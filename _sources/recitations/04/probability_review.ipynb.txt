{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe86988",
   "metadata": {},
   "source": [
    "# R4. Probability review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46774b0",
   "metadata": {},
   "source": [
    "*This recitation was adapted from a probability review document by Rosita Fu. The Jupyter Notebook cheatsheet was written by Grace Liu.*\n",
    "\n",
    "You can [download Rosita's notes](http://bebi103.caltech.edu.s3-website-us-east-1.amazonaws.com/2021a/_downloads/840d1112d4e86cf5d35b0faf73424775/probability_review_rosita.pdf). The go into maximum likelihood estimation as well, which we will cover later in the term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f2337b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contents:\n",
    "\n",
    "In this recitation we will cover:\n",
    "\n",
    "- Axioms of Probability\n",
    "- Conditional Probability and Marginalization\n",
    "- Random Variables, PDFs, PMFs, and CDFs\n",
    "- Expectation and Moment Generating Functions\n",
    "- Common Distributions\n",
    "\n",
    "This notebook will only contain the important formulas and definitions, please see Rosita's notes for more thorough breakdown of the material.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcdffc",
   "metadata": {},
   "source": [
    "## Axioms of Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6928b11",
   "metadata": {},
   "source": [
    "To get started with probability, we can define a few terms:\n",
    "\n",
    "<b> Sample space: </b> Set of all total outcomes $\\Omega$\n",
    "\n",
    "<b> Event: </b> Subset of $\\Omega$,  A $\\in \\Omega$\n",
    "\n",
    "<b> Probability (P): </b> A function that assigns a likelihood to the occurrence all events in $\\Omega$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3218012",
   "metadata": {},
   "source": [
    "There are a number of axioms that must apply to P\n",
    "\n",
    "1. $\\sum_{A_i\\in \\Omega}P(A_i) = 1$\n",
    "\n",
    "\n",
    "2. $P(\\emptyset) = 0$\n",
    "\n",
    "\n",
    "3. $P(A) + P(\\overline{A}) = 1$\n",
    "\n",
    "\n",
    "4. $P(A_i) \\in [0,1]$\n",
    "\n",
    "\n",
    "5. If $A_1, A_2, ...A_n$ disjoint (non-overlapping), then $\\sum P(\\cup A_i) = 1$\n",
    "\n",
    "\n",
    "6. $P(A\\cup B) = P(A) + P(B) - P(A\\cap B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f9dd32",
   "metadata": {},
   "source": [
    "### Conditional Probability and Marginalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e1729",
   "metadata": {},
   "source": [
    "If two events are **independent**, then\n",
    "\n",
    "\\begin{align}\n",
    "P(A,B) = P(A)P(B)\n",
    "\\end{align}\n",
    "\n",
    "The probability of two events both happening if one is dependent on the other is \n",
    "\n",
    "\\begin{align}\n",
    "P(A,B) = P(A\\vert B)P(B)\n",
    "\\end{align}\n",
    "\n",
    "where $P(A\\vert B)$ is the **conditional probability** of A occurring given that B has occurred.\n",
    "\n",
    "Rearranging the above equation, we have that\n",
    "\n",
    "\\begin{align}\n",
    "P(A\\vert B) = \\frac{P(A,B)}{P(B)}\n",
    "\\end{align}\n",
    "\n",
    "**Bayes's Theorem** follows from the fact that $P(A,B) = P(B,A)$ to give\n",
    "\n",
    "\\begin{align}\n",
    "&P(A\\vert B) P(B) = P(B\\vert A) P(A) \\\\[1em]\n",
    "&\\Rightarrow P(A\\vert B) = \\frac{P(B\\vert A)P(A)}{P(B)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d59a049",
   "metadata": {},
   "source": [
    "When you have probabilities expressed in terms of two parameters $P(X\\vert Y)$: you can <b> marginalize </b> out one of the parameters by summing over all possible values of it to get \n",
    "\n",
    "$$P(X) = \\sum_i P(X\\vert Y_i) P(Y_i)$$\n",
    "in the discrete case, or, in the continuous case:\n",
    "$$P(x) = \\int P(x\\vert y) f(y) dy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d8f84-22a2-4b0f-b94f-5abf03d509e3",
   "metadata": {},
   "source": [
    "## Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3874b7d-8771-41d1-a2a6-e2d612139c5c",
   "metadata": {},
   "source": [
    "A <b> random variable </b> is a function that matches outcomes in sample space to real numbers. The probability distribution of $\\Omega$ induces a probability distribution of an R.V. on real numbers. \n",
    "\n",
    "A <b> discrete </b> R.V. can only take on a finite number of values (e.g. integers). Its probability distribution is defined by its <b> probability mass function (PMF) </b> $p(x_i) = P(X=x_i)$\n",
    "\n",
    "A <b> continuous </b> R.V. can take on an infinite number of values, and the <b> probability density function (PDF) </b> defines the likelihood for the value of the R.V. to be near a given value.\n",
    "\n",
    "Properties of the probability density function f(x):\n",
    "1. f(x) $\\geq$ 0\n",
    "2. $\\int_{-\\infty}^{\\infty} f(x) dx = 1$\n",
    "3. $P(X\\in [a,b]) = \\int_{a}^{b}f(x)dx$\n",
    "4. $P(X=a) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c31d27-7d1c-464f-b8c2-df08f3b4a4d3",
   "metadata": {},
   "source": [
    "The <b> cumulative distribution function (CDF) </b> describes the likelihood that the value of the random variable is less than or equal to a given value. \n",
    "\n",
    "$$F(x) = P(X\\leq x)$$\n",
    "\n",
    "It is the integral of the PMF/PDF. In the discrete case:\n",
    "\n",
    "$$F(x) = \\sum_{i, x_i\\leq x}p(x_i)$$\n",
    "\n",
    "In the continuous case:\n",
    "\n",
    "$$F(x) = \\int_{-\\infty}^{x} f(x)dx$$\n",
    "\n",
    "$$f(x) = \\frac{dF(x)}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec34e69b-8d09-403b-a65c-e9034ebf09a8",
   "metadata": {},
   "source": [
    "## Expectation and Moments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e5482-4138-493a-bb06-e1e801f0214e",
   "metadata": {},
   "source": [
    "The <b> expectation </b> of a random variable is the weighted average of all possible values. If you sample out of a distribution a large number of times, the mean of the samples would be close to the expectation value. This is called the <b> Law of Large Numbers </b>\n",
    "\n",
    "- discrete:\n",
    "$$E[X] = \\sum_i x_i\\cdot p(x_i)$$\n",
    "\n",
    "- continuous:\n",
    "$$E[X] = \\int_{-\\infty}^{\\infty} x\\cdot f(x) dx$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc3c91-bb04-41b8-bc1f-108ca7e77fbc",
   "metadata": {},
   "source": [
    "The expectation of a value is also known as the first <b> moment </b>. Additional <b> moment generating functions (MGFs)</b> are calculated as:\n",
    "\n",
    "- discrete:\n",
    "$$E[X^n] = \\sum_i x_i^n\\cdot p(x_i)$$\n",
    "\n",
    "- continuous:\n",
    "$$E[X^n] = \\int_{-\\infty}^{\\infty} x^n\\cdot f(x) dx$$\n",
    "\n",
    "Moment generating functions can also be centered around the mean: $E[(X-\\mu)^n]$, where $\\mu = E[x]$. \n",
    "\n",
    "Notably, the centered second MGF is what we call the <b> variance </b>.\n",
    "\n",
    "The expression of variance is:\n",
    "$$V[X] = E[(X-E[X])^2]$$\n",
    "$$V[X] = E[X^2] - (E[X])^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a8ebf-4041-4eee-b751-c38d9ef22969",
   "metadata": {},
   "source": [
    "## Common Distributions\n",
    "\n",
    "I highly recommend looking at Justin's [Distribution Explorer](https://distribution-explorer.github.io/) and playing with the different distributions yourself, but here are some of the common distributions with stories that you might come across in this class.\n",
    "\n",
    "### Discrete distributions\n",
    "<b> [Bernoulli](https://distribution-explorer.github.io/discrete/bernoulli.html) </b>: There are only two outcomes: success and failure, where the probability of success is $p$. \n",
    "\n",
    "<b> [Binomial](https://distribution-explorer.github.io/discrete/binomial.html) </b>: After performing $n$ independent Bernoulli trials with probability of success $p$, the number of successes is Bernouli distributed \n",
    "\n",
    "<b> [Geometric](https://distribution-explorer.github.io/discrete/geometric.html) </b>: If we perform Bernoulli trials with probability of success $p$, the number of failures before the first success is Geometric distributed\n",
    "\n",
    "<b> [Poisson](https://distribution-explorer.github.io/discrete/poisson.html)</b>: A Poisson process is a memoryless process with rate of arrival $\\lambda$. The number of arrivals in a unit of time is modeled by the Poisson Distribution\n",
    "\n",
    "### Continuous distributions\n",
    "<b> [Uniform](https://distribution-explorer.github.io/continuous/uniform.html)</b>: There is a limited range of possible values $[a,b]$. Within that range, every value has equal probability.\n",
    "\n",
    "<b> [Exponential](https://distribution-explorer.github.io/continuous/exponential.html)</b>: The inter-arrival time for a Poisson process with rate of arrival $\\lambda$ is Exponentially distributed. \n",
    "\n",
    "<b> [Gamma](https://distribution-explorer.github.io/continuous/gamma.html)</b>: The amount of time it takes to accumulate $\\alpha$ arrivals of a Poisson process with arrival rate $\\lambda$\n",
    "\n",
    "<b> [Gaussian/Normal](https://distribution-explorer.github.io/continuous/normal.html)</b>: Parametrized by the mean $\\mu$ and variance $\\sigma^2$. By the <b> Central Limit Theorem </b>, all distributions tend toward the Normal distribution at large enough sample size. Most processes that are a sum of many individual subprocesses will be Normally distributed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
