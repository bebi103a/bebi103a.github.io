[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BE/Bi 103 a 2025",
    "section": "",
    "text": "About the course\nModern biology is a quantitative science, and biological scientists need to be equipped with tools to analyze quantitative data. This course takes a hands-on approach to developing these tools. Together, we will analyze real data. We will learn how to organize, preserve, and share data sets, create informative interactive graphical displays of data, process images to extract actionable data, and perform basic resampling-based statistical inferences.\nImportantly, biological data is often “messy” and there is no one right way to perform an analysis or make a plot. As we work with data, we will discuss various approaches to get a feel for the art of biological data analysis.\nThe sequel to this course goes deeper into statistical modeling, mostly from a Bayesian perspective. This course is foundational for that and further studies in analysis of biological data.\nIf you are enrolled in the course, please read the Course policies. We will not go over them in detail in class, and it is your responsibility to understand them.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#useful-links",
    "href": "index.html#useful-links",
    "title": "BE/Bi 103 a 2025",
    "section": "Useful links",
    "text": "Useful links\n\nEd (used for course communications)\nCanvas (used for assignment submission/return)\nHomework solutions (password protected)",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#personnel",
    "href": "index.html#personnel",
    "title": "BE/Bi 103 a 2025",
    "section": "Personnel",
    "text": "Personnel\n\nCourse instructor\n\nJustin Bois\n\nTeaching assistants\n\nDillan Lau\nZach Martinez\nEmily Meehan\nMatthew Plazola",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#copyright-and-license",
    "href": "index.html#copyright-and-license",
    "title": "BE/Bi 103 a 2025",
    "section": "Copyright and License",
    "text": "Copyright and License\nCopyright 2025, Justin Bois.\nWith the exception of pasted graphics, where the source is noted, this work is licensed under a Creative Commons Attribution License CC BY-NC-SA 4.0. All code contained herein is licensed under an MIT license.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule overview",
    "section": "",
    "text": "The schedule information on this page is subject to changes. All times are Pacific.\n\n\nLab\n\n\nSection 1: Mondays 9 am–noon, Chen 130\nSection 2: Mondays 1–4 pm, Chen 130\n\n\n\n\nLecture\n\n\nWednesdays 9–9:50 am, Chen 100\n\n\n\nInstructor office hours: Wednesdays 3–4 pm, Kerckhoff B123\nTA session: Thursdays 7–10 pm, Chen 130\n\n\n\nHomework due dates\n\nHomework 1: due 5 pm, October 3\nHomework 2: due 5 pm, October 10\nHomework 3: due 5 pm, October 17\nHomework 4: due 5 pm, October 24\nHomework 5: due 5 pm, October 31\nHomework 6: due 5 pm, November 7\nHomework 7: due 5 pm, November 14\nHomework 8: due 5 pm, November 21\nHomework 9: due 5 pm, December 5\nHomework 10: not graded\n\n\n\n\nExam dates\n\nMidterm: In class, November 3, Chen 130\nFinal: 9 am–noon, December 10, Chen 130\n\n\n\n\nLesson exercise due dates\n\nEDA lesson exercise: due noon, October 5\nProbability and sampling lesson exercise: due noon, October 12\nNonparametric stats lesson exercise: due noon, October 26\nNHST lesson exercise: due noon, November 9\nMLE lesson exercise: due noon, November 9\nVariate-covariate models lesson exercise: due noon, November 16\nModel assessment lesson exercise: due noon, November 23\n\n\n\n\nWeekly schedule\nThe notes for each Monday lesson must be read ahead of time and associated lesson exercises submitted by noon on the Sunday before the lesson. For example, the lesson exercises about exploratory data analysis must be submitted by noon on Sunday, October 5.\nIf one were reading through the lessons, the numbering of the lessons represents the most logical order. However, due to the constraints of class meeting times, some of the lessons are presented out of order. This is not a problem, though, as no lesson that strictly depends on another are presented out of order and the order shown in the schedule below is also a reasonable ordering of the lessons.\nWeek 0\n\nAppendix B: Configuring your computer\n\nWeek 1\n\nM 09/29: Course welcome and team set-up\nM 09/29: Lessons 2–9: Exploratory data analysis I\nW 10/01: Lesson 1: What are we doing? (lecture)\n\nWeek 2\n\nM 10/06: Lessons 10–16: Exploratory data analysis II\nW 10/08: Lessons 18 and 19: Introduction to probability (lecture)\n\nWeek 3\n\nM 10/13: Lessons 20–22: Random number generation and its uses\nW 10/15: Guest lecture by Tom Morrell: Good data storage and sharing practices\n\nWeek 4\n\nM 10/20: No reading\nW 10/22: Lessons 24–26: Plug-in estimates and confidence intervals (lecture)\n\nWeek 5\n\nM 10/27: Lessons 27 and 28: Nonparametric inference with hacker stats\nW 10/29: Lessons 30 and 31: Null hypothesis significance testing (lecture)\n\nWeek 6\n\nM 11/03: Midterm exam\nM 11/03: Lesson 32: NHST with hacker stats\nW 11/05: Lessons ###: Parametric inference (lecture)\n\nWeek 7\n\nM 11/10: Lessons ###: Numerical maximum likelihood estimation\nW 11/12: Lessons ###: Variate-covariate models (lecture)\n\nWeek 8\n\nM 11/17: Lessons ###: Confidence intervals of MLEs\nM 11/17: Lessons ###: Implementation of variate-covariate models\nW 11/19: Lessons ###: Model assessment and information criteria (lecture)\n\nWeek 9\n\nM 11/24: Lessons ###: Mixture models\nM 11/24: Lessons ###: Implementation of model assessment\nW 11/26: Lessons ###: Principal components analysis\n\nWeek 10\n\nM 12/01: No reading\nW 12/03: Lessons ###: Statistical watchouts (lecture)\n\nWeek 11\n\nW 12/10: Final exam, 9 am - noon",
    "crumbs": [
      "Course logistics",
      "Schedule overview"
    ]
  },
  {
    "objectID": "policies.html",
    "href": "policies.html",
    "title": "Course policies",
    "section": "",
    "text": "Meetings\nThese are tentative policies that may change before the start of the course.\nAll times are Pacific. Attendance at lectures and lab sessions is required. Weekly lectures are Wednesday mornings in Chen 100, 9-9:50 am. On Mondays, you may attend one of the two lab sessions in Chen 130: 9 am-noon or 1-4 pm. We encourage you to attend the lab session for which you registered.\nTA-led sessions are held 7–10 pm on Thursdays, also in Chen 130. For roughly the first thirty minutes, the TAs will highlight and/or clarify pertinent topics. The remainder of the time will be spent helping individual students.\nInstructor office hours are 3–4 pm on Wednesdays in Kerckhoff B123.\nThere will be no TA session nor instructor office hours the week of Thanksgiving.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#lab-sessions",
    "href": "policies.html#lab-sessions",
    "title": "Course policies",
    "section": "Lab sessions",
    "text": "Lab sessions\nThe lab sessions are spent working on the week’s homework, which typically include working with real data sets with the help of course staff and possibly in collaboration with classmates. You are expected to be working diligently during this time, and it is a golden opportunity to do so.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#submission-of-assignments",
    "href": "policies.html#submission-of-assignments",
    "title": "Course policies",
    "section": "Submission of assignments",
    "text": "Submission of assignments\nAll assignments are submitted (and graded assignments handed back) via Canvas. Lesson exercises are submitted as a single .ipynb file, and each homework problem is submitted separately. (See the Homework section below for more details.)",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#lessons-and-lesson-exercises",
    "href": "policies.html#lessons-and-lesson-exercises",
    "title": "Course policies",
    "section": "Lessons and lesson exercises",
    "text": "Lessons and lesson exercises\nPrior to each lab session, you must go through the lessons listed on the schedule page for the week. These will give you the requisite skills you need to work on the homework problems of the week. To verify completion of the lessons and to identify points of confusion ahead of time, you will submit a small exercise. The file name must be l##_lastname_firstname.ipynb, where ## is the number of the lesson exercise. Lesson exercises are due at noon Pacific time on the Sunday before the lab session.\nThe lesson exercises are not graded for correctness, but for thoughtfulness. A perfectly reasonable answer to a problem in a lesson exercise is, “I am really having trouble with this concept. Can you please go over it in class?”",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#homework",
    "href": "policies.html#homework",
    "title": "Course policies",
    "section": "Homework",
    "text": "Homework\nThere are weekly homework assignments. These consist almost entirely of working up real data sets.\n\nEach homework has a defined due date and time. For most homeworks, this is Friday at 5 pm Pacific time.\nEach homework problem must be submitted as a single Jupyter notebook with file name hw#.#_lastname_firstname.ipynb. All cells in the notebook must be pre-run in order prior to submission. (A tip: You should use Jupyter’s “Restart and Run All” option under the Run pulldown menu to make sure your notebook is in runnable, submittable form.) Additionally, any other files that are necessary for your homework should also be submitted. Note that these are all submitted as separate files, but the TAs will only add graded comments to the Jupyter notebook. There are two exceptions to this rule. Sometimes the notebook must also be submitted as an HTML file, meaning that you submit two files, hw#.#_lastname_firstname.ipynb and hw#.#_lastname_firstname.html, the latter of which is the notebook converted to HTML. This is done for problems that take a long time to run so the TAs do not need to necessarily rerun all cells of the notebook. Such problems will be clearly indicated in the problem statement. The other exception is problems where the instructions specifically say the problem may be submitted as a PDF, in which case a single PDF with file name hw#.#_lastname_firstname.pdf (or, if you prefer, a Jupyter notebook hw#.#_lastname_firstname.ipynb) is submitted.\nSome problems are marked as team problems. For these, all members of the team submit exactly the same notebook (except with different file names containing the name of the submitter), with the names of all team members at the top of the notebook.\nAll code you wrote to do your assignment must be included in the notebook. Code from imported packages that you did not write (e.g., modules distributed by the class instructors) need not be displayed in the notebook. We will run the code in your notebook; all code must run to get credit.\nSince we are running your code to check it, you must have the data sets be accessed in the standard way for the class. That is to say, the following code (or something similar that sets up the correct directory structure) must be at the top of each submitted notebook that uses a data set.\nimport os, sys\nif \"google.colab\" in sys.modules:\n    data_path = \"https://s3.amazonaws.com/bebi103.caltech.edu/data/\"\nelse:\n    data_path = \"../data/\"\nWhen accessing files within your notebooks, do it with something like this: filename = os.path.join(data_path, 'name_of_datafile.csv').\nAll of your results must be clearly explained and all graphics clearly presented and embedded in the Jupyter notebook.\nAny mathematics in your homework must render clearly and properly with MathJax. This essentially means that your equations must be written in correct LaTeX.\nWhere appropriate, you need to give detailed discussion of analysis choices you have made. As an example, you may choose to display data as a jittered strip plot as opposed to, say, a collection of ECDFs. You need to justify that choice.\nTo give a better guideline on how to construct your assignments (and this is good practice in general in your own workflows), you should follow these guidelines.\n\nEach code cell should do only one task or define only one, simple function.\nDo not have any adjacent code cells. Thus, you should have explanatory text describing what is happening in the code cells. This text should not just explain the code, but your reasoning behind why you are doing the calculation.\nShow all equations.\nUse Markdown headers to delineate sections of your notebook. In this class, this at least means using headers to delineate parts of the problem.\n\nBecause this is important to make your work clear, the TAs will deduct points if these rules are not followed.\nThere is seldom a single right way to analyze a set of data. You are encouraged to try different approaches to analysis. If you perform an analysis and find problems with it, clearly write what the problems are and how they came about. Even if your analysis does not completely work, but you demonstrate that you thought carefully about it and understand its difficulties, you will get nearly full credit.\nYou are expected to submit all assignments on time. Late homeworks are accepted up to three days after the due time, in which case 50% of credit is awarded. No assignments are accepted after three days. There are a few exceptions to this rule:\n\nYou have a total of two “grace days” you can use throughout the term. If you use a grace day, your homework may be submitted late without penalty. A grace day is spent for each 24 hours, or portion thereof, that a given homework is late. For example, if a homework is due at 5 pm on Friday, but you turn it in at 7 pm on Sunday, you spend both of your grace days, the first one being spent at 5 pm on Saturday, the second for the remaining two hours on Sunday.\nGrace days may not be applied to lesson exercises or exams. No late lesson exercises will be accepted.\nIf you have a CASS accommodation, you need to communicate it to the instructor within the first week of class. If your accommodation allows for extra time on coursework, you need to let the instructor know you will be exercising that accommodation at least 24 hours before the homework is due.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#exams",
    "href": "policies.html#exams",
    "title": "Course policies",
    "section": "Exams",
    "text": "Exams\nThere will be a midterm and a final exam. The exam format may change, but we expect each exam to consist of a closed, handwritten section, followed by a computational section. The midterm will be completed in class, and the final in class during a specified time during finals week. They will also be submitted via Canvas and have the same file naming and formatting guidelines as for homework, namely midterm_lastname_firstname.ipynb and final_lastname_firstname.ipynb.\nAs you work through the term, it will help to know that, as described below in the AI policies, no AI usage is allowed on the exams. This means also that you will need to be running JupyterLab locally on your machine through the browser (not VS Code or Google Colab) for the exam.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#grading",
    "href": "policies.html#grading",
    "title": "Course policies",
    "section": "Grading",
    "text": "Grading\nYour grade is determined as follows.\n\n5% lesson exercises\n25% homework\n30% midterm exam\n40% final exam\n\nDuring the lab sessions, you are expected to work alone, with your classmates, and/or with course instructors with your full attention. Each unexcused absence from a lab session, or not working on BE/Bi 103 a coursework during a lab session, will result in a 1% deduction from your final grade.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#collaboration-and-ai-policy-and-honor-code",
    "href": "policies.html#collaboration-and-ai-policy-and-honor-code",
    "title": "Course policies",
    "section": "Collaboration and AI policy and Honor Code",
    "text": "Collaboration and AI policy and Honor Code\nSome of the data we will use in this course is unpublished, generously given to us by researchers both from Caltech and from other institutions. They have given us their data in good faith that it will be used only in this class. It is therefore imperative that you do not disseminate data sets that I ask you not to distribute anywhere outside of this class.\nYou may work on your homework assignments with classmates, but each submission must be your own. You must indicate with whom you collaborated on the top of each problem. The exceptions are team problems; you and your teammate(s) submit exactly the same assignment. You may not collaborate on exams.\nYou must complete lesson exercises on your own without any use of large language models (LLMs) or other artificial intelligence (AI).\nYou may not consult solutions of homework or exam problems from previous editions of this course.\nYou are free to consult references, literature, websites, blogs, etc., outside of the materials presented in class. (The exceptions are places where homework problems are completely worked out, such as previous editions of this or other courses, or other students’ work.) If you do, you must properly cite the sources.\nYou may use LLMs or other AI tools to complete your homework. If you use an LLM, at the end of the notebook of your homework submission, you must include which LLM you used (including version number), your prompts, and the response of the AI in readable, nicely formatted text. That said, I strongly encourage you not to use AI at all. I am not trying to deny you an important tool; quite the contrary. In order to effectively use LLMs, you need to have a basal competence to be able to understand what the LLM gives you, and even what to ask of it. In this course, we are building that competence, so using LLMs can result in confusion and are best avoided.\nExams are to be completed on your own with allowed resources. As you plan your term, note that LLMs and other AI tools may not be used on exams. For the computational portion of the exams, there will be a set of internet domains you may use in addition to the course website, mostly consisting of package documentation. You may also use any materials you make yourself. Warning: If you rely heavily on LLMs to complete your homework, it will be very difficult for you to perform well on the exams.\nUse of disallowed resources or other Honor Code violations on either exam may result in failure of the course.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#excused-absences-and-extensions",
    "href": "policies.html#excused-absences-and-extensions",
    "title": "Course policies",
    "section": "Excused absences and extensions",
    "text": "Excused absences and extensions\nUnder very special circumstances, missed lab or lecture sessions will be excused and extensions given on the homework without costing grace days. They must be requested from the course instructor.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#course-communications",
    "href": "policies.html#course-communications",
    "title": "Course policies",
    "section": "Course communications",
    "text": "Course communications\nYou should use the course Ed page for all questions related to the course. Note that you may post private messages to course staff via Ed if you wish. If you need to communicate with course staff about a private matter, you should email the instructor.\nMost of our mass communication with you will be through Ed, so be sure to set your Ed account to give you email alerts if necessary.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#ediquette",
    "href": "policies.html#ediquette",
    "title": "Course policies",
    "section": "“Ediquette”",
    "text": "“Ediquette”\nWhen posting on Ed, please follow these guidelines:\n\nIf you have a question about a coding bug make every attempt to provide a minimal example that demonstrates the problem. A minimal example strips out all other details beyond what is necessary to reproduce the problem or bug. Posting error messages without code is seldom helpful.\nWhen posting code, do not post screen shots. You can post code in Ed by selecting Code instead of Paragraph at the top left of the text entry window when you are making a post.\nIf you feel that posting a minimal example will result in showing too much of your answer to your classmates, you can post your question on Ed privately so that only the course staff can see it.\nWhile you are free to post anonymously to your classmates (course staff will always know who posts), we encourage you to post with your real name. This can spur discussions among students, which can be productive.\nCourse staff strives to answer questions quickly, but students should answer when they can.\n\nThis also spurs more conversation and results in faster answers to questions.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "lessons/what_are_we_doing/what_are_we_doing.html",
    "href": "lessons/what_are_we_doing/what_are_we_doing.html",
    "title": "1  What are we doing?",
    "section": "",
    "text": "1.1 Data analysis pipelines\nIt is always good to start a course thinking about exactly what it is we are doing. Toward that end, we start with a question. What is the goal of doing (biological) experiments? There are many answers you may have for this. Some examples:\nMore obnoxious answers are\nThis question might be better addressed if we zoom out a bit and think about the scientific process as a whole. Below, we have a sketch of the scientific processes. This cycle repeats itself as we explore nature and learn more. In the boxes are milestones, and along the arrows in orange text are the tasks that get us to these milestones.\nWe start to the left with a hypothesis of how a biological system works, often expressed as a sketch, or cartoon, which is something we can have intuition about. Indeed, it is often a cartoon of how a system functions that we have in mind. From these cartoons, we can use deductive inference to move our model from a cartoon to a prediction. We then perform experiments to test our predictions, that is to assault our hypotheses, and the output of experiments is data. Once we have our data, we make inferences from them that enable us to update our hypothesis, thereby pushing human knowledge forward.\nLet’s consider the tasks and their milestones in a little more depth. We start in the lower left.\nLet’s consider the lower right arrow in the figure above, going from a data set to updated hypotheses and parameter estimates, the heart of this course. We zoom in on that arrow, there are several steps, which we will call a data analysis pipeline, depicted below.\nLet’s look at each step.\nIn this course, we will learn about all of these steps. We will spend roughly a third of the class on data validation and wrangling and exploratory data analysis, and the rest of the class on statistical inference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What are we doing?</span>"
    ]
  },
  {
    "objectID": "lessons/what_are_we_doing/what_are_we_doing.html#data-analysis-pipelines",
    "href": "lessons/what_are_we_doing/what_are_we_doing.html#data-analysis-pipelines",
    "title": "1  What are we doing?",
    "section": "",
    "text": "Figure 1.2: Data analysis pipeline. Each block represents a milestone along the pipeline and each arrow a task to bring you there.\n\n\n\n\n\nData validation. We always have assumptions about data coming from a data source into the pipeline. We have assumptions about file formats, data organization, etc. We also have assumptions about the data themselves. For example, fluorescence measurements must all be nonnegative. Before proceeding through the pipeline, we need to make sure the data comply with all of these assumptions, lest we have issues down the pipeline. This process is called data validation.\nData wrangling. This is the process of converting the format of the validated data from the source to a format that is easier to work with. This could involve restructuring tabular data or extracting useful information out of images, etc. There are countless examples.\nExploratory data analysis. Once data sets are tidy and easy to work with, we can start exploring them. Generally, this involves making informative graphics looking at the data set from different angles. Sometimes, this is sufficient to learn from the data, and we can proceed directly to updated hypotheses and publication, but we often also need to perform statistical inference, which is in the next two steps of the pipeline.\nParameter estimation. This is the practice of quantifying the observed effects in the data, and in particular quantifying the uncertainty in our estimates of the effect sizes. This falls under the umbrella of statistical inference.\nHypothesis testing. Also a technique of statistical inference, hypothesis testing involves evaluation of hypotheses about how the data were generated. This usually gives some quantitative measure about the hypotheses, but there are many issues with quantifying the “truth” of a hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What are we doing?</span>"
    ]
  },
  {
    "objectID": "lessons/eda/exploratory_data_analysis.html",
    "href": "lessons/eda/exploratory_data_analysis.html",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "Useful EDA advice from John Tukey\nIn 1977, John Tukey, one of the great statisticians and mathematicians of all time, published a book entitled Exploratory Data Analysis. In it, he laid out general principles on how researchers should handle their first encounters with their data, before formal statistical inference. Most of us spend a lot of time doing exploratory data analysis, or EDA, without really knowing it. Mostly, EDA involves a graphical exploration of a data set.\nWe start off with a few wise words from John Tukey himself.",
    "crumbs": [
      "Exploratory data analysis"
    ]
  },
  {
    "objectID": "lessons/eda/exploratory_data_analysis.html#useful-eda-advice-from-john-tukey",
    "href": "lessons/eda/exploratory_data_analysis.html#useful-eda-advice-from-john-tukey",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "“Exploratory data analysis can never be the whole story, but nothing else can serve as a foundation stone—as the first step.”\n“In exploratory data analysis there can be no substitute for flexibility; for adapting what is calculated—and what we hope plotted—both to the needs of the situation and the clues that the data have already provided.”\n“There is no excuse for failing to plot and look.”\n“There is often no substitute for the detective’s microscope—or for the enlarging graphs.”\n“Graphs force us to note the unexpected; nothing could be more important.”\n“‘Exploratory data analysis’ is an attitude, a state of flexibility, a willingness to look for those things that we believe are not there, as well as those we believe to be there.”",
    "crumbs": [
      "Exploratory data analysis"
    ]
  },
  {
    "objectID": "lessons/eda/exploratory_data_analysis.html#the-tools-of-eda",
    "href": "lessons/eda/exploratory_data_analysis.html#the-tools-of-eda",
    "title": "Exploratory data analysis",
    "section": "The tools of EDA",
    "text": "The tools of EDA\nBeing able to load in a data set and quickly start exploring it graphically enables you to think about your data set instead being mired in the mechanics of producing a plot. In the notebooks that follow in this lesson, we will learn how to use the Python-based tools for EDA. In particular, we will learn how to use Polars to keep the data set organized and accessible and Bokeh to make interactive graphics.\nAlong the way, we will learn key concepts of data organization and display. Importantly, we will learn about tidy data, split-apply-combine, and how to plot all of your data.",
    "crumbs": [
      "Exploratory data analysis"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html",
    "href": "lessons/eda/polars/intro_to_polars.html",
    "title": "2  Introduction to data frames and Polars",
    "section": "",
    "text": "2.1 The data set\n| Download notebook\nData set download\nThroughout your research career, you will undoubtedly need to handle data, possibly lots of data. Once in a usable form, you are empowered to rapidly make graphics and perform statistical inference. Tidy data is an important format and we will discuss that in subsequent sections of this lesson. In an ideal world, data sets would be stored in tidy format and be ready to use. The data comes in lots of formats, and you may have to spend much of your time wrangling the data to get it into a usable format. Wrangling is the topic of the next lesson; for now all data sets will be in tidy format from the get-go.\nWe will explore using data frames with a real data set. We will use a data set published in Beattie, et al., Perceptual impairment in face identification with poor sleep, Royal Society Open Science, 3, 160321, 2016. In this paper, researchers used the Glasgow Facial Matching Test (GMFT) to investigate how sleep deprivation affects a subject’s ability to match faces, as well as the confidence the subject has in those matches. Briefly, the test works by having subjects look at a pair of faces. Two such pairs are shown below.\nThe top two pictures are the same person, the bottom two pictures are different people. For each pair of faces, the subject gets as much time as he or she needs and then says whether or not they are the same person. The subject then rates his or her confidence in the choice.\nIn this study, subjects also took surveys to determine properties about their sleep. The Sleep Condition Indicator (SCI) is a measure of insomnia disorder over the past month (scores of 16 and below indicate insomnia). The Pittsburgh Sleep Quality Index (PSQI) quantifies how well a subject sleeps in terms of interruptions, latency, etc. A higher score indicates poorer sleep. The Epworth Sleepiness Scale (ESS) assesses daytime drowsiness.\nThe data set can be downloaded here. The contents of this file were adapted from the Excel file posted on the public Dryad repository. (Note this: if you want other people to use and explore your data, make it publicly available.) I’ll say it more boldly.\nThe data file is a CSV file, where CSV stands for comma-separated value. This is a text file that is easily read into data structures in many programming languages. You should generally always store your data in such a format, not necessarily CSV, but a format that is open, has a well-defined specification, and is readable in many contexts. Excel files do not meet these criteria. Neither do .mat files. There are other good ways to store data, such as JSON, but we will almost exclusively use CSV files in this class.\nLet’s take a look at the CSV file. We will use the command line program head to look at the first 20 lines of the file.\nfname = os.path.join(data_path, \"gfmt_sleep.csv\")\n\n# This will not work in Colab because the file is not local\n!head {fname}\n\n﻿participant number,gender,age,correct hit percentage,correct reject percentage,percent correct,confidence when correct hit,confidence incorrect hit,confidence correct reject,confidence incorrect reject,confidence when correct,confidence when incorrect,sci,psqi,ess\n8,f,39,65,80,72.5,91,90,93,83.5,93,90,9,13,2\n16,m,42,90,90,90,75.5,55.5,70.5,50,75,50,4,11,7\n18,f,31,90,95,92.5,89.5,90,86,81,89,88,10,9,3\n22,f,35,100,75,87.5,89.5,*,71,80,88,80,13,8,20\n27,f,74,60,65,62.5,68.5,49,61,49,65,49,13,9,12\n28,f,61,80,20,50,71,63,31,72.5,64.5,70.5,15,14,2\n30,m,32,90,75,82.5,67,56.5,66,65,66,64,16,9,3\n33,m,62,45,90,67.5,54,37,65,81.5,62,61,14,9,9\n34,f,33,80,100,90,70.5,76.5,64.5,*,68,76.5,14,12,10\nThe first line contains the headers for each column. They are participant number, gender, age, etc. The data follow. There are two important things to note here. First, notice that the gender column has string data (m or f), while the rest of the data are numeric. Note also that there are some missing data, denoted by the *s in the file.\nGiven the file I/O skills you recently learned, you could write some functions to parse this file and extract the data you want. You can imagine that this might be kind of painful. However, if the file format is nice and clean, like we more or less have here, we can use pre-built tools to read in the data from the file and put it in a convenient data structure. Those structures are data frames.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#the-data-set",
    "href": "lessons/eda/polars/intro_to_polars.html#the-data-set",
    "title": "2  Introduction to data frames and Polars",
    "section": "",
    "text": "Figure 2.1: Two pairs of faces from the Glasgow Facial Matching Test (GFMT).\n\n\n\n\n\n\n\nIf at all possible, share your data freely.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#data-frames",
    "href": "lessons/eda/polars/intro_to_polars.html#data-frames",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.2 Data frames",
    "text": "2.2 Data frames\nThough we will use the word “data frame” over and over again, what a data frame is is actually a bit nebulous. Our working definition of a data frame is that it is a representation of two-dimensional tabular data where each column has a label. We will restrict ourselves to the case where each column has a specific data type (e.g., strings, ints, floats, or even lists).\nOne can think of a data frame as a collection of labeled columns, each one called a series. (A series may be thought of as a single column of data.) Alternatively, it is sometimes convenient to think of a data frame as a collection of rows, where each entry in the row is labeled with the column heading.\nFor more reading on the history of data frames and an attempt (in my opinion a very good attempt) at clearly defining them see section 4 of this paper by Petersohn, et al.\n\n2.2.1 Pandas\nPandas is one of the most widely used tools in the Python ecosystem for handling data. It is worth knowing about. We will, however, not be using Pandas, but instead will use Polars. I prefer Polars because its API is cleaner, in my opinion, but it has the added benefit of generally being much faster than Pandas.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#loading-the-data",
    "href": "lessons/eda/polars/intro_to_polars.html#loading-the-data",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.3 Loading the data",
    "text": "2.3 Loading the data\n\n2.3.1 Using polars.read_csv() to read in data\nWe have imported Polars with the alias pl as is customary. We will use pl.read_csv() to load the data set. The data are stored in a data frame (data type DataFrame), which is one of the data types that makes Polars so convenient for use in data analysis. Data frames offer mixed data types, including incomplete columns, and convenient slicing, among many, many other convenient features. We will use the data frame to look at the data, at the same time demonstrating some of the power of data frames. They are like spreadsheets, only a lot better.\n\ndf = pl.read_csv(fname, null_values=\"*\")\n\nNotice that we used the kwarg null_values=* to specify that entries marked with a * are missing. The resulting data frame is populated with null, wherever this character is present in the file. In this case, we want null_values='*'. So, let’s load in the data set.\nIf you check out the doc string for pl.read_csv(), you will see there are lots of options for reading in the data.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#exploring-the-dataframe",
    "href": "lessons/eda/polars/intro_to_polars.html#exploring-the-dataframe",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.4 Exploring the DataFrame",
    "text": "2.4 Exploring the DataFrame\nLet’s jump right in and look at the contents of the DataFrame. We can look at the first several rows using the df.head() method.\n\n# Look at the contents (first 5 rows)\ndf.head()\n\n\nshape: (5, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\n\n\n\n\n\n\nWe see that the column headings were automatically assigned, as have the data types of the columns, where i64, f64, and str respectively denote integers, floats and strings. Also note (in row 3) that the missing data are denoted as null.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#indexing-data-frames",
    "href": "lessons/eda/polars/intro_to_polars.html#indexing-data-frames",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.5 Indexing data frames",
    "text": "2.5 Indexing data frames\nThe data frame is a convenient data structure for many reasons that will become clear as we start exploring. Let’s start by looking at how data frames are indexed. The rows in Polars data frames are indexed by integers, starting with zero as usual for Python. So, the first row of the data frame may be accessed as follows.\n\ndf[0]\n\n\nshape: (1, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\n\n\n\n\n\n\nIn practice you will almost never use row indices, but rather use Boolean indexing, which is accomplished using Polars’s filter() method.\nBecause row indices in Polars data frames are always integers and column indices are not allowed to be integers (they must be strings), columns are accessed in the same way. If you choose to index with a string, Polars knows you are asking for a column.\n\ndf['percent correct']\n\n\nshape: (102,)\n\n\n\npercent correct\n\n\nf64\n\n\n\n\n72.5\n\n\n90.0\n\n\n92.5\n\n\n87.5\n\n\n62.5\n\n\n…\n\n\n77.5\n\n\n87.5\n\n\n75.0\n\n\n70.0\n\n\n62.5\n\n\n\n\n\n\nFor accessing a single column, I prefer the get_column() method.\n\ndf.get_column('percent correct')\n\n\nshape: (102,)\n\n\n\npercent correct\n\n\nf64\n\n\n\n\n72.5\n\n\n90.0\n\n\n92.5\n\n\n87.5\n\n\n62.5\n\n\n…\n\n\n77.5\n\n\n87.5\n\n\n75.0\n\n\n70.0\n\n\n62.5",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#boolean-indexing-of-data-frames",
    "href": "lessons/eda/polars/intro_to_polars.html#boolean-indexing-of-data-frames",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.6 Boolean indexing of data frames",
    "text": "2.6 Boolean indexing of data frames\nLet’s say I wanted the record for participant number 42. I can use Boolean indexing to specify the row. This is accomplished using the filter() method.\n\ndf.filter(pl.col(\"participant number\") == 42)\n\n\nshape: (1, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n42\n\"m\"\n29\n100\n70\n85.0\n75.0\nnull\n64.5\n43.0\n74.0\n43.0\n32\n1\n6\n\n\n\n\n\n\nThe argument of the filter() method is an expression, pl.col('participant number') == 42, which gives the rows (in this case, one row) for which the value of the 'participant number' column is 42.\nIf I just wanted the percent correct, I can first filter to get the row I want, then extract the 'percent correct' column, and then use the item() method to extract the scalar value.\n\n(\n    df\n    .filter(pl.col('participant number') == 42)\n    .get_column('percent correct')\n    .item()\n)\n\n85.0\n\n\nNote how I expressed this code snippet stylistically. I am doing method chaining, and having each method on its own line adds readability.\nNow, let’s pull out all records of females under the age of 21. We can again use Boolean indexing, but we need to use an & operator, taken to mean logical AND. We did not cover this bitwise operator before, but the syntax is self-explanatory in the example below. Note that it is important that each Boolean operation you are doing is in parentheses because of the precedence of the operators involved. The other bitwise operators you may wish to use for Boolean indexing in data frames are | for OR and ~ for NOT.\n\ndf.filter((pl.col('gender') == 'f') & (pl.col('age') &lt; 21))\n\n\nshape: (5, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n3\n\"f\"\n16\n70\n80\n75.0\n70.0\n57.0\n54.0\n53.0\n57.0\n54.5\n23\n1\n3\n\n\n5\n\"f\"\n18\n90\n100\n95.0\n76.5\n83.0\n80.0\nnull\n80.0\n83.0\n21\n7\n5\n\n\n58\n\"f\"\n16\n85\n85\n85.0\n55.0\n30.0\n50.0\n40.0\n52.5\n35.0\n29\n2\n11\n\n\n72\n\"f\"\n18\n80\n75\n77.5\n67.5\n51.5\n66.0\n57.0\n67.0\n53.0\n29\n4\n6\n\n\n85\n\"f\"\n18\n85\n85\n85.0\n93.0\n92.0\n91.0\n89.0\n91.5\n91.0\n25\n4\n21\n\n\n\n\n\n\nWe can do something even more complicated, like pull out all females under 30 who got more than 85% of the face matching tasks correct.\n\ndf.filter(\n      (pl.col('gender') == 'f') \n    & (pl.col('age') &lt; 30) \n    & (pl.col('percent correct') &gt; 85.0)\n)\n\n\nshape: (8, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n93\n\"f\"\n28\n100\n75\n87.5\n89.5\nnull\n67.0\n60.0\n80.0\n60.0\n16\n7\n4\n\n\n5\n\"f\"\n18\n90\n100\n95.0\n76.5\n83.0\n80.0\nnull\n80.0\n83.0\n21\n7\n5\n\n\n6\n\"f\"\n28\n95\n80\n87.5\n100.0\n85.0\n94.0\n61.0\n99.0\n65.0\n19\n7\n12\n\n\n10\n\"f\"\n25\n100\n100\n100.0\n90.0\nnull\n85.0\nnull\n90.0\nnull\n17\n10\n11\n\n\n44\n\"f\"\n21\n85\n90\n87.5\n66.0\n29.0\n70.0\n29.0\n67.0\n29.0\n26\n7\n18\n\n\n48\n\"f\"\n23\n90\n85\n87.5\n67.0\n47.0\n69.0\n40.0\n67.0\n40.0\n18\n6\n8\n\n\n51\n\"f\"\n24\n85\n95\n90.0\n97.0\n41.0\n74.0\n73.0\n83.0\n55.5\n29\n1\n7\n\n\n67\n\"f\"\n25\n100\n100\n100.0\n61.5\nnull\n58.5\nnull\n60.5\nnull\n28\n8\n9\n\n\n\n\n\n\nOf interest in this exercise in Boolean indexing is that we never had to write a loop. To produce our indices, we could have done the following.\n\n# Initialize array of Boolean indices\ninds = []\n\n# Iterate over the rows of the DataFrame to check if the row should be included\nfor row in df.iter_rows(named=True):\n    inds.append(    \n            row[\"age\"] &lt; 30 \n        and row[\"gender\"] == \"f\" \n        and row[\"percent correct\"] &gt; 85\n    )\n\n# Look at inds\nprint(inds)\n\n[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n\n\nIf we wanted, we could use this inds list of Trues and Falses to filter our values.\n\ndf.filter(inds)\n\n\nshape: (8, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n93\n\"f\"\n28\n100\n75\n87.5\n89.5\nnull\n67.0\n60.0\n80.0\n60.0\n16\n7\n4\n\n\n5\n\"f\"\n18\n90\n100\n95.0\n76.5\n83.0\n80.0\nnull\n80.0\n83.0\n21\n7\n5\n\n\n6\n\"f\"\n28\n95\n80\n87.5\n100.0\n85.0\n94.0\n61.0\n99.0\n65.0\n19\n7\n12\n\n\n10\n\"f\"\n25\n100\n100\n100.0\n90.0\nnull\n85.0\nnull\n90.0\nnull\n17\n10\n11\n\n\n44\n\"f\"\n21\n85\n90\n87.5\n66.0\n29.0\n70.0\n29.0\n67.0\n29.0\n26\n7\n18\n\n\n48\n\"f\"\n23\n90\n85\n87.5\n67.0\n47.0\n69.0\n40.0\n67.0\n40.0\n18\n6\n8\n\n\n51\n\"f\"\n24\n85\n95\n90.0\n97.0\n41.0\n74.0\n73.0\n83.0\n55.5\n29\n1\n7\n\n\n67\n\"f\"\n25\n100\n100\n100.0\n61.5\nnull\n58.5\nnull\n60.5\nnull\n28\n8\n9\n\n\n\n\n\n\nThis feature, where the looping is done automatically on Polars objects like data frames, is very powerful and saves us writing lots of lines of code. This example also showed how to use the iter_rows() method of a data frame. It is actually rare that you will need to do that, and you should generally avoid it, since it is also slow.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#contexts-and-expressions",
    "href": "lessons/eda/polars/intro_to_polars.html#contexts-and-expressions",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.7 Contexts and expressions",
    "text": "2.7 Contexts and expressions\nWe will now be a bit more formal in discussing how to work with Polars data frames. Specifically, Polars features the concepts of expressions and contexts.\n\n2.7.1 The filter context\nAs an example, let us consider our above task of filtering the data frame to extract females under the age of 21. The syntax was\ndf.filter((pl.col('gender') == 'f') & (pl.col('age') &lt; 21))\nConsider first pl.col('gender') == 'f'. This is an example of an expression. An expression consists of a calculation or transformation that can be applied to a series and returns a series. In this case, we are taking a column called 'gender' and we are evaluating whether each element in that column is 'f'. Indeed, if we ask the Python interpreter to tell us the type of the above expression, it is a Polars Expr.\n\ntype(pl.col('gender') == 'f')\n\npolars.expr.expr.Expr\n\n\nSimilarly, pl.col('age') &lt; 21 is also an expression, as is the result when we apply the & bitwise operator.\n\ntype((pl.col('gender') == 'f') & (pl.col('age') &lt; 21))\n\npolars.expr.expr.Expr\n\n\nSo, an expression says what we want to do to data. But now we ask, in what way, i.e., in what context, do we want to use the result of the expression? One way we may wish to use the above expression is to filter the rows in a data frame. The filter context is established by df.filter(). The argument of df.filter() is an expression (or expressions) that evaluate to Booleans. That is how we got our result; in the context of filtering, the expression is evaluated and only entries where the expression gives True are retained.\n\ndf.filter((pl.col('gender') == 'f') & (pl.col('age') &lt; 21))\n\n\nshape: (5, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n3\n\"f\"\n16\n70\n80\n75.0\n70.0\n57.0\n54.0\n53.0\n57.0\n54.5\n23\n1\n3\n\n\n5\n\"f\"\n18\n90\n100\n95.0\n76.5\n83.0\n80.0\nnull\n80.0\n83.0\n21\n7\n5\n\n\n58\n\"f\"\n16\n85\n85\n85.0\n55.0\n30.0\n50.0\n40.0\n52.5\n35.0\n29\n2\n11\n\n\n72\n\"f\"\n18\n80\n75\n77.5\n67.5\n51.5\n66.0\n57.0\n67.0\n53.0\n29\n4\n6\n\n\n85\n\"f\"\n18\n85\n85\n85.0\n93.0\n92.0\n91.0\n89.0\n91.5\n91.0\n25\n4\n21\n\n\n\n\n\n\n\n\n2.7.2 The selection context\nThe simplest way we can use an expression is simply to evaluate the expression and give its result as a new data frame. This is the selection context, in which we get the output of the expression. It can be invoked with df.select().\n\ndf.select((pl.col('gender') == 'f') & (pl.col('age') &lt; 21))\n\n\nshape: (102, 1)\n\n\n\ngender\n\n\nbool\n\n\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n…\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n\n\n\n\nNote that this is a data frame and not a series; it is a data frame containing one column. In this case, the column is named after the first column used in our Boolean expression. We can adjust the column label by applying the alias() method, which does a renaming transformation.\n\nf_under_21 = (pl.col('gender') == 'f') & (pl.col('age') &lt; 21)\n\ndf.select(f_under_21.alias('female under 21'))\n\n\nshape: (102, 1)\n\n\n\nfemale under 21\n\n\nbool\n\n\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n…\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n\n\n\n\nIf we wanted a series instead of a new data frame, we can apply the get_column() method to the data frame returned by df.select().\n\n# Result of expression as a series\ndf.select(f_under_21.alias('female under 21')).get_column('female under 21')\n\n\nshape: (102,)\n\n\n\nfemale under 21\n\n\nbool\n\n\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n…\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n\n\n\n\nWe can also select with multiple expressions. For example, let’s say we additionally wanted to compute the ratio of confidence when correct to confidence when incorrect. First, we can make an expression for that.\n\nconf_ratio = pl.col('confidence when correct') / pl.col('confidence when incorrect')\n\nNow, we can select that as well as the 'female under 21' column.\n\ndf.select(\n    f_under_21.alias('female under 21'),\n    conf_ratio.alias('confidence ratio')\n)\n\n\nshape: (102, 2)\n\n\n\nfemale under 21\nconfidence ratio\n\n\nbool\nf64\n\n\n\n\nfalse\n1.033333\n\n\nfalse\n1.5\n\n\nfalse\n1.011364\n\n\nfalse\n1.1\n\n\nfalse\n1.326531\n\n\n…\n…\n\n\nfalse\n1.040541\n\n\nfalse\n0.925\n\n\nfalse\n0.802469\n\n\nfalse\n1.588235\n\n\nfalse\n1.109589\n\n\n\n\n\n\nNotice that df.select() returns a new data frame containing only the columns that are given by the expressions and the original data frame is discarded. If we want the results of the expressions to instead be added to the data frame (keeping all of its original columns), we use the df.with_columns() method. This is still a selection context; the output is just different, comprising of the original data frame with added columns. (In the output of the cell below, you will find the columns added to the far right of the data frame.)\n\ndf.with_columns(\n    f_under_21.alias('female under 21'),\n    conf_ratio.alias('confidence ratio')\n)\n\n\nshape: (102, 17)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\nfemale under 21\nconfidence ratio\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\nf64\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\nfalse\n1.033333\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\nfalse\n1.5\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\nfalse\n1.011364\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\nfalse\n1.1\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\nfalse\n1.326531\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n97\n\"f\"\n23\n70\n85\n77.5\n77.0\n66.5\n77.0\n77.5\n77.0\n74.0\n20\n8\n10\nfalse\n1.040541\n\n\n98\n\"f\"\n70\n90\n85\n87.5\n65.5\n85.5\n87.0\n80.0\n74.0\n80.0\n19\n8\n7\nfalse\n0.925\n\n\n99\n\"f\"\n24\n70\n80\n75.0\n61.5\n81.0\n70.0\n61.0\n65.0\n81.0\n31\n2\n15\nfalse\n0.802469\n\n\n102\n\"f\"\n40\n75\n65\n70.0\n53.0\n37.0\n84.0\n52.0\n81.0\n51.0\n22\n4\n7\nfalse\n1.588235\n\n\n103\n\"f\"\n33\n85\n40\n62.5\n80.0\n27.0\n31.0\n82.5\n81.0\n73.0\n24\n5\n7\nfalse\n1.109589\n\n\n\n\n\n\nFinally, we will do something we’ll want to use going forward. Recall that a subject is said to suffer from insomnia if he or she has an SCI of 16 or below. We might like to add a column to the data frame that specifies whether or not the subject suffers from insomnia, which we do in the code cell below. Note that until now, we have not updated our data frame. To actually update the data frame, we need an assignment operation, df = ....\n\n# Add a column with `True` for insomnia\ndf = df.with_columns((pl.col('sci') &lt;= 16).alias('insomnia'))\n\n# Take a look (.head() gives first five rows)\ndf.head()\n\n\nshape: (5, 16)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\ninsomnia\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\ntrue\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\ntrue\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\ntrue\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\ntrue\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\ntrue",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#polars-selectors",
    "href": "lessons/eda/polars/intro_to_polars.html#polars-selectors",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.8 Polars selectors",
    "text": "2.8 Polars selectors\nWe have seen that we can choose which columns we want to work with in expressions using pl.col(). Thus far, we have used a single string as an argument, but pl.col() is more capable than that. For example, to select three columns of interest, we can do the following.\n\ndf.select(pl.col('age', 'gender', 'percent correct')).head()\n\n\nshape: (5, 3)\n\n\n\nage\ngender\npercent correct\n\n\ni64\nstr\nf64\n\n\n\n\n39\n\"f\"\n72.5\n\n\n42\n\"m\"\n90.0\n\n\n31\n\"f\"\n92.5\n\n\n35\n\"f\"\n87.5\n\n\n74\n\"f\"\n62.5\n\n\n\n\n\n\nWe can also pass regular expressions into pl.col(). If we want all columns, for example, we can use pl.col('*'). To get all columns containing the string 'confidence', we can use pl.col('^.*confidence.*$').\nPersonally, I always struggle with regular expressions. Fortunately, Polars has powerful selectors which help specify which columns are of interest. In addition to facilitating selection based on the names of the columns, selectors allow selection based on the data type of the column as well (actually, so does pl.col(), but it is simplified with selectors).\nWe have to import the selectors separately, which we have done in the top cell of this notebook via import polars.selectors as cs. The cs alias is suggested by the Polars developers.\nAs an example, we can select all columns that have a column heading containing the string 'confidence'.\n\ndf.select(cs.contains('confidence')).head()\n\n\nshape: (5, 6)\n\n\n\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\n\n\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n\n\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n\n\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n\n\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n\n\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n\n\n\n\n\n\nOr, perhaps we want to exclude all columns that are not indicators of performance on a test (participant number, age, and gender). We can specifically exclude columns with cs.exclude().\n\ndf.select(cs.exclude('gender', 'age', 'participant number')).head()\n\n\nshape: (5, 13)\n\n\n\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\ninsomnia\n\n\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\n\n\n\n\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\ntrue\n\n\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\ntrue\n\n\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\ntrue\n\n\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\ntrue\n\n\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\ntrue\n\n\n\n\n\n\nWhat if we want columns with only a float data type?\n\ndf.select(cs.float()).head()\n\n\nshape: (5, 7)\n\n\n\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\n\n\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n\n\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n\n\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n\n\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n\n\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n\n\n\n\n\n\nNote that the sleep measures were omitted because they are integer data types. We could select everything that is numeric if we want to include those.\n\ndf.select(cs.numeric()).head()\n\n\nshape: (5, 14)\n\n\n\nparticipant number\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n8\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\n\n\n16\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\n\n\n18\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\n\n\n22\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\n\n\n27\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\n\n\n\n\n\n\nUnfortunately, this still gave us participant number and age again. We could exclude those explicitly by chaining methods.\n\ndf.select(cs.numeric().exclude('age', 'participant number')).head()\n\n\nshape: (5, 12)\n\n\n\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\n\n\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\n\n\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\n\n\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\n\n\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\n\n\n\n\n\n\nSelectors also allow for set algebra. As a (contrived) example, let say we want all columns that are not of string data type that have spaces in the column heading and that we also want to exclude the participant number.\n\ndf.select((~cs.string() & cs.contains(' ')).exclude('participant number')).head()\n\n\nshape: (5, 9)\n\n\n\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\n\n\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n\n\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n\n\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n\n\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n\n\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n\n\n\n\n\n\nNotice that we used the complement operator ~ and the intersection operator &. Selectors also support the union operator | and the difference operator -.\nTo close our discussion on selectors, we note that selectors are their own data type:\n\ntype(cs.string())\n\npolars.selectors._selector_proxy_\n\n\nSelectors can be converted to expressions so you can continue computing with the as_expr() method.\n\ntype(cs.string().as_expr())\n\npolars.expr.expr.Expr",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#computing-summary-statistics",
    "href": "lessons/eda/polars/intro_to_polars.html#computing-summary-statistics",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.9 Computing summary statistics",
    "text": "2.9 Computing summary statistics\nTo demonstrate a use of what we have learned so far, we can compute the mean percent correct for insomniacs and normal sleepers. We can filter the data frame according to the insomnia column, select the percent correct column and compute the mean.\nTo put them together in a new data frame, we make a dictionary and convert it to a data frame, which is one of the ways to make a Polars data frame. E.g.,\n\npl.DataFrame(dict(a=[1,2,3], b=[4.5, 5.5, 6.5], c=['one', 'two', 'three']))\n\n\nshape: (3, 3)\n\n\n\na\nb\nc\n\n\ni64\nf64\nstr\n\n\n\n\n1\n4.5\n\"one\"\n\n\n2\n5.5\n\"two\"\n\n\n3\n6.5\n\"three\"\n\n\n\n\n\n\nNow to make our data frame of means for insomniacs and normal sleepers.\n\npl.DataFrame(\n    {\n        'insomniacs': df.filter(pl.col('insomnia')).get_column('percent correct').mean(),\n        'normal sleepers': df.filter(~pl.col('insomnia')).get_column('percent correct').mean()\n    }\n)\n\n\nshape: (1, 2)\n\n\n\ninsomniacs\nnormal sleepers\n\n\nf64\nf64\n\n\n\n\n76.1\n81.461039\n\n\n\n\n\n\nNotice that I used the ~ operator, which is a bit switcher, to get the normal sleepers from the insomnia column. It changes all Trues to Falses and vice versa. In this case, it functions like NOT.\nIt appears as though normal sleepers score better than insomniacs. We will learn techniques to more quantitatively assess that claim when we learn about statistical inference.\nAs we will soon see, what we have done is a split-apply-combine operation, for which there are more elegant and efficient methods using Polars. You will probably never use the approach in the above code cell again.\nWe will do a lot more computing with Polars data frames as the course goes on. For a nifty demonstration demonstration in this lesson, we can quickly compute summary statistics about each column of a data frame using its describe() method.\n\ndf.describe()\n\n\nshape: (9, 17)\n\n\n\nstatistic\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\ninsomnia\n\n\nstr\nf64\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"count\"\n102.0\n\"102\"\n102.0\n102.0\n102.0\n102.0\n102.0\n84.0\n102.0\n93.0\n102.0\n99.0\n102.0\n102.0\n102.0\n102.0\n\n\n\"null_count\"\n0.0\n\"0\"\n0.0\n0.0\n0.0\n0.0\n0.0\n18.0\n0.0\n9.0\n0.0\n3.0\n0.0\n0.0\n0.0\n0.0\n\n\n\"mean\"\n52.04902\nnull\n37.921569\n83.088235\n77.205882\n80.147059\n74.990196\n58.565476\n71.137255\n61.22043\n74.642157\n61.979798\n22.245098\n5.27451\n7.294118\n0.245098\n\n\n\"std\"\n30.020909\nnull\n14.02945\n15.09121\n17.569854\n12.047881\n14.165916\n19.560653\n14.987479\n17.671283\n13.619725\n15.92167\n7.547128\n3.404007\n4.426715\nnull\n\n\n\"min\"\n1.0\n\"f\"\n16.0\n35.0\n20.0\n40.0\n29.5\n7.0\n19.0\n17.0\n24.0\n24.5\n0.0\n0.0\n0.0\n0.0\n\n\n\"25%\"\n26.0\nnull\n26.0\n75.0\n70.0\n72.5\n66.0\n47.0\n64.5\n50.0\n66.0\n51.0\n17.0\n3.0\n4.0\nnull\n\n\n\"50%\"\n53.0\nnull\n37.0\n90.0\n80.0\n85.0\n75.0\n56.5\n71.5\n61.0\n76.0\n61.5\n24.0\n5.0\n7.0\nnull\n\n\n\"75%\"\n78.0\nnull\n45.0\n95.0\n90.0\n87.5\n87.0\n73.0\n80.0\n74.0\n82.5\n73.0\n29.0\n7.0\n10.0\nnull\n\n\n\"max\"\n103.0\n\"m\"\n74.0\n100.0\n100.0\n100.0\n100.0\n92.0\n100.0\n100.0\n100.0\n100.0\n32.0\n15.0\n21.0\n1.0\n\n\n\n\n\n\nThis gives us a data frame with summary statistics.\n\n2.9.1 Outputting a new CSV file\nNow that we added the insomniac column, we might like to save our data frame as a new CSV that we can reload later. We use df.write_csv() for this.\n\ndf.write_csv(\"gfmt_sleep_with_insomnia.csv\")\n\nLet’s take a look at what this file looks like.\n\n!head gfmt_sleep_with_insomnia.csv\n\nparticipant number,gender,age,correct hit percentage,correct reject percentage,percent correct,confidence when correct hit,confidence incorrect hit,confidence correct reject,confidence incorrect reject,confidence when correct,confidence when incorrect,sci,psqi,ess,insomnia\n8,f,39,65,80,72.5,91.0,90.0,93.0,83.5,93.0,90.0,9,13,2,true\n16,m,42,90,90,90.0,75.5,55.5,70.5,50.0,75.0,50.0,4,11,7,true\n18,f,31,90,95,92.5,89.5,90.0,86.0,81.0,89.0,88.0,10,9,3,true\n22,f,35,100,75,87.5,89.5,,71.0,80.0,88.0,80.0,13,8,20,true\n27,f,74,60,65,62.5,68.5,49.0,61.0,49.0,65.0,49.0,13,9,12,true\n28,f,61,80,20,50.0,71.0,63.0,31.0,72.5,64.5,70.5,15,14,2,true\n30,m,32,90,75,82.5,67.0,56.5,66.0,65.0,66.0,64.0,16,9,3,true\n33,m,62,45,90,67.5,54.0,37.0,65.0,81.5,62.0,61.0,14,9,9,true\n34,f,33,80,100,90.0,70.5,76.5,64.5,,68.0,76.5,14,12,10,true\n\n\nVery nice. Notice that by default Polars leaves an empty field for null values, and we do not need the null_values kwarg when we load in this CSV file.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#renaming-columns",
    "href": "lessons/eda/polars/intro_to_polars.html#renaming-columns",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.10 Renaming columns",
    "text": "2.10 Renaming columns\nYou may be annoyed with the rather lengthy syntax of access column names and wish to change them. Actually, you probably do not want to do this. Explicit is better than implicit! And furthermore, high level plotting libraries, as we will soon see, often automatically use column names for axis labels. So, let’s instead lengthen a column name. Say we keep forgetting what ESS stands for an want to rename the ess column to “Epworth Sleepiness Scale.”\nData frames have a nice rename method to do this. To rename the columns, we provide a dictionary where the keys are current column names and the corresponding values are the names we which to check them to. While we are at it, we will choose descriptive names for all three of the sleep quality indices.\n\n# Make a dictionary to rename columns\nrename_dict = {\n    \"ess\": \"Epworth Sleepiness Scale\",\n    \"sci\": \"Sleep Condition Indicator\",\n    \"psqi\": \"Pittsburgh Sleep Quality Index\",\n}\n\n# Rename the columns\ndf = df.rename(rename_dict)\n\ndf.head()\n\n\nshape: (5, 16)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nSleep Condition Indicator\nPittsburgh Sleep Quality Index\nEpworth Sleepiness Scale\ninsomnia\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\ntrue\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\ntrue\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\ntrue\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\ntrue\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\ntrue",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#a-note-on-indexing-and-speed",
    "href": "lessons/eda/polars/intro_to_polars.html#a-note-on-indexing-and-speed",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.11 A note on indexing and speed",
    "text": "2.11 A note on indexing and speed\nAs we have seen Boolean indexing is very convenient and fits nicely into a logical framework which allows us to extract data according to criteria we want. The trade-off is speed. Slicing by Boolean indexing is essentially doing a reverse lookup in a dictionary. We have to loop through all values to find keys that match. This is much slower than directly indexing. Compare the difference in speed for indexing the percent correct by participant number 42 by Boolean indexing versus direct indexing (it’s row 54).\n\nprint(\"Boolean indexing:\")\n%timeit df.filter(pl.col('participant number') == 42)['percent correct'].item()\n\nprint(\"\\nDirect indexing:\")\n%timeit df[54, 'percent correct']\n\nBoolean indexing:\n100 μs ± 471 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\nDirect indexing:\n533 ns ± 5.07 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThe speed difference is stark, differing by two orders of magnitude. For larger data sets, or for analyses that require repeated indexing, this speed consideration may be important. However, Polars does optimization that takes full advantage of parallelization that will accelerate Boolean indexing.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#computing-environment",
    "href": "lessons/eda/polars/intro_to_polars.html#computing-environment",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.12 Computing environment",
    "text": "2.12 Computing environment\n\n\nCode\n%load_ext watermark\n%watermark -v -p numpy,polars,jupyterlab\n\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\npolars    : 1.27.1\njupyterlab: 4.3.6",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html",
    "href": "lessons/eda/polars/tidy_data.html",
    "title": "3  Tidy data and split-apply-combine",
    "section": "",
    "text": "3.1 Tidy data\n| Download notebook\nData set download\nWe have dipped our toe into Polars to see its power. In this lesson, we will continue to harness the power of Polars to pull out subsets of data we are interested in, and of vital importance, will introduce the concept of tidy data. I suspect this will be a demarcation in your life. You will have the times in your life before tidy data and after. Welcome to your bright tomorrow.\nHadley Wickham wrote a great article in favor of “tidy data.” Tidy data frames follow the rules:\nThis is less pretty to visualize as a table, but we rarely look at data in tables. Indeed, the representation of data which is convenient for visualization is different from that which is convenient for analysis. A tidy data frame is almost always much easier to work with than non-tidy formats.\nYou may raise some objections about tidy data. Here are a few, and my responses.\nObjection: Looking at a table of tidy data is ugly. It is not intuitively organized. I would almost never display a tidy data table in a publication.\nResponse: Correct! Having tabular data in a format that is easy to read as a human studying a table is a very different thing than having it in a format that is easy to explore and work with using a computer. As Daniel Chen put it, “There are data formats that are better for reporting and data formats that are better for analysis.” We are using the tidy data frames for analysis, not reporting (though we will see in the coming lessons that having the data in a tidy format makes making plots much easier, and plots are a key medium for reporting.)\nObjection: Isn’t it better to sometimes have data arranged in other ways? Say in a matrix?\nResponse: This is certainly true for things like images, or raster-style data in general. It makes more sense to organize an image in a 2D matrix than to have it organized as a data frame with three columns (row in image, column in image, intensity of pixel), where each row corresponds to a single pixel. For an image, indexing it by row and column is always unambiguous, my_image[i, j] means the pixel at row i and column j.\nFor other data, though, the matrix layout suffers from the fact that there may be more than one way to construct a matrix. If you know a data frame is tidy, you already know its structure. You need only to ask what the columns are, and then you immediately know how to access data using Boolean indexing. In other formats, you might have to read and write extensive comments to understand the structure of the data. Of course, you can read and write comments, but it opens the door for the possibility of misinterpretation or mistakes.\nObjection: But what about time series? Clearly, that can be in matrix format. One column is time, and then subsequent columns are observations made at that time.\nResponse: Yes, that is true. But then the matrix-style described could be considered tidy, since each row is a single observation (time point) that has many facets.\nObjection: Isn’t this an inefficient use of memory? There tend to be lots of repeated entries in tidy data frames.\nResponse: Yes, there are more efficient ways of storing and accessing data. But for data sets that are not “big data,” this is seldom a real issue. The extra expense in memory, as well as the extra expense in access, are small prices to pay for the simplicity and speed of the human user in accessing the data.\nObjection: Once it’s tidy, we pretty much have to use Boolean indexing to get what we want, and that can be slower than other methods of accessing data. What about performance?\nResponse: See the previous response. Speed of access really only becomes a problem with big, high-throughput data sets. In those cases, there are often many things you need to be clever about beyond organization of your data.\nConclusion: I really think that tidying a data set allows for fluid exploration. We will focus on tidy data sets going forward. Bringing untidy data into tidy format can be an annoying challenge that often involves lots of file I/O and text parsing operations. As such, it is wise to arrange your data in tidy format starting at acquisition. If your instrumentation prevents you from doing so, you should develop functions and scripts to parse them and convert them into tidy format.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#tidy-data",
    "href": "lessons/eda/polars/tidy_data.html#tidy-data",
    "title": "3  Tidy data and split-apply-combine",
    "section": "",
    "text": "Each variable is a column.\nEach observation is a row.\nEach type of observation has its own separate data frame.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#the-data-set",
    "href": "lessons/eda/polars/tidy_data.html#the-data-set",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.2 The data set",
    "text": "3.2 The data set\nWe will again use the data set from the Beattie, et al. paper on facial matching under sleep deprivation. Let’s load in the original data set and add the column on insomnia as we did in previous part of this lesson.\n\nfname = os.path.join(data_path, \"gfmt_sleep.csv\")\ndf = pl.read_csv(fname, null_values=\"*\")\ndf = df.with_columns((pl.col('sci') &lt;= 16).alias(\"insomnia\"))\n\n# Take a look\ndf.head()\n\n\nshape: (5, 16)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\ninsomnia\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\ntrue\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\ntrue\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\ntrue\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\ntrue\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\ntrue\n\n\n\n\n\n\nThis data set is in tidy format. Each row represents a single test on a single participant. The aspects of that person’s test are given in each column. We already saw the power of having the data in this format when we did Boolean indexing. Now, we will see how this format allows use to easily do an operation we do again and again with data sets, split-apply-combine.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#split-apply-combine",
    "href": "lessons/eda/polars/tidy_data.html#split-apply-combine",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.3 Split-apply-combine",
    "text": "3.3 Split-apply-combine\nLet’s say we want to compute the median percent correct face matchings for subjects with insomnia and the median percent correct face matchings for those without. Ignoring for the second the mechanics of how we would do this with Python, let’s think about it in English. What do we need to do?\n\nSplit the data set up according to the 'insomnia' field, i.e., split it up so we have a separate data set for the two classes of subjects, those with insomnia and those without.\nApply a median function to the activity in these split data sets.\nCombine the results of these averages on the split data set into a new, summary data set that contains the two classes (insomniac and not) and means for each.\n\nWe see that the strategy we want is a split-apply-combine strategy. This idea was put forward by Hadley Wickham in this paper. It turns out that this is a strategy we want to use very often. Split the data in terms of some criterion. Apply some function to the split-up data. Combine the results into a new data frame.\nNote that if the data are tidy, this procedure makes a lot of sense. Choose the column or columns you want to use to split by. All rows with like entries in the splitting column(s) are then grouped into a new data set. You can then apply any function you want into these new data sets. You can then combine the results into a new data frame.\nPolars’s split-apply-combine operations are achieved in a group by/aggregation context, achieved with df.group_by().agg(). You can think of group_by() as the splitting part. The apply-combine part is done by passing expressions into agg(). The result is a data frame with as many rows as there are groups that we split the data frame into.\n\n3.3.1 Median percent correct\nLet’s go ahead and do our first split-apply-combine on this tidy data set. First, we will split the data set up by insomnia condition and then apply a median function.\n\ndf.group_by('insomnia').median()\n\n\nshape: (2, 16)\n\n\n\ninsomnia\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\nbool\nf64\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\ntrue\n46.0\nnull\n39.0\n90.0\n75.0\n75.0\n76.5\n72.0\n71.0\n68.5\n77.0\n65.0\n14.0\n9.0\n7.0\n\n\nfalse\n54.0\nnull\n36.0\n90.0\n80.0\n85.0\n74.5\n55.5\n71.5\n59.0\n75.0\n59.25\n26.0\n4.0\n6.0\n\n\n\n\n\n\nNote that we used the median() method of the GroupBy object. This computes the median of all columns. This is equivalent the following using agg() with the expression pl.col('*').median().\n\ndf.group_by('insomnia').agg(pl.col('*').median())\n\n\nshape: (2, 16)\n\n\n\ninsomnia\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\nbool\nf64\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\nfalse\n54.0\nnull\n36.0\n90.0\n80.0\n85.0\n74.5\n55.5\n71.5\n59.0\n75.0\n59.25\n26.0\n4.0\n6.0\n\n\ntrue\n46.0\nnull\n39.0\n90.0\n75.0\n75.0\n76.5\n72.0\n71.0\n68.5\n77.0\n65.0\n14.0\n9.0\n7.0\n\n\n\n\n\n\nThe median doesn’t make sense for gender. If we only wanted to compute medians of quantities for which it makes sense to do so, we could use selectors.\n\ndf.group_by('insomnia').agg(cs.numeric().exclude('participant number').median())\n\n\nshape: (2, 14)\n\n\n\ninsomnia\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\nbool\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\nfalse\n36.0\n90.0\n80.0\n85.0\n74.5\n55.5\n71.5\n59.0\n75.0\n59.25\n26.0\n4.0\n6.0\n\n\ntrue\n39.0\n90.0\n75.0\n75.0\n76.5\n72.0\n71.0\n68.5\n77.0\n65.0\n14.0\n9.0\n7.0\n\n\n\n\n\n\nIf instead we only wanted the median of the percent correct and confidence when correct, we could do the following.\n\n(\n    df\n    .group_by('insomnia')\n    .agg(\n        pl.col('percent correct', 'confidence when correct').median(), \n    )\n)\n\n\nshape: (2, 3)\n\n\n\ninsomnia\npercent correct\nconfidence when correct\n\n\nbool\nf64\nf64\n\n\n\n\nfalse\n85.0\n75.0\n\n\ntrue\n75.0\n77.0\n\n\n\n\n\n\nWe can also use multiple columns in our group_by() operation. For example, we may wish to look at four groups, male insomniacs, female insomniacs, male non-insomniacs, and female non-insomniacs. To do this, we simply pass in a list of columns into df.group_by().\n\ndf.group_by([\"gender\", \"insomnia\"]).median()\n\n\nshape: (4, 16)\n\n\n\ngender\ninsomnia\nparticipant number\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\nstr\nbool\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"m\"\ntrue\n55.5\n37.0\n95.0\n82.5\n83.75\n83.75\n55.5\n75.75\n73.25\n81.25\n62.5\n14.0\n9.0\n8.0\n\n\n\"f\"\ntrue\n46.0\n39.0\n80.0\n75.0\n72.5\n76.5\n73.75\n71.0\n68.5\n77.0\n70.5\n14.0\n9.0\n7.0\n\n\n\"f\"\nfalse\n58.0\n36.0\n85.0\n80.0\n85.0\n74.0\n55.0\n70.5\n60.0\n74.0\n58.75\n26.0\n4.0\n7.0\n\n\n\"m\"\nfalse\n41.0\n38.5\n90.0\n80.0\n82.5\n76.0\n57.75\n74.25\n54.75\n76.25\n59.25\n29.0\n3.0\n6.0",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#window-functions",
    "href": "lessons/eda/polars/tidy_data.html#window-functions",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.4 Window functions",
    "text": "3.4 Window functions\nIn the above examples, we split the data frame into groups and applied an aggregating function that gave us back a data frame with as many rows as groups. But what if we do not want to aggregate? For example, say we want to compute the ranking of each participant according to percent correct within each group of insomniacs and normal sleepers. First, let’s see what happens if we use a group by/aggregation context. When we do the grouping, we will retain the order of the entries so that the series of ranks that we acquire match the original order in the data frame. We will also use the 'ordinal' method for ranking, which gives a distinct rank to each entry even in the event of ties.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(\n        pl.col('percent correct')\n        .rank(method='ordinal')\n    )\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\npercent correct\n\n\nbool\nlist[u32]\n\n\n\n\ntrue\n[11, 21, … 10]\n\n\nfalse\n[13, 35, … 7]\n\n\n\n\n\n\nWe see that we indeed get a data frame that has the two rows, one for each group. The 'percent correct' columns has a new data type, a Polars list (not the same as a Python list). If we wanted to convert each entry in the list into a new row of the data frame, we can use the explode() method of a data frame.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(\n        pl.col('percent correct')\n        .rank(method='ordinal')\n    )\n    .explode('percent correct')\n)\n\n\nshape: (102, 2)\n\n\n\ninsomnia\npercent correct\n\n\nbool\nu32\n\n\n\n\ntrue\n11\n\n\ntrue\n21\n\n\ntrue\n23\n\n\ntrue\n19\n\n\ntrue\n3\n\n\n…\n…\n\n\nfalse\n29\n\n\nfalse\n57\n\n\nfalse\n20\n\n\nfalse\n12\n\n\nfalse\n7\n\n\n\n\n\n\nThere are a few annoyances with doing this. First, in the group by/aggregation context, there is no way to maintain the other columns of the data frame as there is in the selection context via the with_columns() method. Second, we have to make a column with a list data type and then explode it.\nHere is where window functions come into play. A window function only operates on a subset of the data, ignoring everything outside of that subset. Since we are applying a function (in our example a rank function) to a subset (group) of the data, we can think of doing a group by followed by a calculation that has the same number of rows in the output as there are rows in the data as a window function.\nWindow functions are implemented in Polars expressions using the over() method. The argument(s) of the over() specify which columns specify groups. The expression is then evaluated individually for each group. To add a column with rankings based on percent correct within each group (insomnia or regular sleeper), we can do the following.\n\n(\n    df\n    .with_columns(\n        pl.col('percent correct')\n        .rank(method='ordinal')\n        .over('insomnia')\n        .alias('percent correct ranked within insomnia groups')\n    )\n)\n\n\nshape: (102, 17)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\ninsomnia\npercent correct ranked within insomnia groups\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\nu32\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\ntrue\n11\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\ntrue\n21\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\ntrue\n23\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\ntrue\n19\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\ntrue\n3\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n97\n\"f\"\n23\n70\n85\n77.5\n77.0\n66.5\n77.0\n77.5\n77.0\n74.0\n20\n8\n10\nfalse\n29\n\n\n98\n\"f\"\n70\n90\n85\n87.5\n65.5\n85.5\n87.0\n80.0\n74.0\n80.0\n19\n8\n7\nfalse\n57\n\n\n99\n\"f\"\n24\n70\n80\n75.0\n61.5\n81.0\n70.0\n61.0\n65.0\n81.0\n31\n2\n15\nfalse\n20\n\n\n102\n\"f\"\n40\n75\n65\n70.0\n53.0\n37.0\n84.0\n52.0\n81.0\n51.0\n22\n4\n7\nfalse\n12\n\n\n103\n\"f\"\n33\n85\n40\n62.5\n80.0\n27.0\n31.0\n82.5\n81.0\n73.0\n24\n5\n7\nfalse\n7\n\n\n\n\n\n\nWindow functions and group by operations are similar. Consider computing the median percent correct for each group, insomniacs and normal sleepers. Using a group by/aggregation context, this is accomplished as follows.\n\n(\n    df\n    .group_by('insomnia')\n    .agg(\n        pl.col('percent correct')\n        .median()\n        .alias('median percent correct')\n    )\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\nmedian percent correct\n\n\nbool\nf64\n\n\n\n\nfalse\n85.0\n\n\ntrue\n75.0\n\n\n\n\n\n\nWe can achieve the same result using a window function with in a select context.\n\n(\n    df\n    .select(\n        pl.col('insomnia'), \n        pl.col('percent correct')\n        .median().over('insomnia')\n        .alias('median percent correct')\n    )\n    .unique('median percent correct')\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\nmedian percent correct\n\n\nbool\nf64\n\n\n\n\nfalse\n85.0\n\n\ntrue\n75.0\n\n\n\n\n\n\nA group by/aggregation context is preferred in this case. As a loose rule of thumb, use a group by/aggregation context when you want a resulting data frame with the number of rows being equal to the number of groups. Use a window function in a selection context when you want a resulting data frame with the same number of rows as your input data frame.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#custom-aggregation-and-window-functions",
    "href": "lessons/eda/polars/tidy_data.html#custom-aggregation-and-window-functions",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.5 Custom aggregation and window functions",
    "text": "3.5 Custom aggregation and window functions\nLet’s say we want to compute the coefficient of variation (CoV, the standard deviation divided by the mean) of data in columns of groups in the data frame. There is no built-in function in Polars to do this, but we could construct an expression that does it.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(\n        (pl.col('percent correct').std(ddof=0) / pl.col('percent correct').mean())\n        .alias('coeff of var percent correct')\n    )\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\ncoeff of var percent correct\n\n\nbool\nf64\n\n\n\n\ntrue\n0.171856\n\n\nfalse\n0.138785\n\n\n\n\n\n\nAlternatively, we could write our own Python function to compute the coefficient of variation using Numpy. Say we had such a function lying around that take as input a Numpy array and returns the coefficient of variation as a Numpy array.\n\ndef coeff_of_var(data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute coefficient of variation from an array of data.\"\"\"\n    return np.std(data) / np.mean(data)\n\nWe can use this function as an aggregating function using the map_batches() method of a Polars expression. The argument of map_batches() is a function that take a series as input and returns either a scalar or a series. We therefore need to pass in a function that converts the series to a Numpy array for use in the coeff_of_var() function, which we can accomplish with a lambda function.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(\n        pl.col('percent correct')\n        .map_batches(\n            lambda s: coeff_of_var(s.to_numpy()), \n            return_dtype=float, \n            returns_scalar=True\n        )\n    )\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\npercent correct\n\n\nbool\nf64\n\n\n\n\ntrue\n0.171856\n\n\nfalse\n0.138785\n\n\n\n\n\n\nNote that it is important to supply the data type of the return of the function you are mapping to ensure that Polars correctly assigns data types in the resulting data frame. It is also important to use the return_scalar=True, lest map_batches() gives each entry as a list.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#polars-structs-and-expressions-using-multiple-columns",
    "href": "lessons/eda/polars/tidy_data.html#polars-structs-and-expressions-using-multiple-columns",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.6 Polars Structs and expressions using multiple columns",
    "text": "3.6 Polars Structs and expressions using multiple columns\nAs discussed earlier, Polars expressions strictly take a series as input and return a series as output. What if we want to perform a calculation that requires two columns? This is where the Polars Struct data type comes in. As a simple example, if we cast our whole data frame into a series, we get a series with a struct data type.\n\nstruct_series = pl.Series(df)\n\n# Take a look\nstruct_series\n\n\nshape: (102,)\n\n\n\n\n\n\nstruct[16]\n\n\n\n\n{8,\"f\",39,65,80,72.5,91.0,90.0,93.0,83.5,93.0,90.0,9,13,2,true}\n\n\n{16,\"m\",42,90,90,90.0,75.5,55.5,70.5,50.0,75.0,50.0,4,11,7,true}\n\n\n{18,\"f\",31,90,95,92.5,89.5,90.0,86.0,81.0,89.0,88.0,10,9,3,true}\n\n\n{22,\"f\",35,100,75,87.5,89.5,null,71.0,80.0,88.0,80.0,13,8,20,true}\n\n\n{27,\"f\",74,60,65,62.5,68.5,49.0,61.0,49.0,65.0,49.0,13,9,12,true}\n\n\n…\n\n\n{97,\"f\",23,70,85,77.5,77.0,66.5,77.0,77.5,77.0,74.0,20,8,10,false}\n\n\n{98,\"f\",70,90,85,87.5,65.5,85.5,87.0,80.0,74.0,80.0,19,8,7,false}\n\n\n{99,\"f\",24,70,80,75.0,61.5,81.0,70.0,61.0,65.0,81.0,31,2,15,false}\n\n\n{102,\"f\",40,75,65,70.0,53.0,37.0,84.0,52.0,81.0,51.0,22,4,7,false}\n\n\n{103,\"f\",33,85,40,62.5,80.0,27.0,31.0,82.5,81.0,73.0,24,5,7,false}\n\n\n\n\n\n\nEach entry is an entire row of a data frame. The labels of each column is still present, as can be seen by considering one entry in the series of structs.\n\nstruct_series[0]\n\n{'participant number': 8,\n 'gender': 'f',\n 'age': 39,\n 'correct hit percentage': 65,\n 'correct reject percentage': 80,\n 'percent correct': 72.5,\n 'confidence when correct hit': 91.0,\n 'confidence incorrect hit': 90.0,\n 'confidence correct reject': 93.0,\n 'confidence incorrect reject': 83.5,\n 'confidence when correct': 93.0,\n 'confidence when incorrect': 90.0,\n 'sci': 9,\n 'psqi': 13,\n 'ess': 2,\n 'insomnia': True}\n\n\nThe labels for each entry in a struct (the keys in the dictionary displated above), are called fields, and we can get a list of the fields for the structs in a series.\n\nstruct_series.struct.fields\n\n['participant number',\n 'gender',\n 'age',\n 'correct hit percentage',\n 'correct reject percentage',\n 'percent correct',\n 'confidence when correct hit',\n 'confidence incorrect hit',\n 'confidence correct reject',\n 'confidence incorrect reject',\n 'confidence when correct',\n 'confidence when incorrect',\n 'sci',\n 'psqi',\n 'ess',\n 'insomnia']\n\n\nThe struct_series.struct.field() method allows us to take entries from all entries corresponding to a given field.\n\nstruct_series.struct.field('percent correct')\n\n\nshape: (102,)\n\n\n\npercent correct\n\n\nf64\n\n\n\n\n72.5\n\n\n90.0\n\n\n92.5\n\n\n87.5\n\n\n62.5\n\n\n…\n\n\n77.5\n\n\n87.5\n\n\n75.0\n\n\n70.0\n\n\n62.5\n\n\n\n\n\n\nThe following two operations give the same result.\npl.Series(df).struct.field('percent correct')\ndf.get_column('percent correct')\nSo, if we need to compute with multiple columns with an expression, we can convert whatever columns we need into a series of data type struct. We can then unpack what we need using the struct_series.struct.field() method.\nAs an example, say we have a function that compute the bivariate (a.k.a. Pearson) correlation coeff between two data sets given as Numpy arrays.\n\ndef bivariate_corr(x: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Compute bivariate correlation coefficient for `x` and `y`.\n    Ignores NaNs.\"\"\"\n    # Masked arrays to handle NaNs\n    x = np.ma.masked_invalid(x)\n    y = np.ma.masked_invalid(y)\n    mask = ~x.mask & ~y.mask\n    \n    return np.corrcoef(x[mask], y[mask])[0, 1]\n\nNow we want to compute the bivariate correlation coefficient for confidence when correct and confidence when incorrect for insomniacs and for normal sleepers. In our call to agg(), we use pl.struct to generate a series of data type struct containing the columns we need for the calculation of the correlation coefficient. We then use map_elements() to use the above function to do the calculation.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(\n        pl.struct(['confidence when correct', 'confidence when incorrect'])\n        .map_elements(\n            lambda s:\n            bivariate_corr(\n                s.struct.field('confidence when correct').to_numpy(), \n                s.struct.field('confidence when incorrect').to_numpy()\n            ), \n           return_dtype=float, returns_scalar=True\n        )\n        .alias('bivariate correlation')\n    )\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\nbivariate correlation\n\n\nbool\nf64\n\n\n\n\ntrue\n0.590435\n\n\nfalse\n0.552045\n\n\n\n\n\n\nWhile this example is instructive to demonstrate how to write your own functions to operate on data frames, as is often the case, Polars has a built-in function that computes the bivariate correlation coefficient.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(pl.corr('confidence when correct', 'confidence when incorrect'))\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\nconfidence when correct\n\n\nbool\nf64\n\n\n\n\ntrue\n0.590435\n\n\nfalse\n0.552045",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#looping-over-a-groupby-object",
    "href": "lessons/eda/polars/tidy_data.html#looping-over-a-groupby-object",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.7 Looping over a GroupBy object",
    "text": "3.7 Looping over a GroupBy object\nWhile the GroupBy methods we have learned so far (like transform() and agg()) are useful and lead to concise code, we sometimes want to loop over the groups of a GroupBy object. This often comes up in plotting applications, as we will see in future lessons. As an example, I will compute the median percent correct for female and males, insomniacs and not (which we already computed using describe()).\n\nfor group_name, sub_df in df.group_by(['gender', 'insomnia']):\n    print(group_name, \": \", sub_df[\"percent correct\"].median())\n\n('m', True) :  83.75\n('f', False) :  85.0\n('m', False) :  82.5\n('f', True) :  72.5\n\n\nBy using the GroupBy object as an iterator, it yields the name of the group (which I assigned as group_name) and the corresponding sub-data frame (which I assigned sub_df). Note that the group name is always given as a tuple, even when only grouping by a single column.\n\nfor (group_name,), sub_df in df.group_by('insomnia'):\n    print(group_name, \": \", sub_df[\"percent correct\"].median())\n\nTrue :  75.0\nFalse :  85.0",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#computing-environment",
    "href": "lessons/eda/polars/tidy_data.html#computing-environment",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.8 Computing environment",
    "text": "3.8 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\npolars    : 1.31.0\njupyterlab: 4.4.5",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/styling_data_frames.html",
    "href": "lessons/eda/polars/styling_data_frames.html",
    "title": "4  Styling data frames",
    "section": "",
    "text": "4.1 Computing environment\n| Download notebook\nData set download\nIt is sometimes useful to highlight features in a data frame when viewing them. (Note that this is generally far less useful than making informative plots, which we will come to shortly.) We sometimes also want to make a table for display. For this purpose, Polars works seamlessly with Great Tables package. To convert a Polars data frame to a Great Tables object that can then be stylized, simply use the df.style attribute of a Polars data frame. You can read about the endless possibilities for styling Polars data frames using Great Tables in the documentation.\nHere, we will do a quick demonstration, again using a data set from Beattie, et al. containing results from a study the effects of sleep quality on performance in the Glasgow Facial Matching Test (GMFT). We will make a pretty-looking table also highlighting rows corresponding to women who scored at 90% or above.\nThere is much more you can do, and the documentation of Great Tables has plenty of examples and tips. In my experience, though, it is rare that you will need to style a data frame; results are usually shown graphically.\n%load_ext watermark\n%watermark -v -p polars,great_tables,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\npolars      : 1.33.1\ngreat_tables: 0.18.0\njupyterlab  : 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Styling data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/polars_for_pandas.html",
    "href": "lessons/eda/polars/polars_for_pandas.html",
    "title": "5  Polars for Pandas users",
    "section": "",
    "text": "5.1 A sample data frame\n| Download notebook\nAs of September 2025, Pandas is far and away the most widely used data frame package for Python. We are using Polars primarily because, in my opinion, the API is more intuitive and therefore easier for beginners and experts alike to use. It is also faster, sometimes much faster. It is, however, important to know about Pandas and how to use it because many of your colleagues use it and many packages you may use do, too.\nTherefore, in this part of the lesson, I discuss how to convert a Polars data frame to Pandas, and vice versa. I also provide syntax for doing common tasks in Polars and Pandas. It is also worth reading the section of the Polars user guide comparing Pandas and Polars.\nFor ease of discussion and comparison, we will use a simple data frame that has two categorical columns, 'c1', and 'c2', two quantitative columns as floats, 'q1', and 'q2', and a column, 'i1' of integer values. It also has an identity column, a unique identifier for each row that is useful when converting the data frame to tall format. Note that 'q1' has a null value and 'q2' has a NaN value.\ndata = dict(\n    id=list(range(1, 9)),\n    c1=['a']*4 + ['b']*4,\n    c2=['c', 'd'] * 4,\n    q1=[1.1, 2.2, 3.1, None, 2.9, 1.7, 3.0, 7.3],\n    q2=[4.5, 2.3, np.nan, 1.1, 7.8, 2.3, 1.1, 0.8],\n    i1=[5, 3, 0, 2, 4, 3, 4, 1],\n)\n\ndf = pl.DataFrame(data)\n\n# Take a look\ndf\n\n\nshape: (8, 6)\n\n\n\nid\nc1\nc2\nq1\nq2\ni1\n\n\ni64\nstr\nstr\nf64\nf64\ni64\n\n\n\n\n1\n\"a\"\n\"c\"\n1.1\n4.5\n5\n\n\n2\n\"a\"\n\"d\"\n2.2\n2.3\n3\n\n\n3\n\"a\"\n\"c\"\n3.1\nNaN\n0\n\n\n4\n\"a\"\n\"d\"\nnull\n1.1\n2\n\n\n5\n\"b\"\n\"c\"\n2.9\n7.8\n4\n\n\n6\n\"b\"\n\"d\"\n1.7\n2.3\n3\n\n\n7\n\"b\"\n\"c\"\n3.0\n1.1\n4\n\n\n8\n\"b\"\n\"d\"\n7.3\n0.8\n1",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Polars for Pandas users</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/polars_for_pandas.html#from-polars-to-pandas-and-from-pandas-to-polars",
    "href": "lessons/eda/polars/polars_for_pandas.html#from-polars-to-pandas-and-from-pandas-to-polars",
    "title": "5  Polars for Pandas users",
    "section": "5.2 From Polars to Pandas and from Pandas to Polars",
    "text": "5.2 From Polars to Pandas and from Pandas to Polars\nIf you have a Polars data frame, you can directly convert it to a Pandas data frame using the to_pandas(), method. Let’s do that for our data frame.\n\ndf.to_pandas()\n\n\n\n\n\n\n\n\nid\nc1\nc2\nq1\nq2\ni1\n\n\n\n\n0\n1\na\nc\n1.1\n4.5\n5\n\n\n1\n2\na\nd\n2.2\n2.3\n3\n\n\n2\n3\na\nc\n3.1\nNaN\n0\n\n\n3\n4\na\nd\nNaN\n1.1\n2\n\n\n4\n5\nb\nc\n2.9\n7.8\n4\n\n\n5\n6\nb\nd\n1.7\n2.3\n3\n\n\n6\n7\nb\nc\n3.0\n1.1\n4\n\n\n7\n8\nb\nd\n7.3\n0.8\n1\n\n\n\n\n\n\n\nNote that the null value becomes a NaN. All missing data in Pandas are NaN. (Well, not really. You can have an object data type for a column that permits None values. However, when Pandas reads in data, when there are missing data, it assigns it to be NaN by default.)\nNote also that Pandas has an index displayed on the left side of the data frame. In general, we will not use Pandas indexes.\nSimilarly, if you have a data frame in Pandas, you can convert it to a Polars data frame using the pl.from_pandas() function.\n\npl.from_pandas(pd.DataFrame(data))\n\n\nshape: (8, 6)\n\n\n\nid\nc1\nc2\nq1\nq2\ni1\n\n\ni64\nstr\nstr\nf64\nf64\ni64\n\n\n\n\n1\n\"a\"\n\"c\"\n1.1\n4.5\n5\n\n\n2\n\"a\"\n\"d\"\n2.2\n2.3\n3\n\n\n3\n\"a\"\n\"c\"\n3.1\nnull\n0\n\n\n4\n\"a\"\n\"d\"\nnull\n1.1\n2\n\n\n5\n\"b\"\n\"c\"\n2.9\n7.8\n4\n\n\n6\n\"b\"\n\"d\"\n1.7\n2.3\n3\n\n\n7\n\"b\"\n\"c\"\n3.0\n1.1\n4\n\n\n8\n\"b\"\n\"d\"\n7.3\n0.8\n1",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Polars for Pandas users</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/polars_for_pandas.html#pandas-and-polars-for-common-tasks",
    "href": "lessons/eda/polars/polars_for_pandas.html#pandas-and-polars-for-common-tasks",
    "title": "5  Polars for Pandas users",
    "section": "5.3 Pandas and Polars for common tasks",
    "text": "5.3 Pandas and Polars for common tasks\nBelow is a table listing common tasks with data frame done using Polars and Pandas.\n\n\n\n\n\n\n\n\nDescription\nPandas\nPolars\n\n\n\n\nConvert dictionary to df\npd.DataFrame(data)\npl.DataFrame(data)\n\n\nMake 2D Numpy array into df\npd.DataFrame(my_ar, columns=['col1', 'col2', 'col3'])\npl.DataFrame(my_ar, schema=['col1', 'col2', 'col3'], orient='row')\n\n\nRead from CSV\npd.read_csv(file_name)\npl.read_csv(file_name)\n\n\nLazily read CSV\n—\npl.scan_csv(file_name)\n\n\nRead from Excel\npd.read_excel(file_name)\npl.read_excel(file_name)\n\n\nRead from JSON\npd.read_json(file_name)\npl.read_json(file_name)\n\n\nRead from HDF5\npd.read_hdf(file_name)\n—\n\n\nWrite CSV\ndf.to_csv(fname, index=False)\ndf.write_csv(fname)\n\n\nRename columns\ndf.rename(columns={'c1': 'cat1', 'c2': 'cat2'})\ndf.rename({'c1': 'cat1', 'c2': 'cat2'})\n\n\nGet column 'q1' as series\ndf['q1'] or df.get_column('q1') or df.loc[:,'q1']\ndf['q1'] or df.get_column('q1')\n\n\nGet column 'q1' as data frame\ndf[['q1']] or df.loc[:,['q1']]\ndf.select('q1')\n\n\nGet columns 'c1' and 'q2'\ndf[['c1', 'q2']] or df.loc[:, ['c1', 'q2']]\ndf.select('c1', 'q2')\n\n\nGet columns containing floats\ndf.select_dtypes(float)\ndf.select(cs.float())\n\n\nGet row 4\ndf.loc[4, :]\ndf.row(4) or df.row(4, named=True)\n\n\nGet row 4 as data frame\ndf.loc[[4], :]\ndf[4]\n\n\nGet row where i1 is 2\ndf.loc[df['i1']==2, :]\ndf.row(by_predicate=pl.col('i1')==2) or df.filter(pl.col('i1')==2)\n\n\nSub df with rows where c1 is 'a'\ndf.loc[df['c1']=='a', :]\ndf.filter(pl.col('c1')=='a')\n\n\nSub df where c1 is 'a' and c2 is 'd'\ndf.loc[(df['c1']=='a') & (df['c2']=='d'), :]\ndf.filter((pl.col('c1')=='a') & (pl.col('c2')=='d'))\n\n\nIterate over columns of df\nfor col, s in df.items()\nfor s in df\n\n\nIterate over rows of df\nfor row_ind, row in df.iterrows()\nfor r in df.iter_rows(named=True)\n\n\nGroup by single column\ndf.groupby('c1')\ndf.group_by('c1')\n\n\nGroup by maintaining order\ndf.groupby('c1', sort=False)\ndf.group_by('c1', maintain_order=True)\n\n\nGroup by multiple columns\ndf.groupby(['c1', 'c2'])\ndf.group_by(['c1', 'c2'])\n\n\nIterate over groups\nfor group, subdf in df.groupby('c1')\nfor (group,), subdf in df.group_by('c1')\n\n\nIterate over nested groups\nfor (g1, g2), subdf in df.groupby(['c1', 'c2'])\nfor (g1, g2), subdf in df.group_by(['c1', 'c2'])\n\n\nGroup by and apply mean¹\ndf.groupby('c1').mean(numeric_only=True)\ndf.group_by('c1').mean()\n\n\nGroup by and apply median to one column\ndf.groupby('c1')['q1'].median()\ndf.group_by('c1').agg(pl.col('q1').median())\n\n\nGroup by and apply mean to two columns\ndf.groupby('c1')[['q1', 'q2']].mean()\ndf.group_by('c1').agg(pl.col('q1', 'q2').mean())\n\n\nGroup by and apply custom func to col²\ndf.groupby('c1')['q1'].apply(my_fun)\ndf.group_by('c1').agg(pl.col('q1').map_batches(my_fun, return_dtype=float, returns_scalar=True))\n\n\nGroup by and apply custom func to 2 cols³\ndf.groupby('c1')[['q1', 'q2']].apply(my_fun)\ndf.group_by('c1').agg(pl.struct(['q1', 'q2']).map_batches(my_fun, return_dtype=float, returns_scalar=True))\n\n\nGroup by and rank within each group\ndf.groupby('c1')['q1'].rank()\ndf.select(pl.col('q1').rank().over('c1'))\n\n\nConvert to tall format\ndf.melt(value_name='value', var_name='var', id_vars='id')\ndf.unpivot(value_name='value', variable_name='var', index='id')\n\n\nPivot tall result above\ndf_tall.pivot(columns='var', index='id').reset_index()\ndf_tall.pivot(on='var', index='id')\n\n\nSelect columns with string in name\ndf.filter(regex='q') or df[df.columns[df.columns.str.contains('q')]]\ndf.select(cs.contains('q'))\n\n\nAdd column of zeros to data frame\ndf['new_col'] = 0 or df.assign(new_col=0)\ndf.with_columns(pl.lit(0).alias('new_col'))\n\n\nAdd a Numpy array as column\ndf['new_col'] = my_array or df.assign(new_col=my_array)\ndf.with_columns(pl.Series(my_array).alias('new_col'))\n\n\nMultiply two columns; make new column\ndf['q1q2'] = df['q1'] * df['q2'] or df.assign(q1q2=df['q1'] * df['q2']\ndf.with_columns((pl.col('q1') * pl.col('q2')).alias('q1q2'))\n\n\nApply a function to each row making new col⁴\ndf.assign(new_col=my_fun)\ndf.with_columns(pl.struct(pl.all()).map_elements(my_fun, return_dtype=float).alias('new_col'))\n\n\nDrop rows with missing data\ndf.dropna()\ndf.drop_nulls()\n\n\nSort according to a column\ndf.sort_values(by='i1')\ndf.sort(by='i1')\n\n\nInner join two data frames⁵\npd.merge(df, df2, on=shared_columns)\ndf.join(df2, on=shared_columns)\n\n\nConcatenate data frames vertically\npd.concat((df, df2))\npl.concat((df, df2), how='diagonal')\n\n\nConcatenate data frames horizontally\npd.concat((df, df2), axis=1)\npl.concat((df, df2), how='horizontal')\n\n\n\nFootnotes\n\nNote that in Pandas, NaNs are omitted from calculations like means. In Polars, NaNs are included, and the result will be NaN. However, nulls are not included.\nFor Pandas, the function my_fun must take an array_like data type (list, Numpy array, Pandas Series, etc.) as input. For Polars, the function my_fun must take a Polars Series as input. It is wise to specify the data type of the output of the function (shown as float in the above example, but can be whatever type my_fun returns). A Pandas example: my_fun = lambda x: np.sum(np.sin(x)). A Polars example: my_fun = lambda s: s.exp().sum().\nFor Pandas, the function must take a Pandas DataFrame as an argument. For Polars, it must take a Polars Series with a struct data type. A Pandas example: my_fun = lambda df: (np.sin(df['q1']) * df['q2']).sum(). A Polars example: my_fun = lambda s: (s.struct.field('q1').sin() * s.struct.field('q2')).sum()\nFor Pandas, my_fun must take as its argument a Pandas Series with an index containing the names of the columns of the original data frame. For Polars, my_fun must take as its argument a dictionary with keys given by the names of the columns of the original data frame. The functions may then have the same syntax (though possibly with different type hints). An example: my_fun = lambda r: r['i1'] * np.sin(r['q2']). However, note that in Polars, a null value is treated as None, which means you cannot apply a function to it, multiply by it, etc.\nFor Polars, the on kwarg for df.join() is required. With Pandas, which columns to join on are inferred based on like-names of columns.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Polars for Pandas users</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/polars_for_pandas.html#hierarchical-indexes",
    "href": "lessons/eda/polars/polars_for_pandas.html#hierarchical-indexes",
    "title": "5  Polars for Pandas users",
    "section": "5.4 Hierarchical indexes",
    "text": "5.4 Hierarchical indexes\nPandas supports hierarchical indexes, called MultiIndexes. This is not supported by Polars. Polars will not read a CSV file with hierarchical indexes. If you have a data set in a CSV file with hierarchical indexes, you can convert it to a CSV file in tall format where the MultiIndex has been converted to columns using the bebi103.utils.unpivot_csv() function. This operation is akin to a df.melt() operation on a data frame with a hierarchical index. You can then read the converted CSV file into Polars and begin working with it.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Polars for Pandas users</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/polars_for_pandas.html#computing-environment",
    "href": "lessons/eda/polars/polars_for_pandas.html#computing-environment",
    "title": "5  Polars for Pandas users",
    "section": "5.5 Computing environment",
    "text": "5.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,pandas,polars,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\npandas    : 2.3.2\npolars    : 1.33.1\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Polars for Pandas users</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting.html",
    "href": "lessons/eda/plotting/plotting.html",
    "title": "6  Data display",
    "section": "",
    "text": "6.1 The Python visualization landscape\nIn 1977, John Tukey, one of the prominent statisticians and mathematicians in history, published a book entitled Exploratory Data Analysis. In it, he laid out general principles on how researchers should handle their first encounters with their data, before formal statistical inference. Most of us spend a lot of time doing exploratory data analysis, or EDA, without really knowing it. Mostly, EDA involves a graphical exploration of a data set.\nWe start off with a few wise words from John Tukey himself, chosen from that brilliant book.\nClearly data display, or plotting, is central to exploratory data analysis.\nLet us start by looking at some of the many plotting packages available in Python. In a talk at PyCon in 2017, Jake VanderPlas, who is one of the authors of one of them (Altair), gave an overview of the Python visualization landscape. That landscape is depicted below, taken from this visualization of it by Nicolas Rougier. (It is from 2017, so it is dated, and definitely not complete, notably missing Panel and domain-specific plotting like napari and Folium, for example.)\nThe landscape is divided into three main pods based on the low-level renderer of the graphics, JavaScript, Matplotlib, and OpenGL (though Matplotlib is higher-level than JavaScript and OpenGL). We will not discuss packages based on OpenGL. Packages that use JavaScript for rendering are particularly well suited for interactivity in browsers. Interactivity and portability (accomplished by rendering in browsers) are key features of modern plotting libraries, so we will use JavaScript-based plotting in the workshop (as I do in my own work).\nThough we will be using Bokeh (and a little bit of HoloViews/Datashader), for a scientist working in Python, it is important to take note of the following.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data display</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting.html#the-python-visualization-landscape",
    "href": "lessons/eda/plotting/plotting.html#the-python-visualization-landscape",
    "title": "6  Data display",
    "section": "",
    "text": "Figure 6.1: A somewhat dated, but reasonably complete, picture of the landscape of data visualization packages in Python.\n\n\n\n\n\n\nMatplotlib is by far the most widely used plotting package in Python. It was even developed a neuroscientist! Seaborn is also widely used as a higher level statistical plotting package that has Matplotlib as its backend (and also developed by a neuroscientist!). We choose to use Bokeh because it is effective\nThere are many domain-specific packages that have plotting modules. For example, for neuroscientists, MNE, Nilearn, and SpikeInterface are all tools with plotting interfaces. Here, we are focusing on general tools. If you have master of lower-level plotting software, you can get much more out of domain-specific packages. You are also unshackled to do the visualization you want to do, and not just those that are available.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data display</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html",
    "title": "7  Making plots with Bokeh",
    "section": "",
    "text": "7.1 High-level and low-level plotting packages\n| Download notebook\nAs a demonstration of what I mean by high-level and low-level plotting packages, let us first think about one of our tasks we did with Polars with the facial matching data set. We computed the median percent correct for those with and without insomnia. Here’s the code to do it.\nfname = os.path.join(data_path, \"gfmt_sleep.csv\")\ndf = pl.read_csv(fname, null_values=\"*\")\ndf = df.with_columns((pl.col('sci') &lt;= 16).alias(\"insomnia\"))\n\ndf.group_by(\"insomnia\").agg(pl.col('percent correct').median())\n\n\nshape: (2, 2)\n\n\n\ninsomnia\npercent correct\n\n\nbool\nf64\n\n\n\n\nfalse\n85.0\n\n\ntrue\n75.0\nLiterally just a few lines of code. Now what if we tried to do it without Polars? I won’t even go through the many lines of code necessary to read in the data. Consider just this one line.\nThere are elementary tasks that go into it if we were to code it up without using Polars’s delicious functionality. We can loop over the rows in the data frame with a for loop, check to see what the value of the insomnia column is with an if statement, put the value in the percent correct field into an appropriate array based on whether or not the subject suffers from insomnia, and then, given those arrays, sort them and pull out the middle value. Under the hood, all of those steps take place, but because we use Polars’s high-level functionality, those details are invisible to us, and glad we are of that.\nNow, say we want to make a plot of some data. You can imagine that there are many many steps to building that. One way you could build a plot is to hand-generate an SVG file that is a set of specifications for lines and circles and text and whatnot that comprises a plot. (I have actually done this before, writing a C program that hand-generated SVG, and it was paaaaainful.) That would be a very low-level way of generating a plot. Plotting libraries in Python usually take care of the rendering part for you, either rendering the plot as SVG, PDF, PNG, or other formats, including interactive ones that use JavaScript and HTML Canvas that can be viewed in a browser. The plotting libraries then vary in their level of abstraction from the data set.\nLower-level plotting libraries typically are more customizable, but require more boilerplate code to get your data plotted and are therefore more cumbersome. Higher-level plotting libraries aim to make it easier to move directly from a data frame to a plot. Because of this streamlining, though, they are often more difficult to customize.\nThe developers of HoloViz made a nice graphic for this concept (Copyright PyViz authors, downloaded here).\nUsing a low-level plotting library, you can get to any graphic you like, but it takes many steps to do so. Using a high level library, you can rapidly get to many, if not most, of the graphics you like in very few steps. However, you cannot get to all graphics. In a layered approach, in which the higher level libraries give you access to the lower level customizations, you can get to any graphic, and can do so quickly. The layered approach requires proficiency in using the low-level and high-level libraries.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#high-level-and-low-level-plotting-packages",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#high-level-and-low-level-plotting-packages",
    "title": "7  Making plots with Bokeh",
    "section": "",
    "text": "df.group_by(\"insomnia\").agg(pl.col('percent correct').median())\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: Routes to making a plot, copyright PyViz authors.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#bokeh-and-this-class",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#bokeh-and-this-class",
    "title": "7  Making plots with Bokeh",
    "section": "7.2 Bokeh and this class",
    "text": "7.2 Bokeh and this class\nOne debate I have every time I teach data visualization is what plotting packages, high-level or low-level, to use. I decided to almost exclusively Bokeh, a low-level plotting library first for a few reasons.\n\nThough low-level, generating plot you might like to construct is fairly straightforward. (Read: it’s not that bad for quickly making plots.)\nBy being familiar with a lower-level plotting package, you can then take a layered approach as you learn a higher-level package.\nWe will discuss at least one higher-level package, iqplot, in short order, discussed in the next part of this lesson.\n\nImportantly, note that Bokeh’s submodules often have to be explicitly imported, as we did in the code cell at the top of this notebook. Note also that if you want your plots to be viewable (and interactive) in the notebook, you need to execute\nbokeh.io.output_notebook()\nat the top of the notebook (as we have done). Finally, note that we also have to have installed the Bokeh JupyterLab extension.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#bokehs-grammar-and-our-first-plot-with-bokeh",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#bokehs-grammar-and-our-first-plot-with-bokeh",
    "title": "7  Making plots with Bokeh",
    "section": "7.3 Bokeh’s grammar and our first plot with Bokeh",
    "text": "7.3 Bokeh’s grammar and our first plot with Bokeh\nConstructing a plot with Bokeh consists of four main steps.\n\nCreating a figure on which to populate glyphs (symbols that represent data, e.g., dots for a scatter plot). Think of this figure as a “canvas” which sets the space on which you will “paint” your glyphs.\nDefining a data source that is the reference used to place the glyphs.\nChoose the kind of glyph you would like.\nAnnotate the columns of data source to determine how they are used to place (and possibly color, scale, etc.) the glyph.\n\nAfter completing these steps, you need to render the graphic.\nLet’s go through these steps in generating a scatter plot of confidence when incorrect versus confidence when correct for the face matching under sleep deprivation study. So you have the concrete example in mind, the final graphic will look like this\n\n\n\n  \n\n\n\n\n\n\nOur first step is creating a figure, our “canvas.” In creating the figure, we are implicitly thinking about what kind of representation for our data we want. That is, we have to specify axes and their labels. We might also want to specify the title of the figure, whether or not to have grid lines, and all sorts of other customizations. Naturally, we also want to specify the shape of the figure.\n\n(Almost) all of this is accomplished in Bokeh by making a call to bokeh.plotting.figure() with the appropriate keyword arguments.\n\n# Create the figure, stored in variable `p`\np = bokeh.plotting.figure(\n    frame_width=400,\n    frame_height=300,\n    x_axis_label='confidence when correct',\n    y_axis_label='condifence when incorrect'\n)\n\nThere are many more keyword attributes you can assign, including all of those listed in the Bokeh Plot class and the additional ones listed in the Bokeh Figure class.\n\nNow that we have set up our canvas, we can decide on the data source. We will use the data frame, df as our data source.\nWe will choose dots (or circles) as our glyph. This kind of glyph requires that we specify which column of the data source will serve to place the glyphs along the \\(x\\)-axis and which will serve to place the glyphs along the \\(y\\)-axis.\nWe choose the 'confidence when correct' column to specify the \\(x\\)-coordinate of the glyph and the 'confidence when incorrect' column to specify the \\(y\\)-coordinate. We already made this decision when we set up our axis labels, but we did not necessarily have to make that decision at that point.\n\nSteps 3 and 4 are accomplished by calling one of the glyph methods of the Bokeh Figure instance, p. Since we are choosing dots, the appropriate method is p.scatter(), and we use the source, x, and y kwargs to specify the positions of the glyphs.\n\np.scatter(\n    source=df.to_dict(),\n    x='confidence when correct',\n    y='confidence when incorrect'\n);\n\nNote that the source must be a ColumnDataSource, a special Bokeh class. If we pass in a dictionary (or Polars data frame) for the source keyword argument, Bokeh converts it to a ColumnDataSource. Therefore, we use the to_dict() method to convert the Polars data frame to a dictionary.\nNow that we have built the plot, we can render it in the notebook using bokeh.io.show().\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nIn looking at the plot, notice a toolbar to right of the plot that enables you to zoom and pan within the plot.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#the-importance-of-tidy-data-frames",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#the-importance-of-tidy-data-frames",
    "title": "7  Making plots with Bokeh",
    "section": "7.4 The importance of tidy data frames",
    "text": "7.4 The importance of tidy data frames\nIt might be clear for you now that building a plot in this way requires that the data frame you use be tidy. The organization of tidy data is really what enables this and high level plotting functionality. There is a well-specified organization of the data.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#code-style-in-plot-specifications",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#code-style-in-plot-specifications",
    "title": "7  Making plots with Bokeh",
    "section": "7.5 Code style in plot specifications",
    "text": "7.5 Code style in plot specifications\nSpecifications of plots often involves calls to functions with lots of keyword arguments to specify the plot, and this can get unwieldy without a clear style. You can develop your own style, maybe reading Trey Hunner’s blog post again. I like to do the following.\n\nPut the function call, like p.scatter( or p = bokeh.plotting.figure( on the first line.\nThe closed parenthesis for the function call is on its own line, unindented.\nAny arguments are given as kwargs (even if they can also be specified as positional arguments) at one level of indentation.\n\nNote that you cannot use method chaining when instantiating figures or populating glyphs.\nIf you adhere to a style (which is roughly the style imposed by Black), it makes your code cleaner and easier to read.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#coloring-with-other-dimensions",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#coloring-with-other-dimensions",
    "title": "7  Making plots with Bokeh",
    "section": "7.6 Coloring with other dimensions",
    "text": "7.6 Coloring with other dimensions\nLet’s say we wanted to make the same plot, but with orange circles for insomniacs and blue circles for normal sleepers. To do this, we take advantage of two features of Bokeh.\n\nWe can make multiple calls to p.scatter() to populate more and more glyphs.\np.scatter(), like all of the glyph methods, has many keyword arguments, including color and legend_label, which will enable us to color the glyphs and include a legend.\n\nWe can loop through the data frame grouped by 'insomnia' and populate the glyphs as we go along.\n\n# For convenience\nx = \"confidence when correct\"\ny = \"confidence when incorrect\"\n\n# Make figure\np = bokeh.plotting.figure(\n    width=400,\n    height=300,\n    x_axis_label=x,\n    y_axis_label=y,\n)\n\n# Add glyphs\ncolors = {'normal sleepers': '#1f77b4', 'insomniacs': 'orange'}\nfor (insom,), sub_df in df.group_by('insomnia'):\n    category = \"insomniacs\" if insom else \"normal sleepers\"\n    p.scatter(\n        source=sub_df.to_dict(),\n        x=x,\n        y=y,\n        color=colors[category],\n        legend_label=category,\n    )\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe got the plot we wanted, but the legend is clashing with the data. Fortunately, Bokeh allows us to set attributes of the figure whenever we like. (We will further discuss styling Bokeh plots in a future lesson.) We can therefore set the legend position to be in the upper left corner. We will also set the click_policy for the legend to be 'hide', which will hide glyphs if you click the legend, which can be convenient for viewing cluttered plots (though this one is not cluttered, really).\n\np.legend.location = 'top_left'\np.legend.click_policy = 'hide'\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#adding-tooltips",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#adding-tooltips",
    "title": "7  Making plots with Bokeh",
    "section": "7.7 Adding tooltips",
    "text": "7.7 Adding tooltips\nBokeh’s interactivity is one of its greatest strengths. While we are plotting confidences when correct and incorrect, we have colored with insomniac status. We might also like to have access to other information in our (tidy) data source if we hover over a glyph. Let’s say we want to know the participant number, gender, and age of each participant. We can tell Bokeh to give us this information by adding tooltips when we instantiate the figure.\nThe syntax for a tooltip is a list of 2-tuples, where each tuple represents the tooltip you want. The first entry in the tuple is the label and the second is the column from the data source that has the values. The second entry must be preceded with an @ symbol signifying that it is a field in the data source and not field that is intrinsic to the plot, which is preceded with a $ sign. If there are spaces in the column heading, enclose the column name in braces. (See the documentation for tooltip specification for more information.)\n\n# For convenience\nx = \"confidence when correct\"\ny = \"confidence when incorrect\"\n\n# Make figure\np = bokeh.plotting.figure(\n    width=400,\n    height=300,\n    x_axis_label=x,\n    y_axis_label=y,\n    tooltips=[\n        (\"p-number\", \"@{participant number}\"),\n        (\"gender\", \"@gender\"),\n        (\"age\", \"@age\"),\n    ],\n)\n\n# Add glyphs\ncolors = {'normal sleepers': '#1f77b4', 'insomniacs': 'orange'}\nfor (insom,), sub_df in df.group_by('insomnia'):\n    category = \"insomniacs\" if insom else \"normal sleepers\"\n    p.scatter(\n        source=sub_df.to_dict(),\n        x=x,\n        y=y,\n        color=colors[category],\n        legend_label=category,\n    )\n\np.legend.location = \"top_left\"\np.legend.click_policy = \"hide\"\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#saving-bokeh-plots",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#saving-bokeh-plots",
    "title": "7  Making plots with Bokeh",
    "section": "7.8 Saving Bokeh plots",
    "text": "7.8 Saving Bokeh plots\nAfter you create your plot, you can save it to a variety of formats. Most commonly you would save them as PNG (for presentations), SVG (for publications in the paper of the past), and HTML (for the paper of the future or sharing with colleagues).\nTo save as a PNG for quick use, you can click the disk icon in the tool bar.\nTo save to SVG, you first change the output backend to 'svg' and then you can click the disk icon again, and you will get an SVG rendering of the plot. After saving the SVG, you should change the output backend back to 'canvas' because it has much better in-browser performance.\n\np.output_backend = 'svg'\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nNow, click the disk icon in the plot above to save it.\nAfter saving, we should switch back to canvas.\n\np.output_backend = 'canvas'\n\nYou can also save the figure programmatically using the bokeh.io.export_svgs() function. This requires additional installations, so we will not do it here, but show the code to do it. Again, this will only work if the output backed is 'svg'.\np.output_backend = 'svg'\nbokeh.io.export_svgs(p, filename='insomniac_confidence_correct.svg')\np.output_backend = 'canvas'\nFinally, to save as HTML, you can use the bokeh.io.save() function. This saves your plot as a standalone HTML page. Note that the title kwarg is not the title of the plot, but the title of the web page that will appear on your Browser tab.\n\nbokeh.io.save(\n    p, \n    filename='insomniac_confidence_correct.html', \n    title='Bokeh plot',\n    resources=bokeh.resources.CDN,\n);\n\nThe resulting HTML page has all of the interactivity of the plot and you can, for example, email it to your collaborators for them to explore.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#computing-environment",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#computing-environment",
    "title": "7  Making plots with Bokeh",
    "section": "7.9 Computing environment",
    "text": "7.9 Computing environment\n\n\nCode\n%load_ext watermark\n%watermark -v -p polars,bokeh,jupyterlab\n\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\npolars    : 1.27.1\nbokeh     : 3.6.2\njupyterlab: 4.3.6",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_smooth_curves.html",
    "href": "lessons/eda/plotting/plotting_smooth_curves.html",
    "title": "8  Plotting smooth curves",
    "section": "",
    "text": "8.1 Computing environment\n| Download notebook\nSometimes you want to plot smooth functions, as opposed to measured data like we have done so far. To do this, you can use Numpy and/or Scipy to generate arrays of values of smooth functions.\nWe will plot the Airy disk, which we encounter in biology when doing microscopy as the diffraction pattern of light passing through a pinhole. Here is a picture of the diffraction pattern from a laser (with the main peak overexposed).\nThe equation for the radial light intensity of an Airy disk is\n\\[\\begin{align}\n\\frac{I(x)}{I_0} = 4 \\left(\\frac{J_1(x)}{x}\\right)^2,\n\\end{align}\\]\nwhere \\(I_0\\) is the maximum intensity (the intensity at the center of the image) and \\(x\\) is the radial distance from the center. Here, \\(J_1(x)\\) is the first order Bessel function of the first kind. Yeesh. How do we plot that?\nFortunately, SciPy has lots of special functions available. Specifically, scipy.special.j1() computes exactly what we are after! We pass in a NumPy array that has the values of \\(x\\) we want to plot and then compute the \\(y\\)-values using the expression for the normalized intensity.\nTo plot a smooth curve, we use the np.linspace() function with lots of points. We then connect the points with straight lines, which to the eye look like a smooth curve. Let’s try it. We’ll use 400 points, which I find is a good rule of thumb for not-too-quickly-oscillating functions.\nNow that we have the values we want to plot, we could construct a Pandas DataFrame to pass in as the source to p.line(). We do not need to take this extra step, though. If we instead leave source unspecified, and pass in NumPy arrays for x and y, Bokeh will directly use those in constructing the plot.\nWe could also plot dots (which doesn’t make sense here, but we’ll show it just to see who the line joining works to make a plot of a smooth function).\nThere is one detail I swept under the rug here. What happens if we compute the function for \\(x = 0\\)?\nWe get a RuntimeWarning because we divided by zero. We know that\n\\[\\begin{align}\n\\lim_{x\\to 0} \\frac{J_1(x)}{x} = \\frac{1}{2},\n\\end{align}\\]\nso we could write a new function that checks if \\(x = 0\\) and returns the appropriate limit for \\(x = 0\\). In the x array I constructed for the plot, we hopped over zero, so it was never evaluated. If we were being careful, we could write our own Airy function that deals with this.\n%load_ext watermark\n%watermark -v -p numpy,scipy,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\nscipy     : 1.16.1\nbokeh     : 3.7.3\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Plotting smooth curves</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/categorical_axes_bokeh.html",
    "href": "lessons/eda/plotting/categorical_axes_bokeh.html",
    "title": "9  Plots with categorical variables",
    "section": "",
    "text": "9.1 Types of data for plots\n| Download notebook\nData set download\nLet us first consider the different kinds of data we may encounter as we think about constructing a plot.\nIn practice, ordinal data can be cast as quantitative or treated as categorical with an ordering enforced on the categories (e.g., categorical data [1, 2, 3] becomes ['1', '2', '3'].). Temporal data can also be cast as quantitative, (e.g., seconds from the start time). We will therefore focus out attention on quantitative and categorical data.\nWhen we made scatter plots, both types of data were quantitative. We did actually incorporate categorical information in the form of colors of the glyph (insomniacs and normal sleepers being colored differently) and in tooltips.\nBut what if we wanted a single type of measurement, such as percent correct in the facial identification, but were interested in delineating performance of insomniacs and normal sleepers. Here, we have the quantitative percent correct data and the categorical sleeper type. One of our axes is now categorical.\nNote that this kind of plot is commonly encountered in the biological sciences. We repeat a measurement many times for given test conditions and wish to compare the results. The different conditions are the categories, and the axis along which the conditions are represented is called a categorical axis. The quantitative axis contains the result of the measurements from each condition.\nThe rest of this lesson is mostly for reference so you can see how to handle categorical axes with Bokeh. In practice, we will mostly be using iqplot to do this and it is done for your automatically. You may therefore skip the rest of this notebook if you like.\nThat said, for some plotting applications, you may need to adjust details or do things outside of iqplot’s capabilities, so the contents of this lesson can be useful.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Plots with categorical variables</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/categorical_axes_bokeh.html#types-of-data-for-plots",
    "href": "lessons/eda/plotting/categorical_axes_bokeh.html#types-of-data-for-plots",
    "title": "9  Plots with categorical variables",
    "section": "",
    "text": "Quantitative data may have continuously varying (and therefore ordered) values.\nCategorical data has discrete, unordered values that a variable can take.\nOrdinal data has discrete, ordered values. Integers are a classic example.\nTemporal data refers to time, which can be represented as dates.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Plots with categorical variables</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/categorical_axes_bokeh.html#making-a-bar-graph-with-bokeh",
    "href": "lessons/eda/plotting/categorical_axes_bokeh.html#making-a-bar-graph-with-bokeh",
    "title": "9  Plots with categorical variables",
    "section": "9.2 Making a bar graph with Bokeh",
    "text": "9.2 Making a bar graph with Bokeh\nTo demonstrate how to set up a categorical axis with Bokeh, I will make a bar graph of the mean percent correct for insomniacs and normal sleepers. But before I even begin this, I will give you the following piece of advice: Don’t make bar graphs. More on that in a moment.\n\n9.2.1 Setting up a data frame for plotting\nBefore making a plot, we need to set up a data frame amenable for the type of plot we want. We start by reading in the data set and computing the 'insomnia' column, which gives Trues and Falses, as we’ve done in the preceding parts of this lesson.\n\nfname = os.path.join(data_path, \"gfmt_sleep.csv\")\ndf = pl.read_csv(fname, null_values=\"*\")\ndf = df.with_columns((pl.col('sci') &lt;= 16).alias('insomnia'))\n\nFor convenience in plotting the categorical axis, we would rather not have the values on the axis be True or False, but something more descriptive, like insomniac and normal. So, let’s make a column in the data frame, 'sleeper' that has that for us. We can use the replace_strict() method, which take a dictionary as an argument, replacing all instances given by a key in the dictionary with the corresponding value. When using this method, it is good practice to specify the data type of the resulting column (though Polars will attempt to infer it).\n\ndf = df.with_columns(\n    pl.col('insomnia')\n    .replace_strict({True: 'insomniac', False: 'normal'}, return_dtype=pl.String)\n    .alias('sleeper')\n)\n\nNext, we need to make a data frame that has the mean percent correct for each of the two categories of sleeper. We have decided that it is the mean of the respective measurements that will set the height of the bars.\n\ndf_mean = df.group_by(\"sleeper\").agg(pl.col(\"percent correct\").mean())\n\n# Take a look\ndf_mean\n\n\nshape: (2, 2)\n\n\n\nsleeper\npercent correct\n\n\nstr\nf64\n\n\n\n\n\"normal\"\n81.461039\n\n\n\"insomniac\"\n76.1\n\n\n\n\n\n\nNow we’re ready to make the bar graph. Note that we now have only two data points that we are showing on the plot. We have decided to throw out a lot of information from the data we collected to display only two values. Does this strike you as a terrible idea? It should. Don’t do this. We’re just doing it to show how categorical axes are set up using Bokeh.\n\n\n9.2.2 Setting up categorical axes\nTo set up a categorical axis, you need to specify the x_range (or y_range if you want the y-axis to be categorical) as a list with the categories you want on the axis when you instantiate the figure. I will make a horizontal bar graph, so I will specify y_range. I also want my quantitative axis (x in this case) to go from zero to 100, since it signifies a percent. Also, when I instantiate this figure, because it is not very tall and I do not want the reset tool cut off, I will also explicitly set the tools I want in the toolbar.\n\np = bokeh.plotting.figure(\n    height=200,\n    width=400,\n    x_axis_label=\"percent correct\",\n    x_range=[0, 100],\n    y_range=['insomniac', 'normal'],\n    tools=\"save\",\n)\n\nNow that we have the figure, we can put the bars on. The p.hbar() method populates the figure with horizontal bar glyphs. The right kwarg says what column of the data source dictates how far to the right to show the bar, while the height kwarg says how think the bars are.\nI will also turn off the grid lines on the categorical axis, which is commonly done.\n\np.hbar(\n    source=df_mean.to_dict(),\n    y=\"sleeper\",\n    right=\"percent correct\",\n    height=0.6,\n)\n\n# Turn off gridlines on categorical axis\np.ygrid.grid_line_color = None\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe similarly make vertical bar graphs specifying x_range and using p.vbar().\n\np = bokeh.plotting.figure(\n    height=250,\n    width=250,\n    x_range=['normal', 'insomniac'],\n    y_range=[0, 100],\n    y_axis_label=\"average percent correct\",\n)\n\np.vbar(\n    source=df_mean.to_dict(),\n    x=\"sleeper\",\n    top=\"percent correct\",\n    width=0.6,\n)\n\np.xgrid.grid_line_color = None\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Plots with categorical variables</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/categorical_axes_bokeh.html#nested-categorical-axes",
    "href": "lessons/eda/plotting/categorical_axes_bokeh.html#nested-categorical-axes",
    "title": "9  Plots with categorical variables",
    "section": "9.3 Nested categorical axes",
    "text": "9.3 Nested categorical axes\nWe may wish to make a bar graph where we have four bars, normal and insomniac for males and also normal and insomniac for females. To start, we will have to re-make the df_mean data frame, now grouping by gender and sleeper. Furthermore, it will be nicer to label the categories as “female” and “male” instead of “f” and “m”.\n\ndf_mean = (\n    df\n    .group_by([\"gender\", \"sleeper\"])\n    .agg(pl.col(\"percent correct\").mean())\n).with_columns(\n    pl.col('gender')\n    .replace_strict(dict(f='female', m='male'), return_dtype=pl.String)\n    .alias('gender')\n)\n\n# Take a look\ndf_mean\n\n\nshape: (4, 3)\n\n\n\ngender\nsleeper\npercent correct\n\n\nstr\nstr\nf64\n\n\n\n\n\"female\"\n\"normal\"\n82.045455\n\n\n\"female\"\n\"insomniac\"\n73.947368\n\n\n\"male\"\n\"normal\"\n80.0\n\n\n\"male\"\n\"insomniac\"\n82.916667\n\n\n\n\n\n\nBecause of the way Bokeh handles nested categories, we need to create a new column that has a list corresponding to the nested category. To make this, we use the pl.concat_list() function.\n\ndf_mean = df_mean.with_columns(pl.concat_list('gender', 'sleeper').alias('cats'))\n\n# Take a look\ndf_mean\n\n\nshape: (4, 4)\n\n\n\ngender\nsleeper\npercent correct\ncats\n\n\nstr\nstr\nf64\nlist[str]\n\n\n\n\n\"female\"\n\"normal\"\n82.045455\n[\"female\", \"normal\"]\n\n\n\"female\"\n\"insomniac\"\n73.947368\n[\"female\", \"insomniac\"]\n\n\n\"male\"\n\"normal\"\n80.0\n[\"male\", \"normal\"]\n\n\n\"male\"\n\"insomniac\"\n82.916667\n[\"male\", \"insomniac\"]\n\n\n\n\n\n\nNext, we need to set up factors, which give the nested categories. We could extract them from the 'cats' column of the data frame as\nfactors = list(df_mean.get_column('cats'))\nInstead, we will specify them by hand to ensure they are ordered as we would like.\n\nfactors = [\n    (\"female\", \"normal\"),\n    (\"female\", \"insomniac\"),\n    (\"male\", \"normal\"),\n    (\"male\", \"insomniac\"),\n]\n\nFinally, to use these factors in a y_range (or x_range), we need to convert them to a factor range using bokeh.models.FactorRange().\n\np = bokeh.plotting.figure(\n    height=200,\n    width=400,\n    x_axis_label=\"average percent correct\",\n    x_range=[0, 100],\n    y_range=bokeh.models.FactorRange(*factors),\n    tools=\"save\",\n)\n\nNow we are ready to add the bars, taking care to specify the 'cats' column for our y-values.\n\np.hbar(\n    source=df_mean.to_dict(),\n    y=\"cats\",\n    right=\"percent correct\",\n    height=0.6,\n)\n\np.ygrid.grid_line_color = None\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Plots with categorical variables</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/categorical_axes_bokeh.html#computing-environment",
    "href": "lessons/eda/plotting/categorical_axes_bokeh.html#computing-environment",
    "title": "9  Plots with categorical variables",
    "section": "9.4 Computing environment",
    "text": "9.4 Computing environment\n\n%load_ext watermark\n%watermark -v -p polars,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\npolars    : 1.33.1\nbokeh     : 3.7.3\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Plots with categorical variables</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/visualizing_distributions.html",
    "href": "lessons/eda/plotting/visualizing_distributions.html",
    "title": "10  Visualizing distributions",
    "section": "",
    "text": "10.1 Box plots\n| Download notebook\nData set download\nYou can think of an experiment as sampling out of a probability distribution. To make this more concrete, imagine measuring the lengths of eggs laid by a given hen. Each egg will be of a different length than others, but all of the eggs will be about six centimeters long. The probability of getting an egg more than eight centimeters long is very small, whereas the probability of getting an egg between 5.5 and 6.5 centimeters is high. The generative probability distribution, the distribution from which experimental data are sampled, then, has high probability mass around six centimeters and low probability mass away from that. We cannot know the generative distribution. We can approximate it with generative models (which we will do in the latter part of the course).\nHere, we will investigate how to make plots to investigate properties about the (unknown) generative distribution of a set of repeated measurements. We will use the iqplot to make the plots, but will withhold dwelling on its syntax until the next notebook of this lesson. Instead, we will make a given plot and then discuss how it helps us visualize the underlying generative probability distribution of a data set.\nWe will continue using the facial recognition data set. We will make the usual adjustments by adding an 'insomnia' column and also a 'sleeper' column that more meaningfully indicates where the subject is an insomniac or a normal sleeper (with the words “normal” or “insomniac” instead of True and False).\nWe saw in the previous part of the lesson that bar graphs throw out most of the information present in a data set. They only give an approximation of the mean of the generative distribution and nothing else. We can instead report more information.\nA box-and-whisker plot, also just called a box plot is a better option than a bar graph. Indeed, it was invented by John Tukey himself. Instead of condensing your measurements into one value (or two, if you include an error bar) like in a bar graph, you condense them into at least five. It is easier to describe a box plot if you have one to look at.\np = iqplot.box(\n    df,\n    \"percent correct\",\n    cats=[\"gender\", \"sleeper\"],\n    box_kwargs=dict(fill_color=\"#1f77b4\"),\n)\n\nbokeh.io.show(p)\nThe top of a box is the 75th percentile of the measured data. That means that 75 percent of the measurements were less than the top of the box. The bottom of the box is the 25th percentile. The line in the middle of the box is the 50th percentile, also called the median. Half of the measured quantities were less than the median, and half were above. The total height of the box encompasses the measurements between the 25th and 75th percentile, and is called the interquartile region, or IQR. The top whisker extends to the minimum of these two quantities: the largest measured data point and the 75th percentile plus 1.5 times the IQR. Similarly, the bottom whisker extends to the maximum of the smallest measured data point and the 25th percentile minus 1.5 times the IQR. Any data points not falling between the whiskers are then plotted individually, and are typically termed outliers. Note that “outlier” is just a name; it does not imply anything special we should consider in those data points.\nSo, box-and-whisker plots give much more information than a bar plot. They give a reasonable summary of how data are distributed by providing quantiles.\nNot to draw our focus away from visualizing how a data set is distributed, going forward in this notebook, we will not split the data set by gender and sleep preference, but will instead look at the entire data set. Here is a box plot for that.\np = iqplot.box(df, \"percent correct\", frame_height=100)\np.yaxis.visible = False\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing distributions</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/visualizing_distributions.html#plot-all-your-data",
    "href": "lessons/eda/plotting/visualizing_distributions.html#plot-all-your-data",
    "title": "10  Visualizing distributions",
    "section": "10.2 Plot all your data",
    "text": "10.2 Plot all your data\nWhile the box plot is better than a bar graph because it shows quantiles and not just a mean, it is still hiding much of the structure of the data set. Conversely, when plotting x-y data in a scatter plot, you plot all of your data points. Shouldn’t the same be true for plots with categorical variables? You went through all the work to get the data; you should show them all!",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing distributions</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/visualizing_distributions.html#strip-plots",
    "href": "lessons/eda/plotting/visualizing_distributions.html#strip-plots",
    "title": "10  Visualizing distributions",
    "section": "10.3 Strip plots",
    "text": "10.3 Strip plots\nOne convenient way to plot all of your data is a strip plot. In a strip plot, every point is plotted.\n\np = iqplot.strip(df, \"percent correct\", frame_height=100)\np.yaxis.visible = False\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nAn obvious problem with this plot is that the data points overlap. We can get around this issue by adding a jitter to the plot. Instead of lining all of the data points up exactly in line with the category, we randomly “jitter” the points about the centerline, of course while maintaining their position along the quantitative axis.\n\np = iqplot.strip(df, \"percent correct\", frame_height=100, spread=\"jitter\")\np.yaxis.visible = False\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nAlternatively, we can deterministically spread the points in a swarm plot, where the glyphs are positioned so as not to overlap, but retain the same position along the quantitative axis.\n\np = iqplot.strip(df, \"percent correct\", frame_height=100, spread=\"swarm\")\np.yaxis.visible = False\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis plot allows us to make out how the data are distributed. We suspect there is more probability density around 85% or so, with a heavy tail heading toward lower values, ending at 40.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing distributions</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/visualizing_distributions.html#spike-plots",
    "href": "lessons/eda/plotting/visualizing_distributions.html#spike-plots",
    "title": "10  Visualizing distributions",
    "section": "10.4 Spike plots",
    "text": "10.4 Spike plots\nThe swarm plot above essentially provides a count of the number of times a given measurement was observed. Because of the nature of the GFMT, the percent correct values are discrete, coming in 2.5% increments. When we have discrete values like that, a spike plot serves to provide the counts of each measurement value.\n\np = iqplot.spike(df, \"percent correct\")\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nEach spike rises to the number of times a given value was measured.\nSpike plots fail, however, when the measurements do not take on discrete values. To demonstrate, we will add random noise to the percent correct data and try replotting. (We will learn about random number generation with NumPy in a future lesson. For now, this is for purposes of discussing plotting options.)\n\n# Add a bit of noise to the % correct measurements so they are no longer discrete\nrng = np.random.default_rng()\n\ndf = df.with_columns(\n    pl.col('percent correct')\n    .map_elements(\n        lambda s: s + rng.normal(0, 0.5),\n        return_dtype=float\n    )\n    .alias(\"percent correct with noise\")\n)\n\n# Replot swarm plot\np = iqplot.strip(df, \"percent correct with noise\", frame_height=100, spread=\"swarm\")\np.yaxis.visible = False\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe can still roughly make out how the data are distributed in the swarm plot, but the spike plot adds little beyond what we would see with a strip plot sans swarm or jitter.\n\np = iqplot.spike(df, \"percent correct with noise\", frame_height=100)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing distributions</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/visualizing_distributions.html#histograms",
    "href": "lessons/eda/plotting/visualizing_distributions.html#histograms",
    "title": "10  Visualizing distributions",
    "section": "10.5 Histograms",
    "text": "10.5 Histograms\nIf we do not have discrete data, we can instead use a histogram. A histogram is constructed by dividing the measurement values into bins and then counting how many measurements fall into each bin. The bins are then displayed graphically.\nA problem with histograms is that they require a choice of binning. Different choices of bins can lead to qualitatively different appearances of the plot. One choice of number of bins is the Freedman-Diaconis rule, which serves to minimize the integral of the squared difference between the (unknown) underlying probability density function and the histogram. This is the default of iqplot.\n\np = iqplot.histogram(df, \"percent correct with noise\")\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe rug plot at the bottom of the histogram shows all of the measurements (following the “plot all your data” rule).",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing distributions</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/visualizing_distributions.html#ecdfs",
    "href": "lessons/eda/plotting/visualizing_distributions.html#ecdfs",
    "title": "10  Visualizing distributions",
    "section": "10.6 ECDFs",
    "text": "10.6 ECDFs\nHistograms are typically used to display how data are distributed. As an example I will generate Normally distributed data and plot the histogram.\n\nx = rng.normal(size=500)\nbokeh.io.show(iqplot.histogram(x, rug=False, style=\"step_filled\"))\n\n\n  \n\n\n\n\n\nThis looks similar to the standard Normal curve we are used to seeing and is a useful comparison to a probability density function (PDF). However, Histograms suffer from binning bias. By binning the data, you are not plotting all of them. In general, if you can plot all of your data, you should. For that reason, I prefer not to use histograms for studying how data are distributed, but rather prefer to use ECDFs, which enable plotting of all data.\nThe ECDF evaluated at x for a set of measurements is defined as\n\\[\\begin{align}\n\\text{ECDF}(x) = \\text{fraction of measurements } \\le x.\n\\end{align}\\]\nWhile the histogram is an attempt to visualize a probability density function (PDF) of a distribution, the ECDF visualizes the cumulative density function (CDF). The CDF, \\(F(x)\\), and PDF, \\(f(x)\\), both completely define a univariate distribution and are related by\n\\[\\begin{align}\nf(x) = \\frac{\\mathrm{d}F}{\\mathrm{d}x}.\n\\end{align}\\]\nThe definition of the ECDF is all that you need for interpretation. For a given value on the x-axis, the value of the ECDF is the fraction of observations that are less than or equal to that value. Once you get used to looking at CDFs, they will become as familiar as PDFs. A peak in a PDF corresponds to an inflection point in a CDF.\nTo make this more clear, let us look at plot of a PDF and ECDF for familiar distributions, the Gaussian and Binomial.\n\n\n\n\nPDFs and CDFs\n\n\n\nNow that we know what an ECDF is, we can plot it.\n\nbokeh.io.show(iqplot.ecdf(x, style='dots'))\n\n\n  \n\n\n\n\n\nEach dot in the ECDF is a single data point that we measured. Given the above definition of the ECDF, it is defined for all real \\(x\\). So, formally, the ECDF is a continuous function (with discontinuous derivatives at each data point). So, it could be plotted like a staircase according to the formal definition.\n\nbokeh.io.show(iqplot.ecdf(x))\n\n\n  \n\n\n\n\n\nEither method of plotting is fine; there is not any less information in one than the other.\nLet us now plot the percent correct data as an ECDF, both with dots and as a staircase. Visualizing it this way helps highlight the relationship between the dots and the staircase.\n\np = iqplot.ecdf(df, \"percent correct\")\np = iqplot.ecdf(df, \"percent correct\", p=p, style='dots', marker_kwargs=dict(color=\"orange\"))\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe circles are on the concave corners of the staircase.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing distributions</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/visualizing_distributions.html#computing-environment",
    "href": "lessons/eda/plotting/visualizing_distributions.html#computing-environment",
    "title": "10  Visualizing distributions",
    "section": "10.7 Computing environment",
    "text": "10.7 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\npolars    : 1.33.1\nbokeh     : 3.7.3\niqplot    : 0.3.7\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing distributions</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html",
    "href": "lessons/eda/plotting/iqplot.html",
    "title": "11  High level plotting with iqplot",
    "section": "",
    "text": "11.1 Plots with categorical variables\n| Download notebook\nIn this lesson, we do some plotting with a high-level package iqplot. For fun, we will use a data set from Kleinteich and Gorb that features data about strikes of the tongues of frogs on a target. Let’s get the data frame loaded in so we can be on our way.\nLet us first consider the different kinds of data we may encounter as we think about constructing a plot.\nIn practice, ordinal data can be cast as quantitative or treated as categorical with an ordering enforced on the categories (e.g., categorical data [1, 2, 3] becomes ['1', '2', '3'].). Temporal data can also be cast as quantitative, (e.g., second from the start time). We will therefore focus out attention on quantitative and categorical data.\nWhen we made scatter plots in the previous lesson, both types of data were quantitative. We did actually incorporate categorical information in the form of colors of the glyph (insomniacs and normal sleepers being colored differently) and in tooltips.\nBut what if we wanted a single type of measurement, say impact force, for each frog? Here, we have the quantitative impact force data and the categorical frog ID data. One of our axes is now categorical.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#plots-with-categorical-variables",
    "href": "lessons/eda/plotting/iqplot.html#plots-with-categorical-variables",
    "title": "11  High level plotting with iqplot",
    "section": "",
    "text": "Quantitative data may have continuously varying (and therefore ordered) values.\nCategorical data has discrete, unordered values that a variable can take.\nOrdinal data has discrete, ordered values. Integers are a classic example.\nTemporal data refers to time, which can be represented as dates.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#bar-graph",
    "href": "lessons/eda/plotting/iqplot.html#bar-graph",
    "title": "11  High level plotting with iqplot",
    "section": "11.2 Bar graph",
    "text": "11.2 Bar graph\nTo demonstrate how to set up a categorical axis with Bokeh, I will make a bar graph of the mean impact force for each of the four frogs. But before I even begin this, I will give you the following piece of advice: Don’t make bar graphs. More on that in a moment.\nBefore we do that, we need to compute the means from the inputted data frame.\n\ndf_mean = df[['ID', 'impact force (mN)']].group_by('ID').mean()\n\n# Take a look\ndf_mean\n\n\nshape: (4, 2)\n\n\n\nID\nimpact force (mN)\n\n\nstr\nf64\n\n\n\n\n\"III\"\n550.1\n\n\n\"I\"\n1530.2\n\n\n\"II\"\n707.35\n\n\n\"IV\"\n419.1\n\n\n\n\n\n\nTo set up a categorical axis, you need to specify the x_range (or y_range if you want the y-axis to be categorical) as a list with the categories you want on the axis when you instantiate the figure. I will make a horizontal bar graph, so I will specify y_range. Also, when I instantiate this figure, because it is not very tall and I do not want the reset tool cut off, I will also explicitly set the tools I want in the toolbar.\n\np = bokeh.plotting.figure(\n    frame_height=200,\n    frame_width=400,\n    x_axis_label='impact force (mN)',\n    y_range=list(df_mean['ID'].sort(descending=True)),\n    tools='pan,wheel_zoom,save,reset'\n)\n\nNow that we have the figure, we can put the bars on. The p.hbar() method populates the figure with horizontal bar glyphs. The right kwarg says what column of the data source dictates how far to the right to show the bar, while the height kwarg says how think the bars are.\nI will also ensure the quantitative axis starts at zero and turn off the grid lines on the categorical axis, which is commonly done.\n\np.hbar(\n    source=df_mean.to_dict(),\n    y='ID',\n    right='impact force (mN)',\n    height=0.6\n)\n\n# Turn off gridlines on categorical axis\np.ygrid.grid_line_color = None\n\n# Start axes at origin on quantitative axis\np.x_range.start = 0\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe similarly make vertical bar graphs specifying x_range and using p.vbar().\n\np = bokeh.plotting.figure(\n    frame_height=250,\n    frame_width=250,\n    y_axis_label='impact force (mN)',\n    x_range=list(df_mean['ID'].sort()),\n)\n\np.vbar(\n    source=df_mean.to_dict(),\n    x='ID',\n    top='impact force (mN)',\n    width=0.6\n)\n\np.xgrid.grid_line_color = None\np.y_range.start = 0\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#iqplot",
    "href": "lessons/eda/plotting/iqplot.html#iqplot",
    "title": "11  High level plotting with iqplot",
    "section": "11.3 iqplot",
    "text": "11.3 iqplot\nGenerating the bar graphs was not too painful, even tough we used Bokeh, a low-level plotting library. Nonetheless, we would like to make plots more declaratively. We do not want to have to explicitly pre-process the data, set up the categorical axis, etc. We would like to just provide a data set, say which column(s) is/are categorical and which is quantitative, and then just get our plot.\niqplot generates plots from tidy data frames where one or more columns contain categorical data and the column of interest in the plot is quantitative.\nThere are seven types of plots that iqplot can generate. As you will see, all four of these modes of plotting are meant to give a picture about how the quantitative measurements are distributed for each category.\n\nBox plots: iqplot.box()\nStrip plots: iqplot.strip()\nSpike plots: iqplot.spike()\nStrip-box plots (strip and box plots overlaid): iqplot.stripbox()\nHistograms: iqplot.histogram()\nStrip-histogram plots (strip and histograms overlaid): iqplot.striphistogram()\nECDFs: iqplot.ecdf()\n\nThis first seven arguments are the same for all plots. They are:\n\ndata: A tidy data frame or Numpy array.\nq: The column of the data frame to be treated as the quantitative variable.\ncats: A list of columns in the data frame that are to be considered as categorical variables in the plot. If None, a single box, strip, histogram, or ECDF is plotted.\nq_axis: Along which axis, x or y that the quantitative variable varies. The default is 'x'.\npalette: A list of hex colors to use for coloring the markers for each category. By default, it uses the Glasbey Category 10 color palette from colorcet.\norder: If specified, the ordering of the categories to use on the categorical axis and legend (if applicable). Otherwise, the order of the inputted data frame is used.\np: If specified, the bokeh.plotting.Figure object to use for the plot. If not specified, a new figure is created.\n\nIf data is given as a Numpy array, it is the only required argument. If data is given as a Pandas DataFrame, q must also be supplied. All other arguments are optional and have reasonably set defaults. Any extra kwargs not in the function call signature are passed to bokeh.plotting.figure() when the figure is instantiated.\nWith this in mind, we will put iqplot to use on facial identification data set to demonstrate how we can make each of the seven kinds of plots using the frog data set.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#box-plots-with-iqplot",
    "href": "lessons/eda/plotting/iqplot.html#box-plots-with-iqplot",
    "title": "11  High level plotting with iqplot",
    "section": "11.4 Box plots with iqplot",
    "text": "11.4 Box plots with iqplot\nAs I discuss below, bar graphs are almost never a good choice for visualization. You distill all of the information in the data set down to one or two summary statistics, and then use giant glyphs to show them. As a start for improvement, you could distill the data set down to five or so summary statistics and show those graphically, as opposed to just one or two.\nBox plots provide such a summary. I will first make one using iqplot.box() and then describe how a box plot is interpreted.\n\np = iqplot.box(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe line in the middle of each box is the median and the top and bottom of the box at the 75th and 25th percentiles, respectively. The distance between the 25th and 75th percentiles is called the interquartile range, or IQR. The whiskers of the box plot extend to the most extreme data point within 1.5 times the interquartile range. If any data points are more extreme than the end of the whisker, they are shown individually, and are often referred to as outliers.\nA box plot can use a useful visualization if you have many data points and it is difficult to plot them all. I rarely find that there are situations where all data cannot be plotted, either with strip plots of ECDFs, which we will cover in a moment, so I generally do not use box plots. Nonetheless, I do not find them too objectionable, as they effectively display important nonparametric summary statistics of your data set.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#plot-all-your-data",
    "href": "lessons/eda/plotting/iqplot.html#plot-all-your-data",
    "title": "11  High level plotting with iqplot",
    "section": "11.5 Plot all your data",
    "text": "11.5 Plot all your data\nBox plots summarize a data set with summary statistics, but what not plot all your data? You work hard to acquire them. You should show them all. This is a mantra to live by.\n\nPlot all of your data.\n\nLet’s do that now.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#strip-plots",
    "href": "lessons/eda/plotting/iqplot.html#strip-plots",
    "title": "11  High level plotting with iqplot",
    "section": "11.6 Strip plots",
    "text": "11.6 Strip plots\nA strip plot is like a scatter plot; it puts a glyph for every measured data point. The only difference is that one of the axes is categorical. In this case, you are plotting all of your data.\n\np = iqplot.strip(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis is a good plot to make since you are plotting all of your data, but it does have the problem that you cannot tell if multiple data points overlap. We will deal with this in the next lesson when we discuss styling plots.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#spike-plots",
    "href": "lessons/eda/plotting/iqplot.html#spike-plots",
    "title": "11  High level plotting with iqplot",
    "section": "11.7 Spike plots",
    "text": "11.7 Spike plots\nA spike plot is useful when you want to see how many times a specific value was encountered in your data set. Let’s make a spike plot for the number of times specific impact times were observed.\n\np = iqplot.spike(\n    df,\n    q='impact time (ms)'\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nNotice that an impact time of 31 ms was observed eight times, though most impact times were observed once or twice. In the above plot, we have not had a categorical variable to demonstrate how a spike plot looks. If we do have a categorical variable, it is more difficult to display counts on the y-axis, so the proportion of the measurements with a given value are displayed.\n\np = iqplot.spike(\n    df,\n    q='impact time (ms)',\n    cats='ID',\n)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#strip-box-plots",
    "href": "lessons/eda/plotting/iqplot.html#strip-box-plots",
    "title": "11  High level plotting with iqplot",
    "section": "11.8 Strip-box plots",
    "text": "11.8 Strip-box plots\nIt is sometimes useful to overlay strip plots with box plots, as the box plots show useful quantile information. This is accomplished using the iqplot.stripbox() function.\n\np = iqplot.stripbox(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#histograms",
    "href": "lessons/eda/plotting/iqplot.html#histograms",
    "title": "11  High level plotting with iqplot",
    "section": "11.9 Histograms",
    "text": "11.9 Histograms\nIn plotting all of our data in a strip plot, we can roughly see how the data are distributed. There are more measurements where there are more glyphs. We ofter seek to visualize the distribution of the data. Histograms are commonly used for this. They are typically interpreted to as an empirical representation of the probability density function.\n\np = iqplot.histogram(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nNote that by default, iqplot includes a rug plot at the bottom of the histogram, showing each measurement. The number of bins are automatically chosen using the Freedman-Diaconis rule.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#strip-histogram",
    "href": "lessons/eda/plotting/iqplot.html#strip-histogram",
    "title": "11  High level plotting with iqplot",
    "section": "11.10 Strip-histogram",
    "text": "11.10 Strip-histogram\nStrip plots may also be combined with histograms. By default, the histograms are normalized and mirrored, similar to a violin plot.\n\np = iqplot.striphistogram(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#ecdfs",
    "href": "lessons/eda/plotting/iqplot.html#ecdfs",
    "title": "11  High level plotting with iqplot",
    "section": "11.11 ECDFs",
    "text": "11.11 ECDFs\nI just mentioned that histograms are typically used to display how data are distributed, but it was hard to make out the distributions in the above plot, partly because we do not have very many measurements. As another example I will generate Normally distributed data and plot the histogram. (We will learn how to generate data like this when we study random number generation with NumPy in a future lesson. For now, this is for purposes of discussing plotting options.)\nNote that iqplot can take a Numpy array as the data argument, and makes a plot assuming it contains a single data set.\n\n# Generate normally distributed data\nrng = np.random.default_rng(3252)\nx = rng.normal(size=500)\n\n# Plot the histogram\np = iqplot.histogram(x, rug=False)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis looks similar to the standard Normal curve we are used to seeing and is a useful comparison to a probability density function (PDF). However, Histograms suffer from binning bias. By binning the data, you are not plotting all of them. In general, if you can plot all of your data, you should. For that reason, I prefer not to use histograms for studying how data are distributed, but rather prefer to use ECDFs, which enable plotting of all data.\nThe ECDF evaluated at x for a set of measurements is defined as\n\\[\\begin{align}\n\\text{ECDF}(x) = \\text{fraction of measurements } \\le x.\n\\end{align}\\]\nWhile the histogram is an attempt to visualize a probability density function (PDF) of a distribution, the ECDF visualizes the cumulative density function (CDF). The CDF, \\(F(x)\\), and PDF, \\(f(x)\\), both completely define a univariate distribution and are related by\n\\[\\begin{align}\nf(x) = \\frac{\\mathrm{d}F}{\\mathrm{d}x}.\n\\end{align}\\]\nThe definition of the ECDF is all that you need for interpretation. Once you get used to looking at CDFs, they will become as familiar to you as PDFs. A peak in a PDF corresponds to an inflection point in a CDF.\nTo make this more clear, let us look at plot of a PDF and ECDF for familiar distributions, the Gaussian and Binomial.\n\n\n\n\n\n\nFigure 11.1: Top: The PDF (left) and CDF (right) of the Normal (a.k.a. Gaussian) distribution. Bottom: The PMF (left) and CDF (right) of the Binomial distribution.\n\n\n\nNow that we know how to interpret ECDFs, lets plot the ECDF for our dummy Normally-distributed data.\n\np = iqplot.ecdf(x)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nLet’s make a set of ECDFs for our frog data.\n\np = iqplot.ecdf(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThough we do not see points, this is still plotting all of your data. The concave corners of the staircase correspond to the measured data. This can be seen by overlaying the “dot” version of the ECDFs.\n\np = iqplot.ecdf(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n    p=p,\n    style='dots',\n    show_legend=False,\n)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#sec-no-bar-graphs",
    "href": "lessons/eda/plotting/iqplot.html#sec-no-bar-graphs",
    "title": "11  High level plotting with iqplot",
    "section": "11.12 Don’t make bar graphs",
    "text": "11.12 Don’t make bar graphs\nBar graphs, especially with error bars (in which case they are called dynamite plots), are typically awful. They are pervasive in biology papers. I have yet to find a single example where a bar graph is the best choice. Strip plots (with jitter) or even box plots, are more informative and almost always preferred. In fact, ECDFs are often better even than these. Here is a simple message:\n\nDon’t make bar graphs.\n\nWhat should I do instead you ask? The answer is simple: plot all of your data when you can. If you can’t, box plots are always better than bar graphs.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#computing-environment",
    "href": "lessons/eda/plotting/iqplot.html#computing-environment",
    "title": "11  High level plotting with iqplot",
    "section": "11.13 Computing environment",
    "text": "11.13 Computing environment\n\n\nCode\n%load_ext watermark\n%watermark -v -p numpy,polars,bokeh,iqplot,jupyterlab\n\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\npolars    : 1.27.1\nbokeh     : 3.6.2\niqplot    : 0.3.7\njupyterlab: 4.3.6",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/styling_bokeh.html",
    "href": "lessons/eda/plotting/styling_bokeh.html",
    "title": "12  Styling Bokeh plots",
    "section": "",
    "text": "12.1 Styling Bokeh plots as they are built\n| Download notebook\nWe have seen how to use Bokeh (and the higher-level plotting package iqplot) to make interactive plots. We have seen how to adjust plot size, axis labels, glyph color, etc. We have also seen how to style plots generated with iqplot. But we have just started to touch the surface of how we might customize plots. In this lesson, we investigate ways to stylize Bokeh plots to our visual preferences.\nWe will again make use of the face-matching data set. We’ll naturally start by loading the data set.\nBokeh figures and renderers (which are essentially the glyphs) have a plethora of attributes pertaining to visual appearance that may be adjusted at instantiation and after making a plot.\nA color palette an ordering of colors that are used for glyphs, usually corresponding to categorical data. Colorcet’s Glasbey Category 10 provides a good palette for categorical data, and we store this as our categorical colors for plotting.\ncat_colors = colorcet.b_glasbey_category10\nNow we can build the plot. Since the data are percentages, we will set the axes to go from zero to 100 and enforce that the figure is square. We will also include a title as well so we can style that.\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=300,\n    x_axis_label=\"confidence when correct\",\n    y_axis_label=\"confidence when incorrect\",\n    title=\"GMFT with sleep conditions\",\n    x_range=[0, 100],\n    y_range=[0, 100],\n)\nIn styling this plot, we will also put the legend outside of the plot area. This is a bit trickier than what we have been doing using the legend_label kwarg in p.scatter(). To get a legend outside of the plot area, we need to:\nNow, we add the glyphs, storing them as variables normal_glyph and insom_glyph.\nnormal_glyph = p.scatter(\n    source=df.filter(~pl.col('insomnia')).to_dict(),\n    x=\"confidence when correct\",\n    y=\"confidence when incorrect\",\n    color=cat_colors[0],\n)\n\ninsom_glyph = p.scatter(\n    source=df.filter(pl.col('insomnia')).to_dict(),\n    x=\"confidence when correct\",\n    y=\"confidence when incorrect\",\n    color=cat_colors[1],\n)\nNow we can construct and add the legend.\n# Construct legend items\nlegend_items = [('normal', [normal_glyph]), ('insomnia', [insom_glyph])]\n\n# Instantiate legend\nlegend = bokeh.models.Legend(items=legend_items, click_policy='hide')\n\n# Add the legend to the right of the plot\np.add_layout(legend, 'right')\nNow, let’s take a look at this beauty!\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Styling Bokeh plots</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/styling_bokeh.html#styling-bokeh-plots-as-they-are-built",
    "href": "lessons/eda/plotting/styling_bokeh.html#styling-bokeh-plots-as-they-are-built",
    "title": "12  Styling Bokeh plots",
    "section": "",
    "text": "Assign each glyph to a variable.\nInstantiate a bokeh.models.Legend object using the stored variables containing the glyphs. This is instantiated as bokeh.models.Legend(items=legend_items), where legend_items is a list of 2-tuples. In each 2-tuple, the first entry is a string with the text used to label the glyph. The second entry is a list of glyphs that have the label.\nAdd the legend to the figure using the add_layout() method.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Styling Bokeh plots</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/styling_bokeh.html#styling-bokeh-plots-after-they-are-built",
    "href": "lessons/eda/plotting/styling_bokeh.html#styling-bokeh-plots-after-they-are-built",
    "title": "12  Styling Bokeh plots",
    "section": "12.2 Styling Bokeh plots after they are built",
    "text": "12.2 Styling Bokeh plots after they are built\nAfter building a plot, we sometimes want to adjust styling. To do so, we need to change attributes of the object p. For example, let’s look at the font of the x-axis label.\n\np.xaxis.axis_label_text_font\n\n'helvetica'\n\n\nWe can also look at the style and size of the font.\n\np.xaxis.axis_label_text_font_style, p.xaxis.axis_label_text_font_size\n\n('italic', '13px')\n\n\nSo, the default axis labels for Bokeh are italicized 13 pt Helvetica. I personally think this choice is fine, but we may have other preferences.\nTo find out all of the available options to tweak, I usually type something like p. and hit tab to see what the options are. Finding p.xaxis is an option, then type p.xaxis. and hit tab again to see the styling option there.\nUsing this technique, we can set some obnoxious styling for this plot. I will make all of the fonts non-italicized, large papyrus. I can also set the background and grid colors. Note that in Bokeh, any named CSS color or any valid HEX code, entered as a string, is a valid color.\nBefore we do the obnoxious styling, we will do one adjustment that is useful. Note in the above plot that the glyphs at the end of the plot are cropped. We would like the whole glyph to show. To do that, we set the level of the glyphs to be 'overlay'. To do that, we extract the first two elements of the list of renderers, which contains the glyphs, and set the level attribute.\n\np.renderers[0].level = 'overlay'\np.renderers[1].level = 'overlay'\n\nNow we can proceed to make our obnoxious styling.\n\n# Obnoxious fonts\np.xaxis.major_label_text_font = 'papyrus'\np.xaxis.major_label_text_font_size = '14pt'\np.xaxis.axis_label_text_font = 'papyrus'\np.xaxis.axis_label_text_font_style = 'normal'\np.xaxis.axis_label_text_font_size = '20pt'\np.yaxis.major_label_text_font = 'papyrus'\np.yaxis.major_label_text_font_size = '14pt'\np.yaxis.axis_label_text_font = 'papyrus'\np.yaxis.axis_label_text_font_style = 'normal'\np.yaxis.axis_label_text_font_size = '20pt'\np.title.text_font = 'papyrus'\np.title.text_font_size = '18pt'\np.legend.label_text_font = 'papyrus'\n\n# Align the title center\np.title.align = 'center'\n\n# Set background and grid color\np.background_fill_color = 'blanchedalmond'\np.legend.background_fill_color = 'chartreuse'\np.xgrid.grid_line_color = 'azure'\np.ygrid.grid_line_color = 'azure'\n\n# Make the ticks point inward (I *hate* this!)\n# Units are pixels that the ticks extend in and out of plot\np.xaxis.major_tick_out = 0\np.xaxis.major_tick_in = 10\np.xaxis.minor_tick_out = 0\np.xaxis.minor_tick_in = 5\np.yaxis.major_tick_out = 0\np.yaxis.major_tick_in = 10\np.yaxis.minor_tick_out = 0\np.yaxis.minor_tick_in = 5\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis is truly hideous, but it demonstrates how we can go about styling plots after they are made.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Styling Bokeh plots</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/styling_bokeh.html#bokeh-themes",
    "href": "lessons/eda/plotting/styling_bokeh.html#bokeh-themes",
    "title": "12  Styling Bokeh plots",
    "section": "12.3 Bokeh themes",
    "text": "12.3 Bokeh themes\nBokeh has several built-in themes which you can apply to all plots in a given document (e.g., in a notebook). Please see the documentation for details about the built-in themes. I personally prefer the default styling to all of their themes, but your opinion may differ.\nYou may also specify custom themes using JSON or YAML. As an example, we can specify a theme such that plots are styled like the default style of the excellent plotting packages Vega-Altair/Vega-Lite/Vega. If we use JSON formatting, we can specify a theme as a dictionary of dictionaries, as below.\n\naltair_theme_dict = {\n    \"attrs\": {\n        \"Axis\": {\n            \"axis_line_color\": \"dimgray\",\n            \"minor_tick_out\": 0,\n            \"major_tick_in\": 0,\n            \"major_tick_line_color\": \"dimgray\",\n            \"major_label_text_font_size\": \"7.5pt\",\n            \"axis_label_text_font_size\": \"8pt\",\n            \"axis_label_text_font_style\": \"bold\",\n        },\n        \"Scatter\": {\n            \"fill_alpha\": 0, \n            \"line_width\": 2, \n            \"size\": 5, \n            \"line_alpha\": 0.7,\n        },\n        \"ContinuousTicker\": {\n            \"desired_num_ticks\": 10\n        },\n        \"figure\": {\n            \"frame_width\": 350, \n            \"frame_height\": 300,\n        },\n        \"Grid\": {\n            \"grid_line_color\": \"lightgray\",\n            \"level\": \"underlay\",\n        },\n        \"Legend\": {\n            \"border_line_color\": None,\n            \"background_fill_color\": None,\n            \"label_text_font_size\": \"7.5pt\",\n            \"title_text_font_size\": \"8pt\",\n            \"title_text_font_style\": \"bold\",\n        },\n        \"Renderer\": {\n            \"level\": \"overlay\"\n        },\n        \"Title\": {\n            \"align\": \"center\",\n        },\n    }\n}\n\nTo activate the theme, we convert it to a Bokeh theme and then add it to the curdoc(), or the current document.\n\naltair_theme = bokeh.themes.Theme(json=altair_theme_dict)\n\nbokeh.io.curdoc().theme = altair_theme\n\nNow the theme is activated, and future plots will have this theme by default. Let’s remake our plot using this theme. For convenience later on, I will write a function to generate this scatter plot that we will use to test various styles.\n\ndef gfmt_plot():\n    \"\"\"Make a plot for testing out styles in this notebook.\"\"\"\n    p = bokeh.plotting.figure(\n        frame_width=300,\n        frame_height=300,\n        x_axis_label=\"confidence when correct\",\n        y_axis_label=\"confidence when incorrect\",\n        title=\"GMFT with sleep conditions\",\n        x_range=[0, 100],\n        y_range=[0, 100],\n    )\n\n    normal_glyph = p.scatter(\n        source=df.filter(~pl.col('insomnia')).to_dict(),\n        x=\"confidence when correct\",\n        y=\"confidence when incorrect\",\n        color=cat_colors[0],\n    )\n\n    insom_glyph = p.scatter(\n        source=df.filter(pl.col('insomnia')).to_dict(),\n        x=\"confidence when correct\",\n        y=\"confidence when incorrect\",\n        color=cat_colors[1],\n    )\n\n    # Construct legend items\n    legend_items = [('normal', [normal_glyph]), ('insomnia', [insom_glyph])]\n\n    # Instantiate legend\n    legend = bokeh.models.Legend(items=legend_items, click_policy='hide')\n\n    # Add the legend to the right of the plot\n    p.add_layout(legend, 'right')\n    \n    return p\n\nbokeh.io.show(gfmt_plot())\n\n\n  \n\n\n\n\n\nWe could also style our plots to resemble the default “dark” styling of Seaborn.\n\nseaborn_theme_dict = {\n    \"attrs\": {\n        \"figure\": {\n            \"background_fill_color\": \"#eaeaf2\",\n            \"frame_height\": 300,\n            \"frame_width\": 350,\n        },\n        \"Axis\": {\n            \"axis_line_color\": None,\n            \"minor_tick_out\": 0,\n            \"major_tick_in\": 0,\n            \"major_tick_out\": 0,\n            \"major_label_text_font_size\": \"7.5pt\",\n            \"axis_label_text_font_size\": \"7.5pt\",\n            \"axis_label_text_font_style\": \"normal\",\n        },\n        \"Legend\": {\n            \"border_line_color\": \"darkgray\",\n            \"background_fill_color\": \"#eaeaf2\",\n            \"border_line_width\": 0.75,\n            \"label_text_font_size\": \"7.5pt\",\n        },\n        \"Grid\": {\n            \"grid_line_color\": \"#FFFFFF\", \n            \"grid_line_width\": 0.75,\n        },\n        \"Title\": {\n            \"align\": \"center\",\n            'text_font_style': 'normal',\n            'text_font_size': \"8pt\",\n        },\n    }\n}\n\nseaborn_theme = bokeh.themes.Theme(json=seaborn_theme_dict)\nbokeh.io.curdoc().theme = seaborn_theme\n\nLet’s make the plot, yet again, with this new styling.\n\nbokeh.io.show(gfmt_plot())\n\n\n  \n\n\n\n\n\nFinally, we can specify a style I like. Note that I do not specify that the glyphs are at an overlay level, since by default Bokeh will scale the axes such that the glyphs are fully contained in the plot area. I also put the toolbar above the plot, which is usually not a problem because I generally prefer not to title my plots, opting instead for good textual description in captions or in surrounding text.\n\njb_theme_dict = {\n    \"attrs\": {\n        \"Axis\": {\n            \"axis_line_color\": \"dimgray\",\n            \"major_tick_line_color\": \"dimgray\",\n            \"major_label_text_font_size\": \"7.5pt\",\n            \"axis_label_text_font_size\": \"9pt\",\n            \"axis_label_text_font_style\": \"bold\",\n        },\n        \"Scatter\": {\n            \"size\": 5, \n            \"fill_alpha\": 0.8,\n            \"line_width\": 0,\n        },\n        \"figure\": {\n            \"frame_height\": 300,\n            \"frame_width\": 350,\n            \"toolbar_location\": \"above\",\n        },\n        \"Grid\": {\n            \"grid_line_color\": \"lightgray\",\n            \"level\": \"underlay\",\n        },\n        \"Legend\": {\n            \"border_line_color\": \"darkgray\",\n            \"border_line_width\": 0.75,\n            \"background_fill_color\": \"#ffffff\",\n            \"background_fill_alpha\": 0.7,\n            \"label_text_font\": \"helvetica\", \n            \"label_text_font_size\": \"7.5pt\",\n            \"title_text_font\": \"helvetica\", \n            \"title_text_font_size\": \"8pt\", \n            \"title_text_font_style\": \"bold\", \n        },\n        \"Renderer\": {\n            \"level\": \"overlay\"\n        },\n        \"Title\": {\n            \"text_font\": \"helvetica\",\n            \"text_font_size\": \"10pt\",\n            'text_font_style': 'bold',\n        },\n    }\n}\n\njb_theme = bokeh.themes.Theme(json=jb_theme_dict)\nbokeh.io.curdoc().theme = jb_theme\n\nbokeh.io.show(gfmt_plot())\n\n\n  \n\n\n\n\n\nFinally, if I were to make this particular plot, I would do it without a title and with axes leaving a little buffer.\n\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=300,\n    x_axis_label=\"confidence when correct\",\n    y_axis_label=\"confidence when incorrect\",\n    x_range=[-2.5, 102.5],\n    y_range=[-2.5, 102.5],\n)\n\nnormal_glyph = p.scatter(\n    source=df.filter(~pl.col('insomnia')).to_dict(),\n    x=\"confidence when correct\",\n    y=\"confidence when incorrect\",\n    color=cat_colors[0],\n)\n\ninsom_glyph = p.scatter(\n    source=df.filter(pl.col('insomnia')).to_dict(),\n    x=\"confidence when correct\",\n    y=\"confidence when incorrect\",\n    color=cat_colors[1],\n)\n\n# Construct legend items\nlegend_items = [('normal', [normal_glyph]), ('insomnia', [insom_glyph])]\n\n# Instantiate legend\nlegend = bokeh.models.Legend(items=legend_items, click_policy='hide')\n\n# Add the legend to the right of the plot\np.add_layout(legend, 'right')\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nYou can play with these themes and develop your own style as you see fit. As you can see, Bokeh is highly configurable, and you can really make the plots your own!",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Styling Bokeh plots</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/styling_bokeh.html#computing-environment",
    "href": "lessons/eda/plotting/styling_bokeh.html#computing-environment",
    "title": "12  Styling Bokeh plots",
    "section": "12.4 Computing environment",
    "text": "12.4 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\npolars    : 1.33.1\nbokeh     : 3.7.3\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Styling Bokeh plots</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/manipulating_data_frames_intro.html",
    "href": "lessons/eda/wrangling/manipulating_data_frames_intro.html",
    "title": "13  Manipulating data frames and wrangling data",
    "section": "",
    "text": "13.1 What’s to come\n| Download notebook\nWe have already seen how to perform calculations with data frames by:\nThese methods are useful for wrangling data frames, doing things like unit conversions or computing statistical point estimates. There are plenty of other methods you can find in the Polars documentation. The Polars docs, and questions on Stack Overflow are among your best references for usage.\nIn the next sections of this lesson, we discuss how to reshape and combine data frames. In these operations, we are not really doing calculations with data, but rather are working to organize our data set into an easily usable, tidy form. We will cover\nAll of these methods are useful to bring inputted data into formats that are convenient to work with.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Manipulating data frames and wrangling data</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/manipulating_data_frames_intro.html#whats-to-come",
    "href": "lessons/eda/wrangling/manipulating_data_frames_intro.html#whats-to-come",
    "title": "13  Manipulating data frames and wrangling data",
    "section": "",
    "text": "Creating a data frame from scratch (as opposed to reading in data from a file)\nJoining and concatenating data frames (putting two or more data frames together into one)\nReshaping data frames by unpivoting and pivoting.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Manipulating data frames and wrangling data</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/manipulating_data_frames_intro.html#polars-documentation",
    "href": "lessons/eda/wrangling/manipulating_data_frames_intro.html#polars-documentation",
    "title": "13  Manipulating data frames and wrangling data",
    "section": "13.2 Polars documentation",
    "text": "13.2 Polars documentation\nThe Pandas documentation is a great resource for learning about these methods. In particular, the section of the user guide on joining concatenating, unpivoting, and pivoting is very useful. They use very small, contrived data frames to demonstrate the concepts. There are benefits to this approach, certainly, but I find it is often easier to grasp the concepts with real examples. In the following sections of this lesson, we will approach these topics with real examples. You can think of the these lessons as valuable introductions and supplements to the official Polars documentation.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Manipulating data frames and wrangling data</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/merging_dataframes.html",
    "href": "lessons/eda/wrangling/merging_dataframes.html",
    "title": "14  Merging and concatenating data frames",
    "section": "",
    "text": "14.1 The frog tongue strike data set\n| Download notebook\nData set download\nIt often happens that experiments consist of multiple data files that need to be brought together into a single data frame to work with in exploratory data analysis and subsequent analyses. Through its concatenation and merging capabilities, Polars provides powerful tools for handling this sort of data.\nAs usual, we will work with a real data set to learn about concatenation and merging of data frames. The data set we will use comes from a fun paper about the adhesive properties of frog tongues. The reference is Kleinteich and Gorb, Tongue adhesion in the horned frog Ceratophrys sp., Sci. Rep., 4, 5225, 2014. You might also want to check out a New York Times feature on the paper here.\nIn this paper, the authors investigated various properties of the adhesive characteristics of the tongues of horned frogs when they strike prey. The authors had a striking pad connected to a cantilever to measure forces. They also used high speed cameras to capture the strike and record relevant data.\nTo get an idea of the experimental set up, you can check out this movie, kindly sent to me by Thomas Kleinteich. If video does not play in your browser, you may download it here.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Merging and concatenating data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/merging_dataframes.html#the-frog-tongue-strike-data-set",
    "href": "lessons/eda/wrangling/merging_dataframes.html#the-frog-tongue-strike-data-set",
    "title": "14  Merging and concatenating data frames",
    "section": "",
    "text": "Your browser does not support display of this video.\n\n\n\n14.1.1 The data files\nI pulled data files from the Kleinteich and Gorb paper. You can download the data files here: https://s3.amazonaws.com/bebi103.caltech.edu/data/frog_strikes.zip.\nThere are four files, one for each of the four frogs, labeled with IDs I, II, III, and IV, that were studied. To see the format of the files, we can look at the content of the file for frog I. You can use\nhead -n 20 ../data/frog_strikes_I.csv\nfrom the command line. Here is the content of the first data file.\n# These data are from Kleinteich and Gorb, Sci. Rep., 4, 5225, 2014.\n# Frog ID: I\n# Age: adult\n# Snout-vent-length (SVL): 63 mm\n# Body weight: 63.1 g\n# Species: Ceratophrys cranwelli crossed with Ceratophrys cornuta\ndate,trial number,impact force (mN),impact time (ms),impact force / body weight,adhesive force (mN),time frog pulls on target (ms),adhesive force / body weight,adhesive impulse (N-s),total contact area (mm2),contact area without mucus (mm2),contact area with mucus / contact area without mucus,contact pressure (Pa),adhesive strength (Pa)\n2013_02_26,3,1205,46,1.95,-785,884,1.27,-0.290,387,70,0.82,3117,-2030\n2013_02_26,4,2527,44,4.08,-983,248,1.59,-0.181,101,94,0.07,24923,-9695\n2013_03_01,1,1745,34,2.82,-850,211,1.37,-0.157,83,79,0.05,21020,-10239\n2013_03_01,2,1556,41,2.51,-455,1025,0.74,-0.170,330,158,0.52,4718,-1381\n2013_03_01,3,493,36,0.80,-974,499,1.57,-0.423,245,216,0.12,2012,-3975\n2013_03_01,4,2276,31,3.68,-592,969,0.96,-0.176,341,106,0.69,6676,-1737\n2013_03_05,1,556,43,0.90,-512,835,0.83,-0.285,359,110,0.69,1550,-1427\n2013_03_05,2,1928,46,3.11,-804,508,1.30,-0.285,246,178,0.28,7832,-3266\n2013_03_05,3,2641,50,4.27,-690,491,1.12,-0.239,269,224,0.17,9824,-2568\n2013_03_05,4,1897,41,3.06,-462,839,0.75,-0.328,266,176,0.34,7122,-1733\n2013_03_12,1,1891,40,3.06,-766,1069,1.24,-0.380,408,33,0.92,4638,-1879\n2013_03_12,2,1545,48,2.50,-715,649,1.15,-0.298,141,112,0.21,10947,-5064\n2013_03_12,3,1307,29,2.11,-613,1845,0.99,-0.768,455,92,0.80,2874,-1348\n2013_03_12,4,1692,31,2.73,-677,917,1.09,-0.457,186,129,0.31,9089,-3636\n2013_03_12,5,1543,38,2.49,-528,750,0.85,-0.353,153,148,0.03,10095,-3453\n2013_03_15,1,1282,31,2.07,-452,785,0.73,-0.253,290,105,0.64,4419,-1557\n2013_03_15,2,775,34,1.25,-430,837,0.70,-0.276,257,124,0.52,3019,-1677\n2013_03_15,3,2032,60,3.28,-652,486,1.05,-0.257,147,134,0.09,13784,-4425\n2013_03_15,4,1240,34,2.00,-692,906,1.12,-0.317,364,260,0.28,3406,-1901\n2013_03_15,5,473,40,0.76,-536,1218,0.87,-0.382,259,168,0.35,1830,-2073\nThe first lines all begin with # signs, signifying that they are comments. They do give important information about the frog, though.\nThe first line after the comments are the headers, giving the column names for the data frame we will load.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Merging and concatenating data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/merging_dataframes.html#concatenating-data-frames",
    "href": "lessons/eda/wrangling/merging_dataframes.html#concatenating-data-frames",
    "title": "14  Merging and concatenating data frames",
    "section": "14.2 Concatenating data frames",
    "text": "14.2 Concatenating data frames\nWe would like to have all of the data frames be together in one data frame so we can conveniently do things like make plots comparing the four frogs. Let’s read in the data sets and make a list of data frames.\n\n# On a local machine, we would do this: fnames = glob.glob('../data/frog_strikes_*.csv')\n# But for Colab compatibility, we will do it by hand\nfnames = [\n    os.path.join(data_path, f\"frog_strikes_{frog_id}.csv\")\n    for frog_id in [\"I\", \"II\", \"III\", \"IV\"]\n]\n\ndfs = [pl.read_csv(f, comment_prefix=\"#\") for f in fnames]\n\n# Take a look at first data frame\ndfs[0].head()\n\n\nshape: (5, 14)\n\n\n\ndate\ntrial number\nimpact force (mN)\nimpact time (ms)\nimpact force / body weight\nadhesive force (mN)\ntime frog pulls on target (ms)\nadhesive force / body weight\nadhesive impulse (N-s)\ntotal contact area (mm2)\ncontact area without mucus (mm2)\ncontact area with mucus / contact area without mucus\ncontact pressure (Pa)\nadhesive strength (Pa)\n\n\nstr\ni64\ni64\ni64\nf64\ni64\ni64\nf64\nf64\ni64\ni64\nf64\ni64\ni64\n\n\n\n\n\"2013_02_26\"\n3\n1205\n46\n1.95\n-785\n884\n1.27\n-0.29\n387\n70\n0.82\n3117\n-2030\n\n\n\"2013_02_26\"\n4\n2527\n44\n4.08\n-983\n248\n1.59\n-0.181\n101\n94\n0.07\n24923\n-9695\n\n\n\"2013_03_01\"\n1\n1745\n34\n2.82\n-850\n211\n1.37\n-0.157\n83\n79\n0.05\n21020\n-10239\n\n\n\"2013_03_01\"\n2\n1556\n41\n2.51\n-455\n1025\n0.74\n-0.17\n330\n158\n0.52\n4718\n-1381\n\n\n\"2013_03_01\"\n3\n493\n36\n0.8\n-974\n499\n1.57\n-0.423\n245\n216\n0.12\n2012\n-3975\n\n\n\n\n\n\nWe have successfully loaded in all of the data frames. They all have the same columns (as given by the CSV files). So, we wish to tape the data frames together vertically. We can use the pl.concat() function to do this.\nBefore we do that, though, we might notice a problem. We will not have information to tell us which frog is which. We might therefore like to add a column to each data frame that has the frog ID, and then concatenate them. We can parse the ID of the frog from the file name, as we can see by looking at the file names.\n\nfnames\n\n['../data/frog_strikes_I.csv',\n '../data/frog_strikes_II.csv',\n '../data/frog_strikes_III.csv',\n '../data/frog_strikes_IV.csv']\n\n\nSo, for each data frame/file name pair, we extract the Roman numeral and add a column to the data frame containing the frog ID. To do this, we use a Polars literal, accessible with pl.lit(), which means that we want to insert a specific value (in this case, \"I\", \"II\", \"III\", or \"IV\") into a data frame as a column.\n\nfor i, f in enumerate(fnames):\n    frog_id = f[f.rfind('_')+1:f.rfind('.')]\n    dfs[i] = dfs[i].with_columns(pl.lit(frog_id).alias('ID'))\n    \n# Take a look\ndfs[0].head()\n\n\nshape: (5, 15)\n\n\n\ndate\ntrial number\nimpact force (mN)\nimpact time (ms)\nimpact force / body weight\nadhesive force (mN)\ntime frog pulls on target (ms)\nadhesive force / body weight\nadhesive impulse (N-s)\ntotal contact area (mm2)\ncontact area without mucus (mm2)\ncontact area with mucus / contact area without mucus\ncontact pressure (Pa)\nadhesive strength (Pa)\nID\n\n\nstr\ni64\ni64\ni64\nf64\ni64\ni64\nf64\nf64\ni64\ni64\nf64\ni64\ni64\nstr\n\n\n\n\n\"2013_02_26\"\n3\n1205\n46\n1.95\n-785\n884\n1.27\n-0.29\n387\n70\n0.82\n3117\n-2030\n\"I\"\n\n\n\"2013_02_26\"\n4\n2527\n44\n4.08\n-983\n248\n1.59\n-0.181\n101\n94\n0.07\n24923\n-9695\n\"I\"\n\n\n\"2013_03_01\"\n1\n1745\n34\n2.82\n-850\n211\n1.37\n-0.157\n83\n79\n0.05\n21020\n-10239\n\"I\"\n\n\n\"2013_03_01\"\n2\n1556\n41\n2.51\n-455\n1025\n0.74\n-0.17\n330\n158\n0.52\n4718\n-1381\n\"I\"\n\n\n\"2013_03_01\"\n3\n493\n36\n0.8\n-974\n499\n1.57\n-0.423\n245\n216\n0.12\n2012\n-3975\n\"I\"\n\n\n\n\n\n\nGood! Now all data frames have an 'ID' column, and we can concatenate. The pl.concat() function takes as input a list of data frames to be concatenated and stacks them on top of each other.\n\n# Concatenate data frames\ndf = pl.concat(dfs)\n\n# Make sure we got them all\nprint(\n    \"Number of rows:\", len(df), \"\\nUnique IDs:\", df.get_column(\"ID\").unique().to_list()\n)\n\nNumber of rows: 80 \nUnique IDs: ['III', 'I', 'II', 'IV']\n\n\nCheck!",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Merging and concatenating data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/merging_dataframes.html#creating-a-dataframe-from-scratch",
    "href": "lessons/eda/wrangling/merging_dataframes.html#creating-a-dataframe-from-scratch",
    "title": "14  Merging and concatenating data frames",
    "section": "14.3 Creating a DataFrame from scratch",
    "text": "14.3 Creating a DataFrame from scratch\nLooking back at the headers of the original data files, we see that there is information present in the header that we would like to have in our data frame. For example, it would be nice to know if each strike came from an adult or juvenile. Or what the snout-vent length was. Working toward the goal of including this in our data frame, we will first construct a new data frame containing information about each frog.\n\n14.3.1 Data frames from dictionaries\nOne way do create this new data frame is to first construct a dictionary with the respective fields. Since these data sets are small, we can look at the files and make the dictionary by hand.\n\ndata_dict = {\n    \"ID\": [\"I\", \"II\", \"III\", \"IV\"],\n    \"age\": [\"adult\", \"adult\", \"juvenile\", \"juvenile\"],\n    \"SVL (mm)\": [63, 70, 28, 31],\n    \"body weight (g)\": [63.1, 72.7, 12.7, 12.7],\n    \"species\": [\"cross\", \"cross\", \"cranwelli\", \"cranwelli\"],\n}\n\nNow that we have this dictionary, we can convert it into a DataFrame by instantiating a pl.DataFrame class with it, using the data kwarg.\n\n# Make it into a DataFrame\ndf_frog_info = pl.DataFrame(data=data_dict)\n\n# Take a look\ndf_frog_info\n\n\nshape: (4, 5)\n\n\n\nID\nage\nSVL (mm)\nbody weight (g)\nspecies\n\n\nstr\nstr\ni64\nf64\nstr\n\n\n\n\n\"I\"\n\"adult\"\n63\n63.1\n\"cross\"\n\n\n\"II\"\n\"adult\"\n70\n72.7\n\"cross\"\n\n\n\"III\"\n\"juvenile\"\n28\n12.7\n\"cranwelli\"\n\n\n\"IV\"\n\"juvenile\"\n31\n12.7\n\"cranwelli\"\n\n\n\n\n\n\nNice!\n\n\n14.3.2 Data frames from numpy arrays\nSometimes the data sets are not small enough to construct a dictionary by hand. Oftentimes, we have a two-dimensional array of data that we want to make into a DataFrame. As an example, let’s say we have a Numpy array where the first column is snout vent length and the second is weight.\n\ndata = np.array([[63, 70, 28, 31], [63.1, 72.7, 12.7, 12.7]]).transpose()\n\n# Verify that it's what we think it is\ndata\n\narray([[63. , 63.1],\n       [70. , 72.7],\n       [28. , 12.7],\n       [31. , 12.7]])\n\n\nTo make this into a DataFrame, we again create pl.DataFrame instance, but this time we also specify the schema keyword argument to label the columns.\n\ndf_demo = pl.DataFrame(data=data, schema=[\"SVL (mm)\", \"weight (g)\"])\n\n# Take a look\ndf_demo\n\n\nshape: (4, 2)\n\n\n\nSVL (mm)\nweight (g)\n\n\nf64\nf64\n\n\n\n\n63.0\n63.1\n\n\n70.0\n72.7\n\n\n28.0\n12.7\n\n\n31.0\n12.7\n\n\n\n\n\n\nThat also works. Generally, any two-dimensional Numpy array can be converted into a DataFrame in this way. You just need to supply column names.\n\n\n14.3.3 Programmatically creating a data frame\nHand-entering data should be minimized. The information about each frog was hand-entered once by the experimenter. We should not hand enter them again. We therefore should parse the comment lines of input files to get the pertinent information.\nNote, though, that in the case of a single experiment with only four data sets, hand entering might be faster and indeed less error prone than doing it programmatically. We should definitely do it programmatically if we have a large number of data files or will ever do an experiment with the same file format again.\nSo, let’s programmatically parse the files. We start by writing a function to parse the metadata from a single file. Recall that the comment lines look like this:\n# These data are from Kleinteich and Gorb, Sci. Rep., 4, 5225, 2014.\n# Frog ID: I\n# Age: adult\n# Snout-vent-length (SVL): 63 mm\n# Body weight: 63.1 g\n# Species: Ceratophrys cranwelli crossed with Ceratophrys cornuta\n(The function below will not work with Colab because open() does not work for files specified by a URL.)\n\ndef parse_frog_metadata(fname):\n    with open(fname, 'r') as f:\n        # Citation line, ignore.\n        f.readline()\n        \n        # Frog ID\n        line = f.readline()\n        frog_id = line[line.find(':')+1:].strip()\n        \n        # Age\n        line = f.readline()\n        age = line[line.find(':')+1:].strip()\n        \n        # SVL, assume units given as mm\n        line = f.readline()\n        svl = line[line.find(':')+1:line.rfind(' ')].strip()\n        \n        # Body weight, assume units given as g\n        line = f.readline()\n        body_weight = line[line.find(':')+1:line.rfind(' ')].strip()\n\n        # Species (either cranwelli or cross)\n        line = f.readline()\n        species = line[line.find(':')+1:].strip()\n        if 'cross' in species:\n            species = 'cross'\n        else:\n            species = 'cranwelli'\n\n    return frog_id, age, svl, body_weight, species\n\nLet’s take it for a spin.\n\nparse_frog_metadata(os.path.join(data_path, 'frog_strikes_I.csv'))\n\n('I', 'adult', '63', '63.1', 'cross')\n\n\nLooks good! Now we can create a list of tuples to use as data for making a data frame.\n\ndata = [parse_frog_metadata(f) for f in fnames]\n    \n# Take a look\ndata\n\n[('I', 'adult', '63', '63.1', 'cross'),\n ('II', 'adult', '70', '72.7', 'cross'),\n ('III', 'juvenile', '28', '12.7', 'cranwelli'),\n ('IV', 'juvenile', '31', '12.7', 'cranwelli')]\n\n\nWe now input this list of tuples, plus the column names, into pl.DataFrame(), and we’ve got our data frame. We do have to specify that this list of tuples is row-oriented, so Polars knows that each tuple is a row and not a column.\n\ndf_frog_info = pl.DataFrame(\n    data=data, \n    schema=[\"ID\", \"age\", \"SVL (mm)\", \"body weight (g)\", \"species\"],\n    orient='row',\n)\n\n# Take a look\ndf_frog_info\n\n\nshape: (4, 5)\n\n\n\nID\nage\nSVL (mm)\nbody weight (g)\nspecies\n\n\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n\"I\"\n\"adult\"\n\"63\"\n\"63.1\"\n\"cross\"\n\n\n\"II\"\n\"adult\"\n\"70\"\n\"72.7\"\n\"cross\"\n\n\n\"III\"\n\"juvenile\"\n\"28\"\n\"12.7\"\n\"cranwelli\"\n\n\n\"IV\"\n\"juvenile\"\n\"31\"\n\"12.7\"\n\"cranwelli\"",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Merging and concatenating data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/merging_dataframes.html#joining-dataframes",
    "href": "lessons/eda/wrangling/merging_dataframes.html#joining-dataframes",
    "title": "14  Merging and concatenating data frames",
    "section": "14.4 Joining DataFrames",
    "text": "14.4 Joining DataFrames\nWe want to add the information about the frogs into our main data frame, df, that we have been working with. Specifically, for each row of the data frame, we also want to include the frog’s age, snout-vent length, body weight, and species. So, we want to take the data frame with all of the information about the tongue strikes and combine it with the data frame containing information about each frog. This combining of data frames is a join operation. In this case, we join on the 'ID' column, since the value of the that column in each data frame indicates the frog we are talking about.\nTo perform a join operation we use the df.join() method (or df.join_asof() method for approximate matches). Its default join strategy is an inner join, in which the entries of a given row are included in the joined data frame if and only if the entry 'ID' column of the respective frames match. You can read more about available join strategies in the documentation.\n\ndf = df.join(df_frog_info, on='ID')\n\n# Take a look\ndf.head()\n\n\nshape: (5, 19)\n\n\n\ndate\ntrial number\nimpact force (mN)\nimpact time (ms)\nimpact force / body weight\nadhesive force (mN)\ntime frog pulls on target (ms)\nadhesive force / body weight\nadhesive impulse (N-s)\ntotal contact area (mm2)\ncontact area without mucus (mm2)\ncontact area with mucus / contact area without mucus\ncontact pressure (Pa)\nadhesive strength (Pa)\nID\nage\nSVL (mm)\nbody weight (g)\nspecies\n\n\nstr\ni64\ni64\ni64\nf64\ni64\ni64\nf64\nf64\ni64\ni64\nf64\ni64\ni64\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n\"2013_02_26\"\n3\n1205\n46\n1.95\n-785\n884\n1.27\n-0.29\n387\n70\n0.82\n3117\n-2030\n\"I\"\n\"adult\"\n\"63\"\n\"63.1\"\n\"cross\"\n\n\n\"2013_02_26\"\n4\n2527\n44\n4.08\n-983\n248\n1.59\n-0.181\n101\n94\n0.07\n24923\n-9695\n\"I\"\n\"adult\"\n\"63\"\n\"63.1\"\n\"cross\"\n\n\n\"2013_03_01\"\n1\n1745\n34\n2.82\n-850\n211\n1.37\n-0.157\n83\n79\n0.05\n21020\n-10239\n\"I\"\n\"adult\"\n\"63\"\n\"63.1\"\n\"cross\"\n\n\n\"2013_03_01\"\n2\n1556\n41\n2.51\n-455\n1025\n0.74\n-0.17\n330\n158\n0.52\n4718\n-1381\n\"I\"\n\"adult\"\n\"63\"\n\"63.1\"\n\"cross\"\n\n\n\"2013_03_01\"\n3\n493\n36\n0.8\n-974\n499\n1.57\n-0.423\n245\n216\n0.12\n2012\n-3975\n\"I\"\n\"adult\"\n\"63\"\n\"63.1\"\n\"cross\"\n\n\n\n\n\n\nNote that the entries for the added columns were repeated appropriately, e.g., the body weight column had 63 for every row corresponding to frog I.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Merging and concatenating data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/merging_dataframes.html#at-long-last-a-plot",
    "href": "lessons/eda/wrangling/merging_dataframes.html#at-long-last-a-plot",
    "title": "14  Merging and concatenating data frames",
    "section": "14.5 At long last, a plot!",
    "text": "14.5 At long last, a plot!\nWhile the purpose of this part of the lesson was to learn how to concatenate and merge data frames, going through all of that wrangling effort would somehow be unsatisfying if we we didn’t generate a plot. Let’s compare the impact force on a per-mass basis for each frog.\n\np = iqplot.strip(\n    df,\n    q=\"impact force / body weight\",\n    cats=\"ID\",\n    color_column=\"age\",\n    spread=\"jitter\",\n    x_axis_label=\"impact force / body weight (mN/g)\",\n    y_axis_label=\"frog ID\"\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nApparently Frog III consistently packs a powerful punch, er…. tongue, for its body weight.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Merging and concatenating data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/merging_dataframes.html#computing-environment",
    "href": "lessons/eda/wrangling/merging_dataframes.html#computing-environment",
    "title": "14  Merging and concatenating data frames",
    "section": "14.6 Computing environment",
    "text": "14.6 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\npolars    : 1.33.1\nbokeh     : 3.7.3\niqplot    : 0.3.7\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Merging and concatenating data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/unpivoting.html",
    "href": "lessons/eda/wrangling/unpivoting.html",
    "title": "15  Making a data frame tall",
    "section": "",
    "text": "15.1 The data set\n| Download notebook\nData set download\nIn this part of the lesson, we will perform some wrangling on a data set that involves:\nWe will use a data set from Angela Stathopoulos’s lab, acquired to study morphogen profiles in developing fruit fly embryos. The original paper is Reeves, Trisnadi, et al., Dorsal-ventral gene expression in the Drosophila embryo reflects the dynamics and precision of the Dorsal nuclear gradient, Dev. Cell., 22, 544-557, 2012, and the data set may be downloaded here: https://s3.amazonaws.com/bebi103.caltech.edu/data/Reeves2012_data.xlsx.\nIn this experiment, Reeves, Trisnadi, and coworkers measured expression levels of a fusion of Dorsal, a morphogen transcription factor important in determining the dorsal-ventral axis of the developing organism, and Venus, a yellow fluorescent protein along the dorsal/ventral– (DV) coordinate. They put this construct on the third chromosome, while wild type dorsal is on the second. Instead of the wild type, they had a homozygous dorsal-null mutant on the second chromosome. The Dorsal-Venus construct rescues wild type behavior, so they could use this construct to study Dorsal gradients.\nDorsal shows higher expression on the ventral side of the organism, thus giving a gradient in expression from dorsal to ventral which can be ascertained by the spatial distribution of Venus fluorescence intensity.\nThis can be seen in the image below, which is a cross-section of a fixed embryo with anti-Dorsal staining. The bottom of the image is the ventral side and the top is the dorsal side of the embryo. The DV coordinate system is defined by the yellow line. The image is adapted from the Reeves, Trisnadi, et al. paper.\nA quick note on nomenclature: Dorsal (capital D) is the name of the protein product of the gene dorsal (italicized). The dorsal (adjective) side of the embryo is its back. The ventral side is its belly. Dorsal is expressed more strongly on the ventral side of the developing embryo. This can be confusing.\nTo quantify the gradient, Reeves, Trisnadi, and coworkers had to first choose a metric for describing it. They chose to fit the measured profile of fluorescence intensity with a Gaussian peak (plus background) and use the standard deviation of that Gaussian as a metric for the width of the Dorsal gradient.\nIn this lesson, we will use the gradient widths as outputted from this procedure. The units of the widths are dimensionless, consistent with the coordinate system shown in the image above. I asked one of the authors for the data sets used in making the figures. She sent me a MS Excel file that had a separate sheet for each of several figures in the paper that I asked about. We will focus on the data used for Fig. 1F of the paper. In this figure, the authors seek to demonstrate that live imaging with their Venus-Dorsal construct gives a Dorsal gradient of similar width as would be obtained by fixing wild type cells and doing Dorsal antibody staining (the gold standard). These wild type embryos were analyzed as whole mounts and also as cross-sections. They also tried anti-Dorsal staining and anti-Venus staining in the Venus-Dorsal construct. Finally, they also measured gradient widths of a GFP-Dorsal construct that fails to complete development.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making a data frame tall</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/unpivoting.html#the-data-set",
    "href": "lessons/eda/wrangling/unpivoting.html#the-data-set",
    "title": "15  Making a data frame tall",
    "section": "",
    "text": "Fluorescently labeled Dorsal in a cross-section of a Drosophila embryo. Adapted from Reeves, Trisnadi, et al., Dev. Cell., 2012.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making a data frame tall</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/unpivoting.html#loading-in-an-excel-sheet",
    "href": "lessons/eda/wrangling/unpivoting.html#loading-in-an-excel-sheet",
    "title": "15  Making a data frame tall",
    "section": "15.2 Loading in an Excel sheet",
    "text": "15.2 Loading in an Excel sheet\nGenerally, you should store your data sets in portable formats, like CSV, JSON, XML, HDF5, OME-TIFF, etc., and not proprietary formats. Nonetheless, software like Microsoft Excel is widely used, and you will often receive data sets in this format. Fortunately, Polars can read Excel files, provided they are from fairly recent versions of Excel.\nTo read in this data set, we will use pl.read_excel(). Importantly, because an Excel document may have many sheets, we need to specify the sheet name we want, in this case 'Fig 1F'.\n\ndf = pl.read_excel(os.path.join(data_path, \"Reeves2012_data.xlsx\"), sheet_name=\"Fig 1F\")\n\ndf.head()\n\n\nshape: (5, 8)\n\n\n\nwt wholemounts\nwt cross-sections\nanti-Dorsal dl1/+dl-venus/+\nanti-gfp dl1/+dl-venus/+\nVenus (live) dl1/+dl-venus/+\nanti-Dorsal dl1/+dl-GFP/+\nanti-gfp dl1/+dl-GFP/+\nGFP (live) dl1/+dl-GFP/+\n\n\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n0.1288\n0.1327\n0.1482\n0.1632\n0.1666\n0.2248\n0.2389\n0.2412\n\n\n0.1554\n0.1457\n0.1503\n0.1671\n0.1753\n0.1891\n0.2035\n0.1942\n\n\n0.1306\n0.1447\n0.1577\n0.1704\n0.1705\n0.1705\n0.1943\n0.2186\n\n\n0.1413\n0.1282\n0.1711\n0.1779\nnull\n0.1735\n0.2\n0.2104\n\n\n0.1557\n0.1487\n0.1342\n0.1483\nnull\n0.2135\n0.256\n0.2463\n\n\n\n\n\n\nThe data frame is not tidy. Each entry corresponds to one observation, not each row. The column headings contain important metadata, the genotype (wt, dl1/+dl-venus/+, or dl1/+dl-GFP/+) and the method (wholemounts, cross-sections, anti-Dorsal, anti-gfp, Venus (live), and GFP (live)).\nThe data set has other issues we need to clean up. The column 'anti-gfp dl1/+dl-venus/+' is mislabeled; it should be 'anti-Venus dl1/+dl-venus/+'. We would also like to clean up the genotypes, putting in a semicolon to separate the chromosomes. The wild type columns have the genotype first ('wt') followed by the method, whereas the other columns have the method first, followed by genotype.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making a data frame tall</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/unpivoting.html#parsing-the-column-names",
    "href": "lessons/eda/wrangling/unpivoting.html#parsing-the-column-names",
    "title": "15  Making a data frame tall",
    "section": "15.3 Parsing the column names",
    "text": "15.3 Parsing the column names\nWe will start our process of tidying this data set by changing the column names. They are pretty messy, so this is best done by hand in this case. We will rename the columns with strings where the genotype comes first, followed by the method for measuring the gradient width, separated by an underscore.\n\ncol_names = {\n    'wt wholemounts': 'WT_whole mount',\n    'wt cross-sections': 'WT_cross-section',\n    'anti-Dorsal dl1/+dl-venus/+': 'dl1/+ ; dl-venus/+_anti-Dorsal',\n    'anti-gfp dl1/+dl-venus/+': 'dl1/+ ; dl-venus/+_anti-Venus',\n    'Venus (live)  dl1/+dl-venus/+': 'dl1/+ ; dl-venus/+_Venus (live)',\n    'anti-Dorsal  dl1/+dl-GFP/+': 'dl1/+ ; dl-gfp/+_anti-Dorsal',\n    'anti-gfp  dl1/+dl-GFP/+ ': 'dl1/+ ; dl-gfp/+_anti-GFP',\n    'GFP (live)  dl1/+dl-GFP/+': 'dl1/+ ; dl-gfp/+_GFP (live)'\n}\n\ndf = df.rename(col_names)\n\ndf.head()\n\n\nshape: (5, 8)\n\n\n\nWT_whole mount\nWT_cross-section\ndl1/+ ; dl-venus/+_anti-Dorsal\ndl1/+ ; dl-venus/+_anti-Venus\ndl1/+ ; dl-venus/+_Venus (live)\ndl1/+ ; dl-gfp/+_anti-Dorsal\ndl1/+ ; dl-gfp/+_anti-GFP\ndl1/+ ; dl-gfp/+_GFP (live)\n\n\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n0.1288\n0.1327\n0.1482\n0.1632\n0.1666\n0.2248\n0.2389\n0.2412\n\n\n0.1554\n0.1457\n0.1503\n0.1671\n0.1753\n0.1891\n0.2035\n0.1942\n\n\n0.1306\n0.1447\n0.1577\n0.1704\n0.1705\n0.1705\n0.1943\n0.2186\n\n\n0.1413\n0.1282\n0.1711\n0.1779\nnull\n0.1735\n0.2\n0.2104\n\n\n0.1557\n0.1487\n0.1342\n0.1483\nnull\n0.2135\n0.256\n0.2463",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making a data frame tall</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/unpivoting.html#unpivot-the-data-frame",
    "href": "lessons/eda/wrangling/unpivoting.html#unpivot-the-data-frame",
    "title": "15  Making a data frame tall",
    "section": "15.4 Unpivot the data frame",
    "text": "15.4 Unpivot the data frame\nWhen we unpivot the data frame, the data within it, called values, become a single column. The column names, called variables also populate a new column. So, to unpivot it, we need to specify what we want to call the values and what we want to call the variable. The unpivot() method does the rest!\n\ndf = df.unpivot(\n    variable_name='genotype_method', \n    value_name='gradient width'\n).drop_nulls()\n\n# Take a look\ndf.head()\n\n\nshape: (5, 2)\n\n\n\ngenotype_method\ngradient width\n\n\nstr\nf64\n\n\n\n\n\"WT_whole mount\"\n0.1288\n\n\n\"WT_whole mount\"\n0.1554\n\n\n\"WT_whole mount\"\n0.1306\n\n\n\"WT_whole mount\"\n0.1413\n\n\n\"WT_whole mount\"\n0.1557\n\n\n\n\n\n\nNice! We now have a tidy data frame. Note that we also dropped the null values, since the nulls from the original columns come along for the ride when unpivoting.\nNote that df.unpivot() has other options. For example, you can specify columns that do not comprise data, but should still be included in the unpivoted data frame using the id_vars keyword argument. That does not apply to this data frame, but comes up often. As a final comment, note that unpivoting is sometimes called melting, as it is when using Pandas.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making a data frame tall</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/unpivoting.html#splitting-the-genotype_method-column",
    "href": "lessons/eda/wrangling/unpivoting.html#splitting-the-genotype_method-column",
    "title": "15  Making a data frame tall",
    "section": "15.5 Splitting the genotype_method column",
    "text": "15.5 Splitting the genotype_method column\nNow our goal is to convert the 'genotype_method' column into two columns, one encoding genotype and the other the method. To do this, we first use Polars’s string methods to split the entries in the column at the underscore. This gives a series of list data types. We then convert the list to a struct, where the fields are the column labels we want when we split the column into two, in this case 'genotype' and 'method'. Finally, we can unnest the column with our structs.\n\ndf = df.with_columns(\n    pl.col('genotype_method')\n    .str.split('_')\n    .list.to_struct(fields=['genotype', 'method'])\n).unnest('genotype_method')\n\n# Take a look\ndf.head()\n\n\nshape: (5, 3)\n\n\n\ngenotype\nmethod\ngradient width\n\n\nstr\nstr\nf64\n\n\n\n\n\"WT\"\n\"whole mount\"\n0.1288\n\n\n\"WT\"\n\"whole mount\"\n0.1554\n\n\n\"WT\"\n\"whole mount\"\n0.1306\n\n\n\"WT\"\n\"whole mount\"\n0.1413\n\n\n\"WT\"\n\"whole mount\"\n0.1557\n\n\n\n\n\n\nLooking at the data frame above, it is very tall. Each row only has a single value, that is, a single measurement, and every other column in the row is metadata associated with that measurement, specifically the genotype and method. Unpivoting operations generally make data frames taller, with more rows than before unpivoting.\n\n15.5.1 Using the data frame\nLet’s make a plot like Fig. 1F of the Reeves, Trisnadi, et al. paper, but not with boxes, rather as a strip plot.\n\np = iqplot.strip(\n    data=df,\n    q='gradient width',\n    cats=['genotype', 'method'],\n    color_column='genotype',\n    spread=\"jitter\",\n    frame_height=350,\n    frame_width=450,\n)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making a data frame tall</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/unpivoting.html#computing-environment",
    "href": "lessons/eda/wrangling/unpivoting.html#computing-environment",
    "title": "15  Making a data frame tall",
    "section": "15.6 Computing environment",
    "text": "15.6 Computing environment\n\n%load_ext watermark\n%watermark -v -p polars,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\npolars    : 1.33.1\nbokeh     : 3.8.0\niqplot    : 0.3.7\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making a data frame tall</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/pivoting.html",
    "href": "lessons/eda/wrangling/pivoting.html",
    "title": "16  Making a data frame wide",
    "section": "",
    "text": "16.1 Exploring the data set\n| Download notebook\nData set download\nWe have seen how unpivoting a data frame can bring it to tidy format, but a tall format is often not the only tidy option nor the easiest to work with. As usual, this is best seen by example, and we will use a subset of the Palmer penguins data set, which you can download here: https://s3.amazonaws.com/bebi103.caltech.edu/data/penguins_subset.csv, to demonstrate. The data set consists of measurements of three different species of penguins acquired at the Palmer Station in Antarctica. The measurements were made between 2007 and 2009 by Kristen Gorman.\nAs we work toward getting the data in a useful tidy format, we will learn some additional wrangling techniques.\nFirst, let’s take a look at the data set stored in the CSV file.\n!head ../data/penguins_subset.csv\n\nGentoo,Gentoo,Gentoo,Gentoo,Adelie,Adelie,Adelie,Adelie,Chinstrap,Chinstrap,Chinstrap,Chinstrap\nbill_depth_mm,bill_length_mm,flipper_length_mm,body_mass_g,bill_depth_mm,bill_length_mm,flipper_length_mm,body_mass_g,bill_depth_mm,bill_length_mm,flipper_length_mm,body_mass_g\n16.3,48.4,220.0,5400.0,18.5,36.8,193.0,3500.0,18.3,47.6,195.0,3850.0\n15.8,46.3,215.0,5050.0,16.9,37.0,185.0,3000.0,16.7,42.5,187.0,3350.0\n14.2,47.5,209.0,4600.0,19.5,42.0,200.0,4050.0,16.6,40.9,187.0,3200.0\n15.7,48.7,208.0,5350.0,18.3,42.7,196.0,4075.0,20.0,52.8,205.0,4550.0\n14.1,48.7,210.0,4450.0,18.0,35.7,202.0,3550.0,18.7,45.4,188.0,3525.0\n15.0,49.6,216.0,4750.0,19.1,39.8,184.0,4650.0,18.2,49.6,193.0,3775.0\n15.7,49.3,217.0,5850.0,18.4,40.8,195.0,3900.0,17.5,48.5,191.0,3400.0\n15.2,49.2,221.0,6300.0,18.4,36.6,184.0,3475.0,18.2,49.2,195.0,4400.0\nWe see that we have two header rows. The first gives the species and the second the quantity that is being measured. Apparently, then, each row of data has information for three different penguins, one from each species. This is not a tidy data set!",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Making a data frame wide</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/pivoting.html#reading-the-data-set-into-a-polars-data-frame",
    "href": "lessons/eda/wrangling/pivoting.html#reading-the-data-set-into-a-polars-data-frame",
    "title": "16  Making a data frame wide",
    "section": "16.2 Reading the data set into a Polars data frame",
    "text": "16.2 Reading the data set into a Polars data frame\nWe start by naively reading this data set using pl.read_csv().\n\ndf = pl.read_csv(os.path.join(data_path, \"penguins_subset.csv\"))\ndf.head()\n\n\nshape: (5, 12)\n\n\n\nGentoo\nGentoo_duplicated_0\nGentoo_duplicated_1\nGentoo_duplicated_2\nAdelie\nAdelie_duplicated_0\nAdelie_duplicated_1\nAdelie_duplicated_2\nChinstrap\nChinstrap_duplicated_0\nChinstrap_duplicated_1\nChinstrap_duplicated_2\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n\"bill_depth_mm\"\n\"bill_length_mm\"\n\"flipper_length_mm\"\n\"body_mass_g\"\n\"bill_depth_mm\"\n\"bill_length_mm\"\n\"flipper_length_mm\"\n\"body_mass_g\"\n\"bill_depth_mm\"\n\"bill_length_mm\"\n\"flipper_length_mm\"\n\"body_mass_g\"\n\n\n\"16.3\"\n\"48.4\"\n\"220.0\"\n\"5400.0\"\n\"18.5\"\n\"36.8\"\n\"193.0\"\n\"3500.0\"\n\"18.3\"\n\"47.6\"\n\"195.0\"\n\"3850.0\"\n\n\n\"15.8\"\n\"46.3\"\n\"215.0\"\n\"5050.0\"\n\"16.9\"\n\"37.0\"\n\"185.0\"\n\"3000.0\"\n\"16.7\"\n\"42.5\"\n\"187.0\"\n\"3350.0\"\n\n\n\"14.2\"\n\"47.5\"\n\"209.0\"\n\"4600.0\"\n\"19.5\"\n\"42.0\"\n\"200.0\"\n\"4050.0\"\n\"16.6\"\n\"40.9\"\n\"187.0\"\n\"3200.0\"\n\n\n\"15.7\"\n\"48.7\"\n\"208.0\"\n\"5350.0\"\n\"18.3\"\n\"42.7\"\n\"196.0\"\n\"4075.0\"\n\"20.0\"\n\"52.8\"\n\"205.0\"\n\"4550.0\"\n\n\n\n\n\n\nOoof! This is nasty. The second header row is included with the data, which results in inferring every column data type to be a string. Polars only allows for a single header row, so we cannot load this data set.\nThis is a fairly common occurrence with human-made tabular data. Researchers will have hierarchical column headings. In this case, the first header row is species and the second is the quantity that is being measured for the respective species. (Note that I did this intentionally for instructional purposes; this is not what the original penguins data set had.)\nTo convert this type of structure into a tidy format, we can perform an unpivoting operation where each of the levels of the hierarchical column headings become rows. Polars will not do this for you, since it forbids hierarchical column headings (hierarchical indexing is not a good idea, in my opinion, so I see why Polars forbids it). I therefore wrote a function for the bebi103 package that takes a CSV file, possibly with a hierarchical index, and unpivots it to give a new CSV file. We can use this on the penguins data set and then load in the result.\n\nbebi103.utils.unpivot_csv(\n    os.path.join(data_path, \"penguins_subset.csv\"), \n    os.path.join(data_path, \"penguins_tall.csv\"),\n    header_names=['species', 'quantity'],\n    retain_row_index=True,\n    row_index_name='penguin_id',\n    force_overwrite=True\n)\n\ndf = pl.read_csv(os.path.join(data_path, \"penguins_tall.csv\"))\n\ndf.head(10)\n\n\nshape: (10, 4)\n\n\n\npenguin_id\nspecies\nquantity\nvalue\n\n\ni64\nstr\nstr\nf64\n\n\n\n\n0\n\"Gentoo\"\n\"bill_depth_mm\"\n16.3\n\n\n0\n\"Gentoo\"\n\"bill_length_mm\"\n48.4\n\n\n0\n\"Gentoo\"\n\"flipper_length_mm\"\n220.0\n\n\n0\n\"Gentoo\"\n\"body_mass_g\"\n5400.0\n\n\n0\n\"Adelie\"\n\"bill_depth_mm\"\n18.5\n\n\n0\n\"Adelie\"\n\"bill_length_mm\"\n36.8\n\n\n0\n\"Adelie\"\n\"flipper_length_mm\"\n193.0\n\n\n0\n\"Adelie\"\n\"body_mass_g\"\n3500.0\n\n\n0\n\"Chinstrap\"\n\"bill_depth_mm\"\n18.3\n\n\n0\n\"Chinstrap\"\n\"bill_length_mm\"\n47.6\n\n\n\n\n\n\nNotice that we kept the row index in the column 'penguin_id' to make sure that we didn’t lose track of what penguin each measurement was associated with. Note also that each penguin has a unique identifier when combined with the species. E.g., Adelie penguin with penguin ID 0 has four measurements associated with it.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Making a data frame wide</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/pivoting.html#pivoting-from-tall-to-wide-format",
    "href": "lessons/eda/wrangling/pivoting.html#pivoting-from-tall-to-wide-format",
    "title": "16  Making a data frame wide",
    "section": "16.3 Pivoting from tall to wide format",
    "text": "16.3 Pivoting from tall to wide format\nLooking at the melded data frame above, it is very tall. Each row only has a single value, that is, a single measurement, and every other column in the row is metadata associated with that measurement, specifically which penguin/species and which quantity was being measured.\nWhile this tall format is tidy, we can imagine using a wide format, in which each row is a specific penguin and each column is a quantity measured on that penguin. This is also a tidy format. In some cases, a tall format is more convenient, and in others a wide format is more convenient.\nTo convert from a tall to a wide format, we perform a pivot. A pivot operation classifies columns of a tall data frame in three ways.\n\nThe column(s) we pivot on gets converted into column headings of the pivoted data frame.\nThe value column’s entries get populated under the new column headings determined by the on column.\nThe index column(s) are essentially along for the ride. They are retained as columns. If the index columns are not unique, then you need to specify an aggregating operation that will be applied to the values associated with rows that have like entries in the index column(s).\n\nLet us now perform the pivot. We want 'quantity' as the “on” column, since it takes a column or columns of a data frame and converts them to column headings. In this case, the value column is 'value', and 'penguin_id' and 'species' columns are the index columns. They uniquely define a penguin, so we do not need to provide any aggregating function.\n\ndf = df.pivot(\n    on='quantity', \n    index=['penguin_id', 'species'], \n    values='value'\n)\n\n# Take a look\ndf.head()\n\n\nshape: (5, 6)\n\n\n\npenguin_id\nspecies\nbill_depth_mm\nbill_length_mm\nflipper_length_mm\nbody_mass_g\n\n\ni64\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n0\n\"Gentoo\"\n16.3\n48.4\n220.0\n5400.0\n\n\n0\n\"Adelie\"\n18.5\n36.8\n193.0\n3500.0\n\n\n0\n\"Chinstrap\"\n18.3\n47.6\n195.0\n3850.0\n\n\n1\n\"Gentoo\"\n15.8\n46.3\n215.0\n5050.0\n\n\n1\n\"Adelie\"\n16.9\n37.0\n185.0\n3000.0\n\n\n\n\n\n\nExcellent! We now have a wide, but still tidy, data frame.\nThe 'penguin_id' column is dispensable, but we will keep it for now, since we will demonstrate a pivot operation from this wide data frame in a moment.\n\n16.3.1 A couple of plots for fun\nNow that we’ve done all this work and our data set is tidy, let’s make a plot for fun. First, we’ll plot the ECDFs of the bill lengths.\n\nbokeh.io.show(\n    iqplot.ecdf(\n        data=df,\n        cats='species',\n        q='bill_length_mm',\n        x_axis_label='bill length (mm)',\n        frame_width=400,\n    )\n)\n\n\n  \n\n\n\n\n\nWe can also plot bill length versus flipper length to see if we can see a difference among the species. It is also useful to have a hover tool that shows bill depth and body mass.\n\n# Create figure\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=300,\n    x_axis_label=\"bill length (mm)\",\n    y_axis_label=\"flipper length (mm)\",\n    toolbar_location=\"above\",\n    tooltips=[('bill depth', '@bill_depth_mm'), ('body mass', '@body_mass_g')]\n)\n\n# Build legend as we populate glyphs\nlegend_items = []\nfor color, ((species,), sub_df) in zip(bokeh.palettes.Category10_3, df.group_by(\"species\")):\n    glyph = p.scatter(\n        source=sub_df.to_dict(), x=\"bill_length_mm\", y=\"flipper_length_mm\", color=color\n    )\n    legend_items.append((species, [glyph]))\n\n# Place legend\nlegend = bokeh.models.Legend(items=legend_items, location=\"center\")\np.add_layout(legend, \"right\")\np.legend.click_policy = \"hide\"\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Making a data frame wide</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/pivoting.html#an-important-note-about-tidiness",
    "href": "lessons/eda/wrangling/pivoting.html#an-important-note-about-tidiness",
    "title": "16  Making a data frame wide",
    "section": "16.4 An important note about tidiness",
    "text": "16.4 An important note about tidiness\nIt is important to note that there is more than one way to make a data set tidy. In the example of the Palmer penguin data set, we saw two legitimate ways of making the data frame tidy. In our preferred wide version, each row corresponded to a measurement of a single penguin, which had several variables associated with it. In the tall version, each row corresponded to a single feature of a penguin.\nTo demonstrate that the tall version is workable, but more cumbersome, we can make the same plots as above. First, we’ll unpivot the data frame to make it tall. We need to specify which columns as index columns for the unpivot because we want the 'penguin_id' and 'species' columns to remain in the tall data frame (which we did not have to do in the previous lesson).\n\ndf = df.unpivot(index=[\"penguin_id\", \"species\"])\n\ndf.head()\n\n\nshape: (5, 4)\n\n\n\npenguin_id\nspecies\nvariable\nvalue\n\n\ni64\nstr\nstr\nf64\n\n\n\n\n0\n\"Gentoo\"\n\"bill_depth_mm\"\n16.3\n\n\n0\n\"Adelie\"\n\"bill_depth_mm\"\n18.5\n\n\n0\n\"Chinstrap\"\n\"bill_depth_mm\"\n18.3\n\n\n1\n\"Gentoo\"\n\"bill_depth_mm\"\n15.8\n\n\n1\n\"Adelie\"\n\"bill_depth_mm\"\n16.9\n\n\n\n\n\n\nPlotting the ECDFs is not really a problem with this form of the data frame. We just need to use a filter context to pull out the bill length rows.\n\nbokeh.io.show(\n    iqplot.ecdf(\n        data=df.filter(pl.col('variable') == 'bill_length_mm'),\n        q=\"value\",\n        cats=\"species\",\n        frame_width=400,\n        x_axis_label=\"bill length (mm)\",\n    )\n)\n\n\n  \n\n\n\n\n\nMaking the scatter plot, however, is much more difficult and involves a lot of filtering by hand.\n\n# Set up figure\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=300,\n    x_axis_label=\"bill length (mm)\",\n    y_axis_label=\"flipper length (mm)\",\n    toolbar_location=\"above\",\n)\n\n# Expressions for filtering\nbill_length = pl.col('variable') == \"bill_length_mm\"\nflipper_length = pl.col('variable') == \"flipper_length_mm\"\n\n# Build legend as we populate glyphs\nlegend_items = []\nfor color, ((species,), sub_df) in zip(bokeh.palettes.Category10_3, df.group_by('species')):\n    # Slice out bill and flipper lengths for species\n    bill = sub_df.filter(bill_length).get_column('value').to_numpy()\n    flipper = sub_df.filter(flipper_length).get_column('value').to_numpy()\n    \n    # Populate glyph\n    glyph = p.scatter(bill, flipper, color=color)\n    legend_items.append((species, [glyph]))\n\n# Build and place legend\nlegend = bokeh.models.Legend(items=legend_items, location=\"center\")\np.add_layout(legend, \"right\")\np.legend.click_policy = \"hide\"\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis works fine, but is more cumbersome. Importantly, we could not use a column data source in the plot to enable display of more data upon hover. The moral of the story is that you should tidy your data, but you should think carefully about in what way you want your data to be tidy.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Making a data frame wide</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/pivoting.html#computing-environment",
    "href": "lessons/eda/wrangling/pivoting.html#computing-environment",
    "title": "16  Making a data frame wide",
    "section": "16.5 Computing environment",
    "text": "16.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,pandas,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\npandas    : 2.3.2\nbokeh     : 3.8.0\niqplot    : 0.3.7\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Making a data frame wide</span>"
    ]
  },
  {
    "objectID": "lessons/eda/eda_lesson_exercise.html",
    "href": "lessons/eda/eda_lesson_exercise.html",
    "title": "17  EDA lesson exercises",
    "section": "",
    "text": "Exercise 1\n| Download notebook\nWhat is split-apply-combine and why is it important that a data set is tidy when doing split-apply-combine operations?",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>EDA lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/eda/eda_lesson_exercise.html#exercise-2",
    "href": "lessons/eda/eda_lesson_exercise.html#exercise-2",
    "title": "17  EDA lesson exercises",
    "section": "Exercise 2",
    "text": "Exercise 2\nWhat is the difference between joining and concatenating data frames?",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>EDA lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/eda/eda_lesson_exercise.html#exercise-3",
    "href": "lessons/eda/eda_lesson_exercise.html#exercise-3",
    "title": "17  EDA lesson exercises",
    "section": "Exercise 3",
    "text": "Exercise 3\nDescribe the difference between categorical and quantitative variables. How are they fundamentally different in the way we plot them?",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>EDA lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/eda/eda_lesson_exercise.html#exercise-4",
    "href": "lessons/eda/eda_lesson_exercise.html#exercise-4",
    "title": "17  EDA lesson exercises",
    "section": "Exercise 4",
    "text": "Exercise 4\nGive pros and cons for using a histogram for display of repeated measurements. Then give pros and cons for using an ECDF.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>EDA lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/eda/eda_lesson_exercise.html#exercise-5",
    "href": "lessons/eda/eda_lesson_exercise.html#exercise-5",
    "title": "17  EDA lesson exercises",
    "section": "Exercise 5",
    "text": "Exercise 5\nWrite down any questions or points of confusion that you have.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>EDA lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_and_sampling.html",
    "href": "lessons/probability_and_sampling/probability_and_sampling.html",
    "title": "Probability: The foundation for generative modeling",
    "section": "",
    "text": "Any data set we encounter was generated by some process, usually a process that involved the ingenuity, blood, sweat, and tears of an experimenter. It we want to learn something more general about nature from acquired data, we need to have a model for the data generation process. Rob Phillips said it beautifully in his book Physical Biology of the Cell, “Quantitative data demand quantitative models.” We call models that describe the process of generating data generative models.\nWe will see in the following lessons that building generative models requires the mathematical machinery of probability and we model data generation with generative probability distributions.\nWhen we perform an experiment and obtain data, we are sampling out of the generative distribution. The true generative distribution is unknown, but by sampling out of it, we gain insights about the generative process. For example, if I measure the heights of a collection of humans, I learn something about the generative distribution just by investigating the samples out of it (the measured data).\nSimilarly, we can learn a lot about probability distributions, including model generative distributions, by sampling out of them directly using random number generation. In this section, we will also learn about the techniques for doing so.\nWe will proceed with a lack of formality, but will nonetheless give useful working definitions of probability and aspects thereof with an eye for putting them to use for modeling and interpreting data.",
    "crumbs": [
      "Probability: The foundation for generative modeling"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html",
    "href": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html",
    "title": "18  Probability: definitions and interpretations",
    "section": "",
    "text": "18.1 Interpretations of probability\nI will be a little formal1 for a moment here as we construct this mathematical notion of probability. First, we need to define the world of possibilities. We denote by \\(\\Omega\\) a sample space, which is the set of all outcomes we could observe in a given experiment. We define an event \\(A\\) to be a subset of \\(\\Omega\\) (\\(A\\subseteq\\Omega\\)). Two events, \\(A_i\\) and \\(A_j\\) are disjoint, also called mutually exclusive, if \\(A_i \\cap A_j = \\emptyset\\). That is to say that two events are disjoint if they do not overlap at all in the sample space; they do not share any outcomes. So, in common terms, the sample space \\(\\Omega\\) contains all possible outcomes of an experiment. An event \\(A\\) is a given outcome or set of outcomes. Two events are disjoint if they are totally different from each other.\nWe define the probability of event \\(A\\) to be \\(P(A)\\), where \\(P\\) is a probability function. It maps the event \\(A\\) to a real number between zero and one. In order to be a probability, the function \\(P\\) must satisfy the following axioms.\nPutting together these axioms, we see that probability consists of positive real numbers that are distributed among the events of a sample space. The sum total of these real numbers over all of the sample space is one. So, a probability function and a sample space go hand-in-hand. For many of our applications, the sample space consists of set of numbers like the real numbers, integers, and subsets of real numbers and integers.\nBefore we go on to talk more about probability, it will help to be thinking about how we can apply it to understand measured data. To do that, we need to think about how probability is interpreted. Note that these are interpretations of probability, not definitions. We have already defined probability, and both of the two dominant interpretations below are valid.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability: definitions and interpretations</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#interpretations-of-probability",
    "href": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#interpretations-of-probability",
    "title": "18  Probability: definitions and interpretations",
    "section": "",
    "text": "18.1.1 Frequentist probability.\nIn the frequentist interpretation of probability, the probability \\(P(A)\\) represents a long-run frequency over a large number of identical repetitions of an experiment. These repetitions can be, and often are, hypothetical. The event \\(A\\) is restricted to propositions about random variables, quantities that can very meaningfully from experiment to experiment.2 So in the frequentist view, we are using probability to understand how the results of an experiment might vary from repetition to repetition.\n\n\n18.1.2 Bayesian probability.\nHere, \\(P(A)\\) is interpreted to directly represent the degree of belief, or plausibility, about \\(A\\). So, \\(A\\) can be any logical proposition, not just a random variable.\nYou may have heard about a split, or even a fight, between people who use Bayesian and frequentist interpretations of probability applied to statistical inference. There is no need for a fight. The two ways of approaching statistical inference differ in their interpretation of probability, the tool we use to quantify uncertainty. Both are valid.\nIn my opinion, the Bayesian interpretation of probability is more intuitive to apply to scientific inference. It always starts with a simple probabilistic expression and proceeds to quantify plausibility. It is conceptually cleaner to me, since we can talk about plausibility of anything, including parameter values. In other words, Bayesian probability serves to quantify our own knowledge, or degree of certainty, about a hypothesis or parameter value. Conversely, in frequentist statistical inference, the parameter values are fixed (they are not random variables; they cannot vary meaningfully from experiment to experiment), and we can only study how repeated experiments will convert the real parameter value to an observation.\nThat is my opinion, and I view fights over such things counterproductive. Frequentist methods are also very useful and powerful, and in this class, we will almost exclusively use them. Next term, we will use almost exclusively Bayesian methods.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability: definitions and interpretations</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#the-sum-rule-the-product-rule-and-conditional-probability",
    "href": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#the-sum-rule-the-product-rule-and-conditional-probability",
    "title": "18  Probability: definitions and interpretations",
    "section": "18.2 The sum rule, the product rule, and conditional probability",
    "text": "18.2 The sum rule, the product rule, and conditional probability\nThe sum rule, which may be derived from the axioms defining probability, says that the probability of all events must add to unity. Let \\(A^c\\) be all events except \\(A\\), called the complement of \\(A\\). Then, the sum rule states that\n\\[\\begin{aligned}\n  P(A) + P(A^c) = 1.\\end{aligned}\\]\nNow, let’s say that we are interested in event \\(A\\) happening given that event \\(B\\) happened. So, \\(A\\) is conditional on \\(B\\). We denote this conditional probability as \\({P(A\\mid B)}\\). Given this notion of conditional probability, we can write the sum rule as\n\\[\\begin{aligned}\n\\text{(sum rule)} \\qquad P(A\\mid B) + P(A^c \\mid B) = 1,\n\\end{aligned}\\]\nfor any \\(B\\).\nThe product rule states that\n\\[\\begin{aligned}\n  P(A, B) = P(A\\mid B)\\, P(B),\\end{aligned}\\]\nwhere \\(P(A,B)\\) is the probability of both \\(A\\) and \\(B\\) happening. (It could be written as \\(P(A\\cap B)\\).) The product rule is also referred to as the definition of conditional probability. It can similarly be expanded as we did with the sum rule.\n\\[\\begin{aligned}\n\\text{(product rule)} \\qquad P(A, B\\mid C) = P(A\\mid B, C)\\, P(B \\mid C),\n\\end{aligned}\\]\nfor any \\(C\\).",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability: definitions and interpretations</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#bayess-theorem",
    "href": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#bayess-theorem",
    "title": "18  Probability: definitions and interpretations",
    "section": "18.3 Bayes’s Theorem",
    "text": "18.3 Bayes’s Theorem\nNote that because “and” is commutative, \\(P(A, B) = P(B, A)\\). We apply the product rule to both sides of this seemingly trivial equality.\n\\[\\begin{aligned}\nP(A \\mid B)\\, P(B) =  P(A, B)\n= P(B,A) = P(B \\mid A)\\, P(A).\n\\end{aligned}\\]\nIf we take the terms at the beginning and end of this equality and rearrange, we get\n\\[\\begin{aligned}\n\\text{(Bayes's theorem)} \\qquad  P(A \\mid B) = \\frac{P(B \\mid A)\\, P(A)}{P(B)}.\n\\end{aligned}\\]\nThis result is called Bayes’s theorem. This result holds for probability, regardless of how it is interpreted, frequentist, Bayesian, or otherwise.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability: definitions and interpretations</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#marginalization",
    "href": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#marginalization",
    "title": "18  Probability: definitions and interpretations",
    "section": "18.4 Marginalization",
    "text": "18.4 Marginalization\nLet \\(\\{A_i\\}\\) be a set of outcomes indexed by \\(i\\). Then,\n\\[\\begin{aligned}\n\\begin{aligned}\n1 &= P(A_j\\mid B) + P(A_j^c \\mid B) \\nonumber \\\\\n&= P(A_j\\mid B) + \\sum_{i\\ne j}P(A_i\\mid B) \\nonumber \\\\\n&= \\sum_iP(A_i\\mid B).\\end{aligned}\n\\end{aligned}\\]\nNow, Bayes’s theorem gives us an expression for \\(P(A_i\\mid B)\\), so we can compute the sum.\n\\[\\begin{aligned}\n\\begin{aligned}\n\\sum_iP(A_i\\mid B) &= \\sum_i\\frac{P(B \\mid A_i)\\, P(A_i)}{P(B)} \\nonumber \\\\\n&= \\frac{1}{P(B)}\\sum_i P(B \\mid A_i)\\, P(A_i) \\nonumber \\\\\n&= 1.\n\\end{aligned}\n\\end{aligned}\\]\nTherefore, we have\n\\[\\begin{aligned}\nP(B) = \\sum_i P(B \\mid A_i)\\, P(A_i).\\end{aligned}\\]\nUsing the definition of conditional probability, we also have\n\\[\\begin{aligned}\nP(B) = \\sum_i P(B,A_i)\n\\end{aligned}\\]\nThis process of eliminating a variable (in this case the \\(A_i\\)'s) in the joint distribution by summing is called marginalization.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability: definitions and interpretations</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#footnotes",
    "href": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#footnotes",
    "title": "18  Probability: definitions and interpretations",
    "section": "",
    "text": "But not too formal. For example, we are not discussing \\(\\sigma\\) algebras, measurability, etc.↩︎\nMore formally, a random variable transforms the possible outcomes of an experiment to real numbers.↩︎",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability: definitions and interpretations</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_distributions.html",
    "href": "lessons/probability_and_sampling/probability_distributions.html",
    "title": "19  Probability distributions",
    "section": "",
    "text": "19.1 Joint and conditional distributions and Bayes’s theorem for PDFs\nSo far we have talked about probability of events, and we have in mind measurements, parameter values and hypotheses as the events. We have a bit of a problem, though, if the sample space consists of real numbers, which we often encounter in our experiments and modeling. The probability of getting a single real value is identically zero. This is my motivation for introducing probability distributions, but the concept is more general and has much more utility than just dealing with sample spaces containing real numbers. Importantly, probability distributions provide the link between outcomes in the sample space to probability. Probability distributions describe both discrete quantities (like integers) and continuous quantities (like real numbers).\nThough we cannot assign a nonzero the probability for an outcome from a sample space containing all of the real numbers, we can assign a probability that the outcome is less than some real number. Notationally, we write this as\n\\[\\begin{aligned}\nP(\\text{having outcome that is}\\le y) = F(y).\n\\end{aligned} \\tag{19.1}\\]\nThe function \\(F(y)\\), which returns a probability, is called a cumulative distribution function (CDF), or just distribution function. It contains all of the information we need to know about how probability is assigned to \\(y\\). A CDF for a Normal distribution is shown in the left panel of the figure below.\nRelated to the CDF for a continuous quantity is the probability density function, or PDF. The PDF is given by the derivative of the CDF,\n\\[\\begin{aligned}\nf(y) = \\frac{\\mathrm{d}F(y)}{\\mathrm{d}y}.\n\\end{aligned} \\tag{19.2}\\]\nNote that \\(f(y)\\) is not the probability of outcome \\(y\\). Rather, the probability that of outcome \\(y\\) lying between \\(y_0\\) and \\(y_1\\) is\n\\[\\begin{aligned}\nP(y_0\\le y \\le y_1) = F(y_1) - F(y_0) = \\int_{y_0}^{y_1}\\mathrm{d}y\\,f(y).\n\\end{aligned} \\tag{19.3}\\]\nNote that with this definition of the probability density function, satisfaction of the axiom that all probabilities sum to zero (equivalently stated as \\(F(y\\to\\infty) = 1\\)) necessitates that the probability density function is normalized. That is,\n\\[\\begin{aligned}\n\\int_{-\\infty}^\\infty \\mathrm{d}t\\, f(y) = 1.\n\\end{aligned} \\tag{19.4}\\]\nConversely, for a discrete quantity, we have a probability mass function, or PMF,\n\\[\\begin{aligned}\nf(y) = P(y).\n\\end{aligned} \\tag{19.5}\\]\nThe PMF is a probability, unlike the PDF. An example of a CDF and a PMF for a discrete distribution are shown in the figure below. In this example, \\(n\\) is the outcome of the roll of a fair die (\\(n\\in\\{1,2,3,4,5,6\\}\\)).\nWe have defined a PDF as \\(f(x)\\), that is, describing a single variable \\(x\\). We can have joint distributions with a PDF \\(f(x, y)\\).\nWe may also have conditional distributions that have PDF \\(f(x\\mid y)\\). This is interpreted similarly to conditional probabilities we have already seen. \\(f(x\\mid y)\\) is the probability density function for \\(x\\), given \\(y\\). As similar relation between joint and conditional PDFs holds as in the case of joint and conditional probabilities.\n\\[\\begin{aligned}\nf(x\\mid y) = \\frac{f(x,y)}{f(y)}.\n\\end{aligned} \\tag{19.6}\\]\nThat this holds is not at all obvious. One immediate issue is that we are conditioning on an event \\(y\\) that has zero probability. We will not carefully derive why this holds, but state it without proof.\nAs a consequence, Bayes's theorem also holds for PDFs, as it does for probabilities.1\n\\[\\begin{aligned}\nf(\\theta\\mid y) = \\frac{f(y\\mid \\theta)\\,f(\\theta)}{f(y)}.\n\\end{aligned} \\tag{19.7}\\]\nNotationally in this course, we will use \\(f\\) to describe a PDF or PMF of a random variable and \\(g\\) to describe the PMF or PDF of a parameter or other logical conjecture that is not measured data or a random variable. For example, \\(f(y \\mid \\theta)\\) is the PDF for a continuous measured quantity and \\(g(\\theta)\\) is the PDF for a parameter value. In this notation, Bayes’s theorem is\n\\[\\begin{aligned}\ng(\\theta\\mid y) = \\frac{f(y\\mid \\theta)\\,g(\\theta)}{f(y)}.\n\\end{aligned} \\tag{19.8}\\]\nFinally, we can marginalize probability distribution functions to get marginalized PDFs.\n\\[\\begin{aligned}\nf(x) = \\int \\mathrm{d}y\\,f(x,y) = \\int\\mathrm{d}y\\,f(x\\mid y)\\,f(y).\n\\end{aligned} \\tag{19.9}\\]\nIn the case of a discrete distribution, we can compute marginal a marginal PMF.\n\\[\\begin{aligned}\nf(x) = \\sum_i\\,f(x,y_i) = \\sum_i f(x\\mid y_i)\\,f(y_i).\n\\end{aligned} \\tag{19.10}\\]",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_distributions.html#change-of-variables-formula-for-continuous-distributions",
    "href": "lessons/probability_and_sampling/probability_distributions.html#change-of-variables-formula-for-continuous-distributions",
    "title": "19  Probability distributions",
    "section": "19.2 Change of variables formula for continuous distributions",
    "text": "19.2 Change of variables formula for continuous distributions\nAs a last note about probability distributions, I discuss the change of variables formula. Say I have a continuous probability distribution with PDF \\(f_X(x)\\). I have included the subscript \\(X\\) to denote that this is a PDF describing the variable \\(X\\). If I wish to change variables to instead get a continuous distribution in \\(y=y(x)\\), or \\(f_Y(y) = f_Y(y(x))\\), how do I get \\(f_Y\\)? We must enforce that the distributions be normalized;\n\\[\\begin{align}\n\\int \\mathrm{d}x\\, f_X(x) =  \\int \\mathrm{d}y\\, f_Y(y) = 1.\n\\end{align} \\tag{19.11}\\]\nThus, we must have \\(\\left|\\mathrm{d}y\\,f_Y(y)\\right| = \\left|\\mathrm{d}x\\,f_x(x)\\right|\\). Equivalently, we have\n\\[\\begin{align}\nf_Y(y) = \\left|\\frac{\\mathrm{d} x}{\\mathrm{d}y}\\right|\\,f_X(x).\n\\end{align} \\tag{19.12}\\]\nThis is the change of variables formula.\n\n19.2.1 Generalization to multiple dimensions\nGenerically, if we have a set of variables \\(\\mathbf{x}\\) that are transformed into a new set of parameters \\(\\mathbf{y} = \\mathbf{y}(\\mathbf{x})\\), then\n\\[\\begin{align}\nf_Y(y) = \\left|\\frac{\\partial(x_1, x_2, \\ldots)}{\\partial(y_1, y_2, \\ldots)}\\right|f_X(x),\n\\end{align} \\tag{19.13}\\]\nwhere the first factor on the right hand side is the Jacobian, which is the absolute value of the determinant of the Jacobi matrix,\n\\[\\begin{aligned}\n\\begin{align}\n\\frac{\\partial(x_1, x_2, \\ldots)}{\\partial(y_1, y_2, \\ldots)} = \\begin{pmatrix}\n\\frac{\\partial x_1}{\\partial y_1} & \\frac{\\partial x_1}{\\partial y_2} & \\cdots \\\\\n\\frac{\\partial x_2}{\\partial y_1} & \\frac{\\partial x_2}{\\partial y_2} & \\cdots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{pmatrix}   .\n\\end{align}\n\\end{aligned} \\tag{19.14}\\]\n\n\n19.2.2 An example of change of variables\nImagine I have a random variable that is Exponentially distributed, such that\n\\[\\begin{align}\nf_X(x) = \\beta \\, \\mathrm{e}^{-\\beta x}.\n\\end{align} \\tag{19.15}\\]\nNow saw that I want to rescale \\(x\\) so that I instead get a distribution in \\(y = a x\\). Here, \\(g(x) = a x\\) and \\(g^{-1}(y) = y/a\\). So, we have\n\\[\\begin{align}\nf_Y(y) = \\left|\\frac{\\mathrm{d}}{\\mathrm{d}y}\\,\\frac{y}{a}\\right|\\,f_X(y/a)\n= \\frac{1}{a}\\,\\beta\\,\\mathrm{e}^{-\\beta y / a}.\n\\end{align} \\tag{19.16}\\]\nThe distribution is again Exponential, but the rate has been rescaled, \\(\\beta \\to \\beta/a\\). This makes sense; we have rescaled \\(x\\) by our change of variables, so the rate should be rescaled accordingly.\n\n\n19.2.3 Another example of change of variables: the Log-Normal distribution\nNow imagine I have a random variable that is Normally distributed and I wish to determine how \\(y = \\mathrm{e}^{x}\\) is distributed.\n\\[\\begin{align}\nf_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\,\\mathrm{e}^{-(x-\\mu)^2/2\\sigma^2}.\n\\end{align} \\tag{19.17}\\]\nHere, \\(g(x) = \\mathrm{e}^x\\) and \\(g^{-1}(y) = \\ln y\\). Again applying the change of variables formula,\n\\[\\begin{align}\nf_Y(y) = \\left|\\frac{\\mathrm{d}\\,\\ln y}{\\mathrm{d}y}\\right|\\,f_X(\\ln y)\n= \\frac{1}{y\\sqrt{2\\pi \\sigma^2}}\\,\\mathrm{e}^{-(\\ln y-\\mu)^2/2\\sigma^2},\n\\end{align} \\tag{19.18}\\]\nwhich is indeed the PDF of the Log-Normal distribution.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_distributions.html#sec-probability-distribution-stories",
    "href": "lessons/probability_and_sampling/probability_distributions.html#sec-probability-distribution-stories",
    "title": "19  Probability distributions",
    "section": "19.3 Probability distributions as stories",
    "text": "19.3 Probability distributions as stories\nThe building blocks of statistical models are probability distributions. Specifying a model amounts to choosing probability distributions that describe the process of data generation. In some cases, you need to derive the distribution based on specific considerations or your experiment or model (or even numerically compute it when it cannot be written in closed form). In many practical cases, though, your model is composed of standard probability distributions. These distributions have stories associated with them. That is, the mathematical particulars of the distribution follow from a description of a data generation process. For example, the story behind the Bernoulli distribution is as follows. The outcome of a coin flip is Bernoulli distributed. So in building models, if your data generation process matches the story of a distribution, you know that this is the distribution to choose for your model.\nThe Distribution Explorer is a useful tool to connect distributions to stories and obtain their PDFs/PMFs and CDFs, as well as syntax for usage in popular software packages. I encourage you to explore the Explorer!",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_distributions.html#footnotes",
    "href": "lessons/probability_and_sampling/probability_distributions.html#footnotes",
    "title": "19  Probability distributions",
    "section": "",
    "text": "This is very subtle. Jaynes’s book, Probability: The Logic of Science, Cambridge University Press, 2003, for more one these subtleties.↩︎",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/random_number_generation.html",
    "href": "lessons/probability_and_sampling/random_number_generation.html",
    "title": "20  Random number generation",
    "section": "",
    "text": "| Download notebook\n\nRandom number generation (RNG) is the process by which a sequence of random numbers may be drawn. When using a computer to draw the random numbers, the numbers are not completely random. The notion of “completely random” is nonsensical because of the infinitude of numbers. Random numbers must be drawn from some probability distribution. Furthermore, in most computer applications, the random numbers are actually pseudorandom. They depend entirely on an input seed and are then generated by a deterministic algorithm from that seed.\nSo what do (pseudo)random number generators do? RNGs are capable of (approximately) drawing integers from a Discrete Uniform distribution. For example, NumPy’s default built-in generator, the PCG64 generator, generates 128 bit numbers, allowing for \\(2^{128}\\), or about \\(10^{38}\\), possible integers. Importantly, each draw of of a random integer is (approximately) independent of all others.\nIn practice, the drawn integers are converted to floating-point numbers (since a double floating-point number has far less than 128 bits) on the interval [0, 1) by dividing a generated random integer by \\(2^{138}\\). Effectively, then, the random number generators provide draws out of a Uniform distribution on the interval [0, 1).\nTo convert from random numbers on a Uniform distribution to random numbers from a nonuniform distribution, we need a transform. For many named distributions convenient transforms exist. For example, the Box-Muller transform is often used to get random draws from a Normal distribution. In the absence of a clever transform, we can use a distribution’s quantile function, also known as a percent-point function, which is the inverse CDF, \\(F^{-1}(y)\\). For example, the quantile function for the Exponential distribution is\n\\[\n\\begin{aligned}\nF^{-1}(p) = -\\beta^{-1}\\,\\ln(1-p),\n\\end{aligned} \\tag{20.1}\\]\nwhere \\(p\\) is the value of the CDF, ranging from zero to one. We first draw \\(p\\) out of a Uniform distribution on [0, 1), and then compute \\(F^{-1}(p)\\) to get a draw from an Exponential distribution. A graphical illustration of using a quantile function to draw 50 random numbers out of Gamma(5, 2) is shown below.\n\n\nCode\nimport numpy as np\nimport scipy.stats as st\n\nimport bokeh.io\nimport bokeh.plotting\nbokeh.io.output_notebook(hide_banner=True)\n\nrng = np.random.default_rng(seed=12341234)\nalpha = 5\nbeta = 2\ny = np.linspace(0, 8, 400)\ncdf = st.gamma.cdf(y, alpha, loc=0, scale=1 / beta)\n\nudraws = rng.uniform(size=50)\n\np = bokeh.plotting.figure(\n    width=300,\n    height=200,\n    x_axis_label=\"y\",\n    y_axis_label=\"F(y; 2, 5)\",\n    x_range=[0, 8],\n    y_range=[0, 1],\n)\np.xgrid.grid_line_color = None\np.ygrid.grid_line_color = None\n\np.line(y, cdf, line_width=2)\n\nfor u in udraws:\n    x_vals = [0] + [st.gamma.ppf(u, alpha, loc=0, scale=1 / beta)] * 2\n    y_vals = [u, u, 0]\n    p.line(x_vals, y_vals, color=\"gray\", line_width=0.5)\n\np.scatter(np.zeros_like(udraws), udraws, marker=\"x\", color=\"black\", line_width=0.5, size=7)\np.scatter(\n    st.gamma.ppf(udraws, alpha, loc=0, scale=1 / beta),\n    np.zeros_like(udraws),\n    line_width=0.5,\n    fill_color=None,\n    line_color=\"black\",\n)\n\np.renderers[0].level = \"overlay\"\np.renderers[-1].level = \"overlay\"\np.renderers[-2].level = \"overlay\"\n\nbokeh.io.show(p)\n\n\n\n  \n\n\n\nEach draw from a Uniform distribution, marked by an × on the vertical axis, is converted to a draw from a Gamma distribution, marked by ○ on the horizontal axis, by computing the inverse CDF. Because the draws of the random integers from which the draws from a Uniform distribution on \\([0, 1)\\) are independent, we get independent draws from the Gamma distribution.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Random number generation</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html",
    "title": "21  Random number generation using Numpy",
    "section": "",
    "text": "21.1 Uniform random numbers\n| Download notebook\nA good portion of the random number generation functionality you will need is in the np.random module. It allows for draws of independent random numbers for many convenient named distributions. The scipy.stats module offers even more distributions, but for most applications, Numpy’s generators suffice and are typically faster than using Scipy, which has more overhead.\nLet’s start by generating random numbers from a Uniform distribution.\nnp.random.uniform(low=0, high=1, size=10)\n\narray([0.40312242, 0.04051189, 0.03851483, 0.41999136, 0.99686689,\n       0.86482983, 0.57146661, 0.71342451, 0.20395884, 0.32728968])\nThe function uniform() in the np.random module generates random numbers on the interval [low, high) from a Uniform distribution. The size keyword argument is how many random numbers you wish to generate, and is a keyword argument in all Numpy’s functions to draw from specific distributions. The random numbers are returned as a Numpy array.\nWe can check to make sure it is appropriately drawing random numbers out of the Uniform distribution by plotting the cumulative distribution function. We’ll generate 1,000 random numbers and plot them along with the CDF of a Uniform distribution.\n# Generate random numbers\nx = np.random.uniform(low=0, high=1, size=1000)\n\n# Plot the ECDF of randomly generated numbers\np = iqplot.ecdf(x, marker_kwargs={\"fill_color\": None},)\n\np.line(\n    x=[0, 1], y=[0, 1], line_width=2, line_color=\"orange\",\n)\n\nbokeh.io.show(p)\nSo, it looks like our random number generator is doing a good job.\nGenerating random numbers on the uniform interval is one of the most commonly used RNG applications. For example, you can simulate flipping a biased (unfair) coin by drawing from a Uniform distribution and then asking if the random number if less than the bias.\n# Generate 20 random numbers on uniform interval\nx = np.random.uniform(low=0, high=1, size=20)\n\n# Make the coin flips (&lt; 0.7 means we have a 70% chance of heads)\nheads = x &lt; 0.7\n\n# Show which were heads, and count the number of heads\nprint(heads)\nprint(\"\\nThere were\", np.sum(heads), \"heads.\")\n\n[ True  True False  True  True  True  True False  True  True  True  True\n  True  True  True False False False  True  True]\n\nThere were 15 heads.\nOf course, you could also do this by drawing out of a Binomial distribution.\nprint(f\"There were {np.random.binomial(20, 0.7)} heads.\")\n\nThere were 12 heads.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html#sec-choice-of-generator",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html#sec-choice-of-generator",
    "title": "21  Random number generation using Numpy",
    "section": "21.2 Choice of generator",
    "text": "21.2 Choice of generator\nAs of version 1.23 of Numpy, the algorithm under the hood of calls to functions like np.random.uniform() is the Mersenne Twister Algorithm for generating random numbers. It is a very widely used and reliable method for generating random numbers. However, starting with version 1.17, the numpy.random module offers random number generators with better speed and statistical performance, including a 64-bit permuted congruential generator (PCG64). Going forward, the preferred approach to doing random number generation is to first instantiate a generator of your choice, and then use its methods to generate numbers out of probability distributions.\nLet’s set up a PCG64 generator, which is Numpy’s default (though this will soon be updated to the PCG64 DXSM, which works better for massively parallel generation, per Numpy’s documentation).\n\nrng = np.random.default_rng()\n\nNow that we have the generator, we can use it to draw numbers out of distributions. The syntax is the same as before, except rng replaces np.random.\n\nrng.uniform(low=0, high=1, size=20)\n\narray([0.83950893, 0.18995913, 0.7455994 , 0.65977687, 0.50251107,\n       0.3920528 , 0.98069072, 0.43620495, 0.42139065, 0.67577344,\n       0.44188521, 0.42143143, 0.64836288, 0.29192271, 0.17708979,\n       0.92594815, 0.04850722, 0.06206417, 0.09710099, 0.75790207])\n\n\nOr, for the Binomial,\n\nrng.binomial(20, 0.7)\n\n18",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html#sec-seeding-rngs",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html#sec-seeding-rngs",
    "title": "21  Random number generation using Numpy",
    "section": "21.3 Seeding random number generators",
    "text": "21.3 Seeding random number generators\nNow, just to demonstrate that random number generation is deterministic, we will explicitly seed the random number generator (which is usually seeded with a number representing the date/time to avoid repeats) to show that we get the same random numbers.\n\n# Instantiate generator with a seed\nrng = np.random.default_rng(seed=3252)\n\n# Draw random numbers\nrng.uniform(size=10)\n\narray([0.18866535, 0.04418857, 0.02961285, 0.22083971, 0.43341773,\n       0.13166813, 0.42112164, 0.43507845, 0.61380912, 0.30627603])\n\n\nIf we reinstantiate with the same seed, we get the same sequence of random numbers.\n\n# Re-seed the RNG\nrng = np.random.default_rng(seed=3252)\n\n# Draw random numbers\nrng.uniform(size=10)\n\narray([0.18866535, 0.04418857, 0.02961285, 0.22083971, 0.43341773,\n       0.13166813, 0.42112164, 0.43507845, 0.61380912, 0.30627603])\n\n\nThe random number sequence is exactly the same. If we choose a different seed, we get totally different random numbers.\n\nrng = np.random.default_rng(seed=3253)\nrng.uniform(size=10)\n\narray([0.31390226, 0.73012457, 0.05800998, 0.01557021, 0.29825701,\n       0.10106784, 0.06329107, 0.58614237, 0.52023168, 0.52779988])\n\n\nIf you are writing tests, it is often useful to seed the random number generator to get reproducible results. Otherwise, it is best to use the default seed, based on the date and time, so that you get a new set of random numbers in your applications each time do computations.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-dists",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-dists",
    "title": "21  Random number generation using Numpy",
    "section": "21.4 Drawing random numbers out of other distributions",
    "text": "21.4 Drawing random numbers out of other distributions\nSay we wanted to draw random samples from a Normal distribution with mean μ and standard deviation σ.\n\n# Set parameters\nmu = 10\nsigma = 1\n\n# Draw 100000 random samples\nx = rng.normal(mu, sigma, size=100000)\n\n# Plot the histogram\np = iqplot.histogram(x, rug=False, density=True, y_axis_label=\"approximate PDF\",)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nIt looks Normal, but, again, comparing the resulting ECDF is a better way to look at this. We’ll check out the ECDF with 1000 samples so as not to choke the browser with the display. I will also make use of the theoretical CDF for the Normal distribution available from the scipy.stats module.\n\n# Compute theoretical CDF\nx_theor = np.linspace(6, 14, 400)\ny_theor = st.norm.cdf(x_theor, mu, sigma)\n\n# Plot the ECDF of randomly generated numbers\np = iqplot.ecdf(x, marker_kwargs={\"fill_color\": None},)\n\np.line(\n    x=x_theor, y=y_theor, line_width=2, line_color=\"orange\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nYup, right on!",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-choice",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-choice",
    "title": "21  Random number generation using Numpy",
    "section": "21.5 Choosing elements from an array",
    "text": "21.5 Choosing elements from an array\nIt is often useful to randomly choose elements from an existing array. (Actually, this is probably the functionality we will use the most, since it is used in bootstrapping.) The rng.choice() function does this. You equivalently could do this using rng.integers(), where the integers represent indices in the array, except rng.choice() has a great keyword argument, replace, which allows random draws with or without replacement. For example, say you had 50 samples that you wanted to send to a facility for analysis, but you can only afford to send 20. If we used rng.integers(), we might have a problem.\n\nrng = np.random.default_rng(seed=126969234)\nrng.integers(0, 51, size=20)\n\narray([12, 31, 47, 26,  3,  5, 46, 49, 26, 38, 24, 17, 46, 26,  6, 17, 35,\n        4, 13, 29])\n\n\nSample 17 was selected twice and sample 26 was selected thrice. This is not unexpected. We can use rng.choice() instead.\n\nrng.choice(np.arange(51), size=20, replace=False)\n\narray([27, 34,  0, 46,  2, 48, 35, 50, 40, 12, 28, 19, 37, 38, 11, 23, 45,\n       15, 29, 32])\n\n\nNow, because we chose replace=False, we do not get any repeats.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-shuffle",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-shuffle",
    "title": "21  Random number generation using Numpy",
    "section": "21.6 Shuffling an array",
    "text": "21.6 Shuffling an array\nSimilarly, the rng.permutation() function is useful. It takes the entries in an array and shuffles them! Let’s shuffle a deck of cards.\n\nrng.permutation(np.arange(53))\n\narray([12, 18,  2, 34, 27, 10,  0, 30, 49,  7,  5, 35, 11, 23, 37, 17,  4,\n       44, 15, 28, 14,  8, 40, 21, 39, 36, 46, 24, 33, 20, 22,  1, 41, 45,\n       50, 26, 16, 42, 52,  3,  9, 48, 38, 25, 43, 51, 19, 47, 32,  6, 13,\n       29, 31])",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-when-do-we-need",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-when-do-we-need",
    "title": "21  Random number generation using Numpy",
    "section": "21.7 When do we need RNG?",
    "text": "21.7 When do we need RNG?\nAnswer: VERY OFTEN! We will use random number generator extensively as we explore probability distributions.\nIn many ways, probability is the language of biology. Molecular processes have energetics that are comparable to the thermal energy, which means they are always influenced by random thermal forces. The processes of the central dogma, including DNA replication, are no exceptions. This gives rise to random mutations, which are central to understanding how evolution works. And of course neuronal firing is also probabilistic. If we want to understand how biology works, it is often useful to use random number generators to model the processes.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html#computing-environment",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html#computing-environment",
    "title": "21  Random number generation using Numpy",
    "section": "21.8 Computing environment",
    "text": "21.8 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\nscipy     : 1.15.2\nbokeh     : 3.6.2\niqplot    : 0.3.7\njupyterlab: 4.3.6",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/luria_delbruck.html",
    "href": "lessons/probability_and_sampling/luria_delbruck.html",
    "title": "22  Simulating the Luria-Delbrück distribution",
    "section": "",
    "text": "22.1 Simulating data generation\n| Download notebook\nWe have seen that we can draw random numbers out of distributions for which we have a convenient transform or access to a quantile function. We have also seen that we can derive a probability mass function or probability density function from the story of a distribution, and those PMFs/PDFs give us complete information about the distribution. Sometimes, though, it is difficult or impossible to derive a PMF or PDF from a story. In other situations, we may know the PMF or PDF buy cannot derive a transform nor an easily evaluated quantile function. In these cases, we can simulate the story of the distribution using random number generation.\nIn this section, as an example, we will learn how to simulate the Luria-Delbrück distribution.\nThe Luria-Delbrück distribution (also known as a jackpot distribution) is a classic example from the biological sciences of a distribution whose story is easy to state, but whose PMF is very difficult to write down. (It was written down by Lea and Colson, but the expression fills a couple pages and is not terribly usable.)\nWhen mutations occur in nature, they are often deleterious to the organism. However, mutations are a critical part of the genetic heritage of living organisms, arising in every type of organism and allowing life to evolve and adapt to new environments. In 1943, the question of how microorganisms acquire mutations was described in a famous paper by Salvador Luria and Max Delbrück (S. E. Luria and M. Delbrück, Genetics, 28, 491–511, 1943). At the time, there were two prominent theories of genetic inheritance, the “random mutation hypothesis,” in which mutations arose randomly in the absence of an environmental cue, and the “adaptive immunity hypothesis, in which mutations occur as an adaptive response to an environmental stimulus. See the figure below.\nTo test these two hypotheses, Luria and Delbrück grew many parallel cultures of bacteria and then plated each culture on agar containing phages (which infect and kill nearly all of the bacteria). Although most bacteria are unable to survive in the presence of phages, often mutations could enable a few survivors to give rise to resistant mutant colonies. If the adaptive immunity hypothesis is correct, mutations occur only after bacteria come in contact with phages, thus only after plating the bacteria on phage-agar plates. Under the random mutation hypothesis, some bacteria already have the immunity before being exposed.\nThe story of the Luria-Delbrück distribution arises from the random mutation hypothesis. Start with a single cell that cannot survive being exposed to phage. When it divides, there is a probability \\(\\theta\\) one of the daughter cells gets a mutation that will impart survivability. This is true for each division. Once a mutation that imparts survival is obtained, it is passed to all subsequent generations. The number \\(n\\) of survivors in \\(N\\) individual cells exposed to the stress is distributed according to the Luria-Delbrück distribution.\nWe will not attempt to write down the PMF, but will instead sample out of the Luria-Delbrück distribution by directly simulating its story. To do the simulation, we note that if we have a population of \\(M\\) unmutated cells, we will have \\(M\\) cell divisions. The number of cell divisions that result in a mutation is Binomially distributed, Binom(M, θ). So, to simulate the a Luria-Delbrück experiment, at each round of cell division, we draw a random number of a Binomial distribution parametrized by the number of cells that have not yet had mutations and \\(\\theta\\). For after the \\(g\\)th set of cell divisions, we have \\(2^g\\) total cells. If we have \\(n_\\mathrm{mut}\\) cells with favorable mutations when we had \\(2^{g-1}\\) cells, then we have \\(2 n_\\mathrm{mut} + n_\\mathrm{mut}^\\mathrm{new}\\), where \\(n_\\mathrm{mut}^\\mathrm{new}\\) is drawn from \\(\\text{Binom}(2^{g-1}-n_\\mathrm{mut}, \\theta)\\), after the \\(g\\)th set of cell divisions. Let’s code it up!\n@numba.njit\ndef draw_random_mutation(n_gen, theta):\n    \"\"\"Draw a sample out of the Luria-Delbruck distribution\n\n    Parameters\n    ----------\n    n_gen : int\n        Number of generations. At the end of the experiment, there are\n        `2**n_gen` cells.\n    theta : float\n        Probability of obtaining a mutation in a single cell division.\n\n    Returns\n    -------\n    output : int\n        Number of cells that will survive stress.\n    \"\"\"\n    # Initialize number of mutants\n    n_mut = 0\n\n    for g in range(n_gen + 1):\n        n_mut = 2 * n_mut + np.random.binomial(2**(g-1) - 2 * n_mut, theta)\n        \n    return n_mut\nThis function draws a single number of survivors. To get a picture of the distribution, we need to make many, many draws, so we write a function to call this function repeatedly to get the samples.\n@numba.njit\ndef sample_random_mutation(n_gen, theta, n_samples=1):\n    \"\"\"Sample out of the Luria-Delbruck distribution\n\n    Parameters\n    ----------\n    n_gen : int\n        Number of generations. At the end of the experiment, there are\n        `2**n_gen` cells.\n    theta : float\n        Probability of obtaining a mutation in a single cell division.\n    n_samples : int\n        Number of samples to draw\n\n    Returns\n    -------\n    output : Numpy array of ints\n        Draws of number of cells that will survive stress.\n    \"\"\"    \n    # Initialize samples\n    samples = np.empty(n_samples)\n    \n    # Draw the samples\n    for i in range(n_samples):\n        samples[i] = draw_random_mutation(n_gen, theta)\n        \n    return samples\nLet’s put it to use! We will draw a million samples for 16 generations with a mutation rate of \\(10^{-5}\\).\nsamples = sample_random_mutation(16, 1e-5, 1_000_000).astype(int)\nLuria and Delbrück knew that if the Fano factor, the ratio of the variance to the mean, was much bigger than one, the adaptive immunity hypothesis (which has a predicted Fano factor of 1—can you explain why?). We can compute the Fano factor of the Luria-Delbrück distribution from the samples.\nprint(\n    \"\"\"\nRandom mutation hypothesis\n--------------------------\nmean:        {mean:.4f}\nvariance:    {var:.4f}\nFano factor: {fano:.4f}\n\"\"\".format(\n        mean=np.mean(samples),\n        var=np.var(samples),\n        fano=np.var(samples) / np.mean(samples),\n    )\n)\n\n\nRandom mutation hypothesis\n--------------------------\nmean:        5.2874\nvariance:    21690.4510\nFano factor: 4102.2952\nWow! Huge variance and bit Fano factor. We can get a feeling for the PMF by making a spike plot (We will avoid an ECDF in this case because we have LOTS of samples and we want to take it easy on the browser.)\n# Print probability of getting zero\nprint(f\"Fraction with zero survivors: {np.sum(samples==0) / len(samples)}\")\n\nbokeh.io.show(\n    iqplot.spike(\n        samples, \n        fraction=True,\n        x_range=[0.5, 2e5],\n        y_range=[1/len(samples), 1],\n        x_axis_type='log', \n        y_axis_type='log', \n        x_axis_label='number of survivors',\n    )\n)\n\nFraction with zero survivors: 0.519538\nWe have a very heavy tail, which decays according to a power law. We also see artifacts due to the discrete nature of the cell divisions, where powers of two are more likely. Using just the powers of two, the apparent power law is approximately \\(P(n) \\sim n^{-1}\\), which is not even normalizable in the limit of an infinite number of plated cells. The fact that we have a finite number of cells keeps the PMF normalizable. A very heavy tail, indeed!",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simulating the Luria-Delbrück distribution</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/luria_delbruck.html#simulating-data-generation",
    "href": "lessons/probability_and_sampling/luria_delbruck.html#simulating-data-generation",
    "title": "22  Simulating the Luria-Delbrück distribution",
    "section": "",
    "text": "Figure 22.1: Luria and Delbrück assessed two competing hypotheses for the role genetic mutations play in evolution by natural selection.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simulating the Luria-Delbrück distribution</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/luria_delbruck.html#computing-environment",
    "href": "lessons/probability_and_sampling/luria_delbruck.html#computing-environment",
    "title": "22  Simulating the Luria-Delbrück distribution",
    "section": "22.2 Computing environment",
    "text": "22.2 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\nbokeh     : 3.7.3\niqplot    : 0.3.7\njupyterlab: 4.4.5",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simulating the Luria-Delbrück distribution</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_and_sampling_lesson_exercise.html",
    "href": "lessons/probability_and_sampling/probability_and_sampling_lesson_exercise.html",
    "title": "23  Probability and sampling lesson exercises",
    "section": "",
    "text": "Exercise 1\n| Download notebook\nWhat is the relationship between a cumulative distribution function (CDF) and a probability density function (PDF)? What is the relationship between as CDF and a probability mass function (PMF)?",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Probability and sampling lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_and_sampling_lesson_exercise.html#exercise-2",
    "href": "lessons/probability_and_sampling/probability_and_sampling_lesson_exercise.html#exercise-2",
    "title": "23  Probability and sampling lesson exercises",
    "section": "Exercise 2",
    "text": "Exercise 2\nHow would you expect each of the following to be distributed?\na) The amount of time between repressor-operator binding events.\nb) The number of times a repressor binds its operator in a given hour.\nc) The amount of time (in total minutes of baseball played) between no-hitters in Major League Baseball.\nd) The number of no-hitters in a Major League Baseball season.\ne) The winning times of the Belmont Stakes.\nTo answer this question, try to match these stories to the stories of named distributions. For those of you not familiar with baseball, a no-hitter is a game in which a team concedes no hits to the opposing team. There have only been a few hundred no-hitters in over 200,000 MLB games. The Belmont Stakes is a major horse race that has been run each year for over 150 years.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Probability and sampling lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_and_sampling_lesson_exercise.html#exercise-3",
    "href": "lessons/probability_and_sampling/probability_and_sampling_lesson_exercise.html#exercise-3",
    "title": "23  Probability and sampling lesson exercises",
    "section": "Exercise 3",
    "text": "Exercise 3\nWrite down any questions or points of confusion that you have.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Probability and sampling lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/nonparametric_inference.html",
    "href": "lessons/nonparametric/nonparametric_inference.html",
    "title": "Nonparametric inference and confidence intervals",
    "section": "",
    "text": "Now that we have a basis in probability theory, we proceed to statistical inference. One of the most widely used concepts in statistical inference is the plug-in principle. In fact, you have probably employed it countless times but didn’t even realize it! In what follows, we will formally define the plug-in principle and the important concept of confidence intervals for our first foray into inference.\nSpecifically, we will learn about how to perform statistical inference when we do not have a generative model in mind. Because there is no generative model, there are no parameters, and, as such, this class of inference problems are referred to as nonparametric. As you may have guessed, we will employ the plug-in principle to use the empirical distribution as an approximation for the unknown true generative distribution.",
    "crumbs": [
      "Nonparametric inference and confidence intervals"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/point_estimates.html",
    "href": "lessons/nonparametric/plugin/point_estimates.html",
    "title": "24  Plug-in estimates",
    "section": "",
    "text": "24.1 The plug-in principle\nWhen we make measurements, we are observing samples out of an underlying, typically unknown, generative distribution. Take as an example measurements of the count of RNA transcripts of a given gene in individual cells. We are likely to observe counts where the probability mass function of the generative distribution are is high and unlikely to observe counts where the PMF of the generative distribution is low.\nIf we have a full understanding of the generative distribution, we have learned how the data were generated, and thereby have an understanding of the physical, chemical, or biological phenomena we are studying. Statistical inference involves deducing properties of these (unknown) generative distributions.\nIn this lecture, we will start with nonparametric inference, which is statistical inference where no model is assumed; conclusions are drawn from the data alone. The approach we will take is heavily inspired by Allen Downey’s wonderful book, Think Stats and from Larry Wasserman’s book All of Statistics.\nLet’s first think about how to get an estimate for a parameter value, given the data. While what we are about to do is general, for now it is useful to have in your mind a concrete example. Imagine we have a data set that is a set of repeated measurements, such as the repeated measurements of lengths of eggs laid by C. elegans worms of a given genotype.\nWe could have a generative model in mind, and we will do this in coming lessons. Instead, we will assume we only know that there is a generative distribution, but nothing about what it may be. Let \\(F(x)\\) be the cumulative distribution function (CDF) for the generative distribution.\nA statistical functional is a functional of the CDF, \\(T(F)\\). A parameter \\(\\theta\\) of a probability distribution can be defined from a functional, \\(\\theta = T(F)\\). For example, the mean, variance, and median are all statistical functionals.\n\\[\\begin{aligned}\n\\begin{aligned}\n    &\\text{mean} \\equiv \\mu = \\int_{-\\infty}^\\infty \\mathrm{d}x\\,x\\,f(x) = \\int_{-\\infty}^\\infty \\mathrm{d}F(x)\\,x, \\\\[1em]\n    &\\text{variance} \\equiv \\sigma^2 = \\int_{-\\infty}^\\infty \\mathrm{d}x\\,(x-\\mu)^2\\,f(x) = \\int_{-\\infty}^\\infty \\mathrm{d}F(x)\\,(x-\\mu)^2, \\\\[1em]\n    &\\text{median} \\equiv m = F^{-1}(1/2).\\end{aligned}\n\\end{aligned}\\]\nNow, say we made a set of \\(n\\) measurements, \\(\\{x_1, x_2, \\ldots x_n\\}\\). You can think of this as a set of C. elegans egg lengths if you want to have an example in your mind. We define the empirical cumulative distribution function, \\(\\hat{F}(x)\\) from our data as\n\\[\\begin{aligned}\n   \\hat{F}(x) = \\frac{1}{n}\\sum_{i=1}^n I(x_i \\le x),\n\\end{aligned}\\]\nwith\n\\[\\begin{aligned}\n\\begin{aligned}\n   I(x_i \\le x) = \\left\\{\n   \\begin{array}{ccl}\n       1 && x_i \\le x \\\\[0.5em]\n       0 && x_i &gt; x.\n   \\end{array}\n   \\right.\n\\end{aligned}\n\\end{aligned}\\]\nWe have already seen this form of the ECDF when we were studying exploratory data analysis. Remember that the probability density function (PDF), \\(f(x)\\), is related to the CDF by\n\\[\\begin{aligned}\n   f(x) = \\frac{\\mathrm{d}F}{\\mathrm{d}x}.\n\\end{aligned}\\]\nWe can then differentiate the ECDF to get the empirical density function, \\(\\hat{f}(x)\\) as\n\\[\\begin{aligned}\n   \\hat{f}(x) = \\frac{1}{n}\\sum_{i=1}^n \\delta(x - x_i),\n\\end{aligned}\\]\nwhere \\(\\delta(x)\\) is the Dirac delta function.\nWith the ECDF (and empirical density function), we have now defined an empirical distribution that is dependent only on the data. We now define a plug-in estimate of a parameter \\(\\theta\\) as\n\\[\\begin{aligned}\n   \\hat{\\theta} = T(\\hat{F}).\n\\end{aligned}\\]\nIn other words, to get a plug-in estimate a parameter \\(\\theta\\), we need only to compute the functional using the empirical distribution. That is, we simply “plug in” the empirical CDF for the actual CDF.\nThe plug-in estimate for the median is easy to calculate.\n\\[\\begin{aligned}\n   \\hat{m} = \\hat{F}^{-1}(1/2),\n\\end{aligned}\\]\nor the middle-ranked data point. The plug-in estimate for the mean or variance seem at face to be a bit more difficult to calculate, but the following general theorem will help. Consider a functional of the form of an expectation value, \\(r(x)\\).\n\\[\\begin{aligned}\n\\begin{aligned}\n   \\int\\mathrm{d}\\hat{F}(x)\\,r(x) &= \\int \\mathrm{d}x \\,r(x)\\, \\hat{f}(x)\n   = \\int \\mathrm{d}x\\, r(x) \\left[\\frac{1}{n}\\sum_{i=1}^n\\delta(x - x_i)\\right] \\nonumber \\\\[1em]\n   &= \\frac{1}{n}\\sum_{i=1}^n\\int \\mathrm{d}x \\,r(x) \\delta(x-x_i)\n   = \\frac{1}{n}\\sum_{i=1}^n r(x_i).\n\\end{aligned}\n\\end{aligned}\\]\nA functional of this form is called a linear statistical functional. The result above means that the plug-in estimate for a linear functional of a distribution is the arithmetic mean of the observed \\(r(x)\\) themselves. The plug-in estimate of the mean, which has \\(r(x) = x\\), is\n\\[\\begin{aligned}\n   \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n x_i \\equiv \\bar{x},\n\\end{aligned}\\]\nwhere we have defined \\(\\bar{x}\\) as the traditional sample mean (the arithmetic mean of the measured data), which we have just shown is the plug-in estimate. This plug-in estimate is implemented in the np.mean() function. The plug-in estimate for the variance is\n\\[\\begin{aligned}\n   \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2\n   = \\frac{1}{n}\\sum_{i=1}^n x_i^2 - \\bar{x}^2.\n\\end{aligned}\\]\nThis plug-in estimate is implemented in the np.var() function.\nNote that we are denoting the mean and variance as \\(\\mu\\) and \\(\\sigma^2\\), but these are not in general the parameters with the same common name and symbols from a Normal distribution. Any distribution has a first moment (called a mean) and a second central moment (called a variance), unless they do not exist, as is the case, e.g., with a Cauchy distribution. In this context, we denote by \\(\\mu\\) and \\(\\sigma^2\\) the mean and variance of the unknown underlying univariate generative distribution.\nWe can compute plug-in estimates for more complicated parameters as well. For example, for a bivariate distribution, the correlation between the two variables \\(x\\) and \\(y\\), is defined with\n\\[\\begin{aligned}\n   \\rho = \\frac{\\left\\langle (x-\\mu_x)(y-\\mu_y)\\right\\rangle}{\\sigma_x \\sigma_y},\n\\end{aligned}\\]\nwhere the expectation in the numerator is called the covariance between \\(x\\) and \\(y\\). It is of large magnitude of \\(x\\) and \\(y\\) vary together and close to zero if they are nearly independent of each other. The plug-in estimate for the correlation is\n\\[\\begin{aligned}\n   \\hat{\\rho} = \\frac{\\sum_i(x_i - \\bar{x})(y_i-\\bar{y})}{\\sqrt{\\left(\\sum_i(x_i-\\bar{x})^2\\right)\\left(\\sum_i(y_i-\\bar{y})^2\\right)}}.\n\\end{aligned}\\]",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Plug-in estimates</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/bias.html",
    "href": "lessons/nonparametric/plugin/bias.html",
    "title": "25  Bias",
    "section": "",
    "text": "25.1 Bias of the plug-in estimate for the mean\nThe bias of an estimate is the difference between the expectation value of the point estimate and value of the parameter.\n\\[\\begin{aligned}\n   \\text{bias}_F(\\hat{\\theta}, \\theta) = \\langle \\hat{\\theta} \\rangle - \\theta\n   = \\int\\mathrm{d}x\\, \\hat{\\theta}f(x) - T(F).\n\\end{aligned}\\]\nNote that the expectation value of \\(\\hat{\\theta}\\) is computed over the (unknown) generative distribution whose PDF is \\(f(x)\\).\nWe often want a small bias because we want to choose estimates that give us back the parameters we expect. Let's first investigate the bias of the plug-in estimate of the mean. As a reminder, the plug-in estimate is\n\\[\\begin{aligned}\n   \\hat{\\mu} = \\bar{x},\n\\end{aligned}\\]\nwhere \\(\\bar{x}\\) is the arithmetic mean of the observed data. To compute the bias of the plug-in estimate, we need to compute \\(\\langle \\hat{\\mu}\\rangle\\) and compare it to \\(\\mu\\).\n\\[\\begin{aligned}\n   \\langle \\hat{\\mu}\\rangle = \\langle \\bar{x}\\rangle = \\frac{1}{n}\\left\\langle\\sum_i x_i\\right\\rangle\n   = \\frac{1}{n}\\sum_i \\left\\langle x_i\\right\\rangle\n   = \\langle x\\rangle\n   = \\mu.\n\\end{aligned}\\]\nBecause \\(\\langle \\hat{\\mu}\\rangle = \\mu\\), the bias in the plug-in estimate for the mean is zero. It is said to be unbiased.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Bias</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/bias.html#bias-of-the-plug-in-estimate-for-the-variance",
    "href": "lessons/nonparametric/plugin/bias.html#bias-of-the-plug-in-estimate-for-the-variance",
    "title": "25  Bias",
    "section": "25.2 Bias of the plug-in estimate for the variance",
    "text": "25.2 Bias of the plug-in estimate for the variance\nTo compute the bias of the plug-in estimate for the variance, first recall that the variance, as the second central moment, is computed as\n\\[\\begin{aligned}\n   \\sigma^2 = \\langle x^2 \\rangle - \\langle x\\rangle^2.\n\\end{aligned}\\]\nSo, the expectation value of the plug-in estimate is\n\\[\\begin{aligned}\n\\begin{aligned}\n\\left\\langle \\hat{\\sigma}^2 \\right\\rangle &= \\left\\langle\\frac{1}{n}\\sum_i x_i^2 - \\bar{x}^2\\right\\rangle \\\\[1em]\n&= \\left\\langle\\frac{1}{n}\\sum_i x_i^2\\right\\rangle - \\left\\langle\\bar{x}^2\\right\\rangle\\\\[1em]\n&= \\frac{1}{n}\\sum_i \\left\\langle x_i^2\\right\\rangle  - \\left\\langle\\bar{x}^2\\right\\rangle \\\\[1em]\n&= \\langle x^2 \\rangle - \\left\\langle\\bar{x}^2\\right\\rangle\\\\[1em]\n&= \\mu^2 + \\sigma^2 - \\left\\langle\\bar{x}^2\\right\\rangle.\n\\end{aligned}\n\\end{aligned}\\]\nWe now need to compute \\(\\left\\langle\\bar{x}^2\\right\\rangle\\), which is a little trickier. We will use the fact that the measurements are independent, so \\(\\left\\langle x_i x_j\\right\\rangle = \\langle x_i \\rangle \\langle x_j\\rangle\\) for \\(i\\ne j\\).\n\\[\\begin{aligned}\n\\begin{aligned}\n\\left\\langle\\bar{x}^2\\right\\rangle\n&= \\left\\langle\\left(\\frac{1}{n}\\sum_ix_i\\right)^2\\right\\rangle \\\\[1em]\n&= \\frac{1}{n^2}\\left\\langle\\left(\\sum_ix_i\\right)^2 \\right\\rangle \\\\[1em]\n&= \\frac{1}{n^2}\\left\\langle\\sum_i x_i^2 + 2\\sum_i\\sum_{j&gt;i}x_i x_j\\right\\rangle \\nonumber \\\\[1em]\n&= \\frac{1}{n^2}\\left(\\sum_i \\left\\langle x_i^2\\right\\rangle\n+ 2\\sum_i\\sum_{j&gt;i}\\left\\langle x_i x_j\\right\\rangle \\right) \\\\[1em]\n&= \\frac{1}{n^2}\\left(n(\\sigma^2 + \\mu^2)\n+ 2\\sum_i\\sum_{j&gt;i}\\langle x_i\\rangle \\langle x_j\\rangle\\right) \\nonumber \\\\[1em]\n&=\\frac{1}{n^2}\\left(n(\\sigma^2 + \\mu^2) + n(n-1)\\langle x\\rangle^2\\right)\\\\[1em]\n&= \\frac{1}{n^2}\\left(n\\sigma^2 + n^2\\mu^2\\right) \\\\[1em]\n&= \\frac{\\sigma^2}{n} + \\mu^2.\n\\end{aligned}\n\\end{aligned}\\]\nThus, we have\n\\[\\begin{aligned}\n\\left\\langle \\hat{\\sigma}^2 \\right\\rangle = \\left(1-\\frac{1}{n}\\right)\\sigma^2.\n\\end{aligned}\\]\nTherefore, the bias is\n\\[\\begin{aligned}\n   \\text{bias} = -\\frac{\\sigma^2}{n}.\n\\end{aligned}\\]\nIf \\(\\hat{\\sigma}^2\\) is the plug-in estimate for the variance, an unbiased estimator would instead be\n\\[\\begin{aligned}\n  \\frac{n}{n-1}\\,\\hat{\\sigma}^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2.\n\\end{aligned}\\]",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Bias</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/bias.html#justification-of-using-plug-in-estimates.",
    "href": "lessons/nonparametric/plugin/bias.html#justification-of-using-plug-in-estimates.",
    "title": "25  Bias",
    "section": "25.3 Justification of using plug-in estimates.",
    "text": "25.3 Justification of using plug-in estimates.\nDespite the apparent bias in the plug-in estimate for the variance, we will normally just use plug-in estimates going forward. (We will use the hat, e.g. \\(\\hat{\\theta}\\), to denote an estimate, which can be either a plug-in estimate or not.) Note that the bootstrap procedures we lay out in what follows do not need to use plug-in estimates, but we will use them for convenience. Why do this? The bias is typically small. We just saw that the biased and unbiased estimators of the variance differ by a factor of \\(n/(n-1)\\), which is negligible for large \\(n\\). In fact, plug-in estimates tend to have much smaller error than the confidence intervals for the parameter estimate, which we will discuss next.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Bias</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/confidence_intervals.html",
    "href": "lessons/nonparametric/plugin/confidence_intervals.html",
    "title": "26  Confidence intervals",
    "section": "",
    "text": "26.1 The frequentist interpretation of probability\nBefore we start taking about confidence intervals, it is important to recall the frequentist interpretation of probability. Under this interpretation, the probability \\(P(A)\\) represents a long-run frequency of event \\(A\\) over a large number of identical repetitions of an experiment. In our calculation of confidence intervals and in performance of null hypothesis significance tests, we will directly apply this interpretation of probability again and again, using our computers to “repeat” experiments many times and tally the frequencies of what we see.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/confidence_intervals.html#confidence-intervals",
    "href": "lessons/nonparametric/plugin/confidence_intervals.html#confidence-intervals",
    "title": "26  Confidence intervals",
    "section": "26.2 Confidence intervals",
    "text": "26.2 Confidence intervals\nConsider the following question. If I were to do the experiment again, what value might I expect for my plug-in estimate for my functional? What if I did it again and again and again? These are reasonable questions because the plug-in estimate is something that can vary meaningfully from experiment to experiment. Remember, with the frequentist interpretation of probability, we cannot assign a probability to a parameter value. A parameter has one value, and that’s that. We can describe the long-term frequency of observing results about random variables. Because a plug-in estimate for a statistical functional does vary from experiment to experiment, it is a random variable. So, we can define a 95% confidence interval as follows.\n\nIf an experiment is repeated over and over again, the estimate I compute for a parameter, \\(\\hat{\\theta}\\), will lie between the bounds of the 95% confidence interval for 95% of the experiments.\n\nWhile this is a correct definition of a confidence interval, some statisticians prefer another. To quote Larry Wasserman from his book, All of Statistics,\n\n[The above definition] is correct but useless since we rarely repeat the same experiment over and over. A better interpretation is this: On day 1, you collect data and construct a 95 percent confidence interval for a parameter \\(\\theta_1\\). On day 2, you collect new data and construct a 95 percent confidence interval for an unrelated parameter \\(\\theta_2\\). On day 3, you collect new data and construct a 95 percent confidence interval for an unrelated parameter \\(\\theta_3\\). You continue this way constructing confidence intervals for a sequence of unrelated parameters \\(\\theta_1, \\theta_2, \\ldots\\). Then 95 percent of your intervals will trap the true parameter value. There us no need to introduce the idea of repeating the same experiment over and over.\n\nIn other words, the confidence interval describes the construction of the confidence interval itself. 95% of the time, it will contain the true (unknown) parameter value. Wasserman’s description contains a reference to the true parameter value, so if you are going to talk about the true parameter value, this description is useful. However, the first definition of the confidence interval is quite useful if you want to think about how repeated experiments will end up.\nWe will use the first definition in thinking about how to construct a confidence interval. To construct the confidence interval, then, we will repeat the experiment over and over again, each time computing \\(\\hat{\\theta}\\). We will then generate an ECDF of our \\(\\hat{\\theta}\\) values, and report the 2.5th and 97.5th percentile to get our 95% confidence interval. But wait, how will we repeat the experiment so many times?",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/confidence_intervals.html#bootstrap-confidence-intervals",
    "href": "lessons/nonparametric/plugin/confidence_intervals.html#bootstrap-confidence-intervals",
    "title": "26  Confidence intervals",
    "section": "26.3 Bootstrap confidence intervals",
    "text": "26.3 Bootstrap confidence intervals\nRemember that the data come from a generative distribution with CDF \\(F(x)\\). Doing an experiment where we make \\(n\\) measurements amounts to drawing \\(n\\) numbers out of \\(F(x)\\)1. So, we could draw out of \\(F(x)\\) over and over again. The problem is, we do not know what \\(F(x)\\) is. However, we do have an empirical approximation for \\(F(x)\\), namely \\(\\hat{F}(x)\\). So, we could draw \\(n\\) samples out of \\(\\hat{F}(x)\\), compute \\(\\hat{\\theta}\\) from these samples, and repeat. This procedure is called bootstrapping.\nTo get the terminology down, a bootstrap sample, \\(\\mathbf{x}^*\\), is a set of \\(n\\) \\(x\\) values drawn from \\(\\hat{F}(x)\\). A bootstrap replicate is the estimate \\(\\hat{\\theta}^*\\) obtained from the bootstrap sample \\(\\mathbf{x}^*\\). To generate a bootstrap sample, consider an array of measured values \\(\\mathbf{x}\\). We draw \\(n\\) values out of this array with replacement to give us \\(\\mathbf{x}^*\\). This is equivalent to sampling out of \\(\\hat{F}(x)\\).\nSo, the recipe for generating a bootstrap confidence interval is as follows.\n\nGenerate \\(B\\) independent bootstrap samples. Each one is generated by drawing \\(n\\) values out of the data array with replacement.\nCompute \\(\\hat{\\theta}^*\\) for each bootstrap sample to get the bootstrap replicates.\nThe central \\(100 (1-\\alpha)\\) percent confidence interval consists of the percentiles \\(100\\alpha/2\\) and \\(100(1-\\alpha/2)\\) of the bootstrap replicates.\n\nThis procedure works for any estimate \\(\\hat{\\theta}\\), be it the mean, median, variance, skewness, kurtosis, or any other thing you can think of. Note that we use the empirical distribution, so there is never any assumption of an underlying \"true\" distribution. We are employing the plug-in principle for repeating experiments. Instead of sampling out of the generative distribution (which is what performing an experiment is), we plug-in the empirical distribution and sample out of it, instead. Thus, we are doing nonparametric inference on what we would expect for parameters coming out of unknown distributions; we only know the data.\nThere are plenty of subtleties and improvements to this procedure, but this is most of the story. We will discuss the mechanics of how to programmatically generate bootstrap replicates in forthcoming lessons, but we have already covered the main idea.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/confidence_intervals.html#footnotes",
    "href": "lessons/nonparametric/plugin/confidence_intervals.html#footnotes",
    "title": "26  Confidence intervals",
    "section": "",
    "text": "We’re being loose with language here. We’re drawing out of the distribution that has CDF \\(F(x)\\), but we’re saying “draw out of F” for short.↩︎",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/intro_bootstrap.html",
    "href": "lessons/nonparametric/bootstrap/intro_bootstrap.html",
    "title": "27  Performing bootstrap calculations",
    "section": "",
    "text": "27.1 The data set\n| Download notebook\nDataset download\nWe have discussed the plug-in principle, in which we approximate the distribution function of the generative model, \\(F\\), with the empirical distribution function, \\(\\hat{F}\\). We then approximate all statistical functionals of \\(F\\) using \\(\\hat{F}\\); \\(T(F)\\approx T(\\hat{F})\\). Using the frequentist perspective, we talked about using the plug-in principle and bootstrapping to compute confidence intervals and do hypothesis testing. In this lesson, we will go over how we implement this in practice. We will work with some real data on the reproductive health of male bees that have been treated with pesticide.\nIt is important as you go through this lesson to remember that you do not know what the generative distribution is. We are only approximating it using the plug-in principle. Since we do not know what \\(F\\) is, we do not know what its parameters are, so we therefore cannot estimate them. Rather, we can study summary statistics, which are plug-in estimates for statistical functionals of the generative distribution, such as means, variances, or even the ECDF itself, and how they may vary from experiment to experiment. In this sense, we are doing nonparametric inference.\nNeonicotinoid pesticides are thought to have inadvertent effects on service-providing insects such as bees. A study of this was featured in the New York Times. The original paper by Straub and coworkers may be found here. The authors put their data in the Dryad repository, which means that it is free to all to work with!\n(Do you see a trend here? If you want people to think deeply about your results, explore them, learn from them, in general further science with them, make your data publicly available. Strongly encourage the members of your lab to do the same.)\nWe will look at the sperm quality of drone bees using this data set. First, let’s load in the data set and check it out.\ndf = pl.read_csv(\n    os.path.join(data_path, \"bee_sperm.csv\"),\n    null_values=[\"NO SPERM\", \"No Sperm\"],\n    comment_prefix=\"#\",\n)\ndf.head()\n\n\nshape: (5, 18)\n\n\n\nSpecimen\nTreatment\nEnvironment\nTreatmentNCSS\nSample ID\nColony\nCage\nSample\nSperm Volume per 500 ul\nQuantity\nViabilityRaw (%)\nQuality\nAge (d)\nInfertil\nAliveSperm\nQuantity Millions\nAlive Sperm Millions\nDead Sperm Millions\n\n\ni64\nstr\nstr\ni64\nstr\ni64\ni64\ni64\ni64\ni64\nf64\nf64\ni64\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n227\n\"Control\"\n\"Cage\"\n1\n\"C2-1-1\"\n2\n1\n1\n2150000\n2150000\n96.726381\n96.726381\n14\n0\n2079617\n2.15\n2.079617\n0.070383\n\n\n228\n\"Control\"\n\"Cage\"\n1\n\"C2-1-2\"\n2\n1\n2\n2287500\n2287500\n96.349808\n96.349808\n14\n0\n2204001\n2.2875\n2.204001\n0.083499\n\n\n229\n\"Control\"\n\"Cage\"\n1\n\"C2-1-3\"\n2\n1\n3\n87500\n87500\n98.75\n98.75\n14\n0\n86406\n0.0875\n0.086406\n0.001094\n\n\n230\n\"Control\"\n\"Cage\"\n1\n\"C2-1-4\"\n2\n1\n4\n1875000\n1875000\n93.287421\n93.287421\n14\n0\n1749139\n1.875\n1.749139\n0.125861\n\n\n231\n\"Control\"\n\"Cage\"\n1\n\"C2-1-5\"\n2\n1\n5\n1587500\n1587500\n97.792506\n97.792506\n14\n0\n1552456\n1.5875\n1.552456\n0.035044\nWe are interested in the number of alive sperm in the samples. Let’s first explore the data by making ECDFs for the two groups we will compare, those treated with pesticide (“Pesticide”) and those that are not (“Control”).\np = iqplot.ecdf(df, q=\"Alive Sperm Millions\", cats=\"Treatment\")\n\nbokeh.io.show(p)\nThe visual inspection of the ECDFs suggests that indeed the control drones have more alive sperm than those treated with pesticide. But how variable would these ECDFs be if we repeated the experiment?",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Performing bootstrap calculations</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/intro_bootstrap.html#bootstrap-samples-and-ecdfs",
    "href": "lessons/nonparametric/bootstrap/intro_bootstrap.html#bootstrap-samples-and-ecdfs",
    "title": "27  Performing bootstrap calculations",
    "section": "27.2 Bootstrap samples and ECDFs",
    "text": "27.2 Bootstrap samples and ECDFs\nTo address this question, we can generate bootstrap samples from the experimental data and make lots of ECDFs. We can then plot them all to see how the ECDF might vary. Recall that a bootstrap sample from a data set of \\(n\\) repeated measurements is generated by drawing \\(n\\) data points out of the original data set with replacement. The rng.choice() function enables random draws of elements out of an array. Let’s generate 100 bootstrap samples and plot their ECDFs to visualize how our data set might change as we repeat the experiment.\n\nrng = np.random.default_rng()\n\n# Set up Numpy arrays for convenience (also much better performance)\ndfs = df.partition_by('Treatment', as_dict=True)\n\nalive_ctrl = dfs[('Control',)].get_column(\"Alive Sperm Millions\").to_numpy()\nalive_pest = dfs[('Pesticide',)].get_column(\"Alive Sperm Millions\").to_numpy()\n\n\n# ECDF values for plotting\nctrl_ecdf = np.arange(1, len(alive_ctrl)+1) / len(alive_ctrl)\npest_ecdf = np.arange(1, len(alive_pest)+1) / len(alive_pest)\n\n# Make 100 bootstrap samples and plot them\nfor _ in range(100):\n    bs_ctrl = rng.choice(alive_ctrl, size=len(alive_ctrl))\n    bs_pest = rng.choice(alive_pest, size=len(alive_pest))\n\n    # Add semitransparent ECDFs to the plot\n    p.scatter(np.sort(bs_ctrl), ctrl_ecdf, color=bokeh.palettes.Category10_3[0], alpha=0.02)\n    p.scatter(np.sort(bs_pest), pest_ecdf, color=bokeh.palettes.Category10_3[1], alpha=0.02)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nFrom this graphical display, we can already see that the ECDFs do not strongly overlap in 100 bootstrap samples, so there is likely a real difference between the two treatments.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Performing bootstrap calculations</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/intro_bootstrap.html#speeding-up-sampling-with-numba",
    "href": "lessons/nonparametric/bootstrap/intro_bootstrap.html#speeding-up-sampling-with-numba",
    "title": "27  Performing bootstrap calculations",
    "section": "27.3 Speeding up sampling with Numba",
    "text": "27.3 Speeding up sampling with Numba\nWe can make sampling faster (though note that the slowness in generating the above plot is not in resampling, but in adding glyphs to the plot) by employing just-in-time compilation, wherein the Python code is compiled at runtime into machine code. This results in a substantial speed boost. Numba is a powerful tool for this purpose, and we will use it. Bear in mind, though, that not all Python code is Numba-able. In order to just-in-time compile a function, we need to decorate its definition with the @numba.njit decorator, which tells the Python interpreter to use Numba to just-in-time compile the function. Note that Numba works on basic Python objects like tuples and Numpy arrays, so be sure to pass Numpy arrays into the respective functions.\nRandom number generation is supported, and instances of Numpy’s generators can be passed into Numba’d functions. However, to avoid always having to pass in an instance of a Numpy RNG into Numba functions, we will use Numpy’s old interface to the the Mersenne Twister bit generator, which uses a syntax like np.random.normal().\n\n@numba.njit\ndef draw_bs_sample(data):\n    \"\"\"Draw a bootstrap sample from a 1D data set.\"\"\"\n    return np.random.choice(data, size=len(data))\n\nLet’s quickly quantify the speed boost. Before we do the timing, we will run the Numba’d version once to give it a chance to do the JIT compilation.\n\n# Run once to JIT\ndraw_bs_sample(alive_ctrl)\n\nprint('Using the Numpy default RNG (PCG64):')\n%timeit rng.choice(alive_ctrl, size=len(alive_ctrl))\n\nprint('\\nUsing the Numpy Mersenne Twister:')\n%timeit np.random.choice(alive_ctrl, size=len(alive_ctrl))\n\nprint(\"\\nUsing a Numba'd Mersenne Twister:\")\n%timeit draw_bs_sample(alive_ctrl)\n\nUsing the Numpy default RNG (PCG64):\n4.33 μs ± 10.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\nUsing the Numpy Mersenne Twister:\n4.87 μs ± 16.9 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\nUsing a Numba'd Mersenne Twister:\n1.73 μs ± 1.81 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThat’s a significant increase and can make a big difference when generating lots of bootstrap samples.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Performing bootstrap calculations</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/intro_bootstrap.html#bootstrap-replicates-and-confidence-intervals",
    "href": "lessons/nonparametric/bootstrap/intro_bootstrap.html#bootstrap-replicates-and-confidence-intervals",
    "title": "27  Performing bootstrap calculations",
    "section": "27.4 Bootstrap replicates and confidence intervals",
    "text": "27.4 Bootstrap replicates and confidence intervals\nWe have plotted the ECDF of the data, which is instructive, but we would also like to get plug-in estimates for the generative distribution. Remember, when doing nonparametric plug-in estimates, we plug in the ECDF for the CDF. We do not need to specify what the distribution (described mathematically by the CDF, or equivalently by the PDF) is, just that we approximate it by the empirical distribution.\nWe have laid out the procedure to compute a confidence interval.\n\nGenerate B independent bootstrap samples.\nCompute the plug-in estimate of the statistical functional of interest for each bootstrap sample to get the bootstrap replicates.\nThe 100(1 – α) percent confidence interval consists of the percentiles 100 α/2 and 100(1 – α/2) of the bootstrap replicates.\n\nA key step here is computing the bootstrap replicate. We will write a function to draw bootstrap replicates of a statistic of interest. To do so, we pass in a function, which we will call stat_fun. Because we do not know what function we will pass in, we cannot just-in-time compile this functions.\n\ndef draw_bs_reps(data, stat_fun, size=1):\n    \"\"\"Draw boostrap replicates computed with stat_fun from 1D data set.\"\"\"\n    return np.array([stat_fun(draw_bs_sample(data)) for _ in range(size)])\n\nWe will also write a few functions for commonly computed statistics, which enables us to use Numba to greatly speed up the process of generating bootstrap replicates. Note that in our decorator, we use the parallel=True keyword argument. Within the for loop to compute the bootstrap replicates, as use numba.prange() as a drop-in replacement for range(). When we do it this way, Numba automatically parallelizes the calculation across available CPUs.\n\n@numba.njit(parallel=True)\ndef draw_bs_reps_mean(data, size=1):\n    \"\"\"Draw boostrap replicates of the mean from 1D data set.\"\"\"\n    out = np.empty(size)\n    for i in numba.prange(size):\n        out[i] = np.mean(draw_bs_sample(data))\n    return out\n\n\n@numba.njit(parallel=True)\ndef draw_bs_reps_median(data, size=1):\n    \"\"\"Draw boostrap replicates of the median from 1D data set.\"\"\"\n    out = np.empty(size)\n    for i in numba.prange(size):\n        out[i] = np.median(draw_bs_sample(data))\n    return out\n\n\n@numba.njit(parallel=True)\ndef draw_bs_reps_std(data, size=1):\n    \"\"\"Draw boostrap replicates of the standard deviation from 1D data set.\"\"\"\n    out = np.empty(size)\n    for i in numba.prange(size):\n        out[i] = np.std(draw_bs_sample(data))\n    return out\n\nNow, let’s get bootstrap replicates for the mean of each of the two treatments.\n\nbs_reps_mean_ctrl = draw_bs_reps_mean(alive_ctrl, size=10000)\nbs_reps_mean_pest = draw_bs_reps_mean(alive_pest, size=10000)\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\nWe can now compute the confidence intervals by computing the percentiles using the np.percentile() function.\n\n# 95% confidence intervals\nmean_ctrl_conf_int = np.percentile(bs_reps_mean_ctrl, [2.5, 97.5])\nmean_pest_conf_int = np.percentile(bs_reps_mean_pest, [2.5, 97.5])\n\nprint(\"\"\"\nMean alive sperm count 95% conf int control (millions):   [{0:.2f}, {1:.2f}]\nMean alive sperm count 95% conf int treatment (millions): [{2:.2f}, {3:.2f}]\n\"\"\".format(*(tuple(mean_ctrl_conf_int) + tuple(mean_pest_conf_int))))\n\n\nMean alive sperm count 95% conf int control (millions):   [1.67, 2.07]\nMean alive sperm count 95% conf int treatment (millions): [1.11, 1.50]\n\n\n\n\n27.4.1 Display of confidence intervals\nWhile the textual confidence intervals shown above are useful, it is often desired to display confidence intervals graphically. The bebi103 package has a utility to do this. It requires as input a list of dictionaries where each dictionary contains an estimate, a confidence interval, and a label.\n\nsummaries = [\n    dict(label=\"control\", estimate=np.mean(alive_ctrl), conf_int=mean_ctrl_conf_int),\n    dict(label=\"treated\", estimate=np.mean(alive_pest), conf_int=mean_pest_conf_int),\n]\n\nbokeh.io.show(bebi103.viz.confints(summaries, x_axis_label=\"alive sperm (millions)\"))\n\n\n  \n\n\n\n\n\n\n\n27.4.2 ECDF of bootstrap replicates\nWe can also use the bootstrap replicates to plot the probability distribution of mean alive sperm count. Remember: This is not a confidence interval on a parameter value. That does not make sense in frequentist statistics, and furthermore we have not assumed any parametric model. It is the confidence interval describing what we would get as a plug-in estimate for the mean if we did the experiment over and over again.\nWhen we plot the ECDF of the bootstrap replicates, we will thin them so as not to choke the browser with too many points. Since we will do this again, we write a quick function to do it.\n\ndef plot_bs_reps_ecdf(ctrl, pest, q, thin=1, **kwargs):\n    \"\"\"Make plot of bootstrap ECDFs for control and pesticide treatment.\"\"\"\n    x_ctrl = np.sort(ctrl)[::thin]\n    x_pest = np.sort(pest)[::thin]\n\n    df = pl.DataFrame(\n        data={\n            \"treatment\": [\"control\"] * len(x_ctrl) + [\"pesticide\"] * len(x_pest),\n            q: np.concatenate((x_ctrl, x_pest)),\n        }\n    )\n\n    p = iqplot.ecdf(\n        df, q=q, cats=\"treatment\", frame_height=200, frame_width=350, **kwargs\n    )\n\n    return p\n\n\np = plot_bs_reps_ecdf(\n    bs_reps_mean_ctrl, bs_reps_mean_pest, \"mean alive sperm (millions)\", thin=50\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThese are both nice, Normal distributions with almost no overlap in the tails. We also saw in the computation of the 95% confidence intervals above that there is no overlap. This is expected, the mean alive sperm count should be Normally distributed as per the central limit theorem.\nWe can do the same procedure for other statistical quantities that do not follow the central limit theorem. The procedure is exactly the same. We will do it for the median.\n\n# Get the bootstrap replicates\nbs_reps_median_ctrl = draw_bs_reps_median(alive_ctrl, size=10000)\nbs_reps_median_pest = draw_bs_reps_median(alive_pest, size=10000)\n\n# 95% confidence intervals\nmedian_ctrl_conf_int = np.percentile(bs_reps_median_ctrl, [2.5, 97.5])\nmedian_pest_conf_int = np.percentile(bs_reps_median_pest, [2.5, 97.5])\n\n# Plot of confidence interval\nsummaries = [\n    dict(label=\"control\", estimate=np.median(alive_ctrl), conf_int=median_ctrl_conf_int),\n    dict(label=\"treated\", estimate=np.median(alive_pest), conf_int=median_pest_conf_int),\n]\n\np_conf_int = bebi103.viz.confints(summaries, x_axis_label=\"alive sperm (millions)\")\n\n# Plot ECDFs of bootstrap replicates\np_ecdf = plot_bs_reps_ecdf(\n    bs_reps_median_ctrl, bs_reps_median_pest, \"median alive sperm (millions)\", thin=50\n)\n\n# Show both plots\nbokeh.io.show(bokeh.layouts.column([p_conf_int, bokeh.models.Spacer(height=50), p]))\n\n\n  \n\n\n\n\n\nThe results are similar, and we clearly see clear non-normality in the ECDFs.\nNote that plotting ECDFs of bootstrap replicates is not a normal reporting mechanism for bootstrap confidence intervals. We show them here to illustrate how bootstrapping works.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Performing bootstrap calculations</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/intro_bootstrap.html#computing-environment",
    "href": "lessons/nonparametric/bootstrap/intro_bootstrap.html#computing-environment",
    "title": "27  Performing bootstrap calculations",
    "section": "27.5 Computing environment",
    "text": "27.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,numba,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\npolars    : 1.31.0\nnumba     : 0.61.2\nbokeh     : 3.7.3\niqplot    : 0.3.7\njupyterlab: 4.4.5",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Performing bootstrap calculations</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/pairs_bootstrap_correlation.html",
    "href": "lessons/nonparametric/bootstrap/pairs_bootstrap_correlation.html",
    "title": "28  Pairs bootstrap and correlation",
    "section": "",
    "text": "28.1 Correlation\n| Download notebook\nDataset download\nWe continue our analysis of the drone sperm quality data set. Let’s load it and remind ourselves of the content.\nWe might wish to investigate how two measured quantities are correlated. For example, if the number of dead sperm and the number of alive sperm are closely correlated, this would mean that a given drone produces some quantity of sperm and some fraction tend to be dead. Let’s take a look at this.\n# Set up plot on log scale\np = bokeh.plotting.figure(\n    frame_height=300,\n    frame_width=300,\n    x_axis_label=\"alive sperm (millions)\",\n    y_axis_label=\"dead sperm (millions)\",\n    x_axis_type=\"log\",\n    y_axis_type=\"log\",\n)\n\n# Only use values greater than zero for log scale\ninds = (pl.col(\"Alive Sperm Millions\") &gt; 0) & (pl.col(\"Dead Sperm Millions\") &gt; 0)\n\n# Populate glyphs\nfor color, ((treatment,), sub_df) in zip(\n    bokeh.palettes.Category10_3, df.filter(inds).group_by(\"Treatment\")\n):\n    p.scatter(\n        source=sub_df.to_dict(),\n        x=\"Alive Sperm Millions\",\n        y=\"Dead Sperm Millions\",\n        color=color,\n        legend_label=treatment,\n    )\n\np.legend.location = \"bottom_right\"\n\nbokeh.io.show(p)\nThere seems to be some correlation (on a log scale), but it is difficult to tell. We can compute the correlation with the bivariate correlation coefficient, also known as the Pearson correlation. It is the plug-in estimate of the correlation between variables (in this case alive and dead sperm). The correlation is the covariance divided by the geometric mean of the individual variances\nThe bivariate correlation coefficient is implemented with np.corrcoef(), but we will code our own and JIT it for speed.\n@numba.njit\ndef bivariate_r(x, y):\n    \"\"\"\n    Compute plug-in estimate for the bivariate correlation coefficient.\n    \"\"\"\n    return (\n        np.sum((x - np.mean(x)) * (y - np.mean(y)))\n        / np.std(x)\n        / np.std(y)\n        / np.sqrt(len(x))\n        / np.sqrt(len(y))\n    )\nWe can use it to compute the bivariate correlation coefficient for the logarithm of alive and dead sperm.\nbivariate_r(\n    np.log(df.filter(inds).get_column(\"Alive Sperm Millions\").to_numpy()),\n    np.log(df.filter(inds).get_column(\"Dead Sperm Millions\").to_numpy()),\n)\n\n0.5219944217488051",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Pairs bootstrap and correlation</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/pairs_bootstrap_correlation.html#pairs-bootstrap-confidence-intervals",
    "href": "lessons/nonparametric/bootstrap/pairs_bootstrap_correlation.html#pairs-bootstrap-confidence-intervals",
    "title": "28  Pairs bootstrap and correlation",
    "section": "28.2 Pairs bootstrap confidence intervals",
    "text": "28.2 Pairs bootstrap confidence intervals\nHow can we get a confidence interval on a correlation coefficient? We can again apply the bootstrap, but this time, the replicate is a pair of data, in this case a dead sperm count/alive sperm count pair. The process of drawing pairs of data points from an experiment and then computing bootstrap replicates from them is called pairs bootstrap. Let’s code it up for this example with the bivariate correlation.\nOur strategy in coding up the pairs bootstrap is to draw bootstrap samples of the indices of measurement and use those indices to select the pairs.\n\n@numba.njit\ndef draw_bs_sample(data):\n    \"\"\"Draw a bootstrap sample from a 1D data set.\"\"\"\n    return np.random.choice(data, size=len(data))\n\n\n@numba.njit\ndef draw_bs_pairs(x, y):\n    \"\"\"Draw a pairs bootstrap sample.\"\"\"\n    inds = np.arange(len(x))\n    bs_inds = draw_bs_sample(inds)\n    \n    return x[bs_inds], y[bs_inds]\n\nWith our pairs sampling function in place, we can write a function to compute replicates.\n\n@numba.njit(parallel=True)\ndef draw_bs_pairs_reps_bivariate(x, y, size=1):\n    \"\"\"\n    Draw bootstrap pairs replicates.\n    \"\"\"\n    out = np.empty(size)\n\n    for i in numba.prange(size):\n        out[i] = bivariate_r(*draw_bs_pairs(x, y))\n\n    return out\n\nFinally, we can put it all together to compute confidence intervals on the correlation. To start, we extract all of the relevant measurements as Numpy arrays to allow for faster resampling (and that’s what our Numba’d functions require).\n\n# Extract NumPy arrays (only use values greater than zero for logs)\ninds = (pl.col(\"Alive Sperm Millions\") &gt; 0) & (pl.col(\"Dead Sperm Millions\") &gt; 0)\ndfs = df.filter(inds).partition_by('Treatment', as_dict=True)\n\nalive_ctrl = dfs[('Control',)].get_column(\"Alive Sperm Millions\").to_numpy()\nalive_pest = dfs[('Pesticide',)].get_column(\"Alive Sperm Millions\").to_numpy()\ndead_ctrl = dfs[('Control',)].get_column(\"Dead Sperm Millions\").to_numpy()\ndead_pest = dfs[('Pesticide',)].get_column(\"Dead Sperm Millions\").to_numpy()\n\nNow we can compute the bootstrap replicates using our draw_bs_pairs_reps_bivariate() function.\n\n# Get reps\nbs_reps_ctrl = draw_bs_pairs_reps_bivariate(\n    np.log(alive_ctrl), np.log(dead_ctrl), size=10_000\n)\n\nbs_reps_pest = draw_bs_pairs_reps_bivariate(\n    np.log(alive_pest), np.log(dead_pest), size=10_000\n)\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\nAnd from the replicates, we can compute and print the 95% confidence interval.\n\n# Get the confidence intervals\nconf_int_ctrl = np.percentile(bs_reps_ctrl, [2.5, 97.5])\nconf_int_pest = np.percentile(bs_reps_pest, [2.5, 97.5])\n\n# Plot confidence intervals\nsummaries = [\n    dict(\n        label=\"control\",\n        estimate=bivariate_r(np.log(alive_ctrl), np.log(dead_ctrl)),\n        conf_int=conf_int_ctrl,\n    ),\n    dict(\n        label=\"treated\",\n        estimate=bivariate_r(np.log(alive_pest), np.log(dead_pest)),\n        conf_int=conf_int_pest,\n    ),\n]\n\nbokeh.io.show(\n    bebi103.viz.confints(summaries, x_axis_label=\"bivariate correlation of logs\")\n)\n\n\n  \n\n\n\n\n\nWe see a clear correlation in both samples, with a wide, but positive, confidence interval. Note that we did this analysis on a log scale, since the data span several orders of magnitude.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Pairs bootstrap and correlation</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/pairs_bootstrap_correlation.html#computing-environment",
    "href": "lessons/nonparametric/bootstrap/pairs_bootstrap_correlation.html#computing-environment",
    "title": "28  Pairs bootstrap and correlation",
    "section": "28.3 Computing environment",
    "text": "28.3 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,numba,bokeh,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\npolars    : 1.31.0\nnumba     : 0.61.2\nbokeh     : 3.7.3\nbebi103   : 0.1.28\njupyterlab: 4.4.5",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Pairs bootstrap and correlation</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/nonparametric_lesson_exercise.html",
    "href": "lessons/nonparametric/nonparametric_lesson_exercise.html",
    "title": "29  Nonparametric inference lesson exercise",
    "section": "",
    "text": "Exercise 1\nWhat is the plug-in principle and how is it used in non-parametric statistics?",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Nonparametric inference lesson exercise</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/nonparametric_lesson_exercise.html#exercise-2",
    "href": "lessons/nonparametric/nonparametric_lesson_exercise.html#exercise-2",
    "title": "29  Nonparametric inference lesson exercise",
    "section": "Exercise 2",
    "text": "Exercise 2\nWhat is a bootstrap sample and what is a bootstrap replicate?",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Nonparametric inference lesson exercise</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/nonparametric_lesson_exercise.html#exercise-3",
    "href": "lessons/nonparametric/nonparametric_lesson_exercise.html#exercise-3",
    "title": "29  Nonparametric inference lesson exercise",
    "section": "Exercise 3",
    "text": "Exercise 3\nWhat is a confidence interval for an estimate? How do the plug-in principle and bootstrapping combine to compute a confidence interval?",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Nonparametric inference lesson exercise</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/nonparametric_lesson_exercise.html#exercise-4",
    "href": "lessons/nonparametric/nonparametric_lesson_exercise.html#exercise-4",
    "title": "29  Nonparametric inference lesson exercise",
    "section": "Exercise 4",
    "text": "Exercise 4\nWrite down any questions or points of confusion that you have.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Nonparametric inference lesson exercise</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/intro_nhst.html",
    "href": "lessons/nhst/intro_nhst.html",
    "title": "Null hypothesis significance testing",
    "section": "",
    "text": "Null hypothesis significance testing, the computing of p-values, which we will define in this section, is widely used throughout the biological sciences in statistical inference procedures. I am not so sure this widespread use is helpful, but the ubiquity of NHST requires us to turn our attention to it.",
    "crumbs": [
      "Null hypothesis significance testing"
    ]
  },
  {
    "objectID": "lessons/nhst/nhst.html",
    "href": "lessons/nhst/nhst.html",
    "title": "30  Null hypothesis significance testing",
    "section": "",
    "text": "30.1 Steps of a NHST\nImagine we do an experiment where we make repeated measurements under control conditions and another set of repeated measurements under test conditions. We may wish to evaluate a hypothesis that there is some similarity in the data generation process between the control and test. If this similar data generating processes cannot produce the observed results, we might like to reject the hypothesis that they are similar. The procedure of null hypothesis significance testing (NHST) formalizes the procedure of evaluating hypotheses in this way.\nA typical hypothesis test consists of these steps.\nWe need to be clear on our definition here. The p-value is the probability of observing a test statistic being at least as extreme as what was measured if the null hypothesis is true. It is exactly that, and nothing else. It is not the probability that the null hypothesis is true. In the frequentist interpretation of probability, we cannot assign a probability to the truth of a hypothesis because a hypothesis is not a random variable.",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Null hypothesis significance testing</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/nhst.html#steps-of-a-nhst",
    "href": "lessons/nhst/nhst.html#steps-of-a-nhst",
    "title": "30  Null hypothesis significance testing",
    "section": "",
    "text": "Clearly state the null hypothesis.\nDefine a test statistic, a scalar value that you can compute from data, almost always a statistical functional of the empirical distribution. Compute it directly from your measured data.\nSimulate data acquisition for the scenario where the null hypothesis is true. Do this many times, computing and storing the value of the test statistic each time.\nThe fraction of simulations for which the test statistic is at least as extreme as the test statistic computed from the measured data is called the p-value, which is what you report.",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Null hypothesis significance testing</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/nhst.html#rejecting-the-null-hypothesis",
    "href": "lessons/nhst/nhst.html#rejecting-the-null-hypothesis",
    "title": "30  Null hypothesis significance testing",
    "section": "30.2 Rejecting the null hypothesis",
    "text": "30.2 Rejecting the null hypothesis\nThe above prescription of a NHST ends with a p-value. Many researchers do not end there, but end with a decision, either a rejection or acceptance of a hypothesis. To take this approach, we adjust the procedure above, the first two steps being the same.\n\nClearly state the null hypothesis.\nDefine a test statistic, a scalar value that you can compute from data, almost always a statistical functional of the empirical distribution. Compute it directly from your measured data.\nDecide on how you will determine the rejection region from the null distribution, which is defined in the next step.\nSimulate data acquisition for the scenario where the null hypothesis is true. Do this many times, computing and storing the value of the test statistic each time. This is sampling out of the null distribution. That is the distribution of the values of the test statistic under the assumption that the null hypothesis is true.\nIf the observed test statistic lies within the rejection region, reject the null hypothesis. Otherwise, do not reject it.\n\nWe are free to choose the rejection region however we like; we just have to be explicit in how we do it. A common method is to choose a significance level \\(\\alpha\\), such that values of the test statistic within the rejection region impart a total probability mass of \\(\\alpha\\), usually with \\(\\alpha\\) being small. This approach is equivalent to computing the p-value, and if it is less than \\(\\alpha\\), the researcher decides to reject the null hypothesis. (I generally do not advocate such black and white decisions, described in the next section of this lesson.)\nImportantly, note that the significance level (or more generally the means of computing the rejection region) is decided before simulating the null hypothesis.",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Null hypothesis significance testing</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/nhst.html#defining-a-test",
    "href": "lessons/nhst/nhst.html#defining-a-test",
    "title": "30  Null hypothesis significance testing",
    "section": "30.3 Defining a test",
    "text": "30.3 Defining a test\nA hypothesis test is defined by the null hypothesis, the test statistic, and what it means to be at least as extreme. That uniquely defines the hypothesis test you are doing. All of the named hypothesis tests, like the Student-t test, the Mann-Whitney U-test, Welch’s t-test, etc., describe a specific hypothesis with a specific test statistic with a specific definition of what it means to be at least as extreme (e.g., one-tailed or two-tailed). I can never remember what these are, nor do I encourage you to; you can always look them up. Rather, you should just clearly write out what your test is in terms of the hypothesis, test statistic, and definition of extremeness.",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Null hypothesis significance testing</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/nhst.html#performing-tests-using-hacker-stats",
    "href": "lessons/nhst/nhst.html#performing-tests-using-hacker-stats",
    "title": "30  Null hypothesis significance testing",
    "section": "30.4 Performing tests using hacker stats",
    "text": "30.4 Performing tests using hacker stats\nNow, the real trick to doing a hypothesis test is simulating data acquisition assuming the null hypothesis is true. For some tests, that is some trio of null hypothesis, test statistic, and meaning of “at least as extreme as,” simulation is unnecessary because analytical results exist. This is the case for many of the named tests. However, in many cases, these are not the hypotheses you wish to test. Because you have random number generation at your finger tips, you can test the hypotheses you want to test, not just the ones that have names.\nI will demonstrate two hypothesis tests and how we can simulate them. For both examples, we will consider the commonly encountered problem of performing the same measurements under two different conditions, control and test. You might have in mind the example of C. elegans egg lengths for well-fed mothers versus starved mothers.\n\n30.4.1 Test and control come from the same distribution.\nHere, the null hypothesis is that the distribution \\(F\\) of the control (wild type) measurements is the same as that \\(G\\) of the test (mutant), or \\(F = G\\). To simulate this, we can do a permutation test. Say we have \\(n\\) control measurements and \\(m\\) test measurements. We concatenate arrays of the control and test measurements to get a single array with \\(n + m\\) entries. We then randomly scramble the order of the entries (this is implemented in rng.permutation()). We take the first \\(n\\) to be labeled “control” and the last \\(m\\) to be labeled “test.” We then compute the test statistic according to these labels. In this way, we are simulating the null hypothesis: whether or not a sample is test or control makes no difference.\nFor this case, we might define our test statistic to be the difference of means, the difference of medians, or anything else that can be computed from the two data sets and has a scalar value.\n\n\n30.4.2 Test and control distributions have the same mean.\nThe null hypothesis here is exactly as I have stated, the test and control distributions have the same mean, and nothing more. To simulate this, we shift the data sets so that they have the same mean. In other words, if the control data are \\(\\mathbf{x}\\) and the test data are \\(\\mathbf{y}\\), then we define the mean of all measurements to be\n\\[\\begin{aligned}\n   \\bar{z} = \\frac{n\\bar{x} + m\\bar{y}}{n+m}.\n\\end{aligned}\n\\]\nThen, we define\n\\[\\begin{aligned}\n   \\mathbf{x}_{\\mathrm{shift}} &= \\mathbf{x} - \\bar{x} + \\bar{z}, \\\\\n   \\mathbf{y}_{\\mathrm{shift}} &= \\mathbf{y} - \\bar{y} + \\bar{z}.\n\\end{aligned}\n\\]\nNow, the data sets \\(\\mathbf{x}_\\mathrm{shift}\\) and \\(\\mathbf{y}_\\mathrm{shift}\\) have the same mean, but everything else about them is the same as \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), respectively. They have exactly the same variance, for example.\nTo simulate the null hypothesis, then, we draw bootstrap samples from \\(\\mathbf{x}_\\mathrm{shift}\\) and \\(\\mathbf{y}_\\mathrm{shift}\\) and compute the test statistic from the bootstrap samples, over and over again.\nIn both of these cases, no assumptions were made about the underlying distributions. Only the empirical distributions were used; these are nonparametric hypothesis tests.",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Null hypothesis significance testing</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/nhst.html#interpretation-of-the-p-value",
    "href": "lessons/nhst/nhst.html#interpretation-of-the-p-value",
    "title": "30  Null hypothesis significance testing",
    "section": "30.5 Interpretation of the p-value",
    "text": "30.5 Interpretation of the p-value\nThe p-value is best interpreted by its definition. It is the probability of observing a test statistic under the null hypothesis that is at least as extreme as what was observed experimentally. That’s it. It is important to interpret a p-value as what it is, and not anything else. Most importantly, it is not the probability that a null hypothesis is true. Even if it small, it is not a confirmation that another hypothesis you have in mind is true.",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Null hypothesis significance testing</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/opinions_about_nhst.html",
    "href": "lessons/nhst/opinions_about_nhst.html",
    "title": "31  Comments and opinions on NHST",
    "section": "",
    "text": "31.1 What does it mean to be significant?\nIf the p-value is small, the effect is said to be statistically significant (hence the term “significance level”). But what is small? What should we choose for the significance level? By choosing a significance level, we draw a line in the sand. On one side of the line is a rejected hypothesis, and on the other an accepted one. I generally discourage a bright line p-value used to deem a result statistically significant or not. You computed the p-value, it has a specific meaning; you should report it. I do not see a need to convert a computed value, the p-value, into a Boolean True/False on whether or not we attach the word “significant” to the result. That is, I do not see a need to make a decision, at least not when doing scientific inquiry.1",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Comments and opinions on NHST</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/opinions_about_nhst.html#do-we-even-want-to-calculate-a-p-value",
    "href": "lessons/nhst/opinions_about_nhst.html#do-we-even-want-to-calculate-a-p-value",
    "title": "31  Comments and opinions on NHST",
    "section": "31.2 Do we even want to calculate a p-value?",
    "text": "31.2 Do we even want to calculate a p-value?\nThe question the NHST and its result, the p-value, address is rarely the question we want to ask. For example, say we are doing a test of the null hypothesis that two sets of measurements have the same mean. In most cases, what we really want is knowledge about the underlying generative distributions of the control and test data. Which of the following two questions better serves that goal?\n\nHow different are the means of the two samples?\nWould we say there is a statistically significant difference of the means of the two samples? Or, more precisely, what is the probability of observing a difference in means of the two samples at least as large as the observed difference in means, if the two samples in fact have the same mean?\n\nThe second question is convoluted and often of little scientific interest. I would say that the first question is much more relevant. To put it in perspective, say we made trillions of measurements of two different samples and their mean differs by one part per million. This difference, though tiny, would still give a low p-value, and therefore often be deemed “statistically significant.” But, ultimately, it is the size of the difference, or the effect size, we care about.",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Comments and opinions on NHST</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/opinions_about_nhst.html#what-is-with-all-those-names",
    "href": "lessons/nhst/opinions_about_nhst.html#what-is-with-all-those-names",
    "title": "31  Comments and opinions on NHST",
    "section": "31.3 What is with all those names?",
    "text": "31.3 What is with all those names?\nYou have no doubt heard of many named hypothesis tests, like the Student-t test, Welch’s t-test, the Mann-Whitney U-test, and countless others. What is with all of those names? It helps to think more generally about how frequentist hypothesis testing is usually done.\nTo do a hypothesis test, people unfortunately do not do what I have laid out here with a resampling-based approach, but typically use a named test. Here is an example of that for a Student-t test (borrowing heavily from the treatment in Phil Gregory’s book).\n\nChoose a null hypothesis. This is the hypothesis you want to test the truth of.\nChoose a suitable test statistic that can be computed from measurements and has a predictable distribution. For the example of two sets of repeated measurements where the null hypothesis is that they come from identical Normal distributions, we can choose as our statistic\n\\[\\begin{aligned}\nT &= \\frac{\\bar{x}_1 - \\bar{x}_2 - (\\mu_1 - \\mu_2)}{S_D\\sqrt{n_1^{-1}+n_2^{-1}}},\\nonumber \\\\[1em]\n\\text{where }S_D^2 &= \\frac{(n_1 - 1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}, \\nonumber \\\\[1em]\n\\text{with } S_1^2 &= \\frac{1}{n_1-1}\\sum_{i\\in \\text{set 1}}(x_{1,i} - \\bar{x}_1)^2, \\nonumber \\\\[1em]\nS_2^2 &= \\frac{1}{n_2-1}\\sum_{i\\in \\text{set 2}}(x_{2,i} - \\bar{x}_2)^2.\n\\end{aligned}\n\\]\nThe T statistic is the difference of the difference of the observed means and the difference of the true means, weighted by the spread in the data. This is a reasonable statistic for determining something about means from data. This is the appropriate statistic when \\(\\sigma_1\\) and \\(\\sigma_2\\) are both unknown but assumed to be equal. (When they are assumed to be unequal, you need to adjust the statistic you use. This test is called Welch’s t-test.) It can be derived that this statistic has the Student-t distribution,\n\\[\\begin{aligned}\nP(t) &= \\frac{1}{\\sqrt{\\pi \\nu}} \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)}\n\\left(1 + \\left(\\frac{t^2}{\\nu}\\right)\\right)^{-\\frac{\\nu + 1}{2}},\\\\[1em]\n\\text{where } \\nu &= n_1+n_2-2.\n\\end{aligned}\\]\nEvaluate the test statistic from measured data. In the case of the Student-t test, we compute \\(T\\).\nPlot \\(P(t)\\). The area under the curve where \\(t &gt; T\\) is the p-value, the probability that we would observe our data under the null hypothesis. Reject the null hypothesis if this is small.\n\nAs you can see from the above prescription, item 2 can be tricky. Coming up with test statistics that also have a distribution that we can write down is difficult2. When such a test statistic is found, the test usually gets a name. The main reason for doing things this way is that most hypothesis tests were developed before computers, so we couldn’t just bootstrap our way through hypothesis tests. (The bootstrap was invented by Brad Efron in 1979.) Conversely, in the approach we have taken, sometimes referred to as “hacker stats,” we can invent any test statistic we want, and we can test it by numerically “repeating” the experiment, in accordance with the frequentist interpretation of probability.\nSo, I would encourage you not to get caught up in names. If someone reports a p-value with a name, simply look up the things you need to define the p-values (the hypothesis being tested, the test statistic, and what it means to be as extreme), and that will give you an understanding of what is going on with the test.\nThat said, many of the tests with names have analytical forms and can be rapidly computed. Most are included in the scipy.stats module. I have chosen to present a method of hypothesis testing that is intuitive with the frequentist interpretation of probability front and center. It also allows you to design your own tests that fit a null hypothesis that you are interested in that might not be “off-the-shelf.” That said, implementations of the named tests are almost always much faster to compute than using resampling methods, so if you have large data sets or computational speed is a consideration for any other reasons, you might want to consider using functions in scipy.stats.\n\n31.3.1 Warnings about hypothesis tests\nThere are many. I will discuss them more when I talk about “statistical pitfalls” later in the course.\n\nAn effect being statistically significant does not mean the effect is significant in practice or even important. It only means exactly what it is defined to mean: an effect is unlikely to have happened by chance under the null hypothesis. Far more important is the effect size.\nThe p-value is not the probability that the null hypothesis is true. It is the probability of observing the test statistic being at least as extreme as what was measured if the null hypothesis is true. I.e., if \\(H_0\\) is the null hypothesis,\n\\[\\begin{aligned}\n    \\text{p-value} = P(\\text{test stat at least as extreme as observed}\\mid H_0).\n\\end{aligned}\n\\]\nIt is not the probability that the null hypothesis is true given that the test statistic was at least as extreme as the data.\n\\[\\begin{aligned}\n    \\text{p-value} \\ne P(H_0\\mid\\text{test stat at least as extreme as observed}).\n\\end{aligned}\n\\]\nWe often actually want the probability that the null hypothesis is true, and the p-value is often erroneously interpreted as this (even though it does not even make sense under the frequentist interpretation of probability) to great peril.\nNull hypothesis significance testing does not say anything about alternative hypotheses. Rejection of the null hypothesis does not mean acceptance of any other hypotheses.\nP-values are not very reproducible, as we will see when we do the “dance of the p-values.”\nRejecting a null hypothesis is also kind of odd, considering you computed\n\\[\\begin{aligned}\nP(\\text{test stat at least as extreme as observed}\\mid H_0).\n\\end{aligned}\n\\]\nThis does not really describe the probability that the hypothesis is true. This, along with point 4, means that the p-value better be really low for you to reject the null hypothesis.\nThroughout the literature, you will see null hypothesis testing when the null hypothesis is not relevant at all. People compute p-values because that’s what they think they are supposed to do. Again, it gets to the point that effect size is waaaaay more important than a null hypothesis significance test.\n\nGiven all these problems with p-values, based on my experience, I would advocate for their abandonment. I am not the only one. They seldom answer the question scientists are asking and lead to great confusion.\nThat said, I know they are widely and effectively used in contexts I am less familiar with. It’s foolish to make strong, blanket statements (like I just did), so consider this a tempering of my advocacy for abandonment.",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Comments and opinions on NHST</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/opinions_about_nhst.html#footnotes",
    "href": "lessons/nhst/opinions_about_nhst.html#footnotes",
    "title": "31  Comments and opinions on NHST",
    "section": "",
    "text": "You may need to make a decision in other contexts, such as clinical settings when deciding treatment. In these cases, though, the results of a NHST are often not the criteria upon which you want to make decisions.↩︎\nI am not going to discuss other important considerations that go into choice of test statistics, especially when you want to make an accept/reject decision, such as power and false positive rates.↩︎",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Comments and opinions on NHST</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/hacker_nhst.html",
    "href": "lessons/nhst/hacker_nhst.html",
    "title": "32  Hacker’s approach to NHST",
    "section": "",
    "text": "32.1 Permutation hypothesis tests\n| Download notebook\nDataset download\nIn this lesson, we will perform null hypothesis significance tests (NHST) using the drone sperm quality data set. Of course, we need to load in the data set first.\nBecause we will be using Numba’d functions to rapidly do our sampling, we will go ahead and slice out the subsets of the data frame we will use for inference as Numpy arrays. We will use the specimens that have a nonzero sperm count (including both alive and dead sperm).\nFinally, so we have them available, we will define some of the utility functions we defined earlier when working with hacker stats.\nRecall the steps for performing a NHST.\nFor one special type of hypothesis, there is a very straightforward way of simulating it. Here is our hypothesis: the control and pesticide-treated samples have exactly the same distribution. To simulate this, we take the following steps for two data sets, a control set with \\(n\\) measurements and a test set the other with \\(m\\).\nThis simulation is exact; it is as if the label of the data set has no meaning; hence the distributions of the two data sets are entirely equal. We test such a null hypothesis with a permutation test. A permutation sample is akin to a bootstrap sample; it is a new pair of data sets generated after scrambling the concatenated data set. A permutation replicate is a value of the test statistic computed from a permutation sample. For this test, we will use the difference of means as the test statistic.\nLet’s code this up. We can write a function to draw a permutation sample, and then a function to draw permutation replicates for an arbitrary user-supplied summary statistic.\n@numba.njit\ndef draw_perm_sample(x, y):\n    \"\"\"Generate a permutation sample.\"\"\"\n    concat_data = np.concatenate((x, y))\n    np.random.shuffle(concat_data)\n    \n    return concat_data[:len(x)], concat_data[len(x):]\n\n\ndef draw_perm_reps(x, y, stat_fun, size=1):\n    \"\"\"Generate array of permuation replicates.\"\"\"\n    return np.array([stat_fun(*draw_perm_sample(x, y)) for _ in range(size)])\nSince we will look at the difference of means specifically, we will also code up a custom function to draw replicates of the difference of means.\n@numba.njit(parallel=True)\ndef draw_perm_reps_diff_mean(x, y, size=1):\n    \"\"\"Generate array of permuation replicates.\"\"\"\n    out = np.empty(size)\n    for i in numba.prange(size):\n        x_perm, y_perm = draw_perm_sample(x, y)\n        out[i] = np.mean(x_perm) - np.mean(y_perm)\n\n    return out\nTo perform the hypothesis test, then, with the difference of means as our test statistic, we have only to draw many replicates and then tally up how many of them are more extreme than the observed difference of means.\n# Compute test statistic for original data set\ndiff_mean = np.mean(alive_ctrl) - np.mean(alive_pest)\n\n# Draw replicates\nperm_reps = draw_perm_reps_diff_mean(alive_ctrl, alive_pest, size=10_000)\n\n# Compute p-value\np_val = np.sum(perm_reps &gt;= diff_mean) / len(perm_reps)\n\nprint('p-value =', p_val)\n\np-value = 0.0\n\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\nWhoa! Wait a minute. The p-value is zero? This just means that in all of the 10,000 replicates we took, not one had a test statistic as extreme as was observed. We cannot resolve p-values much less than 0.001 with 10,000 permutation replicates. Let’s try taking ten million replicates as see how we do. This will take a few seconds to run (which again highlights the utility of using Numpy arrays with Numba).\n# Compute test statistic for original data set\ndiff_mean = np.mean(alive_ctrl) - np.mean(alive_pest)\n\n# Draw replicates\nperm_reps = draw_perm_reps_diff_mean(alive_ctrl, alive_pest, size=10_000_000)\n\n# Compute p-value\np_val = np.sum(perm_reps &gt;= diff_mean) / len(perm_reps)\n\nprint('p-value =', p_val)\n\np-value = 3.41e-05\nSo, our p-value is quite small, less than \\(10^{-4}\\). This means that the probability of getting a difference of means as extreme as was observed under the null hypothesis that the control and test samples were drawn from identical distribution is quite small.",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Hacker's approach to NHST</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/hacker_nhst.html#permutation-hypothesis-tests",
    "href": "lessons/nhst/hacker_nhst.html#permutation-hypothesis-tests",
    "title": "32  Hacker’s approach to NHST",
    "section": "",
    "text": "Clearly state the null hypothesis.\nDefine a test statistic, a scalar value that you can compute from data. Compute it directly from your measured data.\nSimulate data acquisition for the scenario where the null hypothesis is true. Do this many times, computing and storing the value of the test statistic each time.\nThe fraction of simulations for which the test statistic is at least as extreme as the test statistic computed from the measured data is called the p-value, which is what you report.\n\n\n\nConcatenate the two data sets into one.\nRandomly scramble the order of the combined data set.\nDesignate the first \\(n\\) entries in this scrambled array to be “control” and the remaining to be “test.”",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Hacker's approach to NHST</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/hacker_nhst.html#permutation-hypothesis-test-for-correlation",
    "href": "lessons/nhst/hacker_nhst.html#permutation-hypothesis-test-for-correlation",
    "title": "32  Hacker’s approach to NHST",
    "section": "32.2 Permutation hypothesis test for correlation",
    "text": "32.2 Permutation hypothesis test for correlation\nAs we continue to explore the data set, we might want to consider a hypothesis: dead and alive sperm counts are uncorrelated. It is possible that under this hypothesis, we could see correlation at the same level observed in the real data. We can test this hypothesis by permutation. In this case, we scramble the labels “dead” and “alive” and see what correlation we get. We will do 10 million permutation replicates. This will take a while to compute.\n\n# Compute bivariate r for actual data\nr_ctrl = bivariate_r(np.log(alive_ctrl), np.log(dead_ctrl))\nr_pest = bivariate_r(np.log(alive_pest), np.log(dead_pest))\n\n# Get permutation replicates\nperm_reps_ctrl = draw_perm_reps(\n    np.log(alive_ctrl), np.log(dead_ctrl), bivariate_r, size=10_000_000\n)\nperm_reps_pest = draw_perm_reps(\n    np.log(alive_pest), np.log(dead_pest), bivariate_r, size=10_000_000\n)\n\n# Compute p-value\np_ctrl = np.sum(np.abs(perm_reps_ctrl) &gt; np.abs(r_ctrl)) / len(perm_reps_ctrl)\np_pest = np.sum(np.abs(perm_reps_pest) &gt; np.abs(r_pest)) / len(perm_reps_pest)\n\nprint(\n    \"\"\"\np-value control:   {0:.3e}\np-value pesticide: {1:.3e}\n\"\"\".format(\n        p_ctrl, p_pest\n    )\n)\n\n\np-value control:   0.000e+00\np-value pesticide: 1.600e-06\n\n\n\nNote that here we used the absolute value of the permutation replicates. We did this because we are interested to see if we get correlation at all, including anticorrelation. This is another essential piece of a hypothesis test: you need to define what it means to be more extreme.\nThe p-value for both is tiny; not one in 10 million permutations for the control sample gave a correlation as high as was observed, and only 10 or so of the pesticide treated group. So, we would report something like \\(p &lt; 10^{-6}\\) for control and \\(p &lt; 10^{-5}\\) for pesticide.",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Hacker's approach to NHST</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/hacker_nhst.html#bootstrap-hypothesis-tests",
    "href": "lessons/nhst/hacker_nhst.html#bootstrap-hypothesis-tests",
    "title": "32  Hacker’s approach to NHST",
    "section": "32.3 Bootstrap hypothesis tests",
    "text": "32.3 Bootstrap hypothesis tests\nPermutation tests are great: they exactly simulate the null hypothesis that the two samples come from the same generative distribution. But they are limited to this specific null hypothesis. What if we had a different hypothesis, say only that the means of the two distributions we are comparing are equal, but other properties of the generative distributions need not be identical? In this case, we cannot use the permutations to simulate the null hypothesis.\nInstead, we can simulate the null hypothesis by shifting the means of the control and test distributions so that they are equal. We then take a bootstrap sample out of each of the shifted data sets. We compute our test statistic from these two bootstrap samples to get a bootstrap replicate. Then, the number of bootstrap replicates that are at least as extreme as the test statistic computed from the original data is used to compute the p-value. Let’s see this in action. First, we’ll see how to shift the data sets and what the resulting ECDFs look like.\n\n# Shift data sets\ntotal_mean = np.mean(np.concatenate((alive_ctrl, alive_pest)))\nalive_ctrl_shift = alive_ctrl - np.mean(alive_ctrl) + total_mean\nalive_pest_shift = alive_pest - np.mean(alive_pest) + total_mean\n\n# Plot the ECDFs\ndf_shift = pl.DataFrame(\n    data={\n        \"treatment\": [\"control\"] * len(alive_ctrl_shift)\n        + [\"pesticide\"] * len(alive_pest_shift),\n        \"alive sperm (millions)\": np.concatenate((alive_ctrl_shift, alive_pest_shift)),\n    }\n)\np = iqplot.ecdf(df_shift, q=\"alive sperm (millions)\", cats=\"treatment\")\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe distributions now have the same mean, but nothing else about them has changed. They still have the same shape as before. Now, let’s draw bootstrap samples out of these shifted distributions and see how they compare to the original ECDFs.\n\np = iqplot.ecdf(\n    df,\n    q=\"Alive Sperm Millions\",\n    cats=\"Treatment\",\n    x_axis_label=\"alive sperm (millions)\",\n)\n\nfor _ in range(100):\n    ctrl_rep = draw_bs_sample(alive_ctrl_shift)\n    pest_rep = draw_bs_sample(alive_pest_shift)\n    df_rep = pl.DataFrame(\n        data={\n            \"Treatment\": [\"Control\"] * len(ctrl_rep) + [\"Pesticide\"] * len(pest_rep),\n            \"Alive Sperm Millions\": np.concatenate((ctrl_rep, pest_rep)),\n        }\n    )\n\n    p = iqplot.ecdf(\n        df_rep,\n        q=\"Alive Sperm Millions\",\n        cats=\"Treatment\",\n        p=p,\n        line_kwargs=dict(alpha=0.02),\n        show_legend=False,\n    )\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe blue-orange haze in between the ECDFs of the original data are the bootstrap samples under the null hypothesis. Only on rare occurrence does that haze reach the original ECDFs, so we might suspect that the observed data might not be all that probable under the null hypothesis. Let’s perform the hypothesis test.\n\n@numba.njit(parallel=True)\ndef draw_bs_reps_diff_mean(x, y, size=1):\n    \"\"\"\n    Generate bootstrap replicates with difference of means\n    as the test statistic.\n    \"\"\"\n    out = np.empty(size)\n    for i in numba.prange(size):\n        out[i] = np.mean(draw_bs_sample(x)) - np.mean(draw_bs_sample(y))\n\n    return out\n\n\n# Generate samples (10 million again)\nbs_reps = draw_bs_reps_diff_mean(alive_ctrl_shift, alive_pest_shift, size=10_000_000)\n\n# Compute p-value\np_val = np.sum(bs_reps &gt;= diff_mean) / len(bs_reps)\n\nprint(\"p-value =\", p_val)\n\np-value = 6.7e-06\n\n\nThis p-value is of similar magnitude as what we got from the permutation test.",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Hacker's approach to NHST</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/hacker_nhst.html#simulating-a-hypothesis-challenging-and-rewarding",
    "href": "lessons/nhst/hacker_nhst.html#simulating-a-hypothesis-challenging-and-rewarding",
    "title": "32  Hacker’s approach to NHST",
    "section": "32.4 Simulating a hypothesis: challenging and rewarding",
    "text": "32.4 Simulating a hypothesis: challenging and rewarding\nAs you can see from the above examples with permutation and bootstrap hypothesis tests, the part of the procedure that requires the most thought and care is simulating the null hypothesis. For a test where we are comparing means, we needed to shift the distributions so that all other aspects of the distribution remain the same, and then resample from the shifted distribution.\nThe strength of doing these kinds of hypothesis tests is that they are entirely nonparametric. There is no need to specify a probability distribution for the null hypothesis; you can just make statements about how the two distributions you are comparing are related.",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Hacker's approach to NHST</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/hacker_nhst.html#computing-environment",
    "href": "lessons/nhst/hacker_nhst.html#computing-environment",
    "title": "32  Hacker’s approach to NHST",
    "section": "32.5 Computing environment",
    "text": "32.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,numba,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\npolars    : 1.33.1\nnumba     : 0.61.2\nbokeh     : 3.8.0\niqplot    : 0.3.7\njupyterlab: 4.4.9",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Hacker's approach to NHST</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/nhst_lesson_exercise.html",
    "href": "lessons/nhst/nhst_lesson_exercise.html",
    "title": "33  NHST lesson exercises",
    "section": "",
    "text": "33.1 Exercise 1\n| Download notebook\nWhat is a p-value?",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>NHST lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/nhst_lesson_exercise.html#exercise-2",
    "href": "lessons/nhst/nhst_lesson_exercise.html#exercise-2",
    "title": "33  NHST lesson exercises",
    "section": "33.2 Exercise 2",
    "text": "33.2 Exercise 2\nWhat three things need to be clearly specified to define a null hypothesis significance test?",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>NHST lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/nhst_lesson_exercise.html#exercise-3",
    "href": "lessons/nhst/nhst_lesson_exercise.html#exercise-3",
    "title": "33  NHST lesson exercises",
    "section": "33.3 Exercise 3",
    "text": "33.3 Exercise 3\nWhat is the null hypothesis of a permutation test (specifically the kind of permutation tests we have discussed in the preceding lessons)?",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>NHST lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/nhst/nhst_lesson_exercise.html#exercise-4",
    "href": "lessons/nhst/nhst_lesson_exercise.html#exercise-4",
    "title": "33  NHST lesson exercises",
    "section": "33.4 Exercise 4",
    "text": "33.4 Exercise 4\nWrite down any questions or points of confusion that you have.",
    "crumbs": [
      "Null hypothesis significance testing",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>NHST lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/modeling/why_models.html",
    "href": "lessons/modeling/why_models.html",
    "title": "Generative modeling",
    "section": "",
    "text": "As scientists, our goal is to learn about how nature works. We can make observations and measurements, and these are usually in the service of gaining an understanding of how nature works. In their book Physical Biology of the Cell, Phillips, et al. write that “…quantitative data demand quantitative models and, conversely, …quantitative models need to provide experimentally testable quantitative predictions about biological phenomena.” It is in this spirit that we approach parametric inference. Our goal is to model the process of data generation, and using the measured data to learn about the model. This leads to knowledge.",
    "crumbs": [
      "Generative modeling"
    ]
  },
  {
    "objectID": "lessons/modeling/statistical_modeling.html",
    "href": "lessons/modeling/statistical_modeling.html",
    "title": "34  Statistical modeling",
    "section": "",
    "text": "34.1 Cartoon model.\nAs scientists, we often have in mind a generative process by which the data we measure are produced. For example, we might expect the optical density of a solution of E. coli in LB media to grow exponentially over time, with some small measurement error. To model this, we specify a model for how bacteria grow and a probability distribution to describe the variation in the measurements. We can then use the data and statistical inference to learn something about the parameters in the model.\nYou may have noticed the terms \"cartoon model,\" \"mathematical model,\" and \"statistical model\" in the cycle of science we introduced in the first lecture.\nBeing biologists who are doing data analysis, the word \"model\" is used to mean at least three different things in our work (neglecting \"model organism,\" \"model in vitro system,\" etc.). So, for the purposes of this course, we need to clearly define what we are talking about when we use the word \"model.\"\nThese models are the typical cartoons we see in text books or in discussion sections of biological papers. They are a sketch of what we think might be happening in a system of interest, but they do not provide quantifiable predictions.",
    "crumbs": [
      "Generative modeling",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Statistical modeling</span>"
    ]
  },
  {
    "objectID": "lessons/modeling/statistical_modeling.html#mathematical-model.",
    "href": "lessons/modeling/statistical_modeling.html#mathematical-model.",
    "title": "34  Statistical modeling",
    "section": "34.2 Mathematical model.",
    "text": "34.2 Mathematical model.\nThese models give quantifiable predictions that must be true if the hypothesis (which is sketched as a cartoon model) is true. In many cases, getting to predictions from a hypothesis is easy. For example, if I hypothesize that protein A binds protein B, a quantifiable prediction would be that they are colocalized when I image them. However, sometimes harder work and deeper thought is needed to generate quantitative predictions. This often requires \"mathematizing\" the cartoon. This is how a mathematical model is derived from a cartoon model. Oftentimes when biological physicists refer to a \"model,\" they are talking about what we are calling a mathematical model. In the bacterial growth example, the mathematical model is that they grow exponentially.",
    "crumbs": [
      "Generative modeling",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Statistical modeling</span>"
    ]
  },
  {
    "objectID": "lessons/modeling/statistical_modeling.html#statistical-model.",
    "href": "lessons/modeling/statistical_modeling.html#statistical-model.",
    "title": "34  Statistical modeling",
    "section": "34.3 Statistical model.",
    "text": "34.3 Statistical model.\nA statistical model goes a step beyond the mathematical model and uses a probability distribution to describe any measurement error, or stochastic noise in the system being measured. If \\(y\\) is the data set and \\(\\theta\\) is a set of parameters involved in generating the data, this essentially means specifying \\(f(y; \\theta)\\) (and \\(g(\\theta)\\) in the Bayesian case, which we will get to next term). The statistical models we will use are generative in that the encompass the cartoon and mathematical models and any noise to use probability to describe how the data are generated.",
    "crumbs": [
      "Generative modeling",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Statistical modeling</span>"
    ]
  },
  {
    "objectID": "lessons/modeling/building_a_generative_model.html",
    "href": "lessons/modeling/building_a_generative_model.html",
    "title": "35  Building a generative model",
    "section": "",
    "text": "35.1 Examples of generative models\nThe process of model building usually involves starting with a cartoon model, mathematizing it, and the forming that into a statistical model to model noise in measurement. Sometimes this process is very simple, and sometimes it involves careful and difficult modeling.\nIt is often easiest to learn from example. I present here some examples of how we might come up with generative models.",
    "crumbs": [
      "Generative modeling",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Building a generative model</span>"
    ]
  },
  {
    "objectID": "lessons/modeling/building_a_generative_model.html#examples-of-generative-models",
    "href": "lessons/modeling/building_a_generative_model.html#examples-of-generative-models",
    "title": "35  Building a generative model",
    "section": "",
    "text": "35.1.1 The size of eggs laid by C. elegans\nThe experiment here is repeated measurements of the length of eggs laid by C. elegans worms. We do not pretend to know much about how the process of egg generation sets its length. Surely many processes are involved, and we choose to model the egg length as being Normally distributed, as this story roughly matches what we would expect. We further assume that the length of each egg we measure is independent of all of the other eggs we measure, and further that the distribution we use to describe the egg length of any given egg is the same as any other. That is to say that the egg lengths are independent and identically distributed, abbreviated i.i.d.\nWe can then write down the probability density function for the length of egg \\(i\\), \\(y_i\\), as\n\\[\\begin{align}\nf(y_i; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\,\\mathrm{e}^{-(y_i-\\mu)^2/2\\sigma^2},\n\\end{align}\\]\nwhich is the PDF for a Normal distribution. Since each measurement is independent, the PDF for the joint distribution of all measurements, \\(\\mathbf{y} = \\{y_1, y_2, \\ldots, y_n\\}\\), is given by the product of the PDFs of the individual measurements.\n\\[\\begin{aligned}\n\\begin{align}\n   f(\\mathbf{y}; \\mu, \\sigma) &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\,\\mathrm{e}^{-(y_i-\\mu)^2/2\\sigma^2} \\\\\n   &= \\left(\\frac{1}{2\\pi \\sigma^2}\\right)^{n/2}\\,\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\mu)^2\\right].\n\\end{align}\n\\end{aligned}\\]\nThe PDF has all of the information for the generative model. Importantly, the statistical model dictates what parameters you are trying to estimate. In this case, there are two parameters, \\(\\mu\\) and \\(\\sigma\\). The generative model tells us that we can infer the characteristic egg length \\(\\mu\\) and the variance, \\(\\sigma^2\\).\nIn this, model, we skipped through the cartoon model, through mathematization, and went directly to the generative statistical model, since the former two models are trivial.\n\n35.1.1.1 Short-hand model definition\nWriting out the mathematical expression for the PDF can be cumbersome, even for a relatively simple model like we have here. In English, the model is, “The egg lengths are i.i.d. and are Normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).” A shorthand for this is\n\\[\\begin{align}\ny_i \\sim \\text{Norm}(\\mu, \\sigma) \\;\\forall i.\n\\end{align}\\]\nThis is read just like the English sentence describing the model. The tilde symbol means “is distributed as.”\n\n\n\n35.1.2 The amount of time before microtubule catastrophe\nIn your homework, you have already built a model for the time to microtubule catastrophe. We started with a story: Catatstrophe occurs after the arrival of two different successive Poisson processes. The story here is the cartoon model. You derived the probability distribution function for the time it takes for a single catastrophe.\n\\[\\begin{align}\nf(t_i;\\beta_1, \\beta_2) = \\frac{\\beta_1 \\beta_2}{\\beta_2 - \\beta_1}\\left(\\mathrm{e}^{-\\beta_1 t_i} - \\mathrm{e}^{-\\beta_2 t_i}\\right),\n\\end{align}\\]\nwhere we have implicitly assumed that \\(\\beta_1 \\ne \\beta_2\\). We could explicitly model some errors in measurement of catastrophe times, but the experiment is quite clean. It is obvious from the images when catastrophe occurs, so the mathematical model leads directly to the generative statistical model.\nIf we again model the catastrophe events as i.i.d., we can write the joint PDF for a set of measured catastrophe times \\(\\mathbf{t} = \\{t_1, t_2, \\ldots, t_n\\}\\).\n\\[\\begin{align}\nf(\\mathbf{t};\\beta_1, \\beta_2) = \\left(\\frac{\\beta_1 \\beta_2}{\\beta_2 - \\beta_1}\\right)^n\\prod_{i=1}^n\\left(\\mathrm{e}^{-\\beta_1 t_i} - \\mathrm{e}^{-\\beta_2 t_i}\\right).\n\\end{align}\\]\nThis model is more difficult to write in shorthand, but we can.\n\\[\\begin{aligned}\n\\begin{align}\n&t'_i \\sim \\text{Expon}(\\beta_1) \\;\\forall i,\\\\\n&t_i - t'_i \\sim \\text{Expon}(\\beta_2) \\;\\forall i.\n\\end{align}\n\\end{aligned}\\]\nNote that this construction of the model has a latent variable, \\(t_i'\\), a random variable that we can define in the model, but we cannot measure.\n\n35.1.2.1 An alternative model for microtubule catastrophe\nAs an alternative model, we may consider the case where catastrophe is itself a Poisson process (or triggered by the arrival of a single Poisson process). In that case, our model is simpler.\n\\[\\begin{align}\n&t_i \\sim \\text{Expon}(\\beta) \\;\\forall i.\n\\end{align}\\]\n\n\n\n35.1.3 The change in bacterial mass over time\nYou may be familiar with exponential microbial growth. When you put a single cell in growth media, it divides, and then you have two. Those two cells then grow and divide, giving four cells. This continues, and the number of cells grows exponentially with time.\nIn an interesting paper (PNAS, 2014), Iyer-Biswas and coworkers addressed the question of whether or not a single cell exhibits exponential growth (not to be confused with the Exponential distribution). That is, right after a division, does the total mass of a cell grow exponentially before dividing? Even if individual cells grow linearly, in bulk growth will still appear exponential, so we cannot really tell from a growth experiment.\nTheir clever experimental set-up allows imaging of single dividing cells in conditions that are identical through time. This is accomplished by taking advantage of a unique morphological feature of Caulobacter. The mother cell is adherent to the a surface through its stalk. Upon division, one of the daughter cells does not have a stalk and is mobile. The system is part of a microfluidic device that gives a constant flow. So, every time a mother cell divides, the un-stalked daughter cell gets washed away. In such a way, the dividing cells are never in a crowded environment and the buffer is always fresh. Using microscopy and image processing, they have many curves measuring the areas of cells in images (assumed to be proportional to the cell mass), starting from a single mother cell with its growth to division, to assess growth models. The data look like this:\n\n\n\n\n\n\nFigure 35.1: Area of a single Caulobacter cell over time. Jumps from high to low size indicate division events.\n\n\n\nWe can consider two models for growth of an individual cell, linear growth and exponential growth.\n\n35.1.3.1 Linear growth\nWe will start with linear growth; stating that the growth is linear is the cartoon model. More precisely, we model bacterial growth as a constant process for each bacterium; it grows at the same rate regardless of bacterial mass. We can mathematize our model as\n\\[\\begin{align}\na(t) = a^0 + b t,\n\\end{align}\\]\nwhere \\(a(t)\\) is the area of the bacterium over time, and \\(t\\) is the time since the last cell division. So, we now have our mathematical model. The growth rate is \\(b\\), and the area immediately after the last cell division is \\(a^0\\).\nFor the statistical model, we need to model error in measurement. The idea is that the cell grows according to the above equation, but there will be some natural stochastic variation away from that curve. Furthermore, there are errors in measurement for the area at each time point. (We assume that we can measure the time exactly without error.) Thus, the measured area \\(a_i\\) for a bacterium at time point \\(t_i\\) is\n\\[\\begin{align}\na_i = a^0 + b t_i + e_i,\n\\end{align}\\]\nwhere \\(e_i\\) is the variation in the measurement from the mathematical model, called a residual. To complete the statistical model, we need to specify how \\(e_i\\) is distributed, and also the relationship between different time points. We first consider the latter. In time series analysis, the value (in this case the area) at time point \\(t_{i+1}\\) may be influenced by some memory process by the value at time point \\(t_i\\). Nonetheless, we often model measurements at different time points as i.i.d., only being connected with those at previous times by virtue of the fact that there is explicit time dependence in the mathematical model. This is typically a reasonable assumption, as many processes are memoryless.\nGiven that the measurements are i.i.d., we can model the residual, \\(e_i\\). This is commonly modeled as Normal with mean zero and some finite variance. If that variance is the same for all time points, the residuals are said to be homoscedastic. If the variance changes over time, we have heteroscedasticity. So, if we assume homoscedastic error, we could write\n\\[\\begin{align}\nf(e_i;\\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\mathrm{e}^{-e_i^2/2\\sigma^2}\\;\\forall i.\n\\end{align}\\]\nWe can then write the PDF for the joint distribution of all of the measured data, \\((\\mathbf{t}, \\mathbf{a})\\),\n\\[\\begin{align}\nf(\\mathbf{a};\\mathbf{t},a^0, b, \\sigma) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2}\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(a_i - a^0-bt_i)^2\\right].\n\\end{align}\\]\nIt is convenient to write this in shorthand.\n\\[\\begin{aligned}\n\\begin{align}\n&a_i = a^0 + b t_i + e_i \\;\\forall i,\\\\\n&e_i \\sim \\text{Norm}(0, \\sigma)\\;\\forall i,\n\\end{align}\n\\end{aligned}\\]\nor, equivalently,\n\\[\\begin{align}\na_i \\sim \\text{Norm}(a^0 + b t_i, \\sigma)\\;\\forall i.\n\\end{align}\\]\n\n\n35.1.3.2 Exponential growth\nWe can use exactly the same logic as above to write the model for Exponential growth.\n\\[\\begin{aligned}\n\\begin{align}\n&a_i = a^0 \\mathrm{e}^{kt} + e_i \\;\\forall i,\\\\\n&e_i \\sim \\text{Norm}(0, \\sigma)\\;\\forall i,\n\\end{align}\n\\end{aligned}\\]\nor, equivalently,\n\\[\\begin{align}\na_i \\sim \\text{Norm}(a^0 \\mathrm{e}^{kt}, \\sigma)\\;\\forall i.\n\\end{align}\\]\n\n\n35.1.3.3 Variate-covariate models\nThese models for bacterial growth are examples of variate-covariate models. The covariate is the time measurements, assumed to be known exactly. The variate, that which varies with the covariate, is the bacterial area. More generally, covariates are measured quantities that affect the measured value of a variate. In the first model, the variate depends linearly on the covariate, and in the second model, the variate depends exponentially on the covariate.",
    "crumbs": [
      "Generative modeling",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Building a generative model</span>"
    ]
  },
  {
    "objectID": "lessons/modeling/building_a_generative_model.html#important-notes-on-generative-modeling",
    "href": "lessons/modeling/building_a_generative_model.html#important-notes-on-generative-modeling",
    "title": "35  Building a generative model",
    "section": "35.2 Important notes on generative modeling",
    "text": "35.2 Important notes on generative modeling\nIn the three example models presented here, we used our best scientific and statistical insights to put forward a generative model. The model for linear growth of bacteria is in some sense “standard,” in that it leads to linear regression, a widely-used statistical tool. Nonetheless, your modeling should be bespoke. You should choose models that are appropriate for the experiment and data you are analyzing.",
    "crumbs": [
      "Generative modeling",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Building a generative model</span>"
    ]
  },
  {
    "objectID": "lessons/mle/intro_mle.html",
    "href": "lessons/mle/intro_mle.html",
    "title": "Parameter estimation",
    "section": "",
    "text": "Once we have written down our statistical model, we would like to get estimates for the parameters of the model. By far the most often used method for parameter estimation is maximum likelihood estimation. The second-most widely used is probably the method of moments, which we will first briefly discuss.",
    "crumbs": [
      "Parameter estimation"
    ]
  },
  {
    "objectID": "lessons/mle/method_of_moments.html",
    "href": "lessons/mle/method_of_moments.html",
    "title": "36  Method of moments",
    "section": "",
    "text": "36.1 Definition of method of moments\nParameter estimates from the method of moments do not have the same desirable properties as those from maximum likelihood estimation (which we will describe in the next section), so estimates from the method of moments are not as useful in practice in parametric models. They are, however, useful for getting initial guesses for numerical maximum likelihood estimates. For this reason, and because the method of moments is still fairly widely used (though far less than MLE) and you should be familiar with it, I will discuss it here.\nI first introduce the method of moments formally, and then demonstrate with examples.\nFor ease of discussion, we will consider a model of a set of \\(n\\) i.i.d. measurements drawn from a generative distribution with probability density function \\(f(x;\\theta)\\), where \\(\\theta\\) are the parameters. The mth moment of the distribution is\n\\[\\begin{aligned}\n   \\langle x^m \\rangle = \\int \\mathrm{d}x\\,x^m f(x;\\theta).\n\\end{aligned}\\]\nIn the case where \\(x\\) is discrete, the mth moment is\n\\[\\begin{aligned}\n   \\langle x^m \\rangle = \\sum_x\\,x^m f(x;\\theta).\n\\end{aligned}\\]\nThe moments will in general be functions of the parameters \\(\\theta\\).\nRecalling our discussion of plug-in estimates of linear statistical functionals, the plug-in estimate for the mth moment is\n\\[\\begin{aligned}\n   \\widehat{\\langle x^m \\rangle} = \\frac{1}{n}\\sum_{i=1}^n\\,x_i^m,\n\\end{aligned}\\]\nwhere \\(x_i\\) is one of the \\(n\\) empirical measurements.\nIn the method of moments, we equate the plug-in estimates for moments with the analytical expressions for the moments derived from the model generative distribution and then solve for \\(\\theta\\) to get our estimates. This is best seen in practice.",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Method of moments</span>"
    ]
  },
  {
    "objectID": "lessons/mle/method_of_moments.html#examples-of-method-of-moments",
    "href": "lessons/mle/method_of_moments.html#examples-of-method-of-moments",
    "title": "36  Method of moments",
    "section": "36.2 Examples of method of moments",
    "text": "36.2 Examples of method of moments\nWe will demonstrate the method of moments with a few examples.\n\n36.2.1 Estimates for an Exponential distribution\nFor an Exponential distribution, the probability density function is\n\\[\\begin{aligned}\n   f(x;\\beta) = \\beta\\,\\mathrm{e}^{-\\beta x}.\n\\end{aligned}\\]\nThere is a single parameter, \\(\\beta\\). The first moment of the Exponential distribution is \\(\\langle x \\rangle = 1/\\beta\\), which may be computed by evaluating the integral\n\\[\\begin{aligned}\n   \\int_0^\\infty \\mathrm{d}x\\;x\\,f(x;\\beta) = \\int_0^\\infty \\mathrm{d}x\\;x\\,\\beta\\,\\mathrm{e}^{-\\beta x} = 1/\\beta,\n\\end{aligned}\\]\nbut is more easily looked up in the Distribution Explorer. The plug-in estimate for the first moment is\n\\[\\begin{aligned}\n   \\widehat{\\langle x \\rangle} = \\frac{1}{n}\\sum_{i=1}^n\\,x_i = \\bar{x}.\n\\end{aligned}\\]\nEquating the moment from the model generative distribution and the plug-in estimate gives \\(\\beta = 1/\\bar{x}\\), which is our estimate by method of moments for the parameter \\(\\beta\\).\n\n\n36.2.2 Estimates for a Normal distribution\nWe now turn to the Normal distribution. We will again use the first moment, but we need to use a second moment to get a second equation, since the Normal distribution has two parameters, \\(\\mu\\) and \\(\\sigma\\). It is often the case that it is easier to compare the variance and the plug-in estimate for the variance instead of the second moment. Note that the variance can be expressed in terms of the first and second moments,\n\\[\\begin{aligned}\n   \\sigma^2 = \\langle x^2 \\rangle - \\langle x \\rangle^2,\n\\end{aligned}\\]\nso that even though we are comparing the variance and the plug-in estimate for the variance, we are still doing method of moments.\nThe first moment and variance of a Normal distribution are \\(\\mu\\) and \\(\\sigma^2\\), respectively. Equating them with the plug-in estimates gives our method-of-moments estimates for the parameters, \\(\\mu = \\bar{x}\\) and \\(\\sigma = \\hat{\\sigma}\\), where \\(\\hat{\\sigma}\\) is the plug-in estimate for the variance.\n\n\n36.2.3 Estimates for a Gamma distribution\nYou may have noticed that for the Exponential and Normal distributions, the method of moments gives the MLE. This is not generally the case. As a counter example, consider a Gamma distribution, which has probability density function\n\\[\\begin{aligned}\n   f(x;\\alpha, \\beta) = \\frac{1}{\\Gamma(\\alpha)}\\,\\frac{(\\beta x)^\\alpha}{x}\\,\\mathrm{e}^{-\\beta x},\n\\end{aligned}\\]\nwith two parameters, \\(\\alpha\\) and \\(\\beta\\). Equating the first moment and variance with their respective plug-in estimates gives\n\\[\\begin{aligned}\n\\begin{aligned}\n   &\\frac{\\alpha}{\\beta} = \\bar{x},\\\\[1em]\n   &\\frac{\\alpha}{\\beta^2} = \\hat{\\sigma}^2.\n\\end{aligned}\n\\end{aligned}\\]\nSolving for \\(\\alpha\\) and \\(\\beta\\) yields our method-of-moments estimates for the parameters,\n\\[\\begin{aligned}\n\\begin{aligned}\n   &\\alpha = \\frac{\\bar{x}^2}{\\hat{\\sigma}^2},\\\\[1em]\n   &\\beta = \\frac{\\bar{x}}{\\hat{\\sigma}^2}.\n\\end{aligned}\n\\end{aligned}\\]\nThis is not the same result as given by maximum likelihood estimation. In fact, the MLE cannot be written analytically.",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Method of moments</span>"
    ]
  },
  {
    "objectID": "lessons/mle/mle.html",
    "href": "lessons/mle/mle.html",
    "title": "37  Maximum likelihood estimation",
    "section": "",
    "text": "37.1 The likelihood function\nTo understand how MLE works, we first need to define what a likelihood is. Note: Here we are specifically talking about the likelihood function in a frequentist setting, not the likelihood in a Bayesian context.\nSay we have some data \\(\\mathbf{y}\\) that we are modeling parametrically. The generative distribution is parametrized by a set of parameters \\(\\theta\\). Thus, the PDF for the generative distribution of all of the data is \\(f(\\mathbf{y};\\theta)\\).\nWe define the likelihood function \\(L(\\theta;\\mathbf{y})\\) as\n\\[\\begin{align}\nL(\\theta;\\mathbf{y}) = f(\\mathbf{y}; \\theta).\n\\end{align}\\]\nThis may look weird, but it is easier understood as the PDF of the generative distribution, except where it is a function of the parameters \\(\\theta\\) and not of the data \\(\\mathbf{y}\\). The likelihood is therefore not a probability nor is it a probability density.\nWe can define \\(\\ell(\\theta;\\mathbf{y}) = \\ln L(\\theta;\\mathbf{y})\\) as the log-likelihood function, which we will find is often useful to work with.",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Maximum likelihood estimation</span>"
    ]
  },
  {
    "objectID": "lessons/mle/mle.html#maximum-likelihood-estimate",
    "href": "lessons/mle/mle.html#maximum-likelihood-estimate",
    "title": "37  Maximum likelihood estimation",
    "section": "37.2 Maximum likelihood estimate",
    "text": "37.2 Maximum likelihood estimate\nThe set of parameter values \\(\\theta^*\\) for which the likelihood function (and therefore also the log-likelihood function) is maximal is called the maximum likelihood estimate, or MLE. Generically, we can denote the parameter values that maximize the likelihood function as \\(\\theta^*\\). That is,\n\\[\\begin{align}\n\\theta^* = \\mathrm{arg\\;max}_\\theta\\; L(\\theta;\\mathbf{y}) = \\mathrm{arg\\;max}_\\theta \\;\\ell(\\theta;\\mathbf{y}).\n\\end{align}\\]\nThe MLE is not the true value of the parameter. Instead, it is an estimate of the true parameter acquired by the procedure of finding the maximizer of the likelihood function.\nThe MLE has some nice properties, which is probably why it is so widely used to estimate parameters. We will not prove these properties here, but will instead state them.\n\nAs the number of measurements gets very large, the MLE converges to the true parameter value. This is called consistency.\nAlso as the number of measurements gets large, the MLE has the smallest variance among all well-behaved estimators for parameters. This is called asymptotic optimality.\nIf \\(\\theta^*\\) is an MLE for the parameters \\(\\theta\\), then \\(g(\\theta^*)\\) is the MLE for the parameters \\(g(\\theta)\\), where \\(g\\) is an arbitrary function. This is called equivariance.",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Maximum likelihood estimation</span>"
    ]
  },
  {
    "objectID": "lessons/mle/mle.html#confidence-intervals-for-mles",
    "href": "lessons/mle/mle.html#confidence-intervals-for-mles",
    "title": "37  Maximum likelihood estimation",
    "section": "37.3 Confidence intervals for MLEs",
    "text": "37.3 Confidence intervals for MLEs\nThe MLE is not the true value of the parameter, but it is rather an estimate of it computed from data. Therefore, the MLE is a random variable. It can vary meaningfully from experiment to experiment, so we can compute confidence intervals for the MLE using bootstrapping. In essence, we can resample the data set to get a bootstrap sample. We use this bootstrap sample in our procedure for computing the MLE. The MLE computed from the bootstrap sample is a replicate. We gather many replicates and can then compute a bootstrap confidence interval as we have done in the nonparametric setting.\nWe will explore the practicalities of computing confidence intervals for MLEs in detail in forthcoming lessons.",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Maximum likelihood estimation</span>"
    ]
  },
  {
    "objectID": "lessons/mle/mle.html#computing-an-mle",
    "href": "lessons/mle/mle.html#computing-an-mle",
    "title": "37  Maximum likelihood estimation",
    "section": "37.4 Computing an MLE",
    "text": "37.4 Computing an MLE\nComputing an MLE involves writing down the likelihood function (or, more often the log-likelihood function) and finding its maximizer. For all but the simplest of models, this is done numerically, and we will learn about how to do that in coming lessons.\nHowever, it is often quite useful to make analytical progress in characterizing or computing MLEs. The numerical calculation can be difficult for many reasons, including high-dimensionality of the likelihood function, or multiple local maxima. In some cases, we can directly compute the MLE analytically, which can save numerical headaches. We will compute the MLE for two of the models we considered earlier in this lecture.\n\n37.4.1 MLE for rate of arrival of a Poisson process\nWe modeled the time to catastrophe as Exponentially distributed when we considered it to be the result of the arrival of a single Poisson process. So, imagine we have a data set containing \\(n\\) times to catastrophe. Assuming that these times are i.i.d. and Exponentially distributed, the joint PDF is\n\\[\\begin{align}\nf(\\mathbf{t};\\beta) = \\prod_{i=1}^n \\beta\\,\\mathrm{e}^{-\\beta t_i} = \\beta^n\\,\\exp\\left[-\\beta\\sum_{i=1}^n t_i\\right].\n\\end{align}\\]\nThe log-likelihood function is then\n\\[\\begin{align}\n\\ell(\\beta; \\mathbf{t}) =  n\\ln \\beta - \\beta\\sum_{i=1}^n t_i =  n\\ln \\beta - n \\, \\beta\\,\\bar{t},\n\\end{align}\\]\nwhere \\(\\bar{t}\\) is the arithmetic mean of the measured catastrophe times. The log-likelihood is maximal when its first derivative vanishes, or\n\\[\\begin{align}\n\\frac{\\mathrm{d}}{\\mathrm{d}\\beta}\\,\\ell(\\beta; \\mathbf{t}) = \\frac{n}{\\beta^*} - n \\bar{t} = 0.\n\\end{align}\\]\nThis tells us that the MLE is \\(\\beta^* = 1/\\bar{t}\\).\nRecall that the mean of an Exponential distribution is \\(1/\\beta\\), and also that the plug-in estimate for the mean is \\(\\bar{t}\\). This means that for the Exponential distribution, the plug-in estimate for the mean also gives the maximum likelihood estimate for \\(\\beta\\) via the equivariance property of the MLE. (That is, the MLE of \\(1/\\beta\\) is \\(1/\\beta^*\\).)\nNote that this may not be a good model. We assume the model is true, and under that assumption, we find the parameters that maximize the likelihood function. Within the context of the model, we get the (possibly scientifically irrelevant if the model is wrong) maximum likelihood estimate.\n\n\n37.4.2 MLE for the mean and variance of a Normal distribution\nAs a second example, consider the length of C. elegans eggs, modeled as Normally distributed and i.i.d.. In this case, the generative joint PDF, as we have previously written down, is\n\\[\\begin{align}\n   f(\\mathbf{y}; \\mu, \\sigma) &= \\left(\\frac{1}{2\\pi \\sigma^2}\\right)^{n/2}\\,\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\mu)^2\\right].\n\\end{align}\\]\nAfter some algebraic grunge, which I will skip here, it can be shown that\n\\[\\begin{align}\n\\sum_{i=1}^n(y_i-\\mu)^2 = n(\\bar{y} - \\mu)^2 + n\\hat{\\sigma}^2,\n\\end{align}\\]\nwhere\n\\[\\begin{align}\n\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\bar{y})^2\n\\end{align}\\]\nis the plug-in estimate for the variance. Thus, the joint PDF is\n\\[\\begin{align}\n   f(\\mathbf{y}; \\mu, \\sigma) &= \\left(\\frac{1}{2\\pi \\sigma^2}\\right)^{n/2}\\,\\exp\\left[-\\frac{n(\\bar{y} - \\mu)^2 + n\\hat{\\sigma}^2}{2\\sigma^2}\\right].\n\\end{align}\\]\nThe log-likelihood function is then\n\\[\\begin{align}\n   \\ell(\\mu, \\sigma; \\mathbf{y}) &= \\text{constant} -n\\ln \\sigma - \\frac{n(\\bar{y} - \\mu)^2 + n\\hat{\\sigma}^2}{2\\sigma^2}.\n\\end{align}\\]\nThe log-likelihood is maximal when\n\\[\\begin{aligned}\n\\begin{align}\n   \\frac{\\partial\\ell}{\\partial \\mu} &= 0 ,\\\\\n   \\frac{\\partial\\ell}{\\partial \\sigma} &= 0.\n\\end{align}\n\\end{aligned}\\]\nEvaluating the first expression,\n\\[\\begin{align}\n   \\frac{\\partial\\ell}{\\partial \\mu} = -\\frac{n}{(\\sigma^*)^2}(\\bar{y}-\\mu^*) = 0.\n\\end{align}\\]\nThe only solution is \\(\\mu^* = \\bar{y}\\). So, as was the case with the Exponential distribution, the plug-in estimate for the mean gives the MLE for the mean of the Normal distribution.\nNext, to compute the MLE for \\(\\sigma\\), we consider the second equation.\n\\[\\begin{align}\n   \\frac{\\partial\\ell}{\\partial \\sigma} &= - \\frac{n}{\\sigma^*} + \\frac{n(\\bar{y} - \\mu^*)^2 + n\\hat{\\sigma}^2}{(\\sigma^*)^3} = 0.\n\\end{align}\\]\nWe already found, though, that \\(\\mu^* = \\bar{y}\\), so we have\n\\[\\begin{align}\n- \\frac{n}{\\sigma^*} + \\frac{n\\hat{\\sigma}^2}{(\\sigma^*)^3} = 0.\n\\end{align}\\]\nSolving, we get that \\((\\sigma^*)^2 = \\hat{\\sigma}^2\\), which says that the MLE of the variance is also given by its plug-in estimate. This result reveals that the MLE can have bias, as did the plug-in estimate.\nIn all three parameters we estimated (the rate parameter of the Exponential, and the location and scale parameters of the Normal), the MLE was given by the plug-in estimate for the statistical functional whose value is given by the parameter. Note, though, that this is not generally the case.",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Maximum likelihood estimation</span>"
    ]
  },
  {
    "objectID": "lessons/mle/mle.html#next-steps",
    "href": "lessons/mle/mle.html#next-steps",
    "title": "37  Maximum likelihood estimation",
    "section": "37.5 Next steps",
    "text": "37.5 Next steps\nIn the next lessons, you will learn how to numerically compute MLEs for more complicated models, as well as bootstrap confidence intervals for the MLE. We will then proceed to learn about checking validity of models.",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Maximum likelihood estimation</span>"
    ]
  },
  {
    "objectID": "lessons/mle/numerical_mle.html",
    "href": "lessons/mle/numerical_mle.html",
    "title": "38  Numerical maximum likelihood estimation",
    "section": "",
    "text": "38.1 The data set\n| Download notebook\nDataset download\nAs discussed in the previous lesson, maximum likelihood estimates for parameters may sometimes be computed analytically, but often cannot. In those cases, we need to resort to numerical methods. In this lesson, we demonstrate some numerical methods to perform maximum likelihood estimates of parameters form a Negative Binomial distribution.\nThe data come from the Elowitz lab, published in Singer et al., Dynamic Heterogeneity and DNA Methylation in Embryonic Stem Cells, Molec. Cell, 55, 319-331, 2014, available here.\nIn this paper, the authors investigated cell populations of embryonic stem cells using RNA single molecule fluorescence in situ hybridization (smFISH), a technique that enables them to count the number of mRNA transcripts in a cell for a given gene. They were able to measure four different genes in the same cells. So, for one experiment, they get the counts of four different genes in a collection of cells.\nThe authors focused on genes that code for pluripotency-associated regulators to study cell differentiation. Indeed, differing gene expression levels are a hallmark of differentiated cells. The authors do not just look at counts in a given cell at a given time. The temporal nature of gene expression is also important. While the authors do not directly look at temporal data using smFISH (since the technique requires fixing the cells), they did look at time lapse fluorescence movies of other regulators. We will not focus on these experiments here, but will discuss how the distribution of mRNA counts acquired via smFISH can serve to provide some insight about the dynamics of gene expression.\nThe data set we are analyzing now comes from an experiment where smFISH was performed in 279 cells for the genes rex1, rest, nanog, and prdm14. The data set may be downloaded at https://s3.amazonaws.com/bebi103.caltech.edu/data/singer_transcript_counts.csv.",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Numerical maximum likelihood estimation</span>"
    ]
  },
  {
    "objectID": "lessons/mle/numerical_mle.html#exploratory-data-analysis",
    "href": "lessons/mle/numerical_mle.html#exploratory-data-analysis",
    "title": "38  Numerical maximum likelihood estimation",
    "section": "38.2 Exploratory data analysis",
    "text": "38.2 Exploratory data analysis\nWe first load in the data set and generate ECDFs for the mRNA counts for each of the four genes.\n\n# Load DataFrame\ndf = pl.read_csv(os.path.join(data_path, 'singer_transcript_counts.csv'), comment_prefix='#')\n\n# Take a look\ndf.head()\n\n\nshape: (5, 4)\n\n\n\nRex1\nRest\nNanog\nPrdm14\n\n\ni64\ni64\ni64\ni64\n\n\n\n\n11\n34\n39\n0\n\n\n172\n91\n33\n5\n\n\n261\n70\n68\n0\n\n\n178\n54\n88\n1\n\n\n129\n54\n41\n0\n\n\n\n\n\n\nEach of the 279 rows has the mRNA counts for each of the four genes. There may be multiple cell types present, and we do now know how many there are. Our aim here is not to find how many cell types there are, but to demonstrate how MLE works. Nonetheless, we should have some idea of the properties of the data set we are exploring. We can start by plotting ECDFs for each of the four genes. It is useful to have the gene names around for iteration here, and throughout the lesson.\n\ngenes = [\"Nanog\", \"Prdm14\", \"Rest\", \"Rex1\"]\n\nplots = [\n    iqplot.ecdf(\n        data=df.get_column(gene),\n        q=gene,\n        x_axis_label=\"mRNA count\",\n        title=gene,\n        frame_height=150,\n        frame_width=200,\n    )\n    for gene in genes\n]\n\nbokeh.io.show(bokeh.layouts.gridplot(plots, ncols=2))\n\n\n  \n\n\n\n\n\nNote the difference in the x-axis scales. Clearly, prdm14 has far fewer mRNA copies than the other genes. The presence of two inflection points in the Rex1 EDCF implies bimodality, leading us to suspect that there may be two cell types, or at least more than one cell type. We can plot all pairwise combinations of gene counts to explore further.\nFor visualizing the data, it will be useful to have labels for the cells so we can compare expression levels in all four cells at one. We will label them according to the Rex1 levels.\n\n# Add cell label, ranked lowest to highest in Rex1 expression\ndf = df.with_columns(pl.col('Rex1').rank(method='dense').alias('cell'))\n\n# Add colors for plotting, using quantitative to color conversion in bebi103 package\ndf = df.with_columns(pl.Series(name='color', values=bebi103.viz.q_to_color(df[\"cell\"], bokeh.palettes.Viridis256)))\n\nWe will now look at all of the pair-wise plots of mRNA expression. We color the dots according to cell ID, with cell 1 having the lowest count of Rex1 mRNA and the last cell having the highest.\n\ndef pairwise_plot(df, gene1, gene2):\n    p = bokeh.plotting.figure(\n        frame_height=150,\n        frame_width=150,\n        x_axis_label=gene1,\n        y_axis_label=gene2,\n    )\n\n    p.scatter(source=df.to_dict(), x=gene1, y=gene2, color=\"color\", size=2)\n\n    return p\n\n\nplots = [\n    pairwise_plot(df, gene1, gene2) for gene1, gene2 in itertools.combinations(genes, 2)\n]\n\nbokeh.io.show(bokeh.layouts.gridplot(plots, ncols=3))\n\n\n  \n\n\n\n\n\nIt appears as though there is a correlation between Rest, Nanog, and Rex1. They tend to be high or low together. Prdm14, on the other hand, shows less correlation.",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Numerical maximum likelihood estimation</span>"
    ]
  },
  {
    "objectID": "lessons/mle/numerical_mle.html#model-for-mrna-levels",
    "href": "lessons/mle/numerical_mle.html#model-for-mrna-levels",
    "title": "38  Numerical maximum likelihood estimation",
    "section": "38.3 Model for mRNA levels",
    "text": "38.3 Model for mRNA levels\nIn this part of the lesson, we will model gene expression of each of the four genes separately, though they are connected by which cell is being measured. We will discuss that later. For now, we develop a model for the mRNA counts for a given gene.\nIf gene expression is a purely Poisson process, we might expect a Poisson distribution. Or, if the copy number is itself somehow tightly regulated, we might expect a Normal distribution.\nStudy of gene expression dynamics, largely through fluorescence imaging, has lead to a different story. Expression of many important genes can be bursty, which means that the promoter is on for a period of time in which transcripts are made, and then it is off for a while. The “on” periods are called “bursts” and are themselves well-modeled as a Poisson process. That is to say that the amount of time that a promoter is on is Exponentially distributed. Thus, we can think of a burst as a series of Bernoulli trials. A “failure” is production of an mRNA molecule, and a “success” is a switch to an off state. The number of “successes” we get is equal to the number of bursts we get per decay time of the mRNA. We can define the number of bursts before degradation of the mRNA as \\(\\alpha\\). This is the so-called burst frequency. So, we have a series of Bernoulli trials and we wait for \\(\\alpha\\) successes. Then, \\(n\\), the total number of failures (which is the number of mRNA transcripts), is Negative Binomially distributed, since this matches the Negative Binomial story. Referring to the parametrization used in the distribution explorer,\n\\[\\begin{align}\nn \\sim \\text{NBinom}(\\alpha, \\beta),\n\\end{align}\\]\nwhere \\(\\beta\\) is related to the probability \\(\\theta\\) of a success of a Bernoulli trial by \\(\\theta = \\beta/(1+\\beta)\\).\nThe meaning of the parameter \\(\\beta\\), and the related quantity \\(\\theta\\), can be a little mystical here. We would like to relate it to the typical burst size, i.e., the typical number of transcripts made per burst. The size of a single given burst (that is, the number of transcripts made in a burst) is geometrically distributed (since it matches that story), so\n\\[\\begin{align}\nf(n_\\mathrm{burst} ; \\theta) = (1-\\theta)^{n_\\mathrm{burst}}\\,\\theta.\n\\end{align}\\]\nThe mean number of transcripts \\(b\\) in a burst is\n\\[\\begin{align}\nb \\equiv \\left\\langle n_\\mathrm{burst}\\right\\rangle &= \\sum_{n_\\mathrm{burst}=0}^\\infty\nn_\\mathrm{burst}(1-\\theta)^{n_\\mathrm{burst}}\\theta\\\\[1em]\n&= \\theta \\sum_{n_\\mathrm{burst}=0}^\\infty\nn_\\mathrm{burst}(1-\\theta)^{n_\\mathrm{burst}} \\\\[1em]\n&= \\theta(1-\\theta)\\, \\frac{\\mathrm{d}}{\\mathrm{d}(1-\\theta)}\\sum_{n_\\mathrm{burst}=0}^\\infty(1-\\theta)^{n_\\mathrm{burst}} \\\\[1em]\n&= \\theta(1-\\theta)\\, \\frac{\\mathrm{d}}{\\mathrm{d}(1-\\theta)}\\,\\frac{1}{\\theta}\\\\[1em]\n&= -\\theta(1-\\theta)\\, \\frac{\\mathrm{d}}{\\mathrm{d}\\theta}\\,\\frac{1}{\\theta} \\\\[1em]\n&= \\frac{1-\\theta}{\\theta} \\\\[1em]\n&= \\frac{1}{\\beta}.\n\\end{align}\\]\nSo we now see that \\(1/\\beta\\) is the typical burst size. Using the Negative Binomial property of mRNA copy numbers of bursty gene expression, we can characterize the expression levels of a given cell type by the two parameters of the Negative Binomial, the burst frequency \\(\\alpha\\) and the burst size \\(b = 1/\\beta\\). These are the two parameters we would like to infer from transcript count data. The conclusion of all this is that we have have our likelihood.\n\\[\\begin{align}\n&n \\sim \\text{NBinom}(\\alpha, \\beta),\\\\[1em]\n&b = 1/\\beta.\n\\end{align}\\]",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Numerical maximum likelihood estimation</span>"
    ]
  },
  {
    "objectID": "lessons/mle/numerical_mle.html#maximum-likelihood-estimation-by-numerical-optimization",
    "href": "lessons/mle/numerical_mle.html#maximum-likelihood-estimation-by-numerical-optimization",
    "title": "38  Numerical maximum likelihood estimation",
    "section": "38.4 Maximum likelihood estimation by numerical optimization",
    "text": "38.4 Maximum likelihood estimation by numerical optimization\nTo compute the MLE for the two parameters, the burst frequency \\(\\alpha\\) and burst size \\(\\beta\\), we need to define the likelihood function. We make the assumption that the number of transcripts in each cell is i.i.d., giving a statistical model of\n\\[\\begin{align}\nn_i \\sim \\text{NBinom}(\\alpha,\\beta)\\;\\forall i.\n\\end{align}\\]\nReferring to the PMF of the Negative Binomial distribution and making the change of variables \\(b=1/\\beta\\), the likelihood function is\n\\[\\begin{align}\nL(\\alpha, b;\\mathbf{n}) = \\prod_i\\frac{\\Gamma(n_i+\\alpha)}{\\Gamma(\\alpha)n!}\\left(\\frac{1}{1+b}\\right)^\\alpha\\left(\\frac{b}{1+b}\\right)^{n_i},\n\\end{align}\\]\nand the log-likelihood is\n\\[\\begin{align}\n\\ell(\\alpha, b;\\mathbf{n}) = \\ln L(\\alpha, b;\\mathbf{n}) = \\sum_i \\ln \\left(\\frac{\\Gamma(n_i+\\alpha)}{\\Gamma(\\alpha)n!}\\left(\\frac{1}{1+b}\\right)^\\alpha\\left(\\frac{b}{1+b}\\right)^{n_i}\\right).\n\\end{align}\\]\nTo find the MLE, we need to find the values of \\(\\alpha\\) and \\(b\\) that satisfy\n\\[\\begin{align}\n\\frac{\\partial \\ell}{\\partial \\alpha} = \\frac{\\partial \\ell}{\\partial b} = 0.\n\\end{align}\\]\nUnfortunately, no closed form solution exists for this. We therefore need to resort to numerical optimization to find the MLE \\(\\alpha^*\\) and \\(b^*\\).\n\n38.4.1 Numerical optimization\nNumerical optimization is typically implemented to find a minimizers of a function rather than maximizers. The function being minimized is called an objective function. This is not a problem for maximum likelihood estimation; we simply define a negative log-likelihood as our objective function.\nSometimes, we have constraints on the allowed values for the parameters. In our case, both \\(\\alpha\\) and \\(\\beta\\) must be non-negative. So, the statement of the optimization problem to find the MLE is\n\\[\\begin{align}\n\\text{minimize } (-\\ell(\\alpha, \\beta;\\mathbf{n})) \\text{ s.t. } \\alpha, \\beta &gt; 0,\n\\end{align}\\]\nwhere “s.t.” is read “subject to.” If we explicitly consider the constraints, we are performing a constrained optimization problem. Constrained optimization is often considerably more challenging than unconstrained optimization. There are ways around simple positivity constraints such as the ones here. We can instead define new variables \\(\\xi_\\alpha = \\ln \\alpha\\) and \\(\\xi_b = \\ln b\\), and write the log-likelihood in terms of these variables instead. We then find minimizing \\(\\xi_\\alpha\\) and \\(\\xi_b\\) and convert them to \\(\\alpha\\) and \\(\\beta\\) by exponentiation after performing the minimization calculation.\nNumerical optimization is implemented in the scipy.optimize submodule (docs). Most of the functionality you need is in the scipy.optimize.minimize() function. To use the function to find minimizers of an objective function, the standard call signature is\nscipy.optimize.minimize(fun, x0, args=(), method='BFGS')\nThe fun argument is a function with call signature fun(x, *args), where x is the variables used in the optimization. In the case of MLE, the function is the negative log-likelihood function, x is always an array of the parameter values we are trying to estimate, and the remaining arguments are additional arguments passed into the likelihood function, which always include the measured data. Importantly, we have to provide a guess as to which values of the parameters are optimal. This is passed as an array x0. The kwarg args specifies which additional arguments are to be passed to fun(). Note that args must be a tuple. Finally, the method keyword argument specifies which numerical optimization method to use, the default being the Broyden–Fletcher–Goldfarb–Shanno algorithm. This is a good algorithm but does compute derivatives, so it is only useful if the parameter values can take on any real value.\nI have omitted the bounds keyword argument here because we will not usually use them, as we will either do the logarithm trick above, or use Powell’s method, which does not required calculation of derivatives (so we may therefore have discontinuities in the objective function and set the value of the objective function to be infinity for disallowed parameter values).\n\n\n38.4.2 Guess at optimal parameters\nWe will use the method of moments to guess the optimal parameters. Referring to the Distribution Explorer for the first moment and variance, we can equate them with the plug-in estimates.\n\\[\\begin{align}\n&\\langle n \\rangle = \\frac{\\alpha}{\\beta} = \\alpha b = \\bar{n},\\\\[1em]\n&\\sigma^2 = \\frac{\\alpha(1+\\beta)}{\\beta^2} = \\alpha b(1+b) = \\hat{\\sigma}^2.\n\\end{align}\\]\nSolving gives our method-of-moments estimates,\n\\[\\begin{align}\n&\\alpha_\\mathrm{mom} = \\frac{\\bar{n}}{b} = \\frac{\\bar{x}^2}{\\hat{\\sigma}^2 - \\bar{n}},\\\\[1em]\n&b_\\mathrm{mom} = \\frac{\\hat{\\sigma}^2}{\\bar{n}} - 1.\n\\end{align}\\]\nWe can compute the estimates from the plug-in values.\n\n# Extract the values for Nanog\nn = df.get_column('Nanog').to_numpy()\n\n# Compute\nb_mom = np.var(n) / np.mean(n) - 1\nalpha_mom = np.mean(n) / b_mom\n\n# Take a look\nprint(f\"α_mom = {alpha_mom}\\nb_mom = {b_mom}\")\n\nα_mom = 1.7700110004078886\nb_mom = 49.569381904553985\n\n\nNote that the method-of-moments is not the only way to come up with guesses. As an example, if the data are not actually generated from a Negative Binomial distribution (or even if they are and there are not too many data points), then we may have \\(\\hat{\\sigma} &lt; \\bar{n}\\), which would result in a negative estimate for both \\(\\alpha\\) and \\(b\\), which are not even allowed values of the parameters. It turns out that in this case, the optimization works just fine if we arbitrarily pick something like \\(\\alpha \\approx \\beta \\approx 3\\).\n\n38.4.2.1 Solving with the BFGS algorithm\nWe will now solve the minimization problem using the BFGS algorithm, specifying the parameters using logarithms to make sure that the problem is completely unconstrained. First, we have to write a function for the log-likelihood matching the required function signature of the input fun to scipy.optimize.minimize(). Note that we do not have to hand-code the log-likelihood. The scipy.stats module has functions to compute the log-PDF/log-PMF for many distributions. We just need to check the Distribution Explorer to ensure we use the parametrization that the scipy.stats module requires. In this case, it expects parameters alpha and 1/1+b.\n\ndef log_like_iid_nbinom_log_params(log_params, n):\n    \"\"\"Log likelihood for i.i.d. NBinom measurements with \n    input being logarithm of parameters.\n    \n    Parameters\n    ----------\n    log_params : array\n        Logarithm of the parameters alpha and b.\n    n : array\n        Array of counts.\n        \n    Returns\n    -------\n    output : float\n        Negative log-likelihood.    \n    \"\"\"\n    log_alpha, log_b = log_params\n\n    # Convert from log parameters\n    alpha = np.exp(log_alpha)\n    b = np.exp(log_b)\n\n    return np.sum(st.nbinom.logpmf(n, alpha, 1/(1+b)))\n\nWith the log likelihood specified, we simply use -log_like_iid_nbinom_params() as our objective function, which we can succinctly code up as an anonymous (lambda) function. Let’s perform the optimization for the nanog gene and look at the result.\n\n# Solve, making sure to use log parameters\nres = scipy.optimize.minimize(\n    fun=lambda log_params, n: -log_like_iid_nbinom_log_params(log_params, n),\n    x0=np.array([np.log(alpha_mom), np.log(b_mom)]),\n    args=(n,),\n    method='BFGS'\n)\n\nres\n\n  message: Desired error not necessarily achieved due to precision loss.\n  success: False\n   status: 2\n      fun: 1524.9284357729396\n        x: [ 2.338e-01  4.241e+00]\n      nit: 11\n      jac: [ 3.204e-04  1.068e-04]\n hess_inv: [[ 8.545e-09  2.968e-07]\n            [ 2.968e-07  1.043e-05]]\n     nfev: 308\n     njev: 96\n\n\nThe result returned by scipy.optimize.minimize() is an OptimizeResult object that has several attributes about how the optimization calculation went, including if it was successful. Importantly, the optimal log-parameter values are in the array x. We can extract them and exponentiate them to get the MLE.\n\nalpha_mle, b_mle = np.exp(res.x)\n\nprint(\"α: \", alpha_mle)\nprint(\"b: \", b_mle)\n\nα:  1.263433985420747\nb:  69.44437003467432\n\n\nSo, the MLE for the burst frequency is about 1.25 inverse degradation times. The MLE for the burst size is about 70 transcripts per burst.\n\n\n38.4.2.2 Solving using Powell’s method\nAs an alternative to the BFGS method, we can use Powell’s method. This has the advantage that we do not have to use derivatives in the optimization, so we do not have to use logarithms of the parameters. We do, however, need to specify that the log-likelihood is minus infinity for disallowed parameter values.\n\ndef log_like_iid_nbinom(params, n):\n    \"\"\"Log likelihood for i.i.d. NBinom measurements.\"\"\"\n    alpha, b = params\n    \n    if alpha &lt;= 0 or b &lt;= 0:\n        return -np.inf\n\n    return np.sum(st.nbinom.logpmf(n, alpha, 1/(1+b)))\n\nWe take a similar approach to solving using Powell’s method. This time, we will catch warnings because the solver will stumble into regions where the log-likelihood is minus infinity. We know this to be the case, as we designed it that way, so we will suppress the warnings to keep our notebook clean. We will tighten the tolerance on the solver using the tol keyword argument to insist on very small gradients when finding the minimum. This results in a more accurate MLE at the cost of more computational effort.\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    \n    res = scipy.optimize.minimize(\n        fun=lambda params, n: -log_like_iid_nbinom(params, n),\n        x0=np.array([alpha_mom, b_mom]),\n        args=(n,),\n        method='Powell',\n        tol=1e-6,\n    )\n\nif res.success:\n    alpha_mle, b_mle = res.x\nelse:\n    raise RuntimeError('Convergence failed with message', res.message)\n    \nprint(\"α: \", alpha_mle)\nprint(\"b: \", b_mle)\n\nα:  1.2634329776733069\nb:  69.44440337333538\n\n\nThis differs from the result we got with BFGS in the third or fourth decimal place, due to inaccuracies in introducing the logarithms, but the difference is not big and also is small compared to the confidence interval we will for the MLEs of α and b in a subsequent lesson.",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Numerical maximum likelihood estimation</span>"
    ]
  },
  {
    "objectID": "lessons/mle/numerical_mle.html#the-likelihood-function",
    "href": "lessons/mle/numerical_mle.html#the-likelihood-function",
    "title": "38  Numerical maximum likelihood estimation",
    "section": "38.5 The likelihood function",
    "text": "38.5 The likelihood function\nTo help give a picture of what the likelihood function looks like, and what the optimizer is doing, we can plot it. In this case, we have two parameters, so we can make a contour plot. We first compute the log-likelihood for various values of \\(\\alpha\\) and \\(b\\).\n\n# alpha and b values for plotting\nalpha = np.linspace(1, 1.5, 100)\nb = np.linspace(50, 90, 100)\n\n# Compute log-likelihood for each value\nlog_like = np.empty((100, 100))\nfor j, alpha_val in enumerate(alpha):\n    for i, b_val in enumerate(b):\n        log_like[i, j] = log_like_iid_nbinom((alpha_val, b_val), n)\n\nRemember that the likelihood function is not a probability distribution, so it is not normalized. When we exponentiate the log-likelihood, we may get values close to zero, or very large. It is therefore a good idea to first subtract the maximal value of all computed log-likelihoods. This has the effect of multiplying the likelihood function by a constant.\n\nlike = np.exp(log_like - log_like.max())\n\nNow, we can make a contour plot using the bebi103.viz.contour() function.\n\np = bebi103.viz.contour(alpha, b, like, overlaid=True, x_axis_label=\"α*\", y_axis_label=\"b*\")\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nGraphically, we can see that we appropriately found the maximum.",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Numerical maximum likelihood estimation</span>"
    ]
  },
  {
    "objectID": "lessons/mle/numerical_mle.html#a-quick-visualization",
    "href": "lessons/mle/numerical_mle.html#a-quick-visualization",
    "title": "38  Numerical maximum likelihood estimation",
    "section": "38.6 A quick visualization",
    "text": "38.6 A quick visualization\nWe can do a quick visualization of our MLE to see if the model holds up. We will talk more about graphical tests of model predictive accuracy in coming lessons, but for now, we will simply overlay the theoretical CDF parametrized by the MLE. We can conveniently use the scipy.stats module to generate the CDF. It is probably overkill, since we have such a wide range of mRNA counts, but we will take care to make sure we plot the theoretical CDF as a staircase, since it is for a discrete distribution.\n\np = iqplot.ecdf(data=df['Nanog'], q='Nanog', conf_int=True)\nn_theor = np.arange(0, df['Nanog'].max()+1)\ncdf_theor = st.nbinom.cdf(n_theor, alpha_mle, 1/(1+b_mle))\n\n# Weave together to make staircase for discrete distribution\nn_plot = np.empty(2 * len(n_theor))\ncdf_plot = np.empty(2 * len(n_theor))\ncdf_plot[0] = 0\ncdf_plot[1::2] = cdf_theor\ncdf_plot[2::2] = cdf_theor[:-1]\nn_plot[::2] = n_theor\nn_plot[1::2] = n_theor\n\np.line(n_plot, cdf_plot, line_color='orange', line_width=2)\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe MLE curve deviates from the nonparametric ECDF 95% confidence interval. This suggests we may be missing something in our model. We will cover this in more depth in future lessons.",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Numerical maximum likelihood estimation</span>"
    ]
  },
  {
    "objectID": "lessons/mle/numerical_mle.html#computing-environment",
    "href": "lessons/mle/numerical_mle.html#computing-environment",
    "title": "38  Numerical maximum likelihood estimation",
    "section": "38.7 Computing environment",
    "text": "38.7 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,polars,bokeh,iqplot,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\nscipy     : 1.16.0\npolars    : 1.31.0\nbokeh     : 3.7.3\niqplot    : 0.3.7\nbebi103   : 0.1.28\njupyterlab: 4.4.5",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Numerical maximum likelihood estimation</span>"
    ]
  },
  {
    "objectID": "lessons/mle/mle_confidence_intervals.html",
    "href": "lessons/mle/mle_confidence_intervals.html",
    "title": "39  Confidence regions and confidence intervals for a MLE",
    "section": "",
    "text": "39.1 MLE for all genes\n| Download notebook\nDataset download\nIn our previous lesson on numerical maximum likelihood estimation, we computed an MLE for the parameters of a Negative Binomial model for mRNA counts for Nanog using the Singer, et al. dataset. Here, we will compute the MLEs for the other three genes as well and further compute confidence intervals for these MLEs.\nWe start by writing function to compute the maximum likelihood estimate for our model. We have a set of Negative-Binomially distributed i.i.d. counts, parametrized by \\(\\alpha\\) and \\(b=1/\\beta\\), and we wish to compute the MLE. We worked out the code previously, so we can write the functions here.\ndef log_like_iid_nbinom(params, n):\n    \"\"\"Log likelihood for i.i.d. NBinom measurements, parametrized\n    by alpha, b=1/beta.\"\"\"\n    alpha, b = params\n    \n    if alpha &lt;= 0 or b &lt;= 0:\n        return -np.inf\n\n    return np.sum(st.nbinom.logpmf(n, alpha, 1/(1+b)))\n\n\ndef mle_iid_nbinom(n):\n    \"\"\"Perform maximum likelihood estimates for parameters for i.i.d. \n    NBinom measurements, parametrized by alpha, b=1/beta\"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        res = scipy.optimize.minimize(\n            fun=lambda params, n: -log_like_iid_nbinom(params, n),\n            x0=np.array([3, 3]),\n            args=(n,),\n            method='Powell'\n        )\n\n    if res.success:\n        return res.x\n    else:\n        raise RuntimeError('Convergence failed with message', res.message)\nNow, we just have to load in the data and compute the MLEs.\n# Load DataFrame\ndf = pl.read_csv(os.path.join(data_path, 'singer_transcript_counts.csv'), comment_prefix='#')\ngenes = [\"Nanog\", \"Prdm14\", \"Rest\", \"Rex1\"]\n\n# Data frame to store results\ndf_mle = pl.DataFrame(schema=[('gene', str), ('parameter', str), ('mle', float)])\n\n# Perform MLE for each gene\nfor gene in genes:\n    mle = mle_iid_nbinom(df[gene].to_numpy())\n    sub_df = pl.DataFrame({'gene': [gene]*2, 'parameter': ['alpha', 'b'], 'mle': mle})\n    df_mle = pl.concat([df_mle, sub_df])\nLet’s take a look at the results.\ndf_mle\n\n\nshape: (8, 3)\n\n\n\ngene\nparameter\nmle\n\n\nstr\nstr\nf64\n\n\n\n\n\"Nanog\"\n\"alpha\"\n1.263097\n\n\n\"Nanog\"\n\"b\"\n69.347842\n\n\n\"Prdm14\"\n\"alpha\"\n0.552886\n\n\n\"Prdm14\"\n\"b\"\n8.200636\n\n\n\"Rest\"\n\"alpha\"\n4.530335\n\n\n\"Rest\"\n\"b\"\n16.543054\n\n\n\"Rex1\"\n\"alpha\"\n1.634562\n\n\n\"Rex1\"\n\"b\"\n84.680915",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Confidence regions and confidence intervals for a MLE</span>"
    ]
  },
  {
    "objectID": "lessons/mle/mle_confidence_intervals.html#bootstrap-confidence-intervals",
    "href": "lessons/mle/mle_confidence_intervals.html#bootstrap-confidence-intervals",
    "title": "39  Confidence regions and confidence intervals for a MLE",
    "section": "39.2 Bootstrap confidence intervals",
    "text": "39.2 Bootstrap confidence intervals\nRemember that the MLE is a random variable, so we can compute confidence intervals for it. There are several approaches to computing confidence intervals for maximum likelihood estimates. We will explore two methods. They differ in how we use our model to generate bootstrap samples.\n\nIn this case of mRNA counts, the data were generated out of an unknown distribution \\(F(n)\\). We do not know what it is, though we model it as Negative Binomial to compute an MLE. (We chould choose other models, some will be closer to \\(F(n)\\) than others.) To generate a bootstrap sample, we sample out of the empirical distribution \\(\\hat{F}(n;\\mathbf{n})\\). We then use the Negative Binomial model to obtain MLEs for the burst frequency \\(\\alpha\\) and burst size \\(b\\). These MLEs constitute our bootstrap replicates for the MLE, and we can compute confidence intervals from the replicates.\nInstead of approximating the generative distribution \\(F(n)\\) with \\(\\hat{F}(n;\\mathbf{n})\\), we approximate the generative distribution with the model distribution, in this case a Negative Binomial, parametrized by the MLE. In other words, \\(F(n) \\approx F_\\mathrm{NBinom}(n; \\alpha^*, b^*)\\). We then sample out of the model distribution to generate our bootstrap samples, and compute the replicates from these. This is called parametric bootstrap and seems to be more widely used. It operates under the assumption that our model is indeed a good approximation for the generative distribution.\n\nThe first method is nonparametric in that we use the plug-in principle for approximating the generative distribution (and only use the model to compute the statistical functional), so we will refer to it as the nonparametric bootstrap for the MLE, but I have not seen that terminology be widely used, probably because of confusion with the fact that a parametric model is used to compute the MLE.\n\n39.2.1 Nonparametric MLE confidence interval\nFor the nonparametric MLE confidence interval, we simply draw a bootstrap sample out of the original data set, as we have been doing, and then compute the MLE estimates. Let’s do that. We will use TQDM to give us a progress bar, since this calculation takes a few minutes.\n\nrng = np.random.default_rng(3252)\n\ndef draw_bs_sample(data):\n    \"\"\"Draw a bootstrap sample from a 1D data set.\"\"\"\n    return rng.choice(data, size=len(data))\n\n\ndef draw_bs_reps_mle(mle_fun, data, args=(), size=1, progress_bar=False):\n    \"\"\"Draw nonparametric bootstrap replicates of maximum likelihood estimator.\n    \n    Parameters\n    ----------\n    mle_fun : function\n        Function with call signature mle_fun(data, *args) that computes\n        a MLE for the parameters\n    data : one-dimemsional Numpy array\n        Array of measurements\n    args : tuple, default ()\n        Arguments to be passed to `mle_fun()`.\n    size : int, default 1\n        Number of bootstrap replicates to draw.\n    progress_bar : bool, default False\n        Whether or not to display progress bar.\n        \n    Returns\n    -------\n    output : numpy array\n        Bootstrap replicates of MLEs.\n    \"\"\"\n    if progress_bar:\n        iterator = tqdm.tqdm(range(size))\n    else:\n        iterator = range(size)\n\n    return np.array([mle_fun(draw_bs_sample(data), *args) for _ in iterator])\n\nWith these functions in hand, we can draw our bootstrap replicates. Note the the replicates will be returned as a two-dimensional array, with the first column being samples of \\(\\alpha^*\\) and the second being samples of \\(b^*\\). We will do this for Nanog first.\n\nbs_reps = draw_bs_reps_mle(\n    mle_iid_nbinom, df[\"Nanog\"].to_numpy(), size=10000, progress_bar=True\n)\n\n100%|█████████████████████████████████████████████████████| 10000/10000 [01:50&lt;00:00, 90.09it/s]\n\n\nTo get the confidence intervals, we can take the percentiles as we have done before. Note that we can use the axis kwarg to enable computing confidence intervals for both parameters in one statement.\n\nconf_int = np.percentile(bs_reps, [2.5, 97.5], axis=0)\n\n# Take a look\nconf_int\n\narray([[ 1.05756375, 57.16408783],\n       [ 1.54925013, 82.33794692]])\n\n\nSo, the confidence interval for \\(\\alpha\\) lies between 1.06 and 1.55, and that for \\(b\\) lies between 57.1 and 82.3.\n\n\n39.2.2 Confidence regions\nThe confidence intervals we have just reported are the confidence intervals for the respective parameters, but we really should define a confidence region, which is the multidimensional extension of a confidence interval. (A confidence interval is the special case of a confidence region with only one parameter.)\nIndeed, the confidence interval for \\(\\alpha\\) comes from percentiles computed from the probability density function \\(f(\\alpha^*;\\mathbf{n})\\). (Strictly speaking this is a probability mass function in this case because there are a finite number of unique bootstrap samples, but we do not need to worry about that distinction because there are a lot(!) of unique bootstrap samples.) This PDF is computed from the joint PDF of the MLEs as\n\\[\\begin{align}\nf(\\alpha^*;\\mathbf{n}) = \\int \\mathrm{d}b^*\\,f(\\alpha^*, b^*;\\mathbf{n}).\n\\end{align}\n\\]\nThe marginalization was done implicitly by just considering the bootstrap samples of the individual parameters, but bootstrap replicate \\(i\\) of \\(\\alpha^*\\) is related to bootstrap replicate \\(i\\) of \\(b^*\\) because they were found together in a maximization of the likelihood function. We might want to plot the joint distribution. One way to visualize this is to just plot all of the bootstrap replicates.\n\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=300,\n    x_axis_label=\"α*\",\n    y_axis_label=\"b*\",\n)\n\np.scatter(\n    bs_reps[:, 0],\n    bs_reps[:, 1],\n    size=1,\n    alpha=0.1,\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThere is a correlation between our MLE estimates for \\(\\alpha\\) and \\(b\\). We can also visualize the distributions for \\(f(\\alpha^*,b^*;\\mathbf{n})\\) using the bebi103.viz.corner() function.\n\n# Package replicates in data frame for plotting\ndf_res = pl.DataFrame(data=bs_reps, schema=[\"α*\", \"b*\"])\n\np_corner = bebi103.viz.corner(\n    samples=df_res,\n    parameters=[\"α*\", \"b*\"],\n    show_contours=True,\n    levels = [0.95],\n)\n\nbokeh.io.show(p_corner)\n\n\n  \n\n\n\n\n\nThe plots on the diagonal contain the histograms of the bootstrap replicates for the MLE estimates. The off-diagonal plot shows a plot of all the bootstrap replicates of the MLE with a contour plot overlaid. By selecting levels=0.95, we have asked for a 95% confidence region; approximately 95% of all bootstrap replicates lie within the contour. This is not exact, though, as some smoothing is done to generate the contour, which is necessary for confidence regions in two or more dimensions because we have a finite number of samples.\nAn important note: These are not distributions of the true parameters values. In a frequentist setting, that has no meaning. These are distributions of the maximum likelihood estimates for the parameters where we have estimated the generative distribution using the empirical distribution.\nIt is also useful to note that we can access the contour lines directly using the bebi103.viz.contour_lines_from_samples() function and can overlay them on the samples. We may also find such contours useful in other custom visualizations.\n\n# Get contour line\nxs, ys = bebi103.viz.contour_lines_from_samples(bs_reps[:,0], bs_reps[:,1], levels=0.95)\n\np.multi_line(xs, ys, line_width=2, line_color=\"black\")\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\n\n\n39.2.3 Parametric confidence intervals\nWe repeat the calculation for a parametric confidence interval. As we write a function for this, we need to include an mle_fun as for draw_bs_reps_mle(), but we also need to provide a function that draws new data sets out of the parametric model.\n\ndef draw_parametric_bs_reps_mle(\n    mle_fun, gen_fun, data, args=(), size=1, progress_bar=False\n):\n    \"\"\"Draw parametric bootstrap replicates of maximum likelihood estimator.\n    \n    Parameters\n    ----------\n    mle_fun : function\n        Function with call signature mle_fun(data, *args) that computes\n        a MLE for the parameters\n    gen_fun : function\n        Function to randomly draw a new data set out of the model\n        distribution parametrized by the MLE. Must have call\n        signature `gen_fun(*params, size)`.\n    data : one-dimemsional Numpy array\n        Array of measurements\n    args : tuple, default ()\n        Arguments to be passed to `mle_fun()`.\n    size : int, default 1\n        Number of bootstrap replicates to draw.\n    progress_bar : bool, default False\n        Whether or not to display progress bar.\n        \n    Returns\n    -------\n    output : numpy array\n        Bootstrap replicates of MLEs.\n    \"\"\"\n    params = mle_fun(data, *args)\n\n    if progress_bar:\n        iterator = tqdm.tqdm(range(size))\n    else:\n        iterator = range(size)\n\n    return np.array(\n        [mle_fun(gen_fun(*params, size=len(data), *args)) for _ in iterator]\n    )\n\nTo draw parametric bootstrap replicates, we need only to provide the function to generate samples from the model distribution. We directly use the Negative Binomial functionality of the Numpy random module.\n\ndef gen_nbinom(alpha, b, size):\n    return rng.negative_binomial(alpha, 1 / (1 + b), size=size)\n\nLet’s draw them for Nanog.\n\nbs_reps_parametric = draw_parametric_bs_reps_mle(\n    mle_iid_nbinom,\n    gen_nbinom,\n    df[\"Nanog\"].to_numpy(),\n    args=(),\n    size=10000,\n    progress_bar=True,\n)\n\n100%|█████████████████████████████████████████████████████| 10000/10000 [01:43&lt;00:00, 96.26it/s]\n\n\nNow that we have our samples, we can compute the confidence intervals, as we did in the nonparametric case.\n\nnp.percentile(bs_reps_parametric, [2.5, 97.5], axis=0)\n\narray([[ 1.09548128, 57.08437686],\n       [ 1.4909864 , 82.61622427]])\n\n\nWe get similar confidence intervals as for the nonparametric case. We can also look at the corner plot.\n\n# Package replicates in data frame for plotting\ndf_res = pl.DataFrame(data=bs_reps_parametric, schema=[\"α*\", \"b*\"])\n\np = bebi103.viz.corner(\n    samples=df_res,\n    parameters=[\"α*\", \"b*\"],\n    show_contours=True,\n    levels = [0.95],\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe corner plot is indeed similar to the nonparametric case.\nWe could compute confidence intervals for the other genes, but we will not do that here to save time, since each computation takes a few minutes. In the next part of this lesson, we will parallelize the calculations to make them faster.",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Confidence regions and confidence intervals for a MLE</span>"
    ]
  },
  {
    "objectID": "lessons/mle/mle_confidence_intervals.html#computing-environment",
    "href": "lessons/mle/mle_confidence_intervals.html#computing-environment",
    "title": "39  Confidence regions and confidence intervals for a MLE",
    "section": "39.3 Computing environment",
    "text": "39.3 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,polars,tqdm,bokeh,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\nscipy     : 1.16.0\npolars    : 1.31.0\ntqdm      : 4.67.1\nbokeh     : 3.7.3\nbebi103   : 0.1.28\njupyterlab: 4.4.5",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Confidence regions and confidence intervals for a MLE</span>"
    ]
  },
  {
    "objectID": "lessons/mle/parallel_conf_ints.html",
    "href": "lessons/mle/parallel_conf_ints.html",
    "title": "40  Parallel bootstrap calculations",
    "section": "",
    "text": "40.1 Explicit RNGs\n| Download notebook\nDataset download\nIn the last part of this lesson, we saw that computing the MLE confidence intervals can take some time (several minutes). To speed up calculations, we can compute the bootstrap replicates in parallel. That is, we split the sampling job up into parts, and different processing cores work on each part, and then combine the results together at the end.\nTo begin, we load in the data.\nAnd we also need the functions we used for performing the MLE calculation from earlier in the lesson.\nWhen we parallelize the calculation, we have to be very careful about the random number generators we use. If we define a random number generator globally with rng = np.random.default_rng(), then each parallel process will use the same sequence of pseudorandom numbers. We therefore need to make a new RNG for each of the parallel processes. As a result, we need to explicitly supply a random number generator to the function we use to generate samples. This changes the call signature to include the RNG, but requires no other changes.\ndef gen_nbinom(alpha, b, size, rng):\n    return rng.negative_binomial(alpha, 1 / (1 + b), size=size)\nNext, we need to adjust the draw_bs_reps_mle() function to also have explicit specification of a RNG. If we do not supply one, a new one should be created within the function.\nFinally, since the parallel version will supersede the draw_bs_reps_mle() function we have already written, we will prepend its name with an underscore, designating it as a private function (essentially a function an end user will not call), since we will not really call it directly again except for demonstration purposes.\ndef _draw_parametric_bs_reps_mle(\n    mle_fun, gen_fun, data, args=(), size=1, progress_bar=False, rng=None,\n):\n    \"\"\"Draw parametric bootstrap replicates of maximum likelihood estimator.\n    \n    Parameters\n    ----------\n    mle_fun : function\n        Function with call signature mle_fun(data, *args) that computes\n        a MLE for the parameters\n    gen_fun : function\n        Function to randomly draw a new data set out of the model\n        distribution parametrized by the MLE. Must have call\n        signature `gen_fun(*params, size, *args, rng)`.\n    data : one-dimemsional Numpy array\n        Array of measurements\n    args : tuple, default ()\n        Arguments to be passed to `mle_fun()`.\n    size : int, default 1\n        Number of bootstrap replicates to draw.\n    progress_bar : bool, default False\n        Whether or not to display progress bar.\n    rng : numpy.random.Generator instance, default None\n        RNG to be used in bootstrapping. If None, the default\n        Numpy RNG is used with a fresh seed based on the clock.\n        \n    Returns\n    -------\n    output : numpy array\n        Bootstrap replicates of MLEs.\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n        \n    params = mle_fun(data, *args)\n\n    if progress_bar:\n        iterator = tqdm.tqdm(range(size))\n    else:\n        iterator = range(size)\n\n    return np.array(\n        [mle_fun(gen_fun(*params, size=len(data), *args, rng=rng)) for _ in iterator]\n    )\nThese functions still work as advertised. We can call _draw_parametric_bs_reps_mle() exactly as we have earlier in the lesson. To get 100 bootstrap replicates (only 100 so we don’t have to wait too long), we can do the following for Nanog.\nbs_reps = _draw_parametric_bs_reps_mle(\n    mle_iid_nbinom, gen_nbinom, df[\"Nanog\"].to_numpy(), args=(), size=100, progress_bar=True\n)\n\n100%|█████████████████████████████████████████████████████████| 100/100 [00:01&lt;00:00, 99.94it/s]",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Parallel bootstrap calculations</span>"
    ]
  },
  {
    "objectID": "lessons/mle/parallel_conf_ints.html#enabling-parallelization",
    "href": "lessons/mle/parallel_conf_ints.html#enabling-parallelization",
    "title": "40  Parallel bootstrap calculations",
    "section": "40.2 Enabling parallelization",
    "text": "40.2 Enabling parallelization\nTo enable multiple cores to draw bootstrap replicates, we could use Python’s built-in multiprocessing module. This module has some problems running with Jupyter notebooks on some OSs, though. As a drop-in replacement, we could use Mike McKerns’s multiprocess package, which offers improvements on the built-in multiprocessing module.\nTo enable seamless switching between these two multiprocessing packages, I included the following import statement.\ntry:\n    import multiprocess\nexcept:\n    import multiprocessing as multiprocess\nWe can therefore use them interchangably, calling whichever package we are using multiprocess. The syntax is as follows.\nwith multiprocess.Pool(n_jobs) as pool:\n    result = pool.starmap(fun, arg_iterable)\nHere, fun is a function and arg_iterable is an iterable that produces arguments to the fun as tuples that will be splatted when passed to fun. This is best seen with a simple example before we get into the more complicated example of bootstrapping. We will choose a function that adds four arguments. Then, arg_iterable will be an iterable (in this case a list) with tuples, each of which has four elements.\n\ndef fun(a, b, c, d):\n    return a + b + c + d\n\narg_iterable = [(i, i+1, i+2, i+3) for i in range(8)]\n\n# Display arg_iterable\narg_iterable\n\n[(0, 1, 2, 3),\n (1, 2, 3, 4),\n (2, 3, 4, 5),\n (3, 4, 5, 6),\n (4, 5, 6, 7),\n (5, 6, 7, 8),\n (6, 7, 8, 9),\n (7, 8, 9, 10)]\n\n\nIf we call fun with one of the arguments from the arg_iterable using a splat operator, we get the appropriate sum.\n\nfun(*arg_iterable[0])\n\n6\n\n\nTo iterate through the arguments and evaluate them in fun each time, we can do it in parallel, for example using two cores.\n\nwith multiprocess.Pool(2) as pool:\n    result = pool.starmap(fun, arg_iterable)\n    \n# Take a look\nresult\n\n[6, 10, 14, 18, 22, 26, 30, 34]\n\n\nThis gives the same result as if we did the following.\n\n[fun(*args) for args in arg_iterable]\n\n[6, 10, 14, 18, 22, 26, 30, 34]\n\n\nWe can see the equivalence then.\nwith multiprocess.Pool(2) as pool:\n    result = pool.starmap(fun, arg_iterable)\nis the same as\nresult = [fun(*args) for args in arg_iterable]\nWe can use this same structure for our bootstrap replicates. Here, fun is _draw_bs_reps_mle, and we need to make a list of arguments to pass to that function. This means splitting up size into chunks of size size // n_jobs,\n\ndef draw_parametric_bs_reps_mle(\n    mle_fun, gen_fun, data, args=(), size=1, n_jobs=1, progress_bar=False\n):\n    \"\"\"Draw nonparametric bootstrap replicates of maximum likelihood estimator.\n    \n    Parameters\n    ----------\n    mle_fun : function\n        Function with call signature mle_fun(data, *args) that computes\n        a MLE for the parameters\n    gen_fun : function\n        Function to randomly draw a new data set out of the model\n        distribution parametrized by the MLE. Must have call\n        signature `gen_fun(*params, size, *args, rg)`.\n    data : one-dimemsional Numpy array\n        Array of measurements\n    args : tuple, default ()\n        Arguments to be passed to `mle_fun()`.\n    size : int, default 1\n        Number of bootstrap replicates to draw.\n    n_jobs : int, default 1\n        Number of cores to use in drawing bootstrap replicates.\n    progress_bar : bool, default False\n        Whether or not to display progress bar.\n        \n    Returns\n    -------\n    output : numpy array\n        Bootstrap replicates of MLEs.\n    \"\"\"\n    # Just call the original function if n_jobs is 1 (no parallelization)\n    if n_jobs == 1:\n        return _draw_parametric_bs_reps_mle(\n            mle_fun, gen_fun, data, args=args, size=size, progress_bar=progress_bar\n        )\n\n    # Set up sizes of bootstrap replicates for each core, making sure we\n    # get all of them, even if sizes % n_jobs != 0\n    sizes = [size // n_jobs for _ in range(n_jobs)]\n    sizes[-1] += size - sum(sizes)\n\n    # Build arguments\n    arg_iterable = [(mle_fun, gen_fun, data, args, s, progress_bar, None) for s in sizes]\n\n    with multiprocess.Pool(n_jobs) as pool:\n        result = pool.starmap(_draw_parametric_bs_reps_mle, arg_iterable)\n\n    return np.concatenate(result)\n\nNow, we can specify n_jobs to be greater than one to take advantage of multiple cores. It is important to know how many cores you have on your machine. I usually keep one or two cores open for other processes on my computer so it doesn’t die on me. You can check how many cores you have using the multiprocess.cpu_count() function.\n\nmultiprocess.cpu_count()\n\n8\n\n\nMy computer has eight cores; I will therefore use seven cores. And now that I am unleashing nine cores on the problem, I will take 10,000 replicates again, and this time it will only take about 1/7 as long as it did the first time I did the calculation.\n\nbs_reps = draw_parametric_bs_reps_mle(\n    mle_iid_nbinom,\n    gen_nbinom,\n    df[\"Nanog\"].to_numpy(),\n    args=(),\n    size=10000,\n    n_jobs=7,\n    progress_bar=True,\n)\n\n100%|███████████████████████████████████████████████████████| 1428/1428 [00:20&lt;00:00, 69.00it/s]\n100%|███████████████████████████████████████████████████████| 1428/1428 [00:20&lt;00:00, 68.91it/s]\n100%|███████████████████████████████████████████████████████| 1428/1428 [00:20&lt;00:00, 68.75it/s]\n100%|███████████████████████████████████████████████████████| 1428/1428 [00:20&lt;00:00, 68.63it/s]\n100%|███████████████████████████████████████████████████████| 1428/1428 [00:20&lt;00:00, 68.50it/s]\n100%|███████████████████████████████████████████████████████| 1432/1432 [00:20&lt;00:00, 68.46it/s]\n100%|███████████████████████████████████████████████████████| 1428/1428 [00:20&lt;00:00, 68.09it/s]\n\n\nFrom these replicates, we can compute the confidence intervals for \\(\\alpha\\) and \\(b\\) for Nanog.\n\nnp.percentile(bs_reps, [2.5, 97.5], axis=0)\n\narray([[ 1.09689208, 56.59336337],\n       [ 1.49451796, 82.48374033]])",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Parallel bootstrap calculations</span>"
    ]
  },
  {
    "objectID": "lessons/mle/parallel_conf_ints.html#parameter-estimation-for-all-genes",
    "href": "lessons/mle/parallel_conf_ints.html#parameter-estimation-for-all-genes",
    "title": "40  Parallel bootstrap calculations",
    "section": "40.3 Parameter estimation for all genes",
    "text": "40.3 Parameter estimation for all genes\nWe are now empowered to compute the MLE and confidence intervals for all four genes. We will do so with parametric bootstrap. We can ignore the warnings, as they occur when the solver attempts to try illegal (negative) parameter values. Powell’s method is immune to this issue.\n\ngenes = [\"Nanog\", \"Prdm14\", \"Rest\", \"Rex1\"]\n\n# Data frame to store results\ndf_mle = pl.DataFrame(\n    schema=[\n        (\"gene\", str),\n        (\"parameter\", str),\n        (\"mle\", float),\n        (\"conf_int_low\", float),\n        (\"conf_int_high\", float),\n    ]\n)\n\n# Perform MLE for each gene\nfor gene in tqdm.tqdm(genes):\n    mle = mle_iid_nbinom(df[gene].to_numpy())\n\n    bs_reps = draw_parametric_bs_reps_mle(\n        mle_iid_nbinom,\n        gen_nbinom,\n        df[gene].to_numpy(),\n        args=(),\n        size=10000,\n        n_jobs=7,\n        progress_bar=False,\n    )\n    conf_int = np.percentile(bs_reps, [2.5, 97.5], axis=0)\n\n    sub_df = pl.DataFrame(\n        {\n            \"gene\": [gene] * 2,\n            \"parameter\": [\"alpha\", \"b\"],\n            \"mle\": mle,\n            \"conf_int_low\": conf_int[0, :],\n            \"conf_int_high\": conf_int[1, :],\n        }\n    )\n    df_mle = pl.concat([df_mle, sub_df])\n\n100%|█████████████████████████████████████████████████████████████| 4/4 [01:35&lt;00:00, 23.88s/it]\n\n\nWe now have a data frame with the results from our statistical inference. Let’s take a look.\n\ndf_mle\n\n\nshape: (8, 5)\n\n\n\ngene\nparameter\nmle\nconf_int_low\nconf_int_high\n\n\nstr\nstr\nf64\nf64\nf64\n\n\n\n\n\"Nanog\"\n\"alpha\"\n1.263097\n1.091114\n1.487293\n\n\n\"Nanog\"\n\"b\"\n69.347842\n57.072887\n82.716729\n\n\n\"Prdm14\"\n\"alpha\"\n0.552886\n0.453444\n0.685822\n\n\n\"Prdm14\"\n\"b\"\n8.200636\n6.15599\n10.544581\n\n\n\"Rest\"\n\"alpha\"\n4.530335\n3.86649\n5.457839\n\n\n\"Rest\"\n\"b\"\n16.543054\n13.584825\n19.454817\n\n\n\"Rex1\"\n\"alpha\"\n1.634562\n1.413569\n1.933929\n\n\n\"Rex1\"\n\"b\"\n84.680915\n69.805029\n100.023021\n\n\n\n\n\n\nFinally, we can make a plot of the MLEs of the parameter values with confidence interbals.\n\nsub_df = df_mle.filter(pl.col(\"parameter\") == \"alpha\")\nsummaries = [\n    {\"estimate\": est, \"conf_int\": conf, \"label\": label}\n    for est, conf, label in zip(\n        sub_df[\"mle\"].to_numpy(),\n        sub_df[[\"conf_int_low\", \"conf_int_high\"]].to_numpy(),\n        sub_df[\"gene\"],\n    )\n]\n\np1 = bebi103.viz.confints(\n        summaries,\n        x_axis_label=\"α*\",\n        frame_height=75,\n    )\n\nsub_df = df_mle.filter(pl.col(\"parameter\") == \"b\")\nsummaries = [\n    {\"estimate\": est, \"conf_int\": conf, \"label\": label}\n    for est, conf, label in zip(\n        sub_df[\"mle\"].to_numpy(),\n        sub_df[[\"conf_int_low\", \"conf_int_high\"]].to_numpy(),\n        sub_df[\"gene\"],\n    )\n]\np2 = bebi103.viz.confints(\n        summaries,\n        x_axis_label=\"b*\",\n        frame_height=75,\n    )\n\nbokeh.io.show(bokeh.layouts.column(p1, bokeh.models.Spacer(height=20), p2))",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Parallel bootstrap calculations</span>"
    ]
  },
  {
    "objectID": "lessons/mle/parallel_conf_ints.html#implementation-of-mle-bootstrapping-in-the-bebi103-package",
    "href": "lessons/mle/parallel_conf_ints.html#implementation-of-mle-bootstrapping-in-the-bebi103-package",
    "title": "40  Parallel bootstrap calculations",
    "section": "40.4 Implementation of MLE bootstrapping in the bebi103 package",
    "text": "40.4 Implementation of MLE bootstrapping in the bebi103 package\nThe functionality we have developed in this lesson for bootstrapping MLE estimates is available in the function bebi103.bootstrap.draw_bs_reps_mle(). It has call signature\nbebi103.draw_bs_reps_mle(\n    mle_fun,\n    gen_fun,\n    data,\n    mle_args=(),\n    gen_args=(),\n    size=1,\n    n_jobs=1,\n    progress_bar=False,\n    rg=None,\n):\nHere, mle_fun is a function with call signature mle_fun(data, *mle_fun_args) that computes the maximum likelihood estimate from the data. gen_fun is a function with call signature gen_fun(params, *gen_args, size, rg) that generates bootstrap samples to be used to make bootstrap replicates of the maximum likelihood estimates. Here, params is a tuple of the parameters of the model. Note that not all of the inputs for gen_fun are necessarily used, but having them allows for flexibility. The bebi103.bootstrap.draw_bs_reps_mle() function allows for both parametric and nonparametric bootstrapping.\nAs an example, we will repeat the bootstrapped MLE confidence region calculation for Nanog, except we will do it nonparametrically this time.\n\n# This is gen_fun. For nonparametric, just a resampling\n# params = (alpha, beta), but is ignored because we're just resampling\n# gen_params = (n,); this is used for nonparametric sample\ndef resample_fun(params, n, size, rng):\n    return rng.choice(n, size=size)\n\n\nbs_reps = bebi103.bootstrap.draw_bs_reps_mle(\n    mle_iid_nbinom,\n    resample_fun,\n    df['Nanog'].to_numpy(),\n    gen_args=(df['Nanog'].to_numpy(), ),\n    size=10000,\n    n_jobs=7,\n    progress_bar=True,\n)\n\n# Show the confidence interval\nnp.percentile(bs_reps, [2.5, 97.5], axis=0)\n\n100%|███████████████████████████████████████████████████████| 1428/1428 [00:23&lt;00:00, 60.17it/s]\n100%|███████████████████████████████████████████████████████| 1432/1432 [00:23&lt;00:00, 60.34it/s]\n100%|███████████████████████████████████████████████████████| 1428/1428 [00:23&lt;00:00, 59.97it/s]\n100%|███████████████████████████████████████████████████████| 1428/1428 [00:23&lt;00:00, 59.73it/s]\n100%|███████████████████████████████████████████████████████| 1428/1428 [00:23&lt;00:00, 59.64it/s]\n100%|███████████████████████████████████████████████████████| 1428/1428 [00:23&lt;00:00, 59.57it/s]\n100%|███████████████████████████████████████████████████████| 1428/1428 [00:23&lt;00:00, 59.54it/s]\n\n\narray([[ 1.05605495, 57.21452956],\n       [ 1.54194682, 82.706094  ]])\n\n\nThe confidence intervals are slightly different than in the parametric case, but are quite similar.",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Parallel bootstrap calculations</span>"
    ]
  },
  {
    "objectID": "lessons/mle/parallel_conf_ints.html#computing-environment",
    "href": "lessons/mle/parallel_conf_ints.html#computing-environment",
    "title": "40  Parallel bootstrap calculations",
    "section": "40.5 Computing environment",
    "text": "40.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p multiprocess,numpy,scipy,polars,tqdm,bokeh,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nmultiprocess: 0.70.18\nnumpy       : 2.2.6\nscipy       : 1.16.0\npolars      : 1.31.0\ntqdm        : 4.67.1\nbokeh       : 3.7.3\nbebi103     : 0.1.28\njupyterlab  : 4.4.5",
    "crumbs": [
      "Parameter estimation",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Parallel bootstrap calculations</span>"
    ]
  },
  {
    "objectID": "lessons/models/oft_encountered_models.html",
    "href": "lessons/models/oft_encountered_models.html",
    "title": "Oft encountered models",
    "section": "",
    "text": "We have thus far emphasized that model-building should be bespoke, which is to say that whatever models we build should be what we think best describes the data generation process, and not necessarily models that are well-known or with very convenient analytical forms of their maximum-likelihood estimates of p-values. Though, as stated in the Zen of Python, practicality beats purity, and sometimes those handle models and their established results are useful.\nIn the following lessons, we focus on three models, or classes of models, that come up very frequently in model building for statistical inference in the biological sciences. First, in lesson 41  Mixture models, we discuss mixture models. Then, in lessons 42  Variate-covariate models through 45  Implementation of MLE for variate-covariate models, we discuss variate-covariate models, which result in inference procedures commonly referred to as curve fitting. Finally, in lessons 46  Principal component analysis and 47  Implementation of PCA, we present principal components analysis.",
    "crumbs": [
      "Oft encountered models"
    ]
  },
  {
    "objectID": "lessons/models/mixture_models.html",
    "href": "lessons/models/mixture_models.html",
    "title": "41  Mixture models",
    "section": "",
    "text": "41.1 Multiple cell types?\n| Download notebook\nDataset download\nIn a previous lesson, we explored a Negative-Binomial model for mRNA counts. The data set we used came from this paper from the Elowitz lab and featured smFISH performed in 279 cells for the genes rex1, rest, nanog, and prdm14. The data set may be downloaded here.\nWhile the Negative Binomial model could work for some genes, exploratory data analysis shows an obvious bimodality in the distribution of counts of Rex1 transcripts.\ndf = pl.read_csv(os.path.join(data_path, \"singer_transcript_counts.csv\"), comment_prefix=\"#\")\n\np = iqplot.ecdf(\n    data=df, q=\"Rex1\", x_axis_label=\"Rex1 mRNA count\", conf_int=True\n)\n\nbokeh.io.show(p)\nWe can define a cell type by its expression profile. The bimodality of the ECDF suggests that there are more than one cell types present, since the ECDF clearly does not follow a Negative Binomial distribution.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Mixture models</span>"
    ]
  },
  {
    "objectID": "lessons/models/mixture_models.html#a-negative-binomial-mixture-model",
    "href": "lessons/models/mixture_models.html#a-negative-binomial-mixture-model",
    "title": "41  Mixture models",
    "section": "41.2 A Negative Binomial mixture model",
    "text": "41.2 A Negative Binomial mixture model\nWhat is the story for a distribution of mRNA counts coming from more than one cell type? To be concrete, let’s say we ahve two cell types, one with one level of bursty expression of the gene of interest, and another with a different level of bursty expression. We would expect the number of mRNA transcripts to be distributed according to a linear combination of negative binomial distributions. Such a linear combination of distributions is called a mixture model. For this mixture model, we can write out the PMF for an individual measurement of \\(n\\) mRNA transcripts as\n\\[\\begin{align}\nf(n\\mid \\alpha_1, \\beta_1, \\alpha_2, \\beta_2, w) &=\nw\\,\\frac{(n + \\alpha_1 - 1)!}{n!\\,(\\alpha_1-1)!}\\,\\left(\\frac{\\beta_1}{1+\\beta_1}\\right)^{\\alpha_1}\\left(\\frac{1}{1+\\beta_1}\\right)^{n} \\\\[1em]\n&\\;\\;\\;\\;+ (1-w) \\,\\frac{(n + \\alpha_2 - 1)!}{n!\\,(\\alpha_2-1)!}\\,\\left(\\frac{\\beta_2}{1+\\beta_2}\\right)^{\\alpha_2}\\left(\\frac{1}{1+\\beta_2}\\right)^{n}.\n\\end{align}\\]\nWhere, \\(w\\) is the fraction of cells that are of type one (burst size and frequency are determined by \\(1/\\beta_1\\) and \\(\\alpha_1\\)), and \\(1-w\\) is the fraction of cells that are of type two. We can write this likelihood more concisely as\n\\[\\begin{align}\n&n_i \\sim w \\, \\text{NegBinom}(\\alpha_1, \\beta_1) + (1-w)\\,\\text{NegBinom}(\\alpha_2, \\beta_2) \\; \\forall i, \\\\[1em]\n&b_1 = 1/\\beta_1, \\\\[1em]\n&b_2 = 1/\\beta_2.\n\\end{align}\\]\n(Recall from the last lesson that we are choosing to parametrize the Negative Binomial distributions with the burst size \\(b\\), related to \\(\\beta\\) as \\(b = 1/\\beta\\), so going forward we will use \\(b_1\\) and \\(b_2\\).)",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Mixture models</span>"
    ]
  },
  {
    "objectID": "lessons/models/mixture_models.html#mle-for-a-mixture-model",
    "href": "lessons/models/mixture_models.html#mle-for-a-mixture-model",
    "title": "41  Mixture models",
    "section": "41.3 MLE for a mixture model",
    "text": "41.3 MLE for a mixture model\nMixture models offer some unique challenges in modeling. They are revealed by looking at the log likelihood, which I write again here for a single measurement \\(n\\) for clarity.\n\\[\\begin{align}\n\\ell(n\\mid \\alpha_1, b_1, \\alpha_2, b_2, w) = \\ln(w\\,x_1 + (1-w)x_2),\n\\end{align}\\]\nwhere\n\\[\\begin{align}\nx_i = \\frac{(n + \\alpha_i - 1)!}{n!\\,(\\alpha_i-1)!}\\,\\left(\\frac{1}{1+b_i}\\right)^{\\alpha_i}\\left(\\frac{b_i}{1+b_i}\\right)^{n}.\n\\end{align}\\]\n\n41.3.1 The log-sum-exp trick\nWhile the logarithm of a product is conveniently split, we cannot split the logarithm of a sum. If we compute the sum directly, we will get serious underflow errors for parameters for which the terms \\(x_1\\) or \\(x_2\\) are small. To compute this is a more numerically stable way, we need to use the log-sum-exp trick. To do this, we re-write the \\(x_i\\) as \\(x_i = \\mathrm{e}^{\\ln x_i}\\).\n\\[\\begin{align}\n\\ln f(n\\mid \\alpha_1, b_1, \\alpha_2, b_2, w) = \\ln(w\\,\\mathrm{e}^{\\ln x_1} + (1-w)\\mathrm{e}^{\\ln x_2}).\n\\end{align}\\]\nFor a moment, let us assume that \\(x_1 &gt; x_2\\). Then, we can factor the log likelihood as\n\\[\\begin{align}\n\\ln f(n\\mid \\alpha_1, b_1, \\alpha_2, b_2, w) &= \\ln\\left[\\mathrm{e}^{\\ln x_1}\\left(w + (1-w)\\mathrm{e}^{\\ln x_2 - \\ln x_1}\\right)\\right] \\\\[1em]\n&= \\ln x_1 + \\ln\\left(w + (1-w)\\mathrm{e}^{\\ln x_2 - \\ln x_1}\\right).\n\\end{align}\\]\nProvided \\(w\\) is not too close to zero, the argument of the logarithm of the sum is now of order \\(w\\). So, when computing the log of the sum of weighted probabilities, we should factor out the largest term and then proceed. This is known as the log-sum-exp trick. Conveniently, this is implemented in Scipy as scipy.special.logsumexp(). Be sure to read the docs to understand how this works, and we will use it below in our log-likelihood calculation.\n\n\n41.3.2 Model identifiability\nA nonidentifiable model is a model for which we cannot unambiguously determine the parameter values. That is, two or more parameter sets are observationally equivalent. This is the case for this mixture model because of label switching. In this mixture model, it is arbitrary which \\((\\alpha, b)\\) pair we label as \\((\\alpha_1, b_1)\\) or \\((\\alpha_2, b_2)\\). We can switch the labels, and also change \\(w\\) to \\(1-w\\), and we have exactly the same log likelihood. You can imagine that if we have a mixture of more than two distributions, the label switching problem gets worse, as we can have various combinations of label switching.\nThere is no one way to get around this kind of nonidentifiability (or many other kinds of nonidentifiability, for that matter). Since we are finding a maximum likelihood estimate, and the value of the likelihood is identical upon switching labels, we do not really need to worry about it here, though this could become a problem when computing confidence intervals, since we need to do the MLE calculation many times with different bootstrap sample data sets each time. We will proceed to compute the confidence interval and check to see if the label-switching nonidentifiability presents itself in the results.\n\n\n41.3.3 The log-likelihood\nWith these two considerations in mind, let’s code up the log-likelihood.\n\ndef log_like_mix(alpha1, b1, alpha2, b2, w, n):\n    \"\"\"Log-likeihood of binary Negative Binomial mixture model.\"\"\"\n    # Enforce w as a probability\n    if w &lt; 0 or w &gt; 1:\n        return -np.inf\n\n    # Physical bounds on parameters\n    if alpha1 &lt; 0 or alpha2 &lt; 0 or b1 &lt; 0 or b2 &lt; 0:\n        return -np.inf\n\n    # Pieces of log likelihood\n    logx1 = st.nbinom.logpmf(n, alpha1, 1 / (1 + b1))\n    logx2 = st.nbinom.logpmf(n, alpha2, 1 / (1 + b2))\n\n    # Multipliers for log-sum-exp\n    lse_coeffs = np.tile([w, 1 - w], [len(n), 1]).transpose()\n\n    # log-likelihood for each measurement\n    log_likes = scipy.special.logsumexp(np.vstack([logx1, logx2]), axis=0, b=lse_coeffs)\n\n    return np.sum(log_likes)\n\n\n\n41.3.4 Initial guess\nThe optimization problem is more difficult now, since we have five parameters. We should provide the best initial guesses we can. To come up with guesses, we note in the ECDF that the second inflection point occurs around a value of the ECDF of 0.2. We thus can guess \\(w \\approx 0.2\\). To get guesses for \\(\\alpha_1\\) and \\(b_1\\), we take the first 20% of the counts and get an MLE using the single-Negative Binomial model. We do the same for the remaining 80% of counts to get guesses for \\(\\alpha_2\\) and \\(b_2\\). To do this, we need to borrow our MLE function from a previous lesson.\n\ndef log_like_iid_nbinom(params, n):\n    \"\"\"Log likelihood for i.i.d. NBinom measurements, parametrized\n    by alpha, b=1/beta.\"\"\"\n    alpha, b = params\n\n    if alpha &lt;= 0 or b &lt;= 0:\n        return -np.inf\n\n    return np.sum(st.nbinom.logpmf(n, alpha, 1/(1+b)))\n\n\ndef mle_iid_nbinom(n):\n    \"\"\"Perform maximum likelihood estimates for parameters for i.i.d.\n    NBinom measurements, parametrized by alpha, b=1/beta\"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        res = scipy.optimize.minimize(\n            fun=lambda params, n: -log_like_iid_nbinom(params, n),\n            x0=np.array([3, 3]),\n            args=(n,),\n            method='Powell'\n        )\n\n    if res.success:\n        return res.x\n    else:\n        raise RuntimeError('Convergence failed with message', res.message)\n\nWe can now write a function to get an initial guess of the \\(\\alpha\\)’s and \\(b\\)’s.\n\ndef initial_guess_mix(n, w_guess):\n    \"\"\"Generate initial guess for mixture model.\"\"\"\n    n_low = n[n &lt; np.percentile(n, 100 * w_guess)]\n    n_high = n[n &gt;= np.percentile(n, 100 *  w_guess)]\n    \n    alpha1, b1 = mle_iid_nbinom(n_low)\n    alpha2, b2 = mle_iid_nbinom(n_high)\n    \n    return alpha1, b1, alpha2, b2\n\nLet’s give it a test run.\n\n# Extract the values for Rex1\nn = df['Rex1'].to_numpy()\n\n# Guess for w\nw_guess = 0.2\n\n# Generate initial guess\ninitial_guess_mix(n, w_guess)\n\n(2.2311645373503657, 8.87458109227595, 6.5393762491360174, 25.47713821741079)\n\n\nThe guesses seem reasonable, so we will proceed using this method of generating initial guesses.\n\n\n41.3.5 Performing the optimization\nNow we’ll code up a function to generate the initial guess, and then perform the optimization. Note that below I have decreased the tolerance down to \\(10^{-6}\\). I found this to be necessary for this problem; the solver would be off by about 5% if I gave it similar, but different, initial guesses. The lower the tolerance, the closer to zero you require the gradient of the log likelihood to be.\n\ndef mle_mix(n, w_guess):\n    \"\"\"Obtain MLE estimate for parameters for binary mixture \n    of Negative Binomials.\"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        res = scipy.optimize.minimize(\n            fun=lambda params, n: -log_like_mix(*params, n),\n            x0=[*initial_guess_mix(n, w_guess), w_guess],\n            args=(n,),\n            method='Powell',\n            tol=1e-6,\n        )\n\n    if res.success:\n        return res.x\n    else:\n        raise RuntimeError('Convergence failed with message', res.message)    \n\nLet’s get our MLE!\n\npopt = mle_mix(n, w_guess)\n\n# Check out optimal parameters\nprint('α1 = {0:f}\\nb1 = {1:f}\\nα2 = {2:f}\\nb2 = {3:f}\\nw = {4:f}'.format(*popt))\n\nα1 = 3.497007\nb1 = 4.104918\nα2 = 5.089626\nb2 = 31.810367\nw = 0.160422\n\n\n\n\n41.3.6 Comparison to empirical CDF\nLet’s now see how the CDFs parametrized by the MLE compare with the ECDF. The theoretical CDF for a mixture model is computed from the weights of each distribution in the mixture. If the CDF from the first Negative Binomial distribution is \\(F_1(n;\\alpha_1^*,b_1^*)\\) and that of the second is \\(F_2(n;\\alpha_2^*,b_2^*)\\), then the theoretical ECDF for the mixture model, parametrized by the MLE, is\n\\[\\begin{align}\nF(n;\\alpha_1^*, b_1^*, \\alpha_2^*, b_2^*, w^*) = w^* F_1(n;\\alpha_1^*,b_1^*) + (1-w^*) F_2(n;\\alpha_2^*,b_2^*).\n\\end{align}\\]\n\n# Unpack parameters\nalpha1_mle, b1_mle, alpha2_mle, b2_mle, w_mle = popt\n\nn_theor = np.arange(0, n.max()+1)\ncdf_theor = w_mle * st.nbinom.cdf(n_theor, alpha1_mle, 1/(1+b1_mle))\ncdf_theor += (1 - w_mle) * st.nbinom.cdf(n_theor, alpha2_mle, 1/(1+b2_mle))\n\n# Weave together to make staircase for discrete distribution\nn_plot, cdf_plot = bebi103.viz.cdf_to_staircase(n_theor, cdf_theor)\n\np.line(n_plot, cdf_plot, line_color='orange', line_width=2)\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nAlternatively, and in fact better, we can make a predictive ECDF plot. We will not do that here, as predictive ECDFs are covered in future lessons.\nWe can, for comparison, also add the theoretical CDF for the Negative Binomial (non-mixture) model.\n\nalpha_mle, b_mle = mle_iid_nbinom(n)\n\ncdf_theor = st.nbinom.cdf(n_theor, alpha_mle, 1 / (1 + b_mle))\n\n# Weave together to make staircase for discrete distribution\nn_plot, cdf_plot = bebi103.viz.cdf_to_staircase(n_theor, cdf_theor)\n\np.line(n_plot, cdf_plot, line_color='tomato', line_width=2)\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe will implement graphical methods of model assessment later in this lesson, but it is clear from the crude plot above that the Negative Binomial model is not commensurate with the data, but the mixture model is.\n\n\n41.3.7 Confidence interval for the mixture model\nLet’s now compute a confidence interval for the mixture model. To use bebi103.bootstrap.draw_bs_reps_mle(), we need to first write a function to generate data out of the mixture model. To do this, we first draw a random number to see which Negative Binomial distribution to draw the count out of (either the one parametrized with \\(\\alpha_1\\) and \\(b_1\\), or the one parametrized with \\(\\alpha_2\\) and \\(b_2\\)). We then draw a count out of the distribution we chose.\n\ndef gen_mix(params, size, rng):\n    \"\"\"Generate data for the mixture model.\"\"\"\n    alpha1, b1, alpha2, b2, w = params\n    \n    n = np.empty(size)\n    for i in range(size):\n        low_cell_type = rng.uniform() &lt; w\n        \n        if low_cell_type:\n            n[i] = rng.negative_binomial(alpha1, 1/(1+b1))\n        else:\n            n[i] = rng.negative_binomial(alpha2, 1/(1+b2))\n            \n    return n\n\nNow that we have this function in hand, we can generate our bootstrap samples (in parallel). This can take a while.\n\nbs_reps = bebi103.bootstrap.draw_bs_reps_mle(\n    mle_mix,\n    gen_mix,\n    n,\n    mle_args=(w_guess,),\n    gen_args=(),\n    size=10_000,\n    n_jobs=9,\n)\n\nNow that we have the bootstrap samples, we can compute our confidence intervals by taking percentiles. The columns are respectively for \\(\\alpha_1^*\\), \\(b_1^*\\), \\(\\alpha_2^*\\), \\(b_2^*\\), and \\(w^*\\).\n\nnp.percentile(bs_reps, [2.5, 97.5], axis=0)\n\narray([[ 2.00339497,  1.5458858 ,  4.16832771, 25.1146269 ,  0.11619696],\n       [ 8.41091976,  8.45923126,  6.45015086, 38.94580183,  0.20824756]])\n\n\nWe can visualize the confidence region with a corner plot.\n\n# Package replicates in data frame for plotting\ndf_res = pl.DataFrame(data=bs_reps, schema=[\"α1*\", \"b1*\", \"α2*\", \"b2*\", \"w*\"])\n\np = bebi103.viz.corner(\n    samples=df_res,#.loc[inds, :],\n    parameters=[\"α1*\", \"b1*\", \"α2*\", \"b2*\", \"w*\"],\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe corner plot of the confidence region shows that \\(\\alpha_1^*\\) and \\(b_1^*\\) are strongly correlated, as are \\(\\alpha_2^*\\) and \\(b_2^*\\). However, the MLEs of the parameters for the respective Negative Binomials are not correlated with each other.\nWe note that because \\(w\\) is always much smaller than 1/2 and unimodal, we are having problems with the label-switching nonidentifiability.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Mixture models</span>"
    ]
  },
  {
    "objectID": "lessons/models/mixture_models.html#a-mixture-model-for-all-cells",
    "href": "lessons/models/mixture_models.html#a-mixture-model-for-all-cells",
    "title": "41  Mixture models",
    "section": "41.4 A mixture model for all cells",
    "text": "41.4 A mixture model for all cells\nIf we assume a Negative Binomial (non-mixture) model for the mRNA transcript counts, we can independently perform maximum likelihood estimates for the burst frequency and burst size for each gene. However, in the mixture model, we do need to take into account the fact that the mRNA counts for the respective genes are measured in the same cells. In our modeling, that is manifest in the parameter \\(w\\), which specifies the relative fraction of cell types, being shared among all four genes. The generative PMF for a single set of observations \\(\\mathbf{n} = \\{n_\\mathrm{Rex1}, n_\\mathrm{Rest}, n_\\mathrm{Nanog}, n_\\mathrm{Prdm14}\\}\\) is given by\n\\[\\begin{align}\nf(\\mathbf{n};\\boldsymbol{\\alpha}, \\mathbf{b}, w) = w\\prod_j x_{1j} + (1-w)\\prod_j x_{2j},\n\\end{align}\\]\nwhere\n\\[\\begin{align}\nx_{ij} = \\frac{(n + \\alpha_{ij} - 1)!}{n!\\,(\\alpha_{ij}-1)!}\\,\\left(\\frac{1}{1+b_{ij}}\\right)^{\\alpha_{ij}}\\left(\\frac{b_{ij}}{1+b_{ij}}\\right)^{n},\n\\end{align}\\]\nwhere \\(i\\) indexes the distribution of the mixture and \\(j\\) indexes the gene. We can code up the log likelihood of this by modifying our original log_like_mix() function to take in arrays.\n\ndef log_like_mix_all(alpha1, b1, alpha2, b2, w, n):\n    \"\"\"Log-likelihood of a mixuture model for\n    all genes. Here, `alpha1`, `b1`, `alpha2`, and `b2` are\n    all length 4 arrays, and `n` is 279 by 4. The\n    parameter `w`, however is a scalar, shared by all genes.\n    \"\"\"\n    # Fix nonidentifiability be enforcing values of w\n    if w &lt; 0 or w &gt; 1.0:\n        return -np.inf\n\n    # Physical bounds on parameters\n    if np.any(alpha1 &lt; 0) or np.any(alpha2 &lt; 0) or np.any(b1 &lt; 0) or np.any(b2 &lt; 0):\n        return -np.inf\n\n    # The sum is over genes\n    logx1 = st.nbinom.logpmf(n, alpha1, 1 / (1 + b1)).sum(axis=1)\n    logx2 = st.nbinom.logpmf(n, alpha2, 1 / (1 + b2)).sum(axis=1)\n\n    # Multipliers for log-sum-exp\n    lse_coeffs = np.tile([w, 1 - w], [len(n), 1]).transpose()\n\n    # log-likelihood for each measurement\n    log_likes = scipy.special.logsumexp(np.vstack([logx1, logx2]), axis=0, b=lse_coeffs)\n\n    return np.sum(log_likes)\n\nThe initial guess for this model is also a bit more challenging. While we can see immediately from the Rex1 counts that the lower mRNA counts account for about 20% of the cells, this is not obvious in the others. We can, however label a “pseudo cell type” for each row of the data frame; if the Rex1 expression is below the percentile corresponding to our guess for the value of \\(w\\), we consider this one part of the mixture, and if it is above, it is part of the other.\n\ndef initial_guess_mix_all(df, w_guess):\n    pseudo_cell_type = df[\"Rex1\"] &lt; np.percentile(df[\"Rex1\"], 100*w_guess)\n\n    guess = []\n    \n    for gene in df.schema:\n        n = df[gene].to_numpy()\n        n_low = n[pseudo_cell_type]\n        n_high = n[~pseudo_cell_type]\n    \n        alpha1, b1 = mle_iid_nbinom(n_low)\n        alpha2, b2 = mle_iid_nbinom(n_high)\n        \n        guess += [alpha1, b1, alpha2, b2]\n    \n    return guess + [w_guess]\n\nFor speed later when we use bootstrap methods to compute the confidence intervals, we will compute this initial guess once and use it over and over to save on the four optimizations that are necessary to compute the initial guess.\n\nparams_0 = initial_guess_mix_all(df, 0.2)\n\nAlso for speed, we should extract the counts from the data frame as a Numpy array.\n\nn = df.to_numpy()\n\nNow, we can write a function to perform the MLE. First, we need to write a function to compute the negative log likelihood, including unpacking the 17 parameters.\n\ndef neg_log_like_all(params, n):\n    \"\"\"\n    Negative log-likelihood.\n    \"\"\"\n    n_genes = n.shape[1]\n    alpha1 = params[:n_genes]\n    b1 = params[n_genes:2*n_genes]\n    alpha2 = params[2*n_genes:3*n_genes]\n    b2 = params[3*n_genes:4*n_genes]\n    w = params[-1]\n    \n    return -log_like_mix_all(alpha1, b1, alpha2, b2, w, n)\n\nFinally, we can perform the optimization. Because we will use bootstrapping in a moment to compute confidence intervals for the parameters, we will encapsulate the MLE calculation in a function.\n\ndef mle_mix_all(n, params_0):\n    \"\"\"Obtain MLE estimate for parameters for binary mixture \n    of Negative Binomials.\"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        \n        res = scipy.optimize.minimize(\n            fun=neg_log_like_all,\n            x0=params_0,\n            args=(n,),\n            method='powell',\n            tol=1e-6,\n        )\n\n    if res.success:\n        return res.x\n    else:\n        raise RuntimeError('Convergence failed with message', res.message)    \n\nLet’s do it!\n\nmle = mle_mix_all(n, params_0)\n\n# Put in a data frame for ease of reference\ndf_mle = pl.DataFrame(\n    data=np.concatenate((mle, np.array([mle[-1]] * 3))).reshape(5, 4),\n    schema=[(col, float) for col in df.schema],\n)\ndf_mle = df_mle.with_columns(parameter=pl.Series([\"alpha1\", \"b1\", \"alpha2\", \"b2\", \"w\"]))\n\n# Take a look at the results\ndf_mle\n\n\nshape: (5, 5)\n\n\n\nRex1\nRest\nNanog\nPrdm14\nparameter\n\n\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n7.182746\n8.977211\n5.392818\n0.956715\n\"alpha1\"\n\n\n24.600379\n9.607683\n21.07764\n6.468837\n\"b1\"\n\n\n1.112493\n3.56355\n1.209395\n0.440802\n\"alpha2\"\n\n\n45.001282\n13.736508\n18.132925\n1.665332\n\"b2\"\n\n\n0.696519\n0.696519\n0.696519\n0.696519\n\"w\"\n\n\n\n\n\n\nThe MLEs converged on \\(w \\approx 0.7\\), which corresponds to \\(w \\approx 0.3\\) from before (we encountered label switching in this optimization). We can do a quick graphical assessment of the model.\n\ndef plot_theor_cdf(gene, p):\n    \"\"\"Add a theoretical CDF to plot\"\"\"\n    n_theor = np.arange(0, df[gene].max() + 1)\n    w, alpha1, alpha2, b1, b2 = [\n        df_mle.filter(pl.col(\"parameter\") == param)[gene].item()\n        for param in (\"w\", \"alpha1\", \"alpha2\", \"b1\", \"b2\")\n    ]\n    cdf_theor = w * st.nbinom.cdf(n_theor, alpha1, 1 / (1 + b1))\n    cdf_theor += (1 - w) * st.nbinom.cdf(n_theor, alpha2, 1 / (1 + b2))\n\n    # Weave together to make staircase for discrete distribution\n    n_plot, cdf_plot = bebi103.viz.cdf_to_staircase(n_theor, cdf_theor)\n\n    p.line(n_plot, cdf_plot, color=\"orange\", line_width=2)\n\n    return p\n\n\n# Make a list of plots\nplots = [\n    iqplot.ecdf(\n        data=df,\n        q=gene,\n        frame_height=125,\n        frame_width=250,\n        conf_int=True,\n        title=gene,\n    )\n    for gene in df.schema\n]\n\n# Add CDFs\nfor i, gene in enumerate(df.schema):\n    plots[i] = plot_theor_cdf(gene, plots[i])\n\n# Show them as a grid\nbokeh.io.show(bokeh.layouts.gridplot(plots, ncols=2))\n\n\n  \n\n\n\n\n\nThe mixture model seems to fit all four reasonably well, with the theoretical CDF parametrized by the MLE mostly falling within the 95% confidence interval of the ECDF. (We will explore other ways to visualize and assess models in later lessons.) However, we note that the Rex1 data do see some deviations on the tail of the distribution for small counts.\n\n41.4.1 Confidence intervals for the mixture model for all genes\nWe can attempt to compute confidence regions for the MLEs for this model for all four genes. We again start by writing a function to generate the data from the mixture model.\n\ndef gen_mix_all(params, size, rng):\n    \"\"\"Generate data for the mixture model.\"\"\"\n    alpha1 = params[:4]\n    b1 = params[4:8]\n    alpha2 = params[8:12]\n    b2 = params[12:16]\n    w = params[-1]\n    \n    n = np.empty((size, 4))\n    for i in range(size):\n        low_cell_type = rng.uniform() &lt; w\n        \n        if low_cell_type:\n            n[i] = rng.negative_binomial(alpha1, 1 / (1 + b1))\n        else:\n            n[i] = rng.negative_binomial(alpha2, 1 / (1 + b2))\n            \n    return n\n\nWe can now draw our bootstrap replicates. I will only draw a few replicates for now, though, and the reason will be clear when we see the result.\n\nbs_mle_reps = bebi103.bootstrap.draw_bs_reps_mle(\n    mle_mix_all,\n    gen_mix_all,\n    n,\n    mle_args=(params_0,),\n    gen_args=(),\n    size=100,\n    n_jobs=3,\n)\n\n\n---------------------------------------------------------------------------\nRemoteTraceback                           Traceback (most recent call last)\nRemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/bois/miniconda3/envs/bebi103_build/lib/python3.12/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bois/miniconda3/envs/bebi103_build/lib/python3.12/site-packages/multiprocess/pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bois/Dropbox/git/bebi103/bebi103/bootstrap.py\", line 559, in _draw_bs_reps_mle\n    mle_fun(gen_fun(params, *gen_args, size=len(data), rng=rng), *mle_args)\n  File \"/var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/ipykernel_41358/3990052399.py\", line 18, in mle_mix_all\n    raise RuntimeError('Convergence failed with message', res.message)\nRuntimeError: ('Convergence failed with message', 'Maximum number of function evaluations has been exceeded.')\n\"\"\"\n\nThe above exception was the direct cause of the following exception:\n\nRuntimeError                              Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 bs_mle_reps = bebi103.bootstrap.draw_bs_reps_mle(\n      2     mle_mix_all,\n      3     gen_mix_all,\n      4     n,\n      5     mle_args=(params_0,),\n      6     gen_args=(),\n      7     size=100,\n      8     n_jobs=3,\n      9 )\n\nFile ~/Dropbox/git/bebi103/bebi103/bootstrap.py:649, in draw_bs_reps_mle(mle_fun, gen_fun, data, mle_args, gen_args, size, n_jobs, progress_bar, rng)\n    643 arg_iterable = [\n    644     (mle_fun, gen_fun, data, mle_args, gen_args, s, progress_bar, None)\n    645     for s in sizes\n    646 ]\n    648 with multiprocess.Pool(n_jobs) as pool:\n--&gt; 649     result = pool.starmap(_draw_bs_reps_mle, arg_iterable)\n    651 return np.concatenate(result)\n\nFile ~/miniconda3/envs/bebi103_build/lib/python3.12/site-packages/multiprocess/pool.py:375, in Pool.starmap(self, func, iterable, chunksize)\n    369 def starmap(self, func, iterable, chunksize=None):\n    370     '''\n    371     Like `map()` method but the elements of the `iterable` are expected to\n    372     be iterables as well and will be unpacked as arguments. Hence\n    373     `func` and (a, b) becomes func(a, b).\n    374     '''\n--&gt; 375     return self._map_async(func, iterable, starmapstar, chunksize).get()\n\nFile ~/miniconda3/envs/bebi103_build/lib/python3.12/site-packages/multiprocess/pool.py:774, in ApplyResult.get(self, timeout)\n    772     return self._value\n    773 else:\n--&gt; 774     raise self._value\n\nFile ~/miniconda3/envs/bebi103_build/lib/python3.12/site-packages/multiprocess/pool.py:125, in worker()\n    123 job, i, func, args, kwds = task\n    124 try:\n--&gt; 125     result = (True, func(*args, **kwds))\n    126 except Exception as e:\n    127     if wrap_exception and func is not _helper_reraises_exception:\n\nFile ~/miniconda3/envs/bebi103_build/lib/python3.12/site-packages/multiprocess/pool.py:51, in starmapstar()\n     50 def starmapstar(args):\n---&gt; 51     return list(itertools.starmap(args[0], args[1]))\n\nFile ~/Dropbox/git/bebi103/bebi103/bootstrap.py:559, in _draw_bs_reps_mle()\n    554 else:\n    555     iterator = range(size)\n    557 return np.array(\n    558     [\n--&gt; 559         mle_fun(gen_fun(params, *gen_args, size=len(data), rng=rng), *mle_args)\n    560         for _ in iterator\n    561     ]\n    562 )\n\nCell In[21], line 18, in mle_mix_all()\n     16     return res.x\n     17 else:\n---&gt; 18     raise RuntimeError('Convergence failed with message', res.message)\n\nRuntimeError: ('Convergence failed with message', 'Maximum number of function evaluations has been exceeded.')\n\n\n\nWe got a runtime error that the solver did not converge. This can happen; finding MLEs for a large number of parameters is very difficult and can often lead to this kind of failure. We will therefore not pursue confidence intervals for this more complicated mixture model.\nThat said, given the difficulty in finding MLEs for complicated models, we can’t be too sure that the single maximum likelihood estimate we got for the nineteen parameters of this model is even a good one. This should raise a reg flag for you. MLEs are very difficult to obtain for high-dimensional models, and when they are obtained, it is very difficult to verify that they do, in fact, give the maximum likelihood estimate.\nWe will deal with complex models and ways of investigating the failure modes in finding parameter estimates next term. For now, we will proceed by considering only the single-gene case of Rex1.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Mixture models</span>"
    ]
  },
  {
    "objectID": "lessons/models/mixture_models.html#the-expectation-maximization-algorithm",
    "href": "lessons/models/mixture_models.html#the-expectation-maximization-algorithm",
    "title": "41  Mixture models",
    "section": "41.5 The expectation-maximization algorithm",
    "text": "41.5 The expectation-maximization algorithm\nI close this lesson by briefly mentioning that the expectation-maximization (EM) algorithm is a commonly used effective algorithm for performing maximum likelihood estimates for mixture models. It alleviates some (but by no means all) of the problems for finding MLEs of complex models. We will not discuss it, but you should be aware of it.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Mixture models</span>"
    ]
  },
  {
    "objectID": "lessons/models/mixture_models.html#computing-environment",
    "href": "lessons/models/mixture_models.html#computing-environment",
    "title": "41  Mixture models",
    "section": "41.6 Computing environment",
    "text": "41.6 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,scipy,bokeh,iqplot,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.3\nIPython version      : 8.25.0\n\nnumpy     : 1.26.4\npolars    : 1.1.0\nscipy     : 1.13.1\nbokeh     : 3.4.1\niqplot    : 0.3.7\nbebi103   : 0.1.21\njupyterlab: 4.0.13",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Mixture models</span>"
    ]
  },
  {
    "objectID": "lessons/models/variate_covariate/intro_variate_covariate.html",
    "href": "lessons/models/variate_covariate/intro_variate_covariate.html",
    "title": "42  Variate-covariate models",
    "section": "",
    "text": "Consider a titration experiment to determine the dissociation constant of binding partners A and B. The total concentration of B, \\(c_\\mathrm{B}^0\\) is held constant while the total concentration of A, \\(c_\\mathrm{A}^0\\), is varied. The equilibrium concentration of the duplex AB, \\(c_\\mathrm{AB}\\), is measured for each value of \\(c_\\mathrm{A}^0\\). For the reaction AB ⇌ A + B with dissociation constant \\(K_d\\), we can solve for the concentration of the AB duplex using the equation\n\\[\\begin{align}\nK_d = \\frac{c_\\mathrm{A}\\,c_\\mathrm{B}}{c_\\mathrm{AB}} = \\frac{(c_\\mathrm{A}^0 - c_\\mathrm{AB})\\,(c_\\mathrm{A}^0 - c_\\mathrm{AB})}{c_\\mathrm{AB}},\n\\end{align}\n\\]\nwhich is solved to give\n\\[\\begin{align}\nc_\\mathrm{AB} = \\frac{1}{2}\\left(K_d + c_\\mathrm{A}^0 + c_\\mathrm{B}^0 - \\sqrt{\\left(K_d + c_\\mathrm{A}^0 + c_\\mathrm{B}^0\\right)^2 - 4c_\\mathrm{A}^0\\,c_\\mathrm{B}^0}\\right).\n\\end{align}\n\\]\nSo, we have an equation for our measured quantity \\(c_\\mathrm{AB}\\) in terms of our manipulated quantity \\(c_\\mathrm{A}^0\\) (with \\(c_\\mathrm{B}^0\\) held constant). This is an example of a variate-covariate model, where the measured quantity is called the variate and manipulated quantities are called covariates. This constitutes an important class of models that are routinely encountered in science.\nNote that analysis of variate-covariate models is often referred to as “curve fitting.” I eschew this term because I prefer to focus on the generative model, as opposed to whatever technique we may use to obtain estimates of its parameters.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Variate-covariate models</span>"
    ]
  },
  {
    "objectID": "lessons/models/variate_covariate/model_building.html",
    "href": "lessons/models/variate_covariate/model_building.html",
    "title": "43  Building a variate-covariate model",
    "section": "",
    "text": "43.1 End of theory?\nIn this lesson, we will learn how to construct a variate-covariate model and discuss some useful theoretical results related to the maximum likelihood estimates for the parameters of these models. As we usually do, we will have a concrete model in mind as an example. I will take this opportunity to illustrate the process of carefully building a model (this time in the context of variate-covariate modeling) and discuss why modeling is important for scientists.\nAs machine learning methods grow in power and prominence, and as data acquisition becomes more and more facile, we see more and more methods where a machine “learns” directly from data. Over a decade ago, Chris Anderson wrote an article entitled The End of Theory: The Data Deluge Makes the Scientific Method Obsolete in Wired Magazine. Anderson claimed that because we have access to large data sets, we no longer need the scientific method of testable hypotheses. Specifically, he says we do not need models, we can just use lots and lots of data to make predictions. This is absurd because if we just try to learn from data, we do not really learn anything fundamental about how nature works. If you are working for Netflix and trying to figure out what movies people want to watch, learning from data is fine. But if you’re a scientist and want to increase knowledge, you need models.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Building a variate-covariate model</span>"
    ]
  },
  {
    "objectID": "lessons/models/variate_covariate/model_building.html#an-example-exercise-in-modeling",
    "href": "lessons/models/variate_covariate/model_building.html#an-example-exercise-in-modeling",
    "title": "43  Building a variate-covariate model",
    "section": "43.2 An example exercise in modeling",
    "text": "43.2 An example exercise in modeling\nWe will introduce two competing models for how the size of mitotic spindles are set. Matt Good and coworkers (*Science*, 2013)) developed a microfluidic device where they could create droplets of cytoplasm extracted from Xenopus eggs and embryos, as shown the figure below (scale bar 20 µm; image taken from the paper).\n\n\n\nDroplets encapsulating mitotic spindles from Good and coworkers.\n\n\nA remarkable property about Xenopus extract is that mitotic spindles spontaneously form; the extracted cytoplasm has all the ingredients to form them. This makes it an excellent model system for studying spindles. With their device, Good and his colleagues were able to study how the size of the cell affects the dimensions of the mitotic spindle; a simple, yet beautiful, question. The experiment is conceptually simple; they made the droplets and then measured their dimensions and the dimensions of the spindles using microscope images.\nLet’s take a quick look at the result.\n\n\n\n\n\nBokeh Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe now propose two models for how the droplet diameter affects the spindle length.\n\nThe spindles have an inherent length, independent of droplet diameter.\nThe length of spindles is determined by the total amount of tubulin available to make them.\n\n\n43.2.1 Model 1: Spindle size is independent of droplet size\nAs a first model, we propose that the size of a mitotic spindle is inherent to the spindle itself. This means that the size of the spindle is independent of the size of the droplet or cell in which it resides. This would be the case, for example, if construction of the spindle involves length-sensing molecules, such as depolymerizing motor proteins. We define that set length as \\(\\phi\\).\n\n43.2.1.1 The statistical model\nNot all spindles will be measured to be exactly \\(\\phi\\) µm in length. Rather, there may be some variation about \\(\\phi\\) due to natural variation and measurement error. So, we would expect measured length of spindle i to be\n\\[\\begin{align}\nl_i = \\phi + e_i,\n\\end{align}\\]\nwhere \\(e_i\\) is the noise component of the ith datum.\nSo, we have a theoretical model for spindle length, \\(l = \\phi\\), and to get a fully generative model, we need to model the errors \\(e_i\\). A reasonable model assumes\n\nEach measured spindle’s length is independent of all others.\nThe variability in measured spindle length is Normally distributed.\n\nWith these assumptions, we can write the probability density function for \\(l_i\\) as\n\\[\\begin{align}\nf(l_i ; \\phi, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\,\\exp\\left[-\\frac{(l_i - \\phi)^2}{2\\sigma^2}\\right].\n\\end{align}\\]\nSince each measurement is independent, we can write the joint probability density function of the entire data set, which we will define as \\(\\mathbf{l} = \\{l_1, l_2,\\ldots\\}\\), consisting of \\(n\\) total measurements.\n\\[\\begin{align}\nf(\\mathbf{l} ; \\phi, \\sigma) = \\prod_{i} f(l_i ; \\phi, \\sigma) = \\frac{1}{(2\\pi \\sigma^2)^{n/2}}\\,\\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_{i}(l_i - \\phi)^2\\right].\n\\end{align}\\]\nWe can write this more succinctly, and perhaps more intuitively, as\n\\[\\begin{align}\nl_i \\sim \\text{Norm}(\\phi, \\sigma) \\;\\;\\forall i.\n\\end{align}\\]\nWe will generally write our models in this format, which is easier to parse and understand. Note that in writing this generative model, we have necessarily introduced another parameter, \\(\\sigma\\), the standard deviation parametrizing the Normal distribution. So, we have two parameters in our model, \\(\\phi\\) and \\(\\sigma\\).\n\n\n\n43.2.2 Model 2: Spindle length is set by total amount of tubulin\n\n43.2.2.1 The cartoon model\nThe three key principles of this “cartoon” model are:\n\nThe total amount of tubulin in the droplet or cell is conserved.\nThe total length of polymerized microtubules is a function of the total tubulin concentration after assembly of the spindle. This results from the balance of microtubule polymerization rate with catastrophe frequencies.\nThe density of tubulin in the spindle is independent of droplet or cell volume.\n\n\n\n43.2.2.2 The mathematical model\nFrom these principles, we need to derive a mathematical model that will provide us with testable predictions. The derivation follows below (following the derivation presented in the paper), and you may read it if you are interested. Since our main focus here is building a statistical model, you can skip ahead to to the final equation, where we define a mathematical expression relating the spindle length, \\(l\\) to the droplet diameter, \\(d\\), which depends on two parameters, \\(\\gamma\\) and \\(\\phi\\). Nonetheless, it is important to see how a models such as this one is derived.\nPrinciple 1 above (conservation of tubulin) implies\n\\[\\begin{align}\nT_0 V_0 = T_1(V_0 - V_\\mathrm{s}) + T_\\mathrm{s}V_\\mathrm{s},\n\\end{align}\\]\nwhere \\(V_0\\) is the volume of the droplet or cell, \\(V_\\mathrm{s}\\) is the volume of the spindle, \\(T_0\\) is the total tubulin concentration (polymerized or not), \\(T_1\\) is the tubulin concentration in the cytoplasm after the the spindle has formed, and \\(T_\\mathrm{s}\\) is the concentration of tubulin in the spindle. If we assume the spindle does not take up much of the total volume of the droplet or cell (\\(V_0 \\gg V_\\mathrm{s}\\), which is the case as we will see when we look at the data), we have\n\\[\\begin{align}\nT_1 \\approx T_0 - \\frac{V_\\mathrm{s}}{V_0}\\,T_\\mathrm{s}.\n\\end{align}\\]\nThe amount of tubulin in the spindle can we written in terms of the total length of polymerized microtubules, \\(L_\\mathrm{MT}\\) as\n\\[\\begin{align}\nT_s V_\\mathrm{s} = \\alpha L_\\mathrm{MT},\n\\end{align}\\]\nwhere \\(\\alpha\\) is the tubulin concentration per unit microtubule length. (We will see that it is unimportant, but from the known geometry of microtubules, \\(\\alpha \\approx 2.7\\) nmol/µm.)\nWe now formalize assumption 2 into a mathematical expression. Microtubule length should grow with increasing \\(T_1\\). There should also be a minimal threshold \\(T_\\mathrm{min}\\) where polymerization stops. We therefore approximate the total microtubule length as a linear function,\n\\[\\begin{aligned}\n\\begin{align}\nL_\\mathrm{MT} \\approx \\left\\{\\begin{array}{ccl}\n0 & &T_1 \\le T_\\mathrm{min} \\\\\n\\beta(T_1 - T_\\mathrm{min}) & & T_1 &gt; T_\\mathrm{min}.\n\\end{array}\\right.\n\\end{align}\n\\end{aligned}\\]\nBecause spindles form in Xenopus extract, \\(T_0 &gt; T_\\mathrm{min}\\), so there exists a \\(T_1\\) with \\(T_\\mathrm{min} &lt; T_1 &lt; T_0\\). Thus, going forward, we are assured that \\(T_1 &gt; T_\\mathrm{min}\\). So, we have\n\\[\\begin{align}\nV_\\mathrm{s} \\approx \\alpha\\beta\\,\\frac{T_1 - T_\\mathrm{min}}{T_\\mathrm{s}}.\n\\end{align}\\]\nWith insertion of our expression for \\(T_1\\), this becomes\n\\[\\begin{align}\nV_{\\mathrm{s}} \\approx \\alpha \\beta\\left(\\frac{T_0 - T_\\mathrm{min}}{T_\\mathrm{s}} - \\frac{V_\\mathrm{s}}{V_0}\\right).\n\\end{align}\\]\nSolving for \\(V_\\mathrm{s}\\), we have\n\\[\\begin{align}\nV_\\mathrm{s} \\approx \\frac{\\alpha\\beta}{1 + \\alpha\\beta/V_0}\\,\\frac{T_0 - T_\\mathrm{min}}{T_\\mathrm{s}}\n=\\frac{V_0}{1 + V_0/\\alpha\\beta}\\,\\frac{T_0 - T_\\mathrm{min}}{T_\\mathrm{s}}.\n\\end{align}\\]\nWe approximate the shape of the spindle as a prolate spheroid with major axis length \\(l\\) and minor axis length \\(w\\), giving\n\\[\\begin{align}\nV_\\mathrm{s} = \\frac{\\pi}{6}\\,l w^2 = \\frac{\\pi}{6}\\,k^2 l^3,\n\\end{align}\\]\nwhere \\(k \\equiv w/l\\) is the aspect ratio of the spindle. We can now write an expression for the spindle length as\n\\[\\begin{align}\nl \\approx \\left(\\frac{6}{\\pi k^2}\\,\n\\frac{T_0 - T_\\mathrm{min}}{T_\\mathrm{s}}\\,\n\\frac{V_0}{1+V_0/\\alpha\\beta}\\right)^{\\frac{1}{3}}.\n\\end{align}\\]\nFor small droplets, with \\(V_0\\ll \\alpha \\beta\\), this becomes\n\\[\\begin{align}\nl \\approx \\left(\\frac{6}{\\pi k^2}\\,\n\\frac{T_0 - T_\\mathrm{min}}{T_\\mathrm{s}}\\,\nV_0\\right)^{\\frac{1}{3}}\n= \\left(\\frac{T_0 - T_\\mathrm{min}}{k^2T_\\mathrm{s}}\\right)^{\\frac{1}{3}}\\,d,\n\\end{align}\\]\nwhere \\(d\\) is the diameter of the spherical droplet or cell. So, we expect the spindle size to increase linearly with the droplet diameter for small droplets.\nFor large \\(V_0\\), the spindle size becomes independent of droplet size;\n\\[\\begin{align}\nl \\approx \\left(\\frac{6 \\alpha \\beta}{\\pi k^2}\\,\n\\frac{T_0 - T_\\mathrm{min}}{T_\\mathrm{s}}\\right)^{\\frac{1}{3}}.\n\\end{align}\\]\n\n\n43.2.2.3 Indentifiability of parameters\nWe measure the microtubule length \\(l\\) and droplet diameter \\(d\\) directly from the images. We can also measure the spindle aspect ratio \\(k\\) directly from the images. Thus, we have four unknown parameters, since we already know α ≈ 2.7 nmol/µm. The unknown parameters are:\n\n\n\nrameter me\naning\n\n\n\n\n\\(\\beta\\)\nrate constant for MT growth\n\n\n\\(T_0\\)\ntotal tubulin concentration\n\n\n\\(T_\\mathrm{min}\\)\ncritical tubulin concentration for polymerization\n\n\n\\(T_s\\)\ntubulin concentration in the spindle\n\n\n\nWe would like to determine all of these parameters. We could measure them all either in this experiment or in other experiments. We could measure the total tubulin concentration \\(T_0\\) by doing spectroscopic or other quantitative methods on the Xenopus extract. We can \\(T_\\mathrm{min}\\) and \\(T_s\\) might be assessed by other in vitro assays, though these parameters may by strongly dependent on the conditions of the extract.\nImportantly, though, the parameters only appear in combinations with each other in our theoretical model. Specifically, we can define two parameters,\n\\[\\begin{aligned}\n\\begin{align}\n\\gamma &= \\left(\\frac{T_0-T_\\mathrm{min}}{k^2T_\\mathrm{s}}\\right)^\\frac{1}{3} \\\\\n\\phi &= \\gamma\\left(\\frac{6\\alpha\\beta}{\\pi}\\right)^{\\frac{1}{3}}.\n\\end{align}\n\\end{aligned}\\]\nWe can then rewrite the general model expression in terms of these parameters as\n\\[\\begin{align}\nl(d) \\approx \\frac{\\gamma d}{\\left(1+(\\gamma d/\\phi)^3\\right)^{\\frac{1}{3}}}.\n\\end{align}\\]\nIf we tried to determine all four parameters from this experiment only, we would be in trouble. This experiment alone cannot distinguish all of the parameters. Rather, we can only distinguish two combinations of them, which we have defined as \\(\\gamma\\) and \\(\\phi\\). This is an issue of identifiability. We may not be able to distinguish all parameters in a given model, and it is important to think carefully before the analysis about which ones we can identify. Even with our work so far, we are not quite done with characterizing identifiability.\nThis model is a variate-covariate model in which the spindle length is the variate and the droplet diameter is the covariate. The parameters \\(\\gamma\\) and \\(\\phi\\) parametrize the function \\(l(d)\\), which describes how the variate \\(l\\) depends on the covariate \\(d\\).\nNote that variate-covariate modeling and the associated parameter estimation is often referred to as regression. I avoid the use of this term because it is a historical artifact and describes a subset of variate-covariate models. You can read more about the terminology in this short historical description by Michael Betancourt.\nFinding MLEs of parameters for variate-covariate models is also often referred to as curve fitting. I also avoid that term because I prefer the more descriptive term “maximum likelihood estimation,” which further emphasizes that it is the same procedure we have been doing for other models beyond variate-covariate models.\n\n\n43.2.2.4 Visualizing the mathematical model\nLet’s take a quick look at the mathematical model so we can see how the curve looks. It is best to nondimensionalize the diameter by \\(\\phi\\), giving\n\\[\\begin{align}\n\\frac{l(d)}{\\phi} \\approx \\frac{\\gamma d/\\phi}{\\left(1+(\\gamma d/\\phi)^3\\right)^{\\frac{1}{3}}}.\n\\end{align}\\]\nSo, we will plot \\(l(d/\\phi)/\\phi\\) versus \\(d/\\phi\\), which means we choose units of length to be \\(\\phi\\).\n\n\n\n\n\nBokeh Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe curve grows from zero to a plateau at \\(l = \\phi\\), more rapidly for larger \\(\\gamma\\). We can more carefully characterize the limiting behavior.\n\n\n43.2.2.5 Limiting behavior\nFor large droplets, with \\(d \\gg \\phi/\\gamma\\), the spindle size becomes independent of \\(d\\), with\n\\[\\begin{align}\nl \\approx \\phi.\n\\end{align}\\]\nConversely, for \\(d \\ll \\phi/\\gamma\\), the spindle length varies approximately linearly with diameter.\n\\[\\begin{align}\nl(d) \\approx \\gamma\\,d.\n\\end{align}\\]\nNote that the expression for the linear regime gives bounds for \\(\\gamma\\). Obviously, \\(\\gamma &gt; 0\\), lest we get negative spindle lengths. Because \\(l \\le d\\), lest the spindle not fit in the droplet, we also have \\(\\gamma \\le 1\\).\nImportantly, if the experiment is done in the regime where \\(d\\) is large (and we do not really know a priori how large that is since we do not know the parameters \\(\\phi\\) and \\(\\gamma\\)), we cannot tell the difference between the two models, since they are equivalent in that regime. Further, if the experiment is in this regime the model is unidentifiable because we cannot resolve \\(\\gamma\\).\nThis sounds kind of dire, but this is actually a convenient fact. The second model is more complex, but it has the simpler model, model 1, as a limit. Thus, the two models are in fact commensurate with each other. Knowledge of how these limits work also enhances the experimental design. We should strive for small droplets. And perhaps most importantly, if we didn’t consider the second model, we might automatically assume that droplet size has nothing to do with spindle length if we simply did the experiment in larger droplets.\n\n\n43.2.2.6 Generative model\nWe have a theoretical model relating the droplet diameter to the spindle length. Let us now build a generative model. To do so, we need to specify how the measured spindle lengths are distributed.\nWe start by assuming that the measured droplet length-diameter pairs are independent and identically distributed (i.i.d.). This is often a reasonable assumption in variate-covariate models, and certainly seems so in this one. The i.i.d. assumption may break down, for example in some models in which the variate is time and subsequent data points are depended upon each other. Nonetheless, even for many time series, the i.i.d. assumption is reasonable, especially because many processes are memoryless.\nTaking the i.i.d. assumption as given, we now note that the measured spindle length for a given droplet diameter will be close to the theoretical curve, but will not be exactly. The difference between the theoretical curve and the measured value is called a residual. More generally, a residual is the difference between the theoretical value of a covariate and the measured value of a covariate for a given value of the variate(s). So, to complete our generative model, we need to specify how the residuals are distributed.\nFor spindle, droplet pair i, we assume\n\\[\\begin{align}\nl_i = \\frac{\\gamma d_i}{\\left(1+(\\gamma d/\\phi)^3\\right)^{\\frac{1}{3}}} + e_i,\n\\end{align}\\]\nwhere \\(e_i\\) is the residual for point i. We could model the residual in many ways. Here are some examples.\n\nHomoscedastic, Normally distributed residuals: Each data point varies from the theoretical curve according to a Normal distribution with the same variance \\(\\sigma^2\\), such that \\(e_i \\sim \\text{Norm}(0, \\sigma)\\;\\forall i\\).\nHeteroscedastic, Normally distributed residuals: Each data point varies from the theoretical curve according to a Normal distribution with the a variance that is unique to each data point, \\(\\sigma_i^2\\), such that \\(e_i \\sim \\text{Norm}(0, \\sigma_i)\\). This requires an expression for \\(\\sigma_i\\) for each datum. Commonly, the residuals may be proportional to magnitude of the covariate such that \\(\\sigma_i = \\sigma_0\\left|\\mu_i\\right|\\), where \\(\\mu_i\\) is the value of the theoretical curve for data point i, and \\(\\sigma_0\\) is a parameter describing how the values of \\(\\sigma_i\\) scale.\nHomoscedastic, Student-t distributed residuals: Each data point varies from the theoretical curve according to a Student-t distribution with the same scale parameter \\(\\sigma\\) and shape parameter \\(\\nu\\), such that \\(e_i \\sim \\text{Student-t}(\\nu, 0, \\sigma)\\;\\forall i\\). This is commonly used when you suspect there may be strong variation from the theoretical curve, and you need a heavier-tailed distribution to describe the residuals.\n\nHomoscedastic, Normally distributed residuals is the most commonly used model, and we will adopt that here.\n\\[\\begin{aligned}\n\\begin{align}\n&\\mu_i =  \\frac{\\gamma d_i}{\\left(1+(\\gamma d_i/\\phi)^3\\right)^{\\frac{1}{3}}}, \\\\[1em]\n&e_i \\sim \\text{Norm}(0, \\sigma);\\forall i,\\\\[1em]\n&l_i = \\mu_i + e_i \\;\\forall i,\n\\end{align}\n\\end{aligned}\\]\nwhich can be equivalently stated as\n\\[\\begin{aligned}\n\\begin{align}\n&\\mu_i =  \\frac{\\gamma d_i}{\\left(1+(\\gamma d_i/\\phi)^3\\right)^{\\frac{1}{3}}}, \\\\[1em]\n&l_i \\sim \\text{Norm}(\\mu_i, \\sigma) \\;\\forall i,\n\\end{align}\n\\end{aligned}\\]\nor even more compactly as\n\\[\\begin{align}\nl_i \\sim \\text{Norm}\\left(\\frac{\\gamma d_i}{\\left(1+(\\gamma d_i/\\phi)^3\\right)^{\\frac{1}{3}}}, \\sigma\\right) \\;\\forall i.\n\\end{align}\\]\nImportantly, note that this model builds upon our first model. Generally, when doing modeling, it is a good idea to build more complex models on your initial baseline model such that the models are related to each other by limiting behavior. This gives you a continuum of model and a sound basis for making comparisons among models.\nNote that we are assuming the droplet diameters are known. When we generate data sets for prior predictive checks, we will randomly generate them from about 20 µm to 200 µm, since this is the range achievable with the microfluidic device.\n\n\n43.2.2.7 Checking model assumptions\nIn deriving the mathematical model, we made a series of assumptions. It is generally a good idea to check to see if assumptions in the mathematical modeling are realized in the experiment. If they are not, you may need to relax the assumptions and have a potentially more complicated model (which may suffer from identifiability issues). This underscores the interconnection between modeling and experimental design. You can allow for modeling assumptions and identifiability if you design your experimental parameters to meet the assumptions (e.g., choosing the appropriate range of droplet sizes).\n\n43.2.2.7.1 Is \\(V_\\mathrm{s} / V_0 \\ll 1\\)?\nLet’s do a quick verification that the droplet volume is indeed much larger than the spindle volume. Remember, the spindle volume for a prolate spheroid of length \\(l\\) and width \\(w\\) is \\(V_\\mathrm{s} = \\pi l w^2 / 6\\).\n\n\n\n\n\nBokeh Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that for pretty much all spindles that were measured, \\(V_\\mathrm{s} / V_0\\) is small, so this is a sound assumption.\n\n\n43.2.2.7.2 Do all spindles have the same aspect ratio \\(k\\)?\nIn setting up our model, we assumed that all spindles had the same aspect ratio. We can check this assumption because we have the data to do so available to us.\n\n\n\n\n\nBokeh Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe median aspect ratio is about 0.4, and we see spindle lengths about \\(\\pm 25\\%\\) of that. This could be significant variation. We may wish to update the model to account for nonconstant \\(k\\). Going forward, we will assume \\(k\\) is constant, but you may wish to perform the analysis that follows with nonconstant \\(k\\) as an exercise.\nImportantly, these checks of the model highlight the importance of checking your assumptions against your data. Always a good idea!",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Building a variate-covariate model</span>"
    ]
  },
  {
    "objectID": "lessons/models/variate_covariate/mle_for_variate_covariate_models.html",
    "href": "lessons/models/variate_covariate/mle_for_variate_covariate_models.html",
    "title": "44  Maximum likelihood estimation for variate-covariate models",
    "section": "",
    "text": "44.1 Normally distributed residuals\nIn the last part of this lesson, we discussed how to write down a generative model for variate-covariate models. To obtain estimates for the parameters of the model, we can perform a maximum likelihood estimation, precisely as we have done for other models; we minimize the log likelihood function. The optimization is typically done using numerical optimization, as we have already seen in the Numerical maximum likelihood estimation lesson.\nFor some special cases of variate-covariate models, we can work out analytical results that can greatly ease the optimization problem. Fortunately, these special case arise quite frequently.\nIn what follows, we will use a general expression for the theoretical model. Let \\(y(x;\\theta)\\) be the theoretical curve with parameters \\(\\theta = (\\theta_1, \\theta_2, \\ldots)\\), and let \\((x_i,y_i)\\) be a variate-covariate pair of measurements. We assume was have \\(n\\) of them. We will also everywhere assume the data points are i.i.d.\nTo connect this more general notation to the problem of mitotic spindle size we have been working with, \\((x_i, y_i) = (d_i, l_i)\\), \\(\\theta = (\\gamma, \\phi)\\), and\n\\[\\begin{align}\ny(x;\\theta) = l(d;\\gamma, \\phi) = \\frac{\\gamma d}{\\left(1+(\\gamma d/\\phi)^3\\right)^{\\frac{1}{3}}}.\n\\end{align}\\]\nThe results we derive here apply to the case where we have Normally (possibly heteroscedastic) distributed residuals. In the most general case, our model is\n\\[\\begin{align}\ny_i \\sim \\text{Norm}(y(x_i;\\theta), \\sigma_i).\n\\end{align}\\]\nActually, this is not the most general case, since in general \\(x_i\\) may be multi-dimensional, but we are considering scalar \\(x_i\\) for simplicity. We can write the likelihood as\n\\[\\begin{align}\nL(\\theta, \\{\\sigma_i\\}; \\{x_i\\}, \\{y_i\\}) = \\prod_i \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\,\\exp\\left[-\\frac{(y_i - y(x_i;\\theta))^2}{2\\sigma_i^2}\\right],\n\\end{align}\\]\nwhere \\(\\{\\sigma_i\\}\\), \\(\\{x_i\\}\\), and \\(\\{y_i\\}\\) represent the sets of :math:`sigma`, \\(x\\) and \\(y\\): values, respectively. The log-likelihood is then\n\\[\\begin{align}\n\\ell(\\theta, \\{\\sigma_i\\}; \\{x_i\\}, \\{y_i\\}) = -\\frac{n}{2}\\,\\ln 2\\pi - \\sum_i \\ln \\sigma_i - \\frac{1}{2}\\sum_i \\left(\\frac{y_i - y(x_i;\\theta)}{\\sigma_i}\\right)^2.\n\\end{align}\\]\nThis is a general log-likelihood for Normally distributed residuals. We will now investigate special cases.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Maximum likelihood estimation for variate-covariate models</span>"
    ]
  },
  {
    "objectID": "lessons/models/variate_covariate/mle_for_variate_covariate_models.html#known-sigma_i-and-the-levenberg-marquardt-algorithm",
    "href": "lessons/models/variate_covariate/mle_for_variate_covariate_models.html#known-sigma_i-and-the-levenberg-marquardt-algorithm",
    "title": "44  Maximum likelihood estimation for variate-covariate models",
    "section": "44.2 Known \\(\\sigma_i\\) and the Levenberg-Marquardt algorithm",
    "text": "44.2 Known \\(\\sigma_i\\) and the Levenberg-Marquardt algorithm\nIn some cases, we know the values of the \\(\\sigma_i\\)'s a priori. That is, they are not parameters we need estimates for, but are already known. While I often see this in the biological literature, I seldom come across examples like this in my own work. Typically, a measurement for a given \\(x_i\\) is repeated multiple times and then \\(\\sigma_i\\) is estimated. Nonetheless, treating this case is informative, as it introduces to a useful algorithm.\nThe log-likelihood no longer depends on the \\(\\sigma_i\\)'s (since they are known), and is\n\\[\\begin{align}\n\\ell(\\theta; \\{x_i\\}, \\{y_i\\}) = -\\frac{n}{2}\\,\\ln 2\\pi - \\sum_i \\ln \\sigma_i - \\frac{1}{2}\\sum_i \\left(\\frac{y_i - y(x_i;\\theta)}{\\sigma_i}\\right)^2.\n\\end{align}\\]\nSince the first two terms in the above expression for the log-likelihood have no θ-dependence, we may instead seek a value of \\(\\theta\\) that minimizes the quantity\n\\[\\begin{align}\n\\sum_i \\left(\\frac{y_i - y(x_i;\\theta)}{\\sigma_i}\\right)^2.\n\\end{align}\\]\nThis has a convenient structure to utilize the Levenberg-Marquardt algorithm. It robustly and efficiently solves optimization problems of the form\n\\[\\begin{align}\n\\arg \\min_z \\sum_i \\left(f_i(z)\\right)^2,\n\\end{align}\\]\nwith each \\(f_i\\) being scalar-valued and \\(z\\) being scalar or vector-valued.1 The maximum likelihood estimation problem is of this form, with \\(z = \\theta\\) and\n\\[\\begin{align}\nf_i(\\theta) = \\left(\\frac{y_i - y(x_i;\\theta)}{\\sigma_i}\\right)^2.\n\\end{align}\\]\nSo, to find the MLE for the parameters \\(\\theta\\), we can directly employ the Levenberg-Marquardt algorithm. We will discuss implementation using Python-based tools in a forthcoming lesson.\nNote that if the \\(\\sigma_i\\) values are not known, the MLE problem does not match the class of optimization problems for which the Levenberg-Marquardt algorithm can be used.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Maximum likelihood estimation for variate-covariate models</span>"
    ]
  },
  {
    "objectID": "lessons/models/variate_covariate/mle_for_variate_covariate_models.html#homoscedastic-normal-models",
    "href": "lessons/models/variate_covariate/mle_for_variate_covariate_models.html#homoscedastic-normal-models",
    "title": "44  Maximum likelihood estimation for variate-covariate models",
    "section": "44.3 Homoscedastic Normal models",
    "text": "44.3 Homoscedastic Normal models\nNow, consider a homoscedastic model,\n\\[\\begin{align}\ny_i \\sim \\text{Norm}(y(x_i;\\theta), \\sigma) \\;\\forall i.\n\\end{align}\\]\nNow, the log likelihood function is\n\\[\\begin{align}\n\\ell(\\theta, \\sigma; \\{x_i\\}, \\{y_i\\}) = -\\frac{n}{2}\\,\\ln 2\\pi - n \\ln \\sigma - \\frac{1}{2\\sigma}\\sum_i \\left(y_i - y(x_i;\\theta)\\right)^2.\n\\end{align}\\]\nEven though we do not know the value of \\(\\sigma\\), we can still get a maximum likelihood estimate for \\(\\theta\\) without considering \\(\\sigma\\) because of the structure of the log likelihood. Regardless the value of \\(\\sigma\\), the values of \\(\\theta\\) that minimize\n\\[\\begin{align}\n\\text{RSS} \\equiv \\sum_i \\left(y_i - y(x_i;\\theta)\\right)^2,\n\\end{align}\\]\na quantity known as the residual sum of squares, give the maximum likelihood estimate. The MLE for \\(\\theta\\) can therefore be found using the Levenberg-Marquardt algorithm.\nAssume for a moment we find the MLE \\(\\theta^*\\), which gives us a residual sum of squares of \\(\\text{RSS}^*\\). Then, the log-likelihood is\n\\[\\begin{align}\n\\ell(\\theta^*, \\sigma; \\{x_i\\}, \\{y_i\\}) = -\\frac{n}{2}\\,\\ln 2\\pi - n\\ln\\sigma - \\frac{\\text{RSS}^*}{2\\sigma^2}.\n\\end{align}\\]\nThe value of \\(\\sigma\\) that maximizes the log-likelihood is then found by differentiating and setting to zero.\n\\[\\begin{align}\n\\frac{\\partial \\ell}{\\partial \\sigma} = -\\frac{n}{\\sigma} + \\frac{\\text{RSS}^*}{\\sigma^3} = 0.\n\\end{align}\\]\nThis is solved to give \\(\\sigma^* = \\sqrt{\\text{RSS}^*/n}\\).\nSo, we can split the optimization problem in two parts. First, find the values of parameters \\(\\theta\\) that minimize the residual sum of squares, typically using the Levenberg-Marquardt algorithm. Then, using these values, compute the MLE for \\(\\sigma\\) using the analytical result we have just derived.\n\n44.3.1 A very important caveat\nThough commonly encountered, the special case we have just considered is not always the variate-covariate model we might choose. That means that the convenience of the Levenberg-Marquardt algorithm and the convenient result for getting a maximum likelihood estimate for the homoscedastic \\(\\sigma\\) will not in general work. They only work for this special case.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Maximum likelihood estimation for variate-covariate models</span>"
    ]
  },
  {
    "objectID": "lessons/models/variate_covariate/mle_for_variate_covariate_models.html#theoretical-functions-that-are-linear-in-the-parameters",
    "href": "lessons/models/variate_covariate/mle_for_variate_covariate_models.html#theoretical-functions-that-are-linear-in-the-parameters",
    "title": "44  Maximum likelihood estimation for variate-covariate models",
    "section": "44.4 Theoretical functions that are linear in the parameters",
    "text": "44.4 Theoretical functions that are linear in the parameters\nLike other optimization algorithms we have seen so far, the Levenberg-Marquardt algorithm will only provide a local minimizer; it is not guaranteed to find a global minimum. That said, if the function \\(y(x;\\theta)\\) is linear in the parameters \\(\\theta\\), it can be shown that, under certain restrictions, the minimizer \\(\\theta^*\\) of the RSS is unique, and therefore a local minimum is also a global minimum.\nTo show this, since \\(y(x;\\theta)\\) is linear in \\(\\theta\\), we can write it in the form \\(y(x;\\theta) = \\mathbf{z} \\cdot \\theta\\).2 For example, if \\(y(x;\\theta)\\) is a second order polynomial, \\(y(x;\\theta) = \\theta_1 + \\theta_2 x + \\theta_3 x^2\\), then \\(\\mathbf{z} = (1, x, x^2)\\) and \\(\\theta = (\\theta_1, \\theta_2, \\theta_3)^\\mathsf{T}\\). Residual \\(i\\) is \\(y_i - \\mathbf{z}_i \\cdot \\theta\\), and the residual sum of squares is\n\\[\\begin{align}\n\\text{RSS} = \\sum_i (y_i - \\mathbf{z}_i \\theta)^2,\n\\end{align}\\]\nwhere \\(\\mathbf{z}_i = (1, x_i, x_i^2)\\) for the second order polynomial example, and we have omitted the \\(\\cdot\\) symbol, and will continue to do so for notational concision. We can write this in a more convenient form by defining \\(\\mathbf{y} = (y_1, y_2, \\ldots y_n)^\\mathsf{T}\\). Further, for each data point \\(i\\), there is a \\(z_i\\). We define the matrix\n\\[\\begin{aligned}\n\\begin{align}\n\\mathsf{Z} = \\begin{pmatrix}\n\\mathbf{z}_1\\\\\n\\mathbf{z}_2\\\\\n\\vdots\\\\\n\\mathbf{z}_n\n\\end{pmatrix}.\n\\end{align}\n\\end{aligned}\\]\nThe matrix \\(\\mathsf{Z}\\) is sometimes referred to as a design matrix. The residual sum of squares is then\n\\[\\begin{aligned}\n\\begin{align}\n\\text{RSS} &= (\\mathbf{y} - \\mathsf{Z} \\theta)^\\mathsf{T}(\\mathbf{y} - \\mathsf{Z} \\theta) \\\\[1em]\n&= \\mathbf{y}^\\mathsf{T} \\mathbf{y} - \\mathbf{y}^\\mathsf{T} \\mathsf{Z} \\theta - (\\mathsf{Z} \\theta)^\\mathsf{T}\\mathbf{y} + (\\mathsf{Z}\\theta)^\\mathsf{T} (\\mathsf{Z}\\theta) \\\\[1em]\n&= \\mathbf{y}^\\mathsf{T} \\mathbf{y} - 2\\theta^\\mathsf{T}\\mathsf{Z}^\\mathsf{T} \\mathbf{y} + \\theta^\\mathsf{T}\\mathsf{Z}^\\mathsf{T} \\mathsf{Z}\\theta.\n\\end{align}\n\\end{aligned}\\]\nDifferentiating the RSS with respect to \\(\\theta\\) gives\n\\[\\begin{align}\n\\nabla_\\theta \\text{RSS} = -2\\mathsf{Z}^\\mathsf{T}\\mathbf{y} + 2\\mathsf{Z}^\\mathsf{T} \\mathsf{Z}\\theta.\n\\end{align}\\]\nSetting this equal to zero and solving for \\(\\theta^*\\) (the value of \\(\\theta\\) where \\(\\nabla_\\theta \\text{RSS} = \\mathbf{0}\\)) yields\n\\[\\begin{align}\n\\theta^* = (\\mathsf{Z}^\\mathsf{T} \\mathsf{Z})^{-1}\\mathsf{Z}^\\mathsf{T}\\mathbf{y}.\n\\end{align}\\]\nProvided the matrix \\(\\mathsf{Z}^\\mathsf{T}\\mathsf{Z}\\) is nonsingular, this linear system has a unique solution.\nThis is referred to as ordinary least squares, or OLS, and is a subject in many introductory linear algebra courses. Though \\(\\theta^*\\) may be computed using matrix operations according to the above formula, in practice it is often simply calculated using the Levenberg-Marquardt algorithm, which is typically more numerically stable. Because of what we have just shown, whatever local minimum the algorithm finds is, in fact, a global minimum and therefore the unique MLE for \\(\\theta\\).\nNote that this is not generally true! There may be local minima of the RSS as a function of \\(\\theta\\) (and therefore local minima of the log likelihood) is \\(y(x;\\theta)\\) is nonlinear in the parameters \\(\\theta\\).",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Maximum likelihood estimation for variate-covariate models</span>"
    ]
  },
  {
    "objectID": "lessons/models/variate_covariate/mle_for_variate_covariate_models.html#footnotes",
    "href": "lessons/models/variate_covariate/mle_for_variate_covariate_models.html#footnotes",
    "title": "44  Maximum likelihood estimation for variate-covariate models",
    "section": "",
    "text": "Note, though, in the implementation we will use in scipy.optimize.least_squares(), \\(z\\) must be entered as an array, even for a single parameter.↩︎\nIn most books and other literature on the subject, the vector we have defined as \\(\\mathbf{z}\\) is denoted \\(\\mathbf{x}\\). I find that variables named \\(x\\) abound and the symbol is overloaded, which is why I defined it as \\(\\mathbf{z}\\) and the design matrix as \\(\\mathsf{Z}\\) instead of \\(\\mathsf{X}\\) (though the design matrix is often also denoted by \\(\\mathsf{A}\\)).↩︎",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Maximum likelihood estimation for variate-covariate models</span>"
    ]
  },
  {
    "objectID": "lessons/models/variate_covariate/variate_covariate_implementation.html",
    "href": "lessons/models/variate_covariate/variate_covariate_implementation.html",
    "title": "45  Implementation of MLE for variate-covariate models",
    "section": "",
    "text": "45.1 Performing MLE by direct numerical optimization\n| Download notebook\nDataset download\nWe now proceed to do maximum likelihood estimation for the parameters of the second model for spindle length.\n\\[\\begin{align}\n&\\mu_i =  \\frac{\\gamma d_i}{\\left(1+(\\gamma d_i/\\phi)^3\\right)^{\\frac{1}{3}}}, \\\\[1em]\n&l_i \\sim \\text{Norm}(\\mu_i, \\sigma) \\;\\forall i,\n\\end{align}\\]\nThe parameters for this model are \\(\\gamma\\), \\(\\phi\\), and \\(\\sigma\\).\nFinally, before we being doing MLE, as a reminder, we will load in and plot the data.\nIt is also useful to extract the data sets as Numpy arrays for speed in the MLE and bootstrap calculations.\nWe can take the same approach as we did with the mRNA counts from the smFISH experiments. We write a function for the log likelihood and then use numerical optimization to find the parameters.\nThe joint probability density function for the data set is the product of the probability densities of Normals. Therefore, the log likelihood is the sum of log probability densities of Normals. We can code that up using the convenient functions in the scipy.stats module.\ndef theor_spindle_length(gamma, phi, d):\n    \"\"\"Compute spindle length using mathematical model\"\"\"\n    return gamma * d / np.cbrt(1 + (gamma * d / phi)**3)\n    \n    \ndef log_likelihood(params, d, ell):\n    \"\"\"Log likelihood of spindle length model.\"\"\"\n    gamma, phi, sigma = params\n    \n    if gamma &lt;= 0 or gamma &gt; 1 or phi &lt;= 0:\n        return -np.inf\n\n    mu = theor_spindle_length(gamma, phi, d)\n\n    return np.sum(st.norm.logpdf(ell, mu, sigma))\nIn looking at the code for the log likelihood, the convenience of the scipy.stats module is clear. We more or less code up the model as we would say it!\nWe can now code up the MLE calculation. For our initial guesses, we can look at the data an make estimates. The initial slope of the \\(l\\) vs. \\(d\\) curve is about one-half, so we will guess an initial value of \\(\\gamma\\) of 0.5. The plateau seems to be around 40 µm, so we will guess \\(\\phi = 40\\) µm. Eyeballing the standard deviation in the data points, I would guess it is about 5 µm.\ndef spindle_mle(d, ell):\n    \"\"\"Compute MLE for parameters in spindle length model.\"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        res = scipy.optimize.minimize(\n            fun=lambda params, d, ell: -log_likelihood(params, d, ell),\n            x0=np.array([0.5, 35, 5]),\n            args=(d, ell),\n            method='Powell'\n        )\n\n    if res.success:\n        return res.x\n    else:\n        raise RuntimeError('Convergence failed with message', res.message)\nLet’s run the calculation to get MLEs for the parameters.\nmle_params = spindle_mle(d, ell)\n\nmle_params\n\narray([ 0.86047455, 38.23124995,  3.75342168])\nWe can take a quick look as to how the theoretical curve might look with respect to the data.\n# Compute theoretical curve using MLE parameters\nd_theor = np.linspace(0, 250, 200)\nell_theor = theor_spindle_length(*mle_params[:-1], d_theor)\n\n# Add theoretical curve to the plot\np.line(d_theor, ell_theor, line_width=2, line_color=\"orange\")\n\nbokeh.io.show(p)\nOf course, we are not using the full generative model here, since we are not using the parameter \\(\\sigma\\). We will discuss in much more detail in the lesson on model assessment how to visualize the entire generative model as it relates to the measured data.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Implementation of MLE for variate-covariate models</span>"
    ]
  },
  {
    "objectID": "lessons/models/variate_covariate/variate_covariate_implementation.html#least-squares",
    "href": "lessons/models/variate_covariate/variate_covariate_implementation.html#least-squares",
    "title": "45  Implementation of MLE for variate-covariate models",
    "section": "45.2 Least squares",
    "text": "45.2 Least squares\nWe worked out in Lesson 44 that when we have a homoscedastic Normal model for residuals, we may compute the MLE by first minimizing the residual sum of squares and then computing the MLE for the variance of the Normally distributed residuals as \\(\\sigma^* = \\sqrt{\\text{RSS}^* / n}\\), where \\(n\\) is the total number of data points.\nConveniently, this is the form of our generative model, so we can split the optimization problem in two parts. First, find the values of parameters \\(\\gamma\\) and \\(\\phi\\) that minimize the residual sum of squares. Then, using these values, compute the MLE for \\(\\sigma\\) using the analytical result we have just derived.\nThe Levenberg-Marquardt algorithm is a fast, robust algorithm to perform minimization of the RSS. A variant of this is implemented in scipy.optimization.least_squares() (docs). To use that algorithm you need to define a function for the residual; that is a function that returns the quantity \\(l_i - \\mu_i\\). The function has call signature f(params, *args), where params is a Numpy array containing the parameters you will use in the optimization (in our case, \\(\\gamma\\) and \\(\\phi\\)), and args is a tuple of any other arguments passed to the function, which almost always include the data (in our case (d, ell)). We can code up that function.\n\ndef resid(params, d, ell):\n    \"\"\"Residual for spindle length model.\"\"\"\n    return ell - theor_spindle_length(*params, d)\n\nWe can call scipy.optimize.least_squares() to perform the MLE. We can also specify bounds as a tuple containing a list of the lower bounds of the respective parameters and a list of the upper bounds of the respective parameters. We bound \\(\\gamma\\) between zero and one, and \\(\\phi\\) just has to be positive.\n\nres = scipy.optimize.least_squares(\n    resid, np.array([0.5, 35]), args=(d, ell), bounds=([0, 0], [1, np.inf])\n)\n\n# Compute residual sum of squares from optimal params\nrss_mle = np.sum(resid(res.x, d, ell)**2)\n\n# Compute MLE for sigma\nsigma_mle = np.sqrt(rss_mle / len(d))\n\n# Take a look\nres.x, sigma_mle\n\n(array([ 0.8601068 , 38.24676911]), np.float64(3.7549998715440034))\n\n\nIndeed, we got the same results as before. Let’s code this procedure up into a function. Looking ahead to using bebi103.bootstrap.draw_bs_reps_mle(), we need the function to take the data set as a single argument.\n\ndef spindle_mle_lstq(data):\n    \"\"\"Compute MLE for parameters in spindle length model.\"\"\"\n    # Unpack data\n    d = data[:, 0]\n    ell = data[:, 1]\n    \n    res = scipy.optimize.least_squares(\n        resid, np.array([0.5, 35]), args=(d, ell), bounds=([0, 0], [1, np.inf])\n    )\n\n    # Compute residual sum of squares from optimal params\n    rss_mle = np.sum(resid(res.x, d, ell)**2)\n\n    # Compute MLE for sigma\n    sigma_mle = np.sqrt(rss_mle / len(d))\n    \n    return tuple([x for x in res.x] + [sigma_mle])\n\nThe big advantages of doing this are the robustness and speed of the Levenberg-Marquardt algorithm. Let’s do a speed test.\n\nprint(\"Timing for MLE by Powell's method:\")\n%timeit spindle_mle(d, ell)\n\nprint(\"\\nTiming for MLE by least_squares:\")\ndata = df[['Droplet Diameter (um)', 'Spindle Length (um)']].to_numpy()\n%timeit spindle_mle_lstq(data)\n\nTiming for MLE by Powell's method:\n5.09 ms ± 17.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\nTiming for MLE by least_squares:\n1.36 ms ± 2.47 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nThat’s a substantial speed boost of nearly 4×.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Implementation of MLE for variate-covariate models</span>"
    ]
  },
  {
    "objectID": "lessons/models/variate_covariate/variate_covariate_implementation.html#confidence-intervals",
    "href": "lessons/models/variate_covariate/variate_covariate_implementation.html#confidence-intervals",
    "title": "45  Implementation of MLE for variate-covariate models",
    "section": "45.3 Confidence intervals",
    "text": "45.3 Confidence intervals\nWe can now proceed to compute confidence intervals for our parameters. We will do a parametric bootstrap here. We use the generative model parametrized by the MLE to regenerate data sets. For each value of \\(d\\) (which are fixed), we draw values of \\(l\\) using the generative distribution). Then we perform MLE of those to get confidence intervals. To use bebi103.draw_bs_reps_mle(), then, we just are left to supply the bootstrap sample generating function.\n\ndef gen_spindle_data(params, d, size, rng):\n    \"\"\"Generate a new spindle data set.\"\"\"\n    mu = theor_spindle_length(*params[:-1], d)\n    sigma = params[-1]\n\n    return np.vstack((d, rng.normal(mu, sigma))).transpose()\n\nNote that we were careful to make sure the data set has the appropriate dimensions; the row indexes the trial. We can now compute our replicates and confidence intervals.\n\nbs_reps = bebi103.bootstrap.draw_bs_reps_mle(\n    spindle_mle_lstq,\n    gen_spindle_data,\n    data=df[[\"Droplet Diameter (um)\", \"Spindle Length (um)\"]].to_numpy(),\n    mle_args=(),\n    gen_args=(d,),\n    size=10_000,\n    n_jobs=9,\n    progress_bar=False,\n)\n\n# Compute confidence intervals\nconf_ints = np.percentile(bs_reps, [2.5, 97.5], axis=0)\n\n# Print the results\nprint(\n    \"\"\"95% Confidence intervals\nγ: [ {0:.2f},  {1:.2f}]\nφ: [{2:.2f}, {3:.2f}]\nσ: [ {4:.2f},  {5:.2f}]\n\"\"\".format(\n        *conf_ints.transpose().ravel()\n    )\n)\n\n95% Confidence intervals\nγ: [ 0.83,  0.89]\nφ: [37.54, 39.03]\nσ: [ 3.54,  3.95]\n\n\n\nThe confidence intervals are given in each column for \\(\\gamma\\), \\(\\phi\\), and \\(\\sigma\\), respectively, and are quite tight. This is because we have lots of data points. This does not mean that the data are tightly distributed about the theoretical curve, only that the MLE estimates for the parameters will not vary much if we repeated the experiment and analysis.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Implementation of MLE for variate-covariate models</span>"
    ]
  },
  {
    "objectID": "lessons/models/variate_covariate/variate_covariate_implementation.html#other-confidence-intervals",
    "href": "lessons/models/variate_covariate/variate_covariate_implementation.html#other-confidence-intervals",
    "title": "45  Implementation of MLE for variate-covariate models",
    "section": "45.4 Other confidence intervals",
    "text": "45.4 Other confidence intervals\nWe have computed parametric confidence intervals by regenerating spindle lengths for each droplet diameter we measured using the generative model parametrized by the MLE. There are many ways to compute confidence intervals for variate-covariate models, including pairs, residual, and wild bootstrap. Briefly, here are the respective techniques.\nPairs bootstrap: As we saw when doing nonparametric bootstrap confidence intervals for correlation coefficients, we can apply a similar technique here. We resample the data set as, in this case, pairs of droplet diameters and spindle lengths \\((d_i, \\ell_i)\\), and compute MLE estimates for the parameters for each bootstrap sample.\nResidual bootstrap: In residual bootstrap, the residuals are resampled. The procedure goes as follows.\n\nCompute the MLE for the parameters of the model, \\(\\theta^*\\). In the spindle length example, \\(\\theta^* = (\\gamma^*, \\phi^*)\\).\nDefine \\(\\hat{y}_i = y(x_i; \\theta^*)\\) to be the value of \\(y\\) computed as point \\(x_i\\) using the theoretical curve parametrized by the MLE estimate for \\(\\theta\\).\nDefine residual \\(i\\) as \\(\\epsilon_i = y_i - \\hat{y}_i\\).\nDraw a bootstrap sample of the residuals, randomly assigning a new residual to each datum. Let \\(\\epsilon_j\\) be the \\(j\\)th resampled residual (that is the \\(j\\)th residual in the bootstrap sample of residuals).\nGenerate a bootstrap sample of your data set where the \\(j\\)th data point \\((x_j, \\hat{y}_j + \\epsilon_j)\\).\nRecompute the MLE for \\(\\theta\\) from this bootstrap sample.\n\nWild bootstrap: In wild bootstrap, the residuals are resampled parametrically. If residual \\(i\\) is \\(\\epsilon_i = y_i - \\hat{y}_i\\), then a new residual applied to data point \\(i\\) is \\(\\epsilon_i\\,v_i\\), where \\(v_i\\) is drawn from a Normal distribution with mean zero and variance one. Thus, data point \\(i\\) in the bootstrap sample is \\((x_i, \\hat{y}_i + \\epsilon_i\\,v_i)\\). The MLE for \\(\\theta\\) is then computed from this bootstrap sample.\n\n45.4.1 Why I do not advocate for using these other methods\nPairs bootstrap is a nonparametric way of getting bootstrap samples, with the bootstrap replicate computed using a parametric inference technique. This can work well if we have a large number of data points and no x-values are severely undersampled in the experiment. This is not always the case, however, though it is in the case of the spindle length data.\nResidual bootstrap implicitly models the residuals of homoscedastic, since any given residual may be applied to any given data point. Again, this is often not the case.\nWild bootstrap is a twist on residual bootstrap that attempts to model heteroscedasticity. In using wild bootstrap, we have implicitly baked into model that the residual for each data point is normally distributed about the empirical residual with a scale parameter that scales with the size of the empirical residual. This is a way of dealing with heteroscedasticity without seriously considering the variation in measurement in a generative way. But, we really should think generatively. If we are going to model, we should model everything. A full generative model, including how we might expect the variability to change as a function of the measured and/or explanatory variables, is preferable.\nFor variate-covariate models, I usually prefer to do the parametric bootstrap method we have covered in this lesson. If I have enough well-sampled data points, then pairs bootstrap is a reasonable way to nonparametrically get bootstrap samples to compute a confidence interval. If I am sure of homoscedasticity, residual bootstrap is a good method. I generally shy away from wild bootstrap completely, since I prefer to explicitly model heteroscedasticity.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Implementation of MLE for variate-covariate models</span>"
    ]
  },
  {
    "objectID": "lessons/models/variate_covariate/variate_covariate_implementation.html#computing-environment",
    "href": "lessons/models/variate_covariate/variate_covariate_implementation.html#computing-environment",
    "title": "45  Implementation of MLE for variate-covariate models",
    "section": "45.5 Computing environment",
    "text": "45.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,polars,bokeh,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\nscipy     : 1.16.0\npolars    : 1.31.0\nbokeh     : 3.7.3\nbebi103   : 0.1.28\njupyterlab: 4.4.5",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Implementation of MLE for variate-covariate models</span>"
    ]
  },
  {
    "objectID": "lessons/models/pca/pca_theory.html",
    "href": "lessons/models/pca/pca_theory.html",
    "title": "46  Principal component analysis",
    "section": "",
    "text": "46.1 The problem of PCA\n| Download notebook\nWe often obtain high-dimensional data sets, in which each observation has many dimensions. In the parlance of practitioners of machine learning, each multidimensional observation is called a sample and each dimension is called a feature. We will use the observation/dimension and sample/feature nomenclature interchangeably. Considering the Palmer’s penguin data set, each observation (sample) is a penguin, and the dimensions (features) are bill depth, bill length, flipper length, and body mass.\nThe features, though many, may be related. We can imagine that there are unobservable variables, called latent variables at play the influence many features. For example, consider the four features of the penguin. They could all scale with the caloric intake of the penguins, in which case the latent variable of caloric intake could use used to predict all four of the features; it is responsible for most of the variation in what is observed.\nLet us now formalize this. Let \\(\\mathbf{y}\\in \\mathbb{R}^D\\) be a set of observable features. There are \\(D\\) of them. In the case of the penguins, \\(D = 4\\). Let \\(\\mathbf{z} \\in \\mathbb{R}^L\\) be the set of latent variables that influence the observations. Typically, \\(L &lt; D\\) and often \\(L \\ll D\\), and, as such, if we can infer the latent variable \\(\\mathbf{z}\\), we say we have achieved dimensionality reduction. Specifically, we define a model for \\(\\mathbf{y}\\), \\(\\mathbf{y} = h(\\mathbf{z};\\theta)\\), where \\(\\theta\\) is some set of parameters. So, to achieve dimensionality reduction, we want to find a function \\(h(\\mathbf{z};\\theta)\\) that most faithfully reproduces the observed \\(\\mathbf{y}\\).",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Principal component analysis</span>"
    ]
  },
  {
    "objectID": "lessons/models/pca/pca_theory.html#toward-building-a-pca-model-factor-analysis",
    "href": "lessons/models/pca/pca_theory.html#toward-building-a-pca-model-factor-analysis",
    "title": "46  Principal component analysis",
    "section": "46.2 Toward building a PCA model: Factor analysis",
    "text": "46.2 Toward building a PCA model: Factor analysis\nAs we build our model of how the data \\(\\mathbf{y}\\) are generated from the latent variables \\(z\\), we make a series of modeling decisions, many of which are intentionally chosen to be simplifying.\n\n46.2.1 Linear \\(h(\\mathbf{z};\\theta)\\)\nAs is commonly done, we will restrict the functions \\(h(\\mathbf{z};\\theta)\\) we choose to be linear functions. Further, we stipulate that \\(h(\\mathbf{z};\\theta)\\) serves to orthogonally project \\(\\mathbf{z}\\) onto \\(\\mathbf{y}\\).\n\\[\\begin{align}\nh(\\mathbf{z};\\mathsf{W}, \\boldsymbol{\\mu}) = \\mathsf{W}\\cdot \\mathbf{z} + \\boldsymbol{\\mu},\n\\end{align}\n\\]\nwhere the parameters \\(\\theta\\) are given by the \\(D \\times L\\) matrix \\(\\mathsf{W}\\), commonly referred to as a loading matrix, and \\(\\boldsymbol{\\mu}\\), which is an additive term that sets the value of \\(\\mathbf{y}\\) when \\(\\mathbf{z} = 0\\).\n\n\n46.2.2 i.i.d. Normally distributed data points\nIn addition to choosing a linear function \\(h(\\mathbf{z};\\mathsf{W}, \\boldsymbol{\\mu})\\), we assume an i.i.d. Normal generative model,\n\\[\\begin{align}\n\\mathbf{y}_i  \\sim \\text{Norm}(\\mathsf{W}\\cdot \\mathbf{z}_i + \\boldsymbol{\\mu}, \\mathsf{\\Psi})\\;\\forall i,\n\\end{align}\n\\]\nwhere \\(\\mathsf{\\Psi}\\) is a \\(D\\times D\\) covariance matrix describing how the observed data vary from the linear model.\n\n\n46.2.3 A diagonal covariance matrix\nWe typically take \\(\\mathsf{\\Psi}\\) to be diagonal; the residuals of the observations are modeled to be uncorrelated (though the observations are correlated through the latent variables \\(\\mathbf{z}\\)). We therefore will define the vector \\(\\boldsymbol{\\sigma}\\) to be the diagonal elements of \\(\\mathsf{\\Psi}\\) such that \\(\\Psi = \\text{diag}(\\boldsymbol{\\sigma}^2)\\), where \\(\\boldsymbol{\\sigma}^2\\) denotes element-wise squaring of the elements of \\(\\boldsymbol{\\sigma}\\).\n\n\n46.2.4 Modeling the latent variables\nThe latent variables, though unmeasured, are still random variables, as they vary meaningfully from experiment to experiment. As such, we need to specify a generative model for them. We take the latent variables to be \\(L\\)-variate Normally distributed.   \\[\\begin{align}\n\\mathbf{z}_i  \\sim \\text{Norm}(\\boldsymbol{\\mu}_0, \\mathsf{\\Sigma}_0)\\;\\forall i.\n\\end{align}\\]\nNote that since we are using a linear model and \\(\\mathbf{z}_i\\) gets transformed according to \\(\\mathsf{W}\\cdot \\mathbf{z}_i + \\boldsymbol{\\mu}\\), we can absorb \\(\\boldsymbol{\\mu}_0\\) and \\(\\mathsf{\\Sigma}_0\\) into \\(\\mathsf{W}\\) and \\(\\boldsymbol{\\mu}\\). Absorbing these as described, we can write\n\\[\\begin{align}\n\\mathbf{z}_i \\sim \\text{Norm}(\\mathbf{0}, \\mathsf{I})\\;\\forall i,\n\\end{align}\n\\]\nwhere \\(\\mathsf{I}\\) is the \\(L\\times L\\) identity matrix. Our model is then\n\\[\\begin{align}\n&\\mathsf{\\Psi} = \\text{diag}(\\boldsymbol{\\sigma}^2) \\\\[1em]\n&\\mathbf{z}_i \\sim \\text{Norm}(\\mathbf{0}, \\mathsf{I})\\;\\forall i, \\\\[1em]\n&\\mathbf{y}_i  \\sim \\text{Norm}(\\mathsf{W}\\cdot \\mathbf{z}_i + \\boldsymbol{\\mu}, \\mathsf{\\Psi})\\; \\forall i\n\\end{align}\n\\]\n\n\n46.2.5 Restrictions on the loading matrix\nWe will not go into too much detail here, but in general the loading matrix is nonidentifiable. There are several approaches to deal with the nonidentifiability, with a popular one being to enforce that the loading matrix \\(\\mathsf{W}\\) is semiorthogonal and unitary such that the columns are orthonormal vectors and \\(\\mathsf{W}^\\mathsf{T}\\cdot \\mathsf{W} = \\mathsf{I}\\), the identity matrix. If \\(\\mathbf{w}_j\\) is a column of \\(\\mathsf{W}\\), then\n\\[\\begin{align}\n\\mathbf{w}_j^\\mathsf{T}\\cdot \\mathbf{w}_k = \\delta_{jk},\n\\end{align}\n\\]\nwhere \\(\\delta_{jk}\\) is the Kronecker delta, which is equal to one when \\(j = k\\) and zero otherwise.\nFor clarity and for reference, the dimensionality of the variables are shown in the table below. The index \\(i\\) indexes data points, and there are \\(N\\) of them.\n\n\n\n\n\n\n\n\nVariable\nDescription\nDimension\n\n\n\n\n\\(\\boldsymbol{\\mu}\\)\nlocation parameter of measurements\n\\(D\\)-vector\n\n\n\\(\\mathsf{W}\\)\nloading matrix\n\\(D \\times L\\) semiorthogonal unitary matrix\n\n\n\\(\\boldsymbol{\\sigma}\\)\nstandard deviation for each measurement\npositive \\(D\\)-vector\n\n\n\\(\\mathsf{\\Psi}\\)\nmatrix representation of \\(\\boldsymbol{\\sigma}^2\\)\n\\(D \\times D\\) nonnegative diagonal matrix\n\n\n$_i $\nLatent variable for datum \\(i\\)\n\\(L\\)-vector\n\n\n$_i $\nDatum \\(i\\)\n\\(D\\)-vector\n\n\n\nThe model described above is often referred to as a factor analysis model. We have already made simplifications to a latent variable model, namely that dependence of the observations on the latent variables is linear with a semiorthogonal unitary loading matrix, that both the observations and the latent variables are Normally distributed, and that multivariate Normal distribution of the observations has a diagonal covariance matrix.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Principal component analysis</span>"
    ]
  },
  {
    "objectID": "lessons/models/pca/pca_theory.html#probabilistic-pca",
    "href": "lessons/models/pca/pca_theory.html#probabilistic-pca",
    "title": "46  Principal component analysis",
    "section": "46.3 Probabilistic PCA",
    "text": "46.3 Probabilistic PCA\nWe consider first the special case where each entry in \\(\\boldsymbol{\\sigma}\\) is the same, say \\(\\sigma\\). In that case, \\(\\mathsf{\\Psi} = \\sigma\\mathsf{I}\\). We further restrict \\(\\mathsf{W}\\) to have orthonormal columns. The assumption here is that each data point is drawn from a Normal distribution with the same variance, but with mean \\(\\mathsf{W}\\cdot \\mathbf{z}_i + \\boldsymbol{\\mu}\\). This special case is called probabilistic principle component analysis (PPCA).\nThe full model is\n\\[\\begin{align}\n&\\mathbf{z}_i \\sim \\text{Norm}(\\mathbf{0}, \\mathsf{I})\\;\\forall i, \\\\[1em]\n&\\mathbf{y}_i \\sim \\text{Norm}(\\mathsf{W}\\cdot \\mathbf{z}_i + \\boldsymbol{\\mu}, \\sigma^2 \\mathsf{I})\\; \\forall i\n\\end{align}\n\\]\nProbabilistic PCA is a reasonable model; it assumes that each data point results from a linear combination of the latent variables and is uncorrelated with all other data points (except through the latent variables) with homoscedasticity.\n\n46.3.1 Maximum likelihood for PPCA\nPPCA has an added advantage that the maximum likelihood estimates for \\(\\boldsymbol{\\mu}\\), \\(\\mathsf{W}\\), and \\(\\sigma\\) may be computed analytically. The result is\n\\[\\begin{align}\n&\\boldsymbol{\\mu}_\\mathrm{MLE} = \\bar{\\mathbf{y}},\\\\[1em]\n&\\mathsf{W}_\\mathrm{MLE} = \\mathsf{U}\\cdot(\\mathsf{\\Lambda} - \\sigma^2\\mathsf{I})^{\\frac{1}{2}} \\\\[1em]\n&\\sigma_\\mathrm{MLE}^2 = \\frac{1}{D - L}\\sum_{j=L+1}^D \\lambda_j,\n\\end{align}\n\\tag{46.1}\\]\nwhere \\(=(\\lambda_1, \\ldots, \\lambda_D)\\) are the eigenvalues of the empirical covariance matrix (the plug-in estimate for the covariance moatrix),\n\\[\\begin{align}\n\\hat{\\mathsf{\\Sigma}} = \\frac{1}{N}\\sum_{i=1}^N(\\mathbf{y}_i - \\bar{y})\\cdot(\\mathbf{y}_i - \\bar{y})^\\mathsf{T},\n\\end{align}\\]\nordered from largest to smallest, \\(\\mathsf{\\Lambda} = \\text{diag}((\\lambda_1, \\ldots, \\lambda_L))\\), and \\(\\mathsf{U}\\) is a matrix whose columns are the eigenvectors corresponding to eigenvalues \\(\\lambda_1, \\ldots, \\lambda_L\\).\n\n\n46.3.2 Recognition distribution for PPCA\nThough we have MLEs for \\(\\boldsymbol{\\mu}\\), \\(\\mathsf{W}\\), and \\(\\sigma\\), we still do not know how these relate to the latent variables \\(\\mathbf{z}\\). We seek a regonition distribution for \\(\\mathbf{z}_i\\), \\(f(\\mathbf{z}_i \\mid \\mathbf{y}_i)\\). This is the distribution of the latent parameters conditioned on the high-dimensional measurements \\(\\mathbf{y}_i\\). To find it, we can use Bayes’s theorem.\n\\[\\begin{align}\nf(\\mathbf{z}_i \\mid \\mathbf{y}_i) = \\frac{f(\\mathbf{y}_i\\mid  \\mathbf{z}_i)\\,f(\\mathbf{z}_i)}{f(\\mathbf{y}_i)}.\n\\end{align}\n\\]\nBecause the denominator has no \\(\\mathbf{z}_i\\)-dependence, it is a normalization constant. The recognition distribution is then a product of two Normal distributions, since\n\\[\\begin{align}\n&\\mathbf{y}_i \\mid \\mathbf{z}_i \\sim \\text{Norm}(\\mathsf{W}\\cdot \\mathbf{z}_i + \\boldsymbol{\\mu}, \\mathsf{\\Psi}),\\\\[1em]\n&\\mathbf{z}_i \\sim \\text{Norm}(\\mathbf{0}, \\mathsf{I}).\n\\end{align}\n\\]\nAfter some algebraic grunge, we can work out that the recognition distribution parametrized by the MLE values of \\(\\boldsymbol{\\mu}\\), \\(\\mathsf{W}\\), and \\(\\sigma\\) is given by\n\\[\\begin{align}\n\\mathbf{z}_i\\mid \\mathbf{y}_i \\sim \\text{Norm}(\\boldsymbol{\\mu}_{\\mathbf{z}\\mid\\mathbf{y}_i}, \\mathsf{\\Sigma}_{\\mathbf{z}}),\n\\end{align}\n\\tag{46.2}\\]\nwhere\n\\[\\begin{align}\n\\mathsf{\\Sigma}_{\\mathbf{z}} = \\sigma_\\mathrm{MLE}^2\\,(\\sigma_\\mathrm{MLE}^{2}\\,\\mathsf{I} + \\mathsf{W}_\\mathrm{MLE}^\\mathsf{T}\\cdot\\mathsf{W}_\\mathrm{MLE})^{-1}\n\\end{align}\n\\tag{46.3}\\]\nand\n\\[\\begin{align}\n\\boldsymbol{\\mu}_{\\mathbf{z}\\mid\\mathbf{y}_i} = (\\sigma_\\mathrm{MLE}^{2}\\,\\mathsf{I} + \\mathsf{W}_\\mathrm{MLE}^\\mathsf{T}\\cdot\\mathsf{W}_\\mathrm{MLE})^{-1}\\cdot\\mathsf{W}_\\mathrm{MLE}^\\mathsf{T}\\cdot(\\mathbf{y_i - \\boldsymbol{\\mu}_\\mathrm{MLE}}).\n\\end{align}\n\\tag{46.4}\\]\nThe “probabilistic” part of the name probabilistic PCA comes from the fact that we write a probability distribution for the latent parameters, which is given by equations 46.2, 46.3, and 46.4, above.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Principal component analysis</span>"
    ]
  },
  {
    "objectID": "lessons/models/pca/pca_theory.html#pca-as-a-limit-of-ppca",
    "href": "lessons/models/pca/pca_theory.html#pca-as-a-limit-of-ppca",
    "title": "46  Principal component analysis",
    "section": "46.4 PCA as a limit of PPCA",
    "text": "46.4 PCA as a limit of PPCA\nConsider now the limit of PPCA where \\(\\sigma\\) goes to zero. That is, we consider the limit where there is no noise in the measured data. The scale parameter of the recognition distribution vanishes and the location parameter becomes\n\\[\\begin{align}\n\\boldsymbol{\\mu}_{\\mathbf{z}\\mid\\mathbf{y}_i} = (\\mathsf{W}_\\mathrm{MLE}^\\mathsf{T}\\cdot\\mathsf{W}_\\mathrm{MLE})^{-1}\\cdot\\mathsf{W}_\\mathrm{MLE}^\\mathsf{T}\\cdot(\\mathbf{y_i - \\bar{\\mathbf{y}}}),\n\\end{align}\n\\tag{46.5}\\]\nwith the MLE for the loading matrix given by\n\\[\\begin{align}\n\\mathsf{W}_\\mathrm{MLE} = \\mathsf{U}\\cdot\\mathsf{\\Lambda}^{\\frac{1}{2}}.\n\\end{align}\n\\]\nNote that because the scale parameter of the recognition distribution is artificially set to zero in the PCA model, \\(\\boldsymbol{\\mu}_{\\mathbf{z}\\mid\\mathbf{y}_i}\\) is our estimate for \\(\\mathbf{z}_i\\). In this setting, we refer to the estimate of the latent variables \\(\\mathbf{z}_i\\) as the principal components. We could get confidence regions for the principal components by bootstrapping, but that is seldom done. After all, we artifically set \\(\\sigma = 0\\) to get the principal component. Instead of bootstrapping, we could just perform PPCA and get the distribution of the latent parameter values. Though it is important to note that these are themselves functions of the MLE of the parameters \\(\\boldsymbol{\\mu}\\), \\(\\mathsf{W}\\), and \\(\\sigma\\).\nAs a final note, looking at the form of our estimate of the latent variables given in Equation 46.5, we see that it is the solution to the least squares problem\n\\[\\begin{align}\n\\mathbf{z}_i = \\text{arg min}_{\\mathbf{z}_i} \\left\\lVert (\\mathbf{y}_i - \\bar{\\mathbf{y}}_i) - \\mathsf{W}_\\mathrm{MLE}\\cdot\\mathbf{z}_i\\right\\rVert_2^2.\n\\end{align}\n\\]\nThe squared two-norm in the above equation is sometimes referred to as a loss function. The goal of PCA is to find \\(\\mathsf{W}_\\mathrm{MLE}\\) and \\(\\mathbf{z}_i\\) that minimize it, as we have just shown. The idea is that the optimal projection of \\(\\mathbf{z}_i\\) onto \\(\\mathbf{y}_i\\) is the one that minimizes the difference (with the mean subtracted off).\n\n46.4.1 Centering and scaling, the interpretation of \\(\\hat{\\mathsf{\\Sigma}}\\), and maximal variance\nAssume for a moment that the mean of \\(\\mathbf{y}\\) is zero,\n\\[\\begin{align}\n\\bar{\\mathbf{y}} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{y}_i = \\mathbf{0},\n\\end{align}\n\\]\nand the variance of each of the \\(D\\) entries in \\(\\mathbf{y}\\) is one,\n\\[\\begin{align}\ns_j = \\frac{1}{N}\\sum_{i=1}^N (y_{i,j} - \\bar{y}_j)^2 = \\frac{1}{N}\\sum_{i=1}^N y_{i,j}^2 = 1\\;\\forall j\\in[1, D].\n\\end{align}\n\\]\nWe can always center and scale our data such that they have a mean of 0 and a variance of 1 by subtracting the plug-in estimate for the mean from every data point and dividing the result by the plug-in estimate for the standard deviation.\nIn that case, the entries of \\(\\hat{\\mathsf{\\Sigma}}\\) are\n\\[\\begin{align}\n\\hat{\\Sigma}_{jk} = \\frac{1}{N}\\sum_{i=1}^N y_{ij}\\, y_{ik} = \\frac{1}{N}\\sum_{i=1}^N (y_{ij} - \\bar{y_j})(y_{ik} - \\bar{y_k}),\n\\end{align}\n\\tag{46.6}\\]\nwhich we recognize as the elements of the empirical covariance matrix (or, equivalently, since the diagonal entries are all one, an empirical correlation matrix). Thus, \\(\\hat{\\mathsf{\\Sigma}}\\) is the plug-in estimate for the covariance matrix when using centered-and-scaled data.\nConsider a principal component \\(z_{ij}\\). The plug-in estimate for the variance of this is\n\\[\\begin{align}\n\\widehat{\\mathbb{V}(z_{ij})} = \\frac{1}{N}\\sum_{i=1}^N z_{ij}^2 - \\left(\\frac{1}{N}\\sum_{i=1}^Nz_{ij}\\right)^2.\n\\end{align}\n\\]\nLet us compute the first moment.\n\\[\\begin{align}\n\\frac{1}{N}\\sum_{i=1}^Nz_{ij} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{w}_{j}^\\mathsf{T} \\cdot \\mathbf{y}_i = \\mathbf{w}_{j}^\\mathsf{T}\\cdot\\left(\\frac{1}{N}\\sum_{i=1}^N \\mathbf{y}_i\\right) = 0,\n\\end{align}\\]\nwhere we have recognized the average in parentheses to be zero since the data are centered. Therefore, the plug-in estimate for the variance is\n\\[\\begin{align}\n\\widehat{\\mathbb{V}(z_{ij})} = \\frac{1}{N}\\sum_{i=1}^N z_{ij}^2 = \\frac{1}{N}\\sum_{i=1}^N \\frac{1}{N}\\sum_{i=1}^N(\\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{w}_j)^2.\n\\end{align}\\]\nThis is a quadratic form involving the covariance matrix \\(\\hat{\\mathsf{\\Sigma}}\\).\n\\[\\begin{align}\n\\widehat{\\mathbb{V}(z_{ij})} = \\mathbf{w}_j^\\mathsf{T}\\cdot\\hat{\\mathsf{\\Sigma}}\\cdot\\mathbf{w}_j.\n\\end{align}\\]\nRecall that the loss function is\n\\[\\begin{align}\n\\text{loss}(\\mathsf{W}) = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{y}_i^\\mathsf{T}\\cdot\\mathbf{y}_i - \\sum_{j=1}^L \\mathbf{w}_j^\\mathsf{T}\\cdot\\hat{\\mathsf{\\Sigma}}\\cdot\\mathbf{w}_j\n= \\text{constant} - \\widehat{\\mathbb{V}(z_{ij})}.\n\\end{align}\\]\nTherefore, finding the transformation \\(\\mathbf{w}_j\\) that minimizes the loss function is the same as finding the transformation that maximizes the variance of the projection. This is why the principal components are often referred to as the directions that have maximal variance.\nAs a final note, I mention that it is important to center and scale the data set before performing PCA because given directions may have high variance just because of the scaling of the measurement. This is generally not the variance we wish to interpret.\n\n\n46.4.2 Prescription for PCA\nFollowing the above analysis, we arrive at a prescription for PCA. After choosing the number of principal components \\(L\\), do the following.\n\nCenter and scale the data set by subtracting the mean and dividing by the standard deviation along each dimension.\nCompute the empirical covariance matrix.\nCompute the first \\(L\\) eigenvalues and eigenvectors.\nConstruct the loading matrix \\(\\mathsf{W}\\) from the eigenvectors corresponding to the \\(L\\) largest eigenvalues.\nThe principal components are \\(\\mathbf{z}_i = \\mathbf{W}^\\mathsf{T} \\cdot \\mathbf{y}_i \\;\\forall i\\).\n\n\n\n46.4.3 Nonidentifiability of PCA\nThe loading matrix \\(\\mathsf{W}\\) is comprised of the eigenvectors of the empirical covariance matrix and therefore the columns are orthogonal, since the covariance matrix is symmetric. However, any of these eigenvectors can be multiplied by a scalar (which is true in general for an eigenvector; if \\(\\mathbf{v}\\) is an eigenvector of \\(\\mathsf{A}\\), then so is \\(\\alpha \\mathbf{v}\\) for real \\(\\alpha \\ne 0\\).) We can insist that the columns of \\(\\mathsf{W}\\) for an orthonormal basis, such that the magnitude of each eigenvector is 1, which is what is commonly done. However, any given column of \\(\\mathsf{W}\\) may be multiplied by \\(-1\\), still resulting in a nonidentifiability. We therefore cannot uniquely find a loading matrix \\(\\mathsf{W}\\) and therefore a unique set of principal components.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Principal component analysis</span>"
    ]
  },
  {
    "objectID": "lessons/models/pca/pca_implementation.html",
    "href": "lessons/models/pca/pca_implementation.html",
    "title": "47  Implementation of PCA",
    "section": "",
    "text": "47.1 Example data set\n| Download notebook\nData set download\nAs an example of PCA, we will use data from Remedios, et al., 2017. This paper from David Anderson’s lab features a study of mouse behavioral activity (scored by a human) and neuronal activity in the ventrolateral subdivision of the ventromedial hypothalamus (monitored by calcium imaging) of a male mouse when presented with another mouse of a given sex. The authors of the paper have made the data available as a MAT-file, which you can download here: https://s3.amazonaws.com/bebi103.caltech.edu/data/hypothalamus_calcium_imaging_remedios_et_al.mat.\nScipy offers a convenient interface for loading in older versions of MAT files via its scipy.io.loadmat() function. Let us put it to use and see what is in the file.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Implementation of PCA</span>"
    ]
  },
  {
    "objectID": "lessons/models/pca/pca_implementation.html#loading-older-style-mat-files",
    "href": "lessons/models/pca/pca_implementation.html#loading-older-style-mat-files",
    "title": "47  Implementation of PCA",
    "section": "47.2 Loading older-style MAT files",
    "text": "47.2 Loading older-style MAT files\nScipy offers a convenient interface for loading in older versions of MAT files via its scipy.io.loadmat() function. Let us put it to use and see what is in the file.\n\ndata = scipy.io.loadmat(\n    os.path.join(data_path, 'hypothalamus_calcium_imaging_remedios_et_al.mat')\n)\n\n# Look at what is in the Python variable `data`\ndata\n\n{'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Mon Jun 27 10:14:33 2022',\n '__version__': '1.0',\n '__globals__': [],\n 'attack_vector': array([[0, 0, 0, ..., 0, 0, 0]], shape=(1, 18561), dtype=uint8),\n 'neural_data': array([[-0.88539819, -0.80569246, -0.73758439, ..., -0.98890073,\n         -1.03551113, -0.99196781],\n        [-0.14700497, -0.32165216, -0.33878817, ...,  3.78779148,\n          3.83222242,  3.85200285],\n        [-1.19636847, -1.18637515, -1.16444654, ..., -0.34082998,\n         -0.39085209, -0.38183045],\n        ...,\n        [-1.07157774, -0.58209432, -0.17334395, ..., -0.8675362 ,\n         -0.87503357, -1.08967763],\n        [ 1.82225675,  1.30727208,  1.03393152, ...,  1.30556881,\n          1.41885948,  0.99312731],\n        [ 0.5958771 ,  0.36866712,  0.28664763, ...,  1.78433094,\n          1.47429182,  1.85403792]], shape=(115, 18561)),\n 'sex_vector': array([[1, 1, 1, ..., 0, 0, 0]], shape=(1, 18561), dtype=uint8)}\n\n\nIt is apparent that scipy.io.loadmat() returns a dictionary with the various Matlab variables as well as some metadata. As given in the '__header__' entry of the dictionary, this is a version 5.0 MAT-file, which is indeed an earlier version than 7.3, which uses HDF5.\nAccording to metadata associated with the paper, data were acquired at 30 Hz, to the time between each measurement is 1/30th of a second. The variables have the following meanings.\n\nattack_vector: An entry is zero if the mouse was not attacking his companion in the arena at that time point and one if he was.\nsex_vector: An entry is one if the presented mouse was female and zero at a given time point and zero if male.\nneural_data: Each row of this Numpy array corresponds to a single neuron as measured by calcium imaging and each column corresponds to a given time point. The reported value is the relative fluorescent change, \\((F(t) - F_0) / F_0\\), where \\(F_0\\) is the average fluorescent intensity over all frames. The reported values are normalized over the standard deviation of a baseline trial and are in units of that standard deviation.\n\nWe can pop those data out.\n\nneural_data = data.pop('neural_data') # Row is time, column is neuron\nsex_vector = data.pop('sex_vector').squeeze()\nattack_vector = data.pop('attack_vector').squeeze()\n\n# Set up time points\nt = np.arange(neural_data.shape[1]) / 30\n\nNow we can construct a visualization.\n\n# Kwargs for plotting\nkwargs = dict(\n    x_interpixel_distance=1/30,\n    y_interpixel_distance=1,\n    frame_height=150,\n    frame_width=600,\n    x_axis_label=\"time (s)\",\n    y_axis_label=\"neuron\",\n)\n\n# Plot neural data\np_neuron = bebi103.image.imshow(neural_data, **kwargs)\n\n# Sex of presented mouse\np_sex = bokeh.plotting.figure(\n    frame_height=50,\n    frame_width=600,\n    x_axis_label=None,\n    y_axis_label='sex',\n    y_range=['m', 'f'],\n    toolbar_location=None,\n)\nsex_color = ['orchid' if s else 'dodgerblue' for s in sex_vector]\np_sex.scatter(t, ['f' if s else 'm' for s in sex_vector], size=2, color=sex_color)\np_sex.xaxis.major_label_text_font_size = '0pt'\n\n# Attack encoding\np_attack = bokeh.plotting.figure(\n    frame_height=50,\n    frame_width=600,\n    x_axis_label=None,\n    y_axis_label='attack',\n    y_range=['no', 'yes'],\n    toolbar_location=None,\n)\nattack_color = ['tomato' if a else 'gray' for a in attack_vector]\np_attack.scatter(t, ['yes' if a else 'no' for a in attack_vector], size=2, color=attack_color)\np_attack.xaxis.major_label_text_font_size = '0pt'\n\n# Link x-axes\np_attack.x_range = p_neuron.x_range\np_sex.x_range = p_neuron.x_range\n\n# Show the plots together\nbokeh.io.show(bokeh.layouts.column(p_sex, p_attack, p_neuron))",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Implementation of PCA</span>"
    ]
  },
  {
    "objectID": "lessons/models/pca/pca_implementation.html#pca-by-direct-eigenvalue-calculation",
    "href": "lessons/models/pca/pca_implementation.html#pca-by-direct-eigenvalue-calculation",
    "title": "47  Implementation of PCA",
    "section": "47.3 PCA by direct eigenvalue calculation",
    "text": "47.3 PCA by direct eigenvalue calculation\nFor each time point, we have 115 neuron measurements. In our notation, the observed neuronal activity at time point \\(i\\) is a \\(D = 115\\) dimensional vector \\(\\mathbf{y}_i\\). We assume there are only \\(L = 2\\) latent variables responsible for the observed neuronal activity.\nTo start, we can center and scale the data. We will also transpose the neural data so that it has dimension \\(N\\times D\\) for ease in comparing to the above theoretical results.\n\n# Transpose\ny = neural_data.T\n\n# Center data set by subtracting the mean from each time point and dividing by the standard deviation\ny = (y - np.mean(y, axis=0)) / np.std(y, axis=0)\n\nWe can now carry out the prescription of PCA. We are hoping to visualize the trajectory of the neuronal activity of the mouse on a 2D plot, so we will specify that there are \\(L = 2\\) latent variables (principal componenents).\n\n# Two principle components\nL = 2\n\n# Compute the plugin estimate for the covariance matrix\ncov = np.cov(y.T)\n\n# Compute the eigensystem\neigenvalues, eigenvectors = np.linalg.eig(cov)\n\n# Sort highest to lowest\nidx = eigenvalues.argsort()[::-1]\n\n# Extract largest L to build eigenvalues and loading matrix\nlam = eigenvalues[idx][:L]\nW = eigenvectors[:, idx][:, :L]\n\n# Compute principal components\nz = np.dot(W.T, y.T)\n\nWe will plot the visualization of the principal components in a moment, but for now, we will take the opportunity to compute the fraction of the variance that each principal component contributes. Given that the eigenvalues are the eigenvalues of the covariance matrix, each eigenvalue gives the scale of the variance along the given eigenvector. Thus, the fraction of the total variance that runs along principal component \\(j\\) is \\(\\lambda_j / \\sum_{j=1}^L \\lambda_j\\).\n\nlam / eigenvalues.sum()\n\narray([0.22153642, 0.15257567])\n\n\nEvidently, the first two principal components account for nearly 40% of the total variance.\nNow, we will proceed to plot the principal components. We will plot them against each other and color the glyphs by time or by sex of the presented mouse. We will also plot each of the two principal components vs time.\n\np1 = bokeh.plotting.figure(\n    frame_height=350,\n    frame_width=350,\n    x_axis_label=\"PC 1\",\n    y_axis_label=\"PC 2\",\n)\n\np2 = bokeh.plotting.figure(\n    frame_height=150,\n    frame_width=550,\n    x_axis_label=\"time\",\n    y_axis_label=\"PC\",\n)\n\n# Set up date for the plot\ntime_color = bebi103.viz.q_to_color(t, bokeh.palettes.Viridis256)\nsex_color = [\"orchid\" if s else \"dodgerblue\" for s in sex_vector]\ncds = bokeh.models.ColumnDataSource(dict(pc1=z[0], pc2=z[1], t=t, color=time_color))\n\n# We'll allow selection of which color we want to visualize\ncolor_selector = bokeh.models.RadioButtonGroup(labels=[\"time\", \"sex\"], active=0)\njs_code = \"\"\"\ncds.data['color'] = color_selector.active == 0 ? time_color : sex_color;\ncds.change.emit();\n\"\"\"\n\ncolor_selector.js_on_event(\n    \"button_click\",\n    bokeh.models.CustomJS(\n        code=js_code,\n        args=dict(\n            time_color=time_color,\n            sex_color=sex_color,\n            cds=cds,\n            color_selector=color_selector,\n        ),\n    ),\n)\n\n# Populate glyphs\np1.scatter(source=cds, x=\"pc1\", y=\"pc2\", color=\"color\")\np2.line(source=cds, x='t', y='pc1', legend_label='PC 1')\np2.line(source=cds, x='t', y='pc2', color=\"orange\", legend_label='PC 2')\n\n# Lay out and show!\nbokeh.io.show(\n    bokeh.layouts.column(\n        bokeh.layouts.row(p1, bokeh.layouts.column(bokeh.models.Div(text=\"Color by\"), color_selector)),\n        p2\n    )\n)\n\n\n  \n\n\n\n\n\nWhen looking at the coloring by sex, it is clear that there is a difference along the PC 1-axis depending on what sex of mouse is presented. When the male mouse is introduced about 250 seconds into the experiment, PC 1 shifts to exceed PC 2.\n\n47.3.1 Reconstruction of neuronal measurements from principal components\nBy restricting ourselves to only two latent variables, we necessarily have omitted some of the dynamics observed in the system. To assess what we have lost, we can reconstruct the neuronal signal from the estimates of the latent variables and the corresponding loading matrix \\(\\mathsf{W}\\). Recall that the reconstructed data set is\n\\[\\begin{align}\n\\hat{\\mathbf{y}}_i = h(\\mathbf{z}_i; \\mathsf{W}) = \\mathsf{W} \\cdot \\mathbf{z}_i\\;\\forall i.\n\\end{align}\n\\]\nWe can directly compute this and compare to the measured signal.\n\n# Compute the reconstruction\ny_hat = np.dot(W, z)\n\n# Uncenter and unscale the reconstruction\ny_hat = np.std(neural_data, axis=0) * y_hat + np.mean(neural_data, axis=0)\n\n# Kwargs for plotting\nkwargs = dict(\n    x_interpixel_distance=1/30,\n    y_interpixel_distance=1,\n    frame_height=150,\n    frame_width=600,\n    x_axis_label=\"time (s)\",\n    y_axis_label=\"neuron\",\n)\n\n# Plot original data\np_neuron = bebi103.image.imshow(neural_data, **kwargs)\n\n# Plot reconstructed data\np_hat = bebi103.image.imshow(y_hat, **kwargs)\n\np_neuron.yaxis.major_label_text_font_size = '0pt'\np_hat.yaxis.major_label_text_font_size = '0pt'\n\nbokeh.io.show(bokeh.layouts.column(p_neuron, p_hat))\n\n\n  \n\n\n\n\n\nQualitatively, we see that the reconstruction recapitulates the key features of the measured data.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Implementation of PCA</span>"
    ]
  },
  {
    "objectID": "lessons/models/pca/pca_implementation.html#principal-component-analysis-using-singular-value-decomposition",
    "href": "lessons/models/pca/pca_implementation.html#principal-component-analysis-using-singular-value-decomposition",
    "title": "47  Implementation of PCA",
    "section": "47.4 Principal component analysis using singular value decomposition",
    "text": "47.4 Principal component analysis using singular value decomposition\nPerforming PCA by directly computing eigenvalues is instructive. It gives clear, direct interpretation of what the estimates latent variables mean. Here, we show that PCA may be equivalently computed using singular value decomposition. We will not go into the details here, but will instead state a few theorems from linear algebra and comment that using SVD can result in faster calculations that directly computing the eigenvalues. First, the theorems.\n\nAny real \\(m\\times n\\) matrix \\(\\mathsf{A}\\) may be factored as \\(\\mathsf{A} = \\mathsf{U}\\cdot\\mathsf{S}\\cdot\\mathsf{V}^\\mathsf{T}\\), where \\(\\mathsf{U}\\) is a real \\(m\\times m\\) orthogonal unitary matrix (such that the columns of \\(\\mathsf{U}\\) for orthogonal vectors and \\(\\mathsf{U}^{-1} = \\mathsf{U}^\\mathsf{T}\\)), \\(\\mathsf{S}\\) is an \\(m\\times n\\) diagonal matrix with nonnegative real entries, and \\(\\mathsf{V}\\) is an \\(n\\times n\\) real unitary orthogonal matrix. This factorization is called a singular value decomposition. The diagonal entries of \\(\\mathsf{S}\\) are referred to as the singular values of \\(\\mathsf{A}\\).\nGiven a singular value decomposition, the columns of \\(\\mathsf{U}\\) are the eigenvectors of \\(\\mathsf{A}\\cdot \\mathsf{A}^\\mathsf{T}\\), the diagonal entries of \\(\\mathsf{S}\\) are the eigenvalues of \\(\\mathsf{A}^\\mathsf{T}\\cdot \\mathsf{A}\\) (which are equal to the eigenvalues of \\(\\mathsf{A}\\cdot \\mathsf{A}^\\mathsf{T}\\)), and the columns of \\(\\mathsf{V}\\) are the eigenvectors of \\(\\mathsf{A}^\\mathsf{T} \\cdot \\mathsf{A}\\).\nAny \\(n\\times n\\) matrix \\(\\mathsf{B}\\) with \\(n\\) linearly independent eigenvectors may be factored as \\(\\mathsf{Q}\\cdot\\mathsf{\\Lambda}\\cdot\\mathsf{Q}^{-1}\\), where \\(\\mathsf{Q}\\) is \\(n\\times n\\) and its columns are the eigenvectors of \\(\\mathsf{B}\\) and \\(\\mathsf{\\Lambda}\\) is a diagonal matrix whose diagonal entries are the eigenvalues of \\(\\mathsf{B}\\). Such a factorization is called an eigendecomposition.\nIf \\(\\mathsf{B}\\) is real symmetric positive definite, then \\(\\mathsf{Q}\\) is a unitary matrix, and \\(\\mathbf{B} = \\mathsf{Q}\\cdot\\mathsf{\\Lambda}\\cdot\\mathsf{Q}^\\mathsf{T}\\).\n\nNow, consider \\(\\hat{\\mathsf{\\Sigma}}\\), the plug-in estimate for the covariance matrix. We wrote it element by element in Equation 46.6, but we can write it in a convenient matrix form by defining a \\(N\\times D\\) matrix \\(\\mathsf{Y}\\) where row \\(i\\) is \\(\\mathbf{y}_i\\). Then, given that the data are centered, \\(\\hat{\\mathsf{\\Sigma}} = \\mathsf{Y}^\\mathsf{T}\\cdot\\mathsf{Y}/N\\).\n\\[\\begin{align}\n\\hat{\\mathsf{\\Sigma}} = \\frac{1}{N}\\,\\mathsf{Y}^\\mathsf{T}\\cdot\\mathsf{Y} = \\frac{1}{N}\\mathsf{Q}\\cdot \\mathsf{\\Lambda}\\cdot\\mathsf{Q}^\\mathsf{T},\n\\end{align}\n\\]\nwhere we have used the fact that \\(\\mathsf{Q}\\) is unitary since \\(\\mathsf{Y}^\\mathsf{T}\\cdot\\mathsf{Y}\\) is positive definite. We have already worked out that the loading matrix \\(\\mathsf{W}\\) is given by the first \\(L\\) eigenvectors of \\(\\hat{\\Sigma}\\), so that \\(\\mathsf{Q} = \\mathsf{W}\\).\nNow, consider the singular value decomposition of \\(\\mathsf{Y}\\),\n\\[\\begin{align}\n\\mathsf{Y} = \\mathsf{U}\\cdot \\mathsf{S}\\cdot\\mathsf{V}^\\mathsf{T}.\n\\end{align}\n\\]\nThe matrix \\(\\mathsf{V}\\) is comprised of the eigenvectors of \\(\\mathsf{Y}^\\mathsf{T} \\cdot \\mathsf{Y}\\). But this is also \\(\\mathsf{W}\\). So, we can use SVD computing eigenvalues/vectors to find \\(\\mathsf{W}\\).\nComputing the eigenvectors of an \\(D\\times D\\) matrix (which is what \\(\\hat{\\mathsf{\\Sigma}}\\) is) is an \\(\\mathcal{O}(D^3)\\) calculation. The calculation of \\(\\hat{\\mathsf{\\Sigma}} = \\mathsf{Y}^\\mathsf{T}\\cdot\\mathsf{Y}\\) is itself an \\(\\mathcal{O}(ND^2)\\) calculation, making the computational complexity of performing PCA by eigendecomposition as \\(\\mathcal{O}(ND^2)\\) + \\(\\mathcal{D^3}\\). Computing the SVD for an \\(N\\times D\\) matrix, which we have to do in this case (the SVD of \\(\\mathsf{Y}\\)), is an \\(\\mathcal{O}(ND^2) + \\mathcal{O}(D^3)\\) calculation, which is the same complexity of the eigendecomposition method. However, if we only need to compute the first \\(L\\) eigenvectors, the randomized SVD algorithm developed by Caltech’s own Joel Tropp can do the calculation in \\(\\mathcal{O}(NL^2) + \\mathcal{O}(L^3)\\) time, which is a big improvement if \\(L \\ll D\\) as it often is, and indeed is in this case.",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Implementation of PCA</span>"
    ]
  },
  {
    "objectID": "lessons/models/pca/pca_implementation.html#sec-pca-sklean",
    "href": "lessons/models/pca/pca_implementation.html#sec-pca-sklean",
    "title": "47  Implementation of PCA",
    "section": "47.5 Principal component analysis using scikit-learn",
    "text": "47.5 Principal component analysis using scikit-learn\nScikit-learn implements PCA using the aforementioned randomized SVD algorithm. A PCA instance is instantiated using sklearn.decomposition.PCA(), with \\(L\\), the number of components, provided.\n\n# Instantiate a PCA object\npca = sklearn.decomposition.PCA(n_components=2)\n\nAs usual with the scikit-learn API, we call the fit() method to perform PCA. The input data is expected to be \\(N\\times D\\), as we have defined for \\(\\mathsf{Y}\\) above and as we have stored in the variable \\(y\\).\n\n# Perform the fit\npca.fit(y)\n\nPCA(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA?Documentation for PCAiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_components \n2\n\n\n\ncopy \nTrue\n\n\n\nwhiten \nFalse\n\n\n\nsvd_solver \n'auto'\n\n\n\ntol \n0.0\n\n\n\niterated_power \n'auto'\n\n\n\nn_oversamples \n10\n\n\n\npower_iteration_normalizer \n'auto'\n\n\n\nrandom_state \nNone\n\n\n\n\n            \n        \n    \n\n\nWe got a couple of warnings encountered during some of the matrix calculations, but the PCA fit was accomplished. We can extract the columns of the loading matrix \\(\\mathsf{W}\\) using the components_ attribute. Here, I will show that we get the same result as we did for the eigenvalue-based calculation we did above, which should be the case potentially up to a multiplicative constant of \\(-1\\).\n\nnp.isclose(np.abs(pca.components_ / W.T), 1).all()\n\nnp.True_\n\n\nSuccess!\nWe have already seen that we can recover the loading matrix \\(\\mathsf{W}\\) using pca.components_. To recover other pertinent results, we can use the following attributes/methods.\n\nExplained variance, \\(\\lambda_j/\\sum_{k=1}^L\\lambda_k\\): explained_var = pca.explained_variance_ratio_.\nProjection onto principal components, \\(\\mathbf{z}\\): z = pca.transform(y).\nReconstructed data, \\(\\hat{\\mathbf{y}}\\): y_hat = pca.inverse_transform(z).\n\nAs with the analysis we did by hand above, we have to center and scale (or uncenter and unscale if we are computing reconstructed data).",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Implementation of PCA</span>"
    ]
  },
  {
    "objectID": "lessons/models/pca/pca_implementation.html#sec-center-scale-sklearn",
    "href": "lessons/models/pca/pca_implementation.html#sec-center-scale-sklearn",
    "title": "47  Implementation of PCA",
    "section": "47.6 Centering and scaling with scikit-learn",
    "text": "47.6 Centering and scaling with scikit-learn\nAs a final note, I’ll mention that scikit-learn offers automated centering and scaling. A StandardScaler instance is instantiated and “fit” based on the data set.\n\nscaler = sklearn.preprocessing.StandardScaler().fit(neural_data)\n\nWith the scaler in place, the data may be scaled and unscaled.\n\ny = scaler.transform(neural_data)\noriginal_data = scaler.inverse_transform(y)\n\nWe can automate the PCA process to get reconstructed data as follows. (We will silence warnings just for aesthetic purposes of these notes. In general, it is a bad idea to silence warnings in your workflow unless you really know what you are doing.)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    \n    scaler = sklearn.preprocessing.StandardScaler().fit(neural_data)\n    y = scaler.transform(neural_data)\n    pca = sklearn.decomposition.PCA(n_components=2).fit(y)\n    z = pca.transform(y)\n\n    reconstructed_neural_data = scaler.inverse_transform(pca.inverse_transform(z))",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Implementation of PCA</span>"
    ]
  },
  {
    "objectID": "lessons/models/pca/pca_implementation.html#computing-environment",
    "href": "lessons/models/pca/pca_implementation.html#computing-environment",
    "title": "47  Implementation of PCA",
    "section": "47.7 Computing environment",
    "text": "47.7 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,sklearn,bokeh,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\nscipy     : 1.16.2\nsklearn   : 1.7.2\nbokeh     : 3.8.0\nbebi103   : 0.1.28\njupyterlab: 4.4.9",
    "crumbs": [
      "Oft encountered models",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Implementation of PCA</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/intro_model_assessment.html",
    "href": "lessons/model_assessment/intro_model_assessment.html",
    "title": "Model assessment",
    "section": "",
    "text": "Thus far, we have not been terribly principled in how we have assessed models. By model assessment, I mean some metric, quantitative or qualitative, or how well matched the proposed generative model and the actual (unknown) generative model are. In my opinion, many of the most effective methods are graphical, meaning that we make plots of data generated from the theoretical generative model along with the measured data. We will nonetheless also explore information criteria for model selection.",
    "crumbs": [
      "Model assessment"
    ]
  },
  {
    "objectID": "lessons/model_assessment/graphical_model_assessment.html",
    "href": "lessons/model_assessment/graphical_model_assessment.html",
    "title": "48  Graphical model assessment",
    "section": "",
    "text": "48.1 Overlaying the theoretical CDF\nFor this lesson, we will consider two method of graphical model assessment for repeated measurements, Q-Q plots and predictive ECDFs. We will also briefly revisit what we have done so far, the less-effective and soon-to-be-jettisoned method or plotting the CDF of the generative model parametrized by the MLE along with the ECDF. I note that we will not go over the implementation of these in Python here, but will rather cover the procedure and look at example plots. We will cover how to implement these methods in Python in the next lesson.\nTo have data sets in mind, we will revisit the data set from when we learned about the effects of neonicotinoid pesticides on bee sperm counts (Straub, et al., 2016). We considered the quantity of alive sperm in drone bees treated with pesticide and those that were not.\nBefore seeing the data, we may think that the number of alive sperm in untreated (control) drones would be Normally distributed. That is, defining \\(y\\) as the number of alive sperm in millions,\n\\[\\begin{align}\ny \\sim \\mathrm{Norm}(\\mu, \\sigma).\n\\end{align}\\]\nWe already derived that the maximum likelihood estimate for the parameters \\(\\mu\\) and \\(\\sigma\\) of a Normal distribution are the plug-in estimates. Calculating these using np.mean() and np.std() yields \\(\\mu^* = 1.87\\) million and \\(\\sigma^* = 1.23\\) million.\nThus far, when we needed to make a quick sanity check to see if data are distributed as we suspect, we have generated the ECDF and overlaid the theoretical CDF. I do this here for reference.\nThis plot does highlight the fact that the sperm counts seem to be Normally distributed for intermediate to high sperm counts, but strongly deviate from Normality for low counts. It suggests that the bees with low sperm counts are somehow abnormal (no pun intended).\nThough not useless, there are better options for graphical model assessment. At the heart of these methods is the idea that we should not compare a theoretical description of the model to the data, but rather we should compare data generated by the model to the observed data.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Graphical model assessment</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/graphical_model_assessment.html#overlaying-the-theoretical-cdf",
    "href": "lessons/model_assessment/graphical_model_assessment.html#overlaying-the-theoretical-cdf",
    "title": "48  Graphical model assessment",
    "section": "",
    "text": "Bokeh Plot",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Graphical model assessment</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/graphical_model_assessment.html#q-q-plots",
    "href": "lessons/model_assessment/graphical_model_assessment.html#q-q-plots",
    "title": "48  Graphical model assessment",
    "section": "48.2 Q-Q plots",
    "text": "48.2 Q-Q plots\nQ-Q plots (the “Q” stands for “quantile”) are convenient ways to graphically compare two probability distributions. The variant of a Q-Q plot we discuss here compares data generated by the model generative distribution parametrized by the MLE and the empirical distribution (defined entirely by the measured data).\nThere are many ways to generate Q-Q plots, and many of the descriptions out there are kind of convoluted. Here is a procedure/description I like for \\(N\\) total empirical measurements.\n\nSort your measurements from lowest to highest.\nDraw \\(N\\) samples from the generative distribution and sort them. This constitutes a sorted parametric bootstrap sample.\nPlot your samples against the samples from the theoretical distribution.\n\nIf the plotted points fall on a straight line of slope one and intercept zero (“the diagonal”), the distributions are similar. Deviations from the diagonal highlight differences in the distributions.\nI actually like to generate many many samples from the theoretical distribution and then plot the 95% confidence region of the Q-Q plot. This plot gives a feel of how plausible it is that the observed data were drawn out of the theoretical distribution. Below is the Q-Q plot for the control bee sperm data using a Normal model parametrized by \\(\\mu^*\\) and \\(\\sigma^*\\). In making the plot, I use a modified generative model: and negative sperm count drawn out of the Normal distribution is set to zero.\n\n\n\n\n\nBokeh Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe envelope containing 95% of the Q-Q lines is fairly thick, and closely matches the normal distribution in the mid-range sperm counts. We do see the deviation from Normality at the low counts, but it is not as striking as we might have through from our previous plot overlaying the theoretical CDF.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Graphical model assessment</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/graphical_model_assessment.html#predictive-ecdfs",
    "href": "lessons/model_assessment/graphical_model_assessment.html#predictive-ecdfs",
    "title": "48  Graphical model assessment",
    "section": "48.3 Predictive ECDFs",
    "text": "48.3 Predictive ECDFs\nWhile Q-Q plots are widely used, I think it is more intuitive to directly compare a ECDFs of data made from the generative model to the ECDF of the measured data. The procedure for generating this kind of plot is similar to that of Q-Q plots. It involves acquiring parametric bootstrap replicates of the ECDF, and then plotting confidence intervals of the resulting ECDFs. I call ECDFs that come from the generative model predictive ECDFs. Below is a predictive ECDF for the bee sperm counts.\n\n\n\n\n\nBokeh Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this representation, the dark blue line in the middle is the median of all of the parametric bootstrap replicates of the ECDF. The darker shaded blue region is the 68% confidence interval, and the light blue region is the 95% confidence interval. Again, we see deviation at low sperm counts, but good agreement with the model otherwise.\nSometimes it is difficult to resolve where the confidence intervals of the ECDF and the measured ECDF diverge. This is especially true with a large number of observations. To clarify, we can instead plot the difference between the predictive and measured ECDFs.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Graphical model assessment</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/graphical_model_assessment.html#predictive-regressions",
    "href": "lessons/model_assessment/graphical_model_assessment.html#predictive-regressions",
    "title": "48  Graphical model assessment",
    "section": "48.4 Predictive regressions",
    "text": "48.4 Predictive regressions\nPredictive ECDFs makes an apples-to-apples comparison of the data that could be generated by a model and the data that were observed. We would like to be able to do the same for variate-covariate models. The approach is the same; we use the generative model parametrized by the MLE to make data sets for the variate for given values of the covariate. We then plot percentiles of these data sets along with the observed data. Such a plot is shown below for the now very familiar spindle-size data set from Good, et al.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Graphical model assessment</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/graphical_model_assessment.html#general-graphical-model-assessment",
    "href": "lessons/model_assessment/graphical_model_assessment.html#general-graphical-model-assessment",
    "title": "48  Graphical model assessment",
    "section": "48.5 General graphical model assessment",
    "text": "48.5 General graphical model assessment\nThe examples of predictive ECDFs and predictive regression plots we have just seen are examples of graphical model assessment. The idea in both plots is to compare data that can be generated by the generative model and the data that were observed. Different kinds of measurements and different kinds of models may not lend themselves to the two example plots we have made. This does not mean you cannot do graphical model comparison! You can be creative in displays that allow comparison of generated and observed data, and many measurement/model pairs require bespoke visualizations for this purpose.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Graphical model assessment</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/graphical_model_assessment.html#graphical-model-comparison-vs.-nhst",
    "href": "lessons/model_assessment/graphical_model_assessment.html#graphical-model-comparison-vs.-nhst",
    "title": "48  Graphical model assessment",
    "section": "48.6 Graphical model comparison vs. NHST",
    "text": "48.6 Graphical model comparison vs. NHST\nNull hypothesis significance testing seeks to check if the data produced in the experiment are commensurate with those that could be produced from a generative model. The approach is to do that through a single scalar test statistic. The graphical methods of model assessment shown here also check to see if the data produced in the experiment are commensurate with those that could be produced from the generative model, but we check it against the entire data set, and not just a single test statistic. Importantly, graphical model assessment does not tell us how probable it is that the generative model is correct. What it does tell us is if the generative model could have produced the data we observed.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Graphical model assessment</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/information_criteria.html",
    "href": "lessons/model_assessment/information_criteria.html",
    "title": "49  Model comparison with the Akaike information criterion",
    "section": "",
    "text": "49.1 The Akaike information criterion\nWe have seen that we can assess models graphically. There are many non-graphical ways to assess models, including likelihood-ratio tests and cross-validation. Both of these are involved topics (especially cross-validation; there is a lot to learn there), and we will not cover them in much depth here.\nWe will go into much more depth on model selection in the sequel of this course.\nFor now, we will consider a metric for model selection called the Akaike information criterion, or AIC. We will not show it, but the AIC can be loosely interpreted as an estimate of a quantity related to the distance between the true generative distribution and the model distribution. (I am speaking loosely here, there are important details missing, and in fact I have abused some language.) Given two models, the one with the lesser AIC is likely closer to the true generative distribution.\nFor a set of parameters \\(\\theta\\) with MLE \\(\\theta^*\\) and a model with log-likelihood \\(\\ell(\\theta;\\text{data})\\), the AIC is given by\n\\[\\begin{align}\n\\text{AIC} = -2\\ell(\\theta^*;\\text{data}) + 2p,\n\\end{align}\\]\nwhere \\(p\\) is the number of free parameters in a model. Thus, more complicated models, those with more parameters, are penalized for having too many parameters. Furthermore, models where the likelihood function is broad, meaning that many parameter values are viable, have a lower log likelihood and are also penalized. So, models with few, meaningful parameters and a high likelihood function at the MLE have lower AIC and are favored.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Model comparison with the Akaike information criterion</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/information_criteria.html#akaike-weights",
    "href": "lessons/model_assessment/information_criteria.html#akaike-weights",
    "title": "49  Model comparison with the Akaike information criterion",
    "section": "49.2 Akaike weights",
    "text": "49.2 Akaike weights\nIf we compare two models, say model 1 and model 2, then the Akaike weight for model 1 is\n\\[\\begin{align}\nw_1 = \\frac{\\mathrm{e}^{-\\text{AIC}_1/2}}{\\mathrm{e}^{-\\text{AIC}_1/2} +     \\mathrm{e}^{-\\text{AIC}_2/2}}.\n\\end{align}\\]\nBecause the values of the AIC can be large, it is more numerically stable to compute the Akaike weight as\n\\[\\begin{align}\nw_1 = \\frac{\\mathrm{e}^{-(\\text{AIC}_1 -     \\text{AIC}_\\mathrm{max})/2}}{\\mathrm{e}^{-(\\text{AIC}_1 -     \\text{AIC}_\\mathrm{max})/2}+\\mathrm{e}^{-(\\text{AIC}_2 - \\text{AIC}_\\mathrm{max})/2}},\n\\end{align}\\]\nwhere \\(\\text{AIC}_\\mathrm{max}\\) is the maximum of the two AICs. More generally, the Akaike weight of model \\(i\\) among many is\n\\[\\begin{align}\nw_i = \\frac{\\mathrm{e}^{-(\\text{AIC}_i -     \\text{AIC}_\\mathrm{max})/2}}{\\sum_j\\mathrm{e}^{-(\\text{AIC}_j -     \\text{AIC}_\\mathrm{max})/2}}.\n\\end{align}\\]\nInterpretation of the Akaike weights is lacking a consensus, but we can loosely interpret the Akaike weight as the probability that a model will give the best predictions of new data, assuming that we have considered all possible models.\nImportantly, computing the AIC and the Akaike weights is trivial once the MLE calculation is complete. You simply evaluate the log-likelihood at the MLE (which you had to do anyway to perform the optimization to find the MLE), multiply by negative two, and then add twice the number of parameters to it!",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Model comparison with the Akaike information criterion</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/implementation_of_graphical_model_selection.html",
    "href": "lessons/model_assessment/implementation_of_graphical_model_selection.html",
    "title": "50  Graphical model assessment for univariate models",
    "section": "",
    "text": "50.1 Graphical model assessment\n| Download notebook\nDataset download\nWe have already discussed model assessment. In this lesson, we will discuss implementation of graphical model assessment.\nWe will use the transcript counts from Singer, et al., this time focusing on the Rex1 gene, which shows clear biomodality. As a reminder, here is the ECDF for the transcript counts.\ndf = pl.read_csv(os.path.join(data_path, \"singer_transcript_counts.csv\"), comment_prefix=\"#\")\n\np = iqplot.ecdf(\n    data=df, q=\"Rex1\", x_axis_label=\"Rex1 mRNA count\", conf_int=True\n)\n\nbokeh.io.show(p)\nBefore we can proceed to the model assessments, we need to write functions to calculate the MLE.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Graphical model assessment for univariate models</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/implementation_of_graphical_model_selection.html#likelihood-calculators",
    "href": "lessons/model_assessment/implementation_of_graphical_model_selection.html#likelihood-calculators",
    "title": "50  Graphical model assessment for univariate models",
    "section": "50.2 Likelihood calculators",
    "text": "50.2 Likelihood calculators\nIn this and the following parts of this lesson, we will need the functions we have written to perform maximum likelihood estimation for a Negative Binomial distribution and for a mixture of two Negative Binomials. These are copied directly from the Lesson 41, so you do not need to focus on this rather long code cell.\n\ndef log_like_iid_nbinom(params, n):\n    \"\"\"Log likelihood for i.i.d. NBinom measurements, parametrized\n    by alpha, b=1/beta.\"\"\"\n    alpha, b = params\n\n    if alpha &lt;= 0 or b &lt;= 0:\n        return -np.inf\n\n    return np.sum(st.nbinom.logpmf(n, alpha, 1 / (1 + b)))\n\n\ndef mle_iid_nbinom(n):\n    \"\"\"Perform maximum likelihood estimates for parameters for i.i.d.\n    NBinom measurements, parametrized by alpha, b=1/beta\"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        res = scipy.optimize.minimize(\n            fun=lambda params, n: -log_like_iid_nbinom(params, n),\n            x0=np.array([3, 3]),\n            args=(n,),\n            method=\"Powell\",\n        )\n\n    if res.success:\n        return res.x\n    else:\n        raise RuntimeError(\"Convergence failed with message\", res.message)\n\n\ndef initial_guess_mix(n, w_guess):\n    \"\"\"Generate initial guess for mixture model.\"\"\"\n    n_low = n[n &lt; np.percentile(n, 100 * w_guess)]\n    n_high = n[n &gt;= np.percentile(n, 100 * w_guess)]\n\n    alpha1, b1 = mle_iid_nbinom(n_low)\n    alpha2, b2 = mle_iid_nbinom(n_high)\n\n    return alpha1, b1, alpha2, b2\n\n\ndef log_like_mix(alpha1, b1, alpha2, b2, w, n):\n    \"\"\"Log-likeihood of binary Negative Binomial mixture model.\"\"\"\n    # Fix nonidentifieability be enforcing values of w\n    if w &lt; 0 or w &gt; 1:\n        return -np.inf\n\n    # Physical bounds on parameters\n    if alpha1 &lt; 0 or alpha2 &lt; 0 or b1 &lt; 0 or b2 &lt; 0:\n        return -np.inf\n\n    logx1 = st.nbinom.logpmf(n, alpha1, 1 / (1 + b1))\n    logx2 = st.nbinom.logpmf(n, alpha2, 1 / (1 + b2))\n\n    # Multipliers for log-sum-exp\n    lse_coeffs = np.tile([w, 1 - w], [len(n), 1]).transpose()\n\n    # log-likelihood for each measurement\n    log_likes = scipy.special.logsumexp(np.vstack([logx1, logx2]), axis=0, b=lse_coeffs)\n\n    return np.sum(log_likes)\n\n\ndef mle_mix(n, w_guess):\n    \"\"\"Obtain MLE estimate for parameters for binary mixture \n    of Negative Binomials.\"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        res = scipy.optimize.minimize(\n            fun=lambda params, n: -log_like_mix(*params, n),\n            x0=[*initial_guess_mix(n, w_guess), w_guess],\n            args=(n,),\n            method=\"Powell\",\n            tol=1e-6,\n        )\n\n    if res.success:\n        return res.x\n    else:\n        raise RuntimeError(\"Convergence failed with message\", res.message)\n\nWe can now carry out the MLE for both models.\n\n# Load in data\nn = df['Rex1'].to_numpy()\n\n# Single Negative Binomial MLE\nalpha, b = mle_iid_nbinom(n)\n\n# Mixture model MLE\nalpha1, b1, alpha2, b2, w = mle_mix(n, 0.2)",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Graphical model assessment for univariate models</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/implementation_of_graphical_model_selection.html#constructing-a-q-q-plot",
    "href": "lessons/model_assessment/implementation_of_graphical_model_selection.html#constructing-a-q-q-plot",
    "title": "50  Graphical model assessment for univariate models",
    "section": "50.3 Constructing a Q-Q plot",
    "text": "50.3 Constructing a Q-Q plot\nWe will start by making a Q-Q plot for the single Negative-Binomial case. The function bebi103.viz.qqplot() generates Q-Q plots. In order to provide the function with samples out of the generative distribution, let’s draw out of a single Negative Binomial with our \\(\\alpha\\), \\(b\\) parametrization.\n\nrng = np.random.default_rng()\n\nsingle_samples = np.array(\n    [rng.negative_binomial(alpha, 1 / (1 + b), size=len(n)) for _ in range(100000)]\n)\n\nWith that in place, we can call bebi103.viz.qqplot() to generate the plot.\n\np = bebi103.viz.qqplot(\n    data=n,\n    samples=single_samples,\n    x_axis_label=\"transcript counts\",\n    y_axis_label=\"transcript counts\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nClearly, the generative model cannot produce the observed transcript bounds, since the Q-Q plot shows strong separation of the generative quantiles from the observed quantiles.\n\n50.3.1 Q-Q plot for the mixture model\nWe can also make a Q-Q plot for the mixture model. We need to write a function to generate data out of the mixture model. As we did in a previous lesson, we first randomly choose a cell type with weight \\(w\\), and then draw out of a Negative Binomial distribution with parameters corresponding to that cell type.\n\ndef draw_mix(alpha1, b1, alpha2, b2, w, size):\n    \"\"\"Draw samples from Negative Binomial binary mixture model.\"\"\"\n    n = np.empty(size)\n    for i in range(size):\n        low_cell_type = rng.random() &lt; w\n\n        if low_cell_type:\n            n[i] = rng.negative_binomial(alpha1, 1 / (1 + b1))\n        else:\n            n[i] = rng.negative_binomial(alpha2, 1 / (1 + b2))\n\n    return n\n\n\nmixture_samples = np.array(\n    [draw_mix(alpha1, b1, alpha2, b2, w, size=len(n)) for _ in range(100_000)]\n)\n\nNow, we can make the Q-Q plot!\n\np = bebi103.viz.qqplot(\n    data=n,\n    samples=mixture_samples,\n    x_axis_label=\"transcript counts\",\n    y_axis_label=\"transcript counts\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe Q-Q plot shows a much stronger agreement between the theoretical distribution and the observed.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Graphical model assessment for univariate models</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/implementation_of_graphical_model_selection.html#predictive-ecdfs",
    "href": "lessons/model_assessment/implementation_of_graphical_model_selection.html#predictive-ecdfs",
    "title": "50  Graphical model assessment for univariate models",
    "section": "50.4 Predictive ECDFs",
    "text": "50.4 Predictive ECDFs\nIf we wish to accept a model with the MLE for the parameters as a good representation of the true generative model, we would like to see how data generated from that model would look. We can do a parametric bootstrap to generate data sets and then plot confidence intervals of the resulting ECDFs. We call ECDFs that come from the generative model predictive ECDFs. We can try that for the single Negative Binomial model.\nGiven the samples (which we already computed and stored in single_samples), we can then compute the predictive ECDFs for each of these samples. We want the values of the predictive ECDF at each possible value of \\(n\\), which requires us to compute the value of the ECDF at an arbitrary value. The function below accomplishes this.\n\n@numba.njit\ndef ecdf(x, data):\n    \"\"\"Give the value of an ECDF at arbitrary points x.\"\"\"\n    y = np.arange(len(data) + 1) / len(data)\n    return y[np.searchsorted(np.sort(data), x, side=\"right\")]\n\nNow we can compute the value of the ECDF for the respective values of \\(n\\).\n\nn_theor = np.arange(0, single_samples.max() + 1)\n\necdfs = np.array([ecdf(n_theor, sample) for sample in single_samples])\n\nNow that we have the ECDF values, we can compute a 95th-percentile of the value of the ECDF.\n\necdf_low, ecdf_high = np.percentile(ecdfs, [2.5, 97.5], axis=0)\n\nAnd now, we can make a plot.\n\np = bebi103.viz.fill_between(\n    x1=n_theor,\n    y1=ecdf_high,\n    x2=n_theor,\n    y2=ecdf_low,\n    patch_kwargs={\"fill_alpha\": 0.5},\n    x_axis_label=\"n\",\n    y_axis_label=\"ECDF\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis is a predictive ECDF. Is is the envelope in which we would expect 95% of the data points to lie if the generative model were true.\n\n50.4.1 Comparing with data\nWe can overlay the ECDF of the data to see if it falls within this envelope.\n\np = iqplot.ecdf(data=df, q=\"Rex1\", palette=[\"orange\"], p=p)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe Negative Binomial model generated some counts that were quite large, which is why the plot extends past counts of 1000. If you zoom in on the region where the data lie, the deviation of the measured ECDF from the ECDF generated by the model is clear.\nWe can also compare with the mixture model.\n\n\n50.4.2 Using predictive ECDFs in the bebi103 package\nThe bebi103 package enables automated generation of predictive ECDFs. To demonstrate its usage, we can generate samples from the mixture model and plot a predictive ECDF.\n\n# Use discrete=True for discrete data\np = bebi103.viz.predictive_ecdf(\n    samples=mixture_samples, data=n, discrete=True, x_axis_label=\"n\"\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nHere we see that the predictive ECDF captures the measured data much better. If you zoom into the plot, you will notice that the bebi103.viz.predictive_ecdf() function plots quantiles in various shades of blue. By default, the outermost region (lightest blue) contains the middle 95% of the generated ECDFs, and the next one in captures the middle 68%. The median ECDF from the generative model is a dark blue line in the middle.\n\n\n50.4.3 ECDF difference\nWhile the predictive ECDF plot is useful, it is perhaps more illuminating to plot the difference between the predictive ECDF and the measured ECDF. We can do this with the diff='ecdf' kwarg.\n\np = bebi103.viz.predictive_ecdf(\n    samples=mixture_samples, data=n, diff=\"ecdf\", discrete=True, x_axis_label=\"n\"\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nIt is now clear that very few of the 279 data points lie outside the 95% envelope of the predictive ECDF. We might expect more to lie outside; it is conceivable that the model is too flexible.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Graphical model assessment for univariate models</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/implementation_of_graphical_model_selection.html#computing-environment",
    "href": "lessons/model_assessment/implementation_of_graphical_model_selection.html#computing-environment",
    "title": "50  Graphical model assessment for univariate models",
    "section": "50.5 Computing environment",
    "text": "50.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,scipy,bokeh,iqplot,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\npolars    : 1.31.0\nscipy     : 1.16.0\nbokeh     : 3.7.3\niqplot    : 0.3.7\nbebi103   : 0.1.28\njupyterlab: 4.4.5",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Graphical model assessment for univariate models</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/predictive_regression.html",
    "href": "lessons/model_assessment/predictive_regression.html",
    "title": "51  Graphical model assessment: Predictive regression",
    "section": "",
    "text": "51.1 The data set and models\n| Download notebook\nDataset download\nWe have just seen how a predictive ECDF can be used to graphically assess a model for the case of repeated measurements drawn out of a generative distribution. Now, we turn to a model that involves a regression.\nWe again use the data set from Good and coworkers that we used in our lessons on model building and variate-covariate modeling. In the experiment, Good and coworkers measured the length of mitotic spindles in droplets of varying diameter.\nLet’s take a quick look at the data.\ndf = pl.read_csv(os.path.join(data_path, \"good_invitro_droplet_data.csv\"), comment_prefix=\"#\")\n\np = bokeh.plotting.figure(\n    frame_height=300,\n    frame_width=400,\n    x_axis_label=\"droplet diameter (µm)\",\n    y_axis_label=\"spindle length (µm)\",\n    x_range=[0, 250],\n    y_range=[0, 50],\n)\n\np.scatter(\n    source=df.to_dict(),\n    x=\"Droplet Diameter (um)\",\n    y=\"Spindle Length (um)\",\n    alpha=0.3,\n)\n\nbokeh.io.show(p)\nGood and coworkers proposed two models for the relationship between spindle length and droplet diameter. In the first model, the spindle length is independent of the droplet diameter and drawn out of a Normal distribution.\n\\[\\begin{align}\nl_i \\sim \\text{Norm}(\\phi, \\sigma)\\;\\forall i.\n\\end{align}\\]\nIn the second model, the spindle length depends on the droplet diameter according to the equation below, and the spindle lengths vary from the theoretical model in a Normal fashion.\n\\[\\begin{align}\n&\\mu_i = \\frac{\\gamma d_i}{\\left(1 + (\\gamma d_i/\\phi)^3\\right)^{1/3}}\\\\[1em]\n&l_i \\sim \\text{Norm}(\\mu_i, \\sigma)\\;\\forall i.\n\\end{align}\\]\nThese are the two models we will graphically assess, starting with the second model, which establishes a relationship between spindle length and droplet diameter.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Graphical model assessment: Predictive regression</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/predictive_regression.html#assessing-the-model-for-spindle-length-dependent-on-droplet-diameter",
    "href": "lessons/model_assessment/predictive_regression.html#assessing-the-model-for-spindle-length-dependent-on-droplet-diameter",
    "title": "51  Graphical model assessment: Predictive regression",
    "section": "51.2 Assessing the model for spindle length dependent on droplet diameter",
    "text": "51.2 Assessing the model for spindle length dependent on droplet diameter\nAs was the case with our studies of the Singer, et al. data on mRNA counts, we do also need functions from previous lessons here to compute the MLE. You can skip this, since it is copied directly from previous lessons. The important result is that we get the MLE for the three parameters, \\(\\gamma\\), \\(\\phi\\), and \\(\\sigma\\).\n\ndef theor_spindle_length(gamma, phi, d):\n    \"\"\"Compute spindle length using mathematical model\"\"\"\n    return gamma * d / np.cbrt(1 + (gamma * d / phi)**3)\n\n\ndef log_likelihood(params, d, ell):\n    \"\"\"Log likelihood of spindle length model.\"\"\"\n    gamma, phi, sigma = params\n\n    if gamma &lt;= 0 or gamma &gt; 1 or phi &lt;= 0:\n        return -np.inf\n\n    mu = theor_spindle_length(gamma, phi, d)\n    return np.sum(st.norm.logpdf(ell, mu, sigma))\n\n\ndef spindle_mle(d, ell):\n    \"\"\"Compute MLE for parameters in spindle length model.\"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        res = scipy.optimize.minimize(\n            fun=lambda params, d, ell: -log_likelihood(params, d, ell),\n            x0=np.array([0.5, 35, 5]),\n            args=(d, ell),\n            method='Powell'\n        )\n\n    if res.success:\n        return res.x\n    else:\n        raise RuntimeError('Convergence failed with message', res.message)\n        \n\nmle_params = spindle_mle(\n    df['Droplet Diameter (um)'].to_numpy(), df['Spindle Length (um)'].to_numpy()\n)\n\n# Take a look\nprint(\"γ* = {0:f}\\nφ* = {1:f}\\nσ* = {2:f}\".format(*mle_params))\n\nγ* = 0.860475\nφ* = 38.231250\nσ* = 3.753422\n\n\n\n51.2.1 Sampling out of the generative model\nAs before, we want to see what kind of data sets are predicted by the generative model when parametrized by the MLEs of the parameters. We therefore need to write a function to obtain the samples. The extra wrinkle here is that we need to also provide values for the droplet diameter for which we want the samples. In this model, we assume that the droplet diameters are measured exactly and that they determine the spindle length (of course neglecting the variation we model in the residuals).\n\nrng = np.random.default_rng()\n\ndef sample_spindle(gamma, phi, sigma, d, size=1):\n    \"\"\"Generate samples of spindle length vs droplet diameter.\"\"\"\n    samples = np.empty((size, len(d)))\n\n    for i in range(size):\n        mu = theor_spindle_length(gamma, phi, d)\n        samples[i] = np.maximum(0, rng.normal(mu, sigma))\n\n    return samples\n\nNote that this function returns a array of shape (size, len(d)). That is, each row of the outputted array of samples corresponds to one set of measurements for prescribed droplet diameters d.\nWe will generate samples for the droplet diameters going from zero to 250 microns.\n\nd_theor = np.linspace(0, 250, 200)\nsamples = sample_spindle(*mle_params, d_theor, size=10000)\n\n\n\n51.2.2 Making the predictive regression plot\nNow that we have our samples, we compute the percentiles of the spindle length for each value of \\(d\\). We then generate the plot from these percentiles, with the data overlaid. The bebi103.viz.predictive_regression() function accomplishes this for us. We need to provide it with the samples of spindle length (with the shape we have already prescribed), as well as the \\(d\\)-values for which the samples were generated. We also need to provide the data as an array with two columns, the first being the diameter and the second the spindle length.\n(Note that we use the term “regression” here, as is commonly done for variate-covariate modeling, even though it is in many ways a misnomer.)\n\np = bebi103.viz.predictive_regression(\n    samples=samples,\n    samples_x=d_theor,\n    data=df[['Droplet Diameter (um)', 'Spindle Length (um)']].to_numpy(),\n    x_axis_label='droplet diameter (µm)',\n    y_axis_label='spindle length (µm)',\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nIn this plot, we see that roughly 5% of the data points (there are 670 total points) lie outside the middle 95%, so the model is consistent with the measured data.\nIn my view, this is how variate-covariate modeling results should be plotted, not simple with a “best fit line”, which does not reveal the full generative model.\n\n\n51.2.3 Viewing differences\nWe may also wish to get a plot of how different the data are from the model. In this case, the median spindle length is subtracted off of all samples such that the median is zero. We also need to subtract it from the data to make a comparison. That means that we have to draw our samples from the generative distribution for values of the droplet diameter that were actually measured.\n\nsamples = sample_spindle(*mle_params, df['Droplet Diameter (um)'].to_numpy(), size=10000)\n\np = bebi103.viz.predictive_regression(\n    samples=samples,\n    samples_x=df['Droplet Diameter (um)'].to_numpy(),\n    data=df[['Droplet Diameter (um)', 'Spindle Length (um)']].to_numpy(),\n    diff=True,\n    x_axis_label='droplet diameter (µm)',\n    y_axis_label='diff. spindle length (µm)',\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nImportantly, there does not seem to be any systematic way in which the measurements deviate from the model. In other words, whether a data point lies outside the 80% envelope and whether it lies above or below looks independent of droplet diameter.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Graphical model assessment: Predictive regression</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/predictive_regression.html#graphical-assessment-of-the-normal-model",
    "href": "lessons/model_assessment/predictive_regression.html#graphical-assessment-of-the-normal-model",
    "title": "51  Graphical model assessment: Predictive regression",
    "section": "51.3 Graphical assessment of the Normal model",
    "text": "51.3 Graphical assessment of the Normal model\nAs we have learned, the MLE for the parameters for the Normal model are the same as the plug-in estimates.\n\nphi = df['Spindle Length (um)'].mean()\nsigma = df['Spindle Length (um)'].std()\n\nWe can then draw our samples directly out of a Normal distribution with location parameter \\(\\phi^*\\) and scale parameter \\(\\sigma^*\\).\n\nsamples = rng.normal(phi, sigma, size=(5000, len(df)))\n\nGiven the samples, we can again make a predictive regression plot.\n\np = bebi103.viz.predictive_regression(\n    samples=samples,\n    samples_x=df['Droplet Diameter (um)'].to_numpy(),\n    percentiles=[68, 80, 95],\n    data=df[['Droplet Diameter (um)', 'Spindle Length (um)']].to_numpy(),\n    x_axis_label='droplet diameter (µm)',\n    y_axis_label='spindle length (µm)',\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nHere, I have also included the 80% envelope. Again, only about 5% of the data points lie outside the 95% envelope. However, the data points tend to fall below the 80% envelope only for small droplet diameters and above only for large droplet diameters, suggesting that the model is missing the droplet-diameter dependence.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Graphical model assessment: Predictive regression</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/predictive_regression.html#general-method-of-graphical-predictive-model-assessment",
    "href": "lessons/model_assessment/predictive_regression.html#general-method-of-graphical-predictive-model-assessment",
    "title": "51  Graphical model assessment: Predictive regression",
    "section": "51.4 General method of graphical predictive model assessment",
    "text": "51.4 General method of graphical predictive model assessment\nWhen using both predictive ECDFs and predictive regression curves, we did the same procedure, which you can imagine generalizing.\n\nCompute the MLE of the parameters for the generative model in question.\nUse the MLE to parametrize the model and generate many data sets out of the model.\nMake a plot showing percentile regions of the generated data sets.\nOverlay the measured data set.\nEvaluate how many of the measured data points lie outside the percentile regions of the generated data sets.\n\nPredictive ECDFs and regression curves are just two (often used) examples of this prescription, and you can develop your own as is useful for the model and data set you are working with.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Graphical model assessment: Predictive regression</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/predictive_regression.html#computing-environment",
    "href": "lessons/model_assessment/predictive_regression.html#computing-environment",
    "title": "51  Graphical model assessment: Predictive regression",
    "section": "51.5 Computing environment",
    "text": "51.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,scipy,bokeh,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\npolars    : 1.31.0\nscipy     : 1.16.0\nbokeh     : 3.7.3\nbebi103   : 0.1.28\njupyterlab: 4.4.5",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Graphical model assessment: Predictive regression</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/aic.html",
    "href": "lessons/model_assessment/aic.html",
    "title": "52  Model comparison with the AIC",
    "section": "",
    "text": "52.1 AIC for mRNA counts\n| Download notebook\nmRNA count dataset download\nSpindle length dataset download\nWe have previously introduced the Akaike information criterion. Here, we will demonstrate its use in model comparison and the mechanics of how to calculated it.\nAs a reminder, for a set of parameters \\(\\theta\\) with MLE \\(\\theta^*\\) and a model with log-likelihood \\(\\ell(\\theta;\\text{data})\\), the AIC is given by\n\\[\\begin{align}\n\\text{AIC} = -2\\ell(\\theta^*;\\text{data}) + 2p,\n\\end{align}\\]\nwhere \\(p\\) is the number of free parameters in a model. The Akaike weight of model \\(i\\) in a collection of models is\n\\[\\begin{align}\nw_i = \\frac{\\mathrm{e}^{-(\\text{AIC}_i - \\text{AIC}_\\mathrm{max})/2}}{\\sum_j\\mathrm{e}^{-(\\text{AIC}_j - \\text{AIC}_\\mathrm{max})/2}}.\n\\end{align}\\]\nTo begin, we will use the AIC to compare a single Negative Binomial model to a mixture of two Negative Binomials for smFISH data from Singer, et al.\nLet us now compare the single Negative Binomial to the mixture model for mRNA counts. We again need our functions for computing the MLE and computing the log-likelihood from previous lessons.\ndef log_like_iid_nbinom(params, n):\n    \"\"\"Log likelihood for i.i.d. NBinom measurements, parametrized\n    by alpha, b=1/beta.\"\"\"\n    alpha, b = params\n\n    if alpha &lt;= 0 or b &lt;= 0:\n        return -np.inf\n\n    return np.sum(st.nbinom.logpmf(n, alpha, 1/(1+b)))\n\n\ndef mle_iid_nbinom(n):\n    \"\"\"Perform maximum likelihood estimates for parameters for i.i.d.\n    NBinom measurements, parametrized by alpha, b=1/beta\"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        res = scipy.optimize.minimize(\n            fun=lambda params, n: -log_like_iid_nbinom(params, n),\n            x0=np.array([3, 3]),\n            args=(n,),\n            method='Powell'\n        )\n\n    if res.success:\n        return res.x\n    else:\n        raise RuntimeError('Convergence failed with message', res.message)\n        \n\ndef initial_guess_mix(n, w_guess):\n    \"\"\"Generate initial guess for mixture model.\"\"\"\n    n_low = n[n &lt; np.percentile(n, 100*w_guess)]\n    n_high = n[n &gt;= np.percentile(n, 100*w_guess)]\n    \n    alpha1, b1 = mle_iid_nbinom(n_low)\n    alpha2, b2 = mle_iid_nbinom(n_high)\n    \n    return alpha1, b1, alpha2, b2\n\n\ndef log_like_mix(alpha1, b1, alpha2, b2, w, n):\n    \"\"\"Log-likeihood of binary Negative Binomial mixture model.\"\"\"\n    # Fix nonidentifiability be enforcing values of w\n    if w &lt; 0 or w &gt; 1:\n        return -np.inf\n    \n    # Physical bounds on parameters\n    if alpha1 &lt; 0 or alpha2 &lt; 0 or b1 &lt; 0 or b2 &lt; 0:\n        return -np.inf\n\n    logx1 = st.nbinom.logpmf(n, alpha1, 1/(1+b1))\n    logx2 = st.nbinom.logpmf(n, alpha2, 1/(1+b2))\n\n    # Multipliers for log-sum-exp\n    lse_coeffs = np.tile([w, 1-w], [len(n), 1]).transpose()\n\n    # log-likelihood for each measurement\n    log_likes = scipy.special.logsumexp(np.vstack([logx1, logx2]), axis=0, b=lse_coeffs)\n    \n    return np.sum(log_likes)\n\n\ndef mle_mix(n, w_guess):\n    \"\"\"Obtain MLE estimate for parameters for binary mixture \n    of Negative Binomials.\"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        res = scipy.optimize.minimize(\n            fun=lambda params, n: -log_like_mix(*params, n),\n            x0=[*initial_guess_mix(n, w_guess), w_guess],\n            args=(n,),\n            method='Powell',\n            tol=1e-6,\n        )\n\n    if res.success:\n        return res.x\n    else:\n        raise RuntimeError('Convergence failed with message', res.message)\nNow we can load in the data and compute the MLEs for each of the four genes.\n# Load in data\ndf = pl.read_csv(\n    os.path.join(data_path, \"singer_transcript_counts.csv\"), comment_prefix=\"#\"\n)\n\ndf_mle = pl.DataFrame(\n    schema=[(\"gene\", str)]\n    + [(param, float) for param in [\"alpha\", \"b\", \"alpha1\", \"b1\", \"alpha2\", \"b2\", \"w\"]]\n)\n\nfor gene in df.schema:\n    n = df[\"Nanog\"].to_numpy()\n\n    # Single Negative Binomial MLE\n    alpha, b = mle_iid_nbinom(df[gene].to_numpy())\n\n    # Mixture model MLE\n    alpha1, b1, alpha2, b2, w = mle_mix(df[gene].to_numpy(), 0.2)\n\n    # Store results in data frame\n    df_mle = pl.concat(\n        (\n            df_mle,\n            pl.DataFrame(\n                data=[[gene, alpha, b, alpha1, b1, alpha2, b2, w]],\n                schema=df_mle.schema,\n                orient=\"row\",\n            ),\n        )\n    )\n\n# Take a look\ndf_mle\n\n\nshape: (4, 8)\n\n\n\ngene\nalpha\nb\nalpha1\nb1\nalpha2\nb2\nw\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Rex1\"\n1.634562\n84.680915\n3.497009\n4.104916\n5.089625\n31.810375\n0.160422\n\n\n\"Rest\"\n4.530335\n16.543054\n2.786601\n12.395701\n6.683424\n11.953265\n0.108772\n\n\n\"Nanog\"\n1.263097\n69.347842\n0.834832\n66.535947\n4.127488\n28.133048\n0.466636\n\n\n\"Prdm14\"\n0.552886\n8.200636\n2.385858\n4.747279\n0.558672\n4.872751\n0.210606\nFor each of the two models, we can compute the log likelihood evaluated at the MLEs for the parameters.\n# Define funcitons taking Polars structs for computing log likelihoods\ndef pl_log_like_iid_nbinom(s):\n    return log_like_iid_nbinom((s[\"alpha\"], s[\"b\"]), df[s[\"gene\"]].to_numpy())\n\n\ndef pl_log_like_mix(s):\n    return log_like_mix(\n        s[\"alpha1\"], s[\"b1\"], s[\"alpha2\"], s[\"b2\"], s[\"w\"], df[s[\"gene\"]].to_numpy()\n    )\n\n\n# Apply the functions\ndf_mle = df_mle.with_columns(\n    # Single negative binomial\n    pl.struct([\"alpha\", \"b\", \"gene\"])\n    .map_elements(pl_log_like_iid_nbinom, return_dtype=float)\n    .alias(\"log_like_single\"),\n\n    # Mixture model\n    pl.struct([\"alpha1\", \"b1\", \"alpha2\", \"b2\", \"w\", \"gene\"])\n    .map_elements(pl_log_like_mix, return_dtype=float)\n    .alias(\"log_like_mix\"),\n)\n\n# Take a look\ndf_mle\n\n\nshape: (4, 10)\n\n\n\ngene\nalpha\nb\nalpha1\nb1\nalpha2\nb2\nw\nlog_like_single\nlog_like_mix\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Rex1\"\n1.634562\n84.680915\n3.497009\n4.104916\n5.089625\n31.810375\n0.160422\n-1638.678482\n-1590.353743\n\n\n\"Rest\"\n4.530335\n16.543054\n2.786601\n12.395701\n6.683424\n11.953265\n0.108772\n-1376.748398\n-1372.108896\n\n\n\"Nanog\"\n1.263097\n69.347842\n0.834832\n66.535947\n4.127488\n28.133048\n0.466636\n-1524.928918\n-1512.444558\n\n\n\"Prdm14\"\n0.552886\n8.200636\n2.385858\n4.747279\n0.558672\n4.872751\n0.210606\n-713.091587\n-712.702876\nWe can already see a very large difference between the log likelihood evaluated at the MLE for Rex1, but not much difference for Prdm14. The mixture model has \\(p = 5\\) parameters, while the single Negative Binomial model has \\(p = 2\\). With these numbers, we can compute the AIC and then also the Akaike weights.\ndf_mle = df_mle.with_columns(\n    (-2 * (pl.col('log_like_single') - 2)).alias('AIC_single'),\n    (-2 * (pl.col('log_like_mix') - 5)).alias('AIC_mix'),\n)\n\n# Take a look\ndf_mle\n\n\nshape: (4, 12)\n\n\n\ngene\nalpha\nb\nalpha1\nb1\nalpha2\nb2\nw\nlog_like_single\nlog_like_mix\nAIC_single\nAIC_mix\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"Rex1\"\n1.634562\n84.680915\n3.497009\n4.104916\n5.089625\n31.810375\n0.160422\n-1638.678482\n-1590.353743\n3281.356963\n3190.707487\n\n\n\"Rest\"\n4.530335\n16.543054\n2.786601\n12.395701\n6.683424\n11.953265\n0.108772\n-1376.748398\n-1372.108896\n2757.496797\n2754.217792\n\n\n\"Nanog\"\n1.263097\n69.347842\n0.834832\n66.535947\n4.127488\n28.133048\n0.466636\n-1524.928918\n-1512.444558\n3053.857837\n3034.889116\n\n\n\"Prdm14\"\n0.552886\n8.200636\n2.385858\n4.747279\n0.558672\n4.872751\n0.210606\n-713.091587\n-712.702876\n1430.183173\n1435.405751\nFinally, we can compute the Akaike weight for the model with a single Negative Binomial (the weight for the mixture model is \\(1-w_\\mathrm{single}\\)).\ndf_mle = df_mle.with_columns(\n    max_AIC := pl.max_horizontal(pl.col('AIC_single', 'AIC_mix')).alias('max_AIC'),\n    num := (-(pl.col('AIC_single') - max_AIC) / 2).exp().alias('num'),\n    (num / (num + (-(pl.col('AIC_mix') - max_AIC) / 2).exp())).alias('w_single')\n).select(\n    pl.exclude('max_AIC', 'num')\n)\n\n# Look at Akaike weights\ndf_mle[['gene', 'w_single']]\n\n\nshape: (4, 2)\n\n\n\ngene\nw_single\n\n\nstr\nf64\n\n\n\n\n\"Rex1\"\n2.0688e-20\n\n\n\"Rest\"\n0.162533\n\n\n\"Nanog\"\n0.000076\n\n\n\"Prdm14\"\n0.931585\nIn looking at the Akaike weight for the mixture (1 – w_single), is it clear that the mixture model is strongly preferred for Rex1 and Nanog. There is not strong preference for Rest, and a preference for the single Negative Binomial model for Prdm14. Reminding ourselves of the ECDFs, this makes sense.\ngenes = [\"Nanog\", \"Prdm14\", \"Rest\", \"Rex1\"]\n\nplots = [\n    iqplot.ecdf(\n        data=df[gene].to_numpy(),\n        q=gene,\n        x_axis_label=\"mRNA count\",\n        title=gene,\n        frame_height=150,\n        frame_width=200,\n    )\n    for gene in genes\n]\n\nbokeh.io.show(\n    bokeh.layouts.column(bokeh.layouts.row(*plots[:2]), bokeh.layouts.row(*plots[2:]))\n)\nRex1 clearly is bimodal, and Nanog appears to have a second inflection point where the ECDF reaches a value of about 0.4, which is what we see in the MLE estimates in the mixture model. Rest and Prdm14 both appear to be unimodal, agreeing with what we saw with the AIC analysis.\nNote that this underscores something we’ve been stressing all along. You should do good exploratory data analysis first, and the EDA often tells much of the story!",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Model comparison with the AIC</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/aic.html#aic-for-mrna-counts",
    "href": "lessons/model_assessment/aic.html#aic-for-mrna-counts",
    "title": "52  Model comparison with the AIC",
    "section": "",
    "text": "52.1.1 Caveat\nRemember, though, that we did not take into account that the measurements of the four genes were done in the same cells. We modeled that when we presented the mixture models at the beginning of this lesson. The analysis of a more complicated model with MLE proved to be out of reach due to computational difficulty. So, we should not make strong conclusions about what the relative quality of the mixture of single Negative Binomial models mean in this context. We will address these kinds of modeling issues in the sequel of this course.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Model comparison with the AIC</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/aic.html#aic-for-the-spindle-model",
    "href": "lessons/model_assessment/aic.html#aic-for-the-spindle-model",
    "title": "52  Model comparison with the AIC",
    "section": "52.2 AIC for the spindle model",
    "text": "52.2 AIC for the spindle model\nWe can do a similar analysis for the two competing models for mitotic spindle size. We need our functions from earlier.\n\ndef theor_spindle_length(gamma, phi, d):\n    \"\"\"Compute spindle length using mathematical model\"\"\"\n    return gamma * d / np.cbrt(1 + (gamma * d / phi)**3)\n\n\ndef log_likelihood(params, d, ell):\n    \"\"\"Log likelihood of spindle length model.\"\"\"\n    gamma, phi, sigma = params\n\n    if gamma &lt;= 0 or gamma &gt; 1 or phi &lt;= 0:\n        return -np.inf\n\n    mu = theor_spindle_length(gamma, phi, d)\n    return np.sum(st.norm.logpdf(ell, mu, sigma))\n\n\ndef spindle_mle(d, ell):\n    \"\"\"Compute MLE for parameters in spindle length model.\"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        res = scipy.optimize.minimize(\n            fun=lambda params, d, ell: -log_likelihood(params, d, ell),\n            x0=np.array([0.5, 35, 5]),\n            args=(d, ell),\n            method='Powell'\n        )\n\n    if res.success:\n        return res.x\n    else:\n        raise RuntimeError('Convergence failed with message', res.message)        \n\nWe can now perform MLE to get the parameters for each model and store the results.\n\ndf = pl.read_csv(os.path.join(data_path, \"good_invitro_droplet_data.csv\"), comment_prefix=\"#\")\n\nmle_1 = df.select(\n    pl.col(\"Spindle Length (um)\").mean().alias('phi_1'), \n    pl.col(\"Spindle Length (um)\").std().alias('sigma_1')\n)\n\nmle_2 = pl.DataFrame(\n    data=spindle_mle(\n        df[\"Droplet Diameter (um)\"].to_numpy(), \n        df[\"Spindle Length (um)\"].to_numpy()\n    ).reshape((1, 3)),\n    orient='row',\n    schema=['gamma', 'phi_2', 'sigma_2']\n)\n\nNext, we can compute the log likelihood evaluated at the MLE.\n\nlog_like_1 = st.norm.logpdf(\n    df[\"Spindle Length (um)\"], \n    mle_1[\"phi_1\"].item(), \n    mle_1[\"sigma_1\"].item()\n).sum()\n\nlog_like_2 = log_likelihood(\n    mle_2.to_numpy().flatten(),\n    df[\"Droplet Diameter (um)\"],\n    df[\"Spindle Length (um)\"],\n)\n\n# Take a look\nlog_like_1, log_like_2\n\n(np.float64(-1999.5179249272933), np.float64(-1837.1589821363168))\n\n\nThe log likeihood for model 2, with spindle size depending on droplet diameter, is much greater than for model 1. And now we can compute the AIC,noting that there are two parameters for model 1 and three for model 2.\n\nAIC_1 = -2 * (log_like_1 - 2)\nAIC_2 = -2 * (log_like_2 - 3)\n\n# Look at the AICs\nAIC_1, AIC_2\n\n(np.float64(4003.0358498545866), np.float64(3680.3179642726336))\n\n\nThere is a massive disparity in the AICs, so we know that model 2 is strongly preferred. Nonetheless, we can compute the Akaike weight for model 1 to compare.\n\nAIC_max = max(AIC_1, AIC_2)\nnumerator = np.exp(-(AIC_1 - AIC_max)/2)\ndenominator = numerator + np.exp(-(AIC_2 - AIC_max)/2)\nw_single = numerator / denominator\n\n# Check the Akaike weight\nw_single\n\nnp.float64(8.369539052514859e-71)\n\n\nModel 1 is completely out of the question, with a tiny Akaike weight!",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Model comparison with the AIC</span>"
    ]
  },
  {
    "objectID": "lessons/model_assessment/aic.html#computing-environment",
    "href": "lessons/model_assessment/aic.html#computing-environment",
    "title": "52  Model comparison with the AIC",
    "section": "52.3 Computing environment",
    "text": "52.3 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,scipy,bokeh,iqplot,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\npolars    : 1.31.0\nscipy     : 1.16.0\nbokeh     : 3.7.3\niqplot    : 0.3.7\nbebi103   : 0.1.28\njupyterlab: 4.4.5",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>Model comparison with the AIC</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/statistical_watchouts.html",
    "href": "lessons/statistical_watchouts/statistical_watchouts.html",
    "title": "Statistical watchouts",
    "section": "",
    "text": "I hope by now it has been impressed upon you that statistical inference is fragile. There are many failure modes, and much of doing good inference involves using techniques to avoid, or at least mitigate, those failure modes.\nHere, we present some statistical watchouts to further solidify this notion.",
    "crumbs": [
      "Statistical watchouts"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/olkin_petkau_zidek.html",
    "href": "lessons/statistical_watchouts/olkin_petkau_zidek.html",
    "title": "53  The Olkin-Petkau-Zidek example of MLE fragility",
    "section": "",
    "text": "53.1 Computing environment\n| Download notebook\nIn 1981, Olkin, Petkau, and Zidek demonstrated an example in which MLE estimates can very wildly with only small changes in the data. Here, we will work through their example.\nThe problem is thusly stated: Say you are measuring the outcomes of \\(N\\) Bernoulli trials, but you can only measure a positive result; negative results are not detected in your experiment. You do know, however that \\(N\\), while unknown, is the same for all experiments. The number of positive results you get from a set of measurements (sorted for convenience) are n = 16, 18, 22, 25, 27. Modeling the generative process with Binomial distribution, \\(n_i \\sim \\text{Binom}(\\theta, N)\\;\\;\\forall i\\), we can obtain maximum likelihood estimates for \\(\\theta\\) and \\(N\\).\nLet us first attempt to make some analytical progress on computing the MLE. We start by writing the likelihood and log likelihood. Let \\(M\\) be the number of measurements made; in this case, \\(M = 5\\).\n\\[\\begin{align}\n&L(\\theta, N;\\mathbf{n}) = \\prod_{i=1}^M \\frac{N!}{n_i!(N-n_i)!}\\,\\theta^{n_i}(1-\\theta)^{N-n_i},\\\\[1em]\n&\\ell(\\theta, N;\\mathbf{n}) = M\\ln N! + \\sum_{i=1}^M \\left[-\\ln n_i! - \\ln\\left(N-n_i\\right)! + n_i \\ln \\theta + (N-n_i)\\ln(1-\\theta)\\right].\n\\end{align}\\]\nA necessary condition for the MLE is that\n\\[\\begin{align}\n\\frac{\\partial \\ell}{\\partial \\theta} = \\sum_{i=1}^M\\,\\frac{n_i}{\\theta} - \\frac{N-n_i}{1-\\theta}\n= M\\left(\\frac{\\bar{n}}{\\theta} - \\frac{N}{1-\\theta} + \\frac{\\bar{n}}{1-\\theta}\\right)= 0,\n\\end{align}\\]\nwhere \\(\\bar{n}\\) denotes the average of the \\(M\\) observed \\(n\\) values. Solving for the MLE value of \\(\\theta\\) yields\n\\[\\begin{align}\n\\theta^* = \\frac{\\bar{n}}{N},\n\\end{align}\\]\nwhich is perhaps not surprising. We are left to find the MLE for \\(N\\), which is the value of \\(N\\) for which the log likelihood\n\\[\\begin{align}\n\\ell = M\\ln N! + \\sum_{i=1}^M \\left[-\\ln n_i! - \\ln\\left(N-n_i\\right)! + n_i\\,\\ln \\frac{\\bar{n}}{N} + (N-n_i)\\ln\\left(1-\\frac{\\bar{n}}{N}\\right)\\right]\n\\end{align}\\]\nis maximal. Equivalently, we seek \\(N\\) such that the quantity\n\\[\\begin{align}\n\\ln N! - \\bar{n}\\,\\ln N + (N-\\bar{n}) \\ln\\left(1-\\frac{\\bar{n}}{N}\\right) - \\frac{1}{M}\\sum_{i=1}^M \\ln(N-n_i)!\n\\end{align}\\]\nis maximal.\nComputing the MLE for a discrete parameter can be a bit tricky. We could take the approach of exhaustively enumerating the log likelihood for many values of \\(N\\) and then selecting the value of \\(N\\) that give the largest log likelihood. Here is an implementation of that.\nLet’s put our function to use and compute the MLE!\nWe get \\(N^* = 99\\) and \\(\\theta^* = 0.22\\).\nBut now, let’s say that the final measurement has 28 positive results instead of 27. That’s a change of a single positive result in just one of the five experiments. Now let’s compute the MLE for this new data set.\nOof! \\(N^*\\) almost doubled to 191 and \\(\\theta^*\\) halved to 0.11. With just a tiny change in the data, we can get wildly different estimates for the parameters. This is an example of the fragility of the MLE.\nTo get a handle on how our MLE would change with newly acquired data sets, we will do parametric bootstrap to compute confidence intervals for \\(N^*\\) and \\(\\theta^*\\) using the original data set with 27 successes for the fifth trial. Computing the bootstrapped confidence interval becomes difficult because exhaustive enumeration is slow and \\(N\\) can be very large. Another approach is to consider \\(N\\) as a continuous variable such that \\(N!= \\Gamma(N+1)\\), and instead search for a maximizer of\n\\[\\begin{align}\nh(N) = \\ln \\Gamma(N+1) - \\bar{n}\\,\\ln N + (N-\\bar{n}) \\ln\\left(1-\\frac{\\bar{n}}{N}\\right) - \\frac{1}{M}\\sum_{i=1}^M \\ln \\Gamma(N-n_i+1).\n\\end{align}\\]\nNoting that the derivative of the log of a Gamma function is a polygamma function,\n\\[\\begin{align}\n\\frac{\\mathrm{d}}{\\mathrm{d}x}\\,\\ln \\Gamma(x) = \\psi_0(x),\n\\end{align}\\]\nwe can write\n\\[\\begin{align}\n\\frac{\\mathrm{d}h}{\\mathrm{d}N} = \\ln\\left(1 - \\frac{\\bar{n}}{N}\\right) + \\psi_0(N+1) - \\frac{1}{M}\\sum_{i=1}^M\\psi_0(N-n_i+1) = 0.\n\\end{align}\\]\nOur strategy to find \\(N^*\\) is to solve the above root finding problem for continuous \\(N\\), and then find which integer \\(N\\) on either side of the root gives the maximal log likelihood. Note that in the limit of \\(N\\to \\infty\\), \\(\\mathrm{d}h/\\mathrm{d}N = 0\\). A finite root may not exist, meaning that the MLE is \\(N\\to\\infty\\) and \\(\\theta = 0\\). In this limit, the number of positive results are Poisson distributed (the \\(N\\to\\infty\\) and \\(\\theta \\to 0\\) limit where \\(N\\theta\\) remains finite). In this case, the MLE for \\(N\\theta\\) is \\(\\bar{n}\\). We will not include those results in our bootstrap samples.\nLet’s now compute the bootstrap replicates.\nLet’s first check what fraction of the bootstrap replicates had \\(N\\to\\infty\\).\nWow. Nearly 25% of the bootstrap replicates had \\(N\\to\\infty\\), such that we can’t even compute an MLE for the Binomial model. Now, computing a confidence interval with the samples where an MLE exists, first for \\(N\\).\nAnd now for theta.\nThese results verify that from observing first positive results from five sets of experiments, we cannot really know anything about the parameters of the model.\n%load_ext watermark\n%watermark -v -p numpy,scipy,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\nscipy     : 1.16.0\njupyterlab: 4.4.5",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>The Olkin-Petkau-Zidek example of MLE fragility</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/nonidentifiable.html",
    "href": "lessons/statistical_watchouts/nonidentifiable.html",
    "title": "54  Nonidentifiable models",
    "section": "",
    "text": "54.1 The theoretical model\n| Download notebook\nData set download\nIn this portion of the lesson, we will show how nonidentifiability can be lurking in an analysis.\nA human immunodeficiency virus (HIV) is a virus that causes host organisms to develop a weaker, and sometimes ineffective, immune system. HIV inserts its viral RNA into a target cell of the immune system. This virus gets reverse transcribed into DNA and integrated into the host cell’s chromosomes. The host cell will then transcribe the integrated DNA into mRNA for the production of viral proteins and produce new HIV. Newly created viruses then exit the cell by budding off from the host. HIV carries the danger of ongoing replication and cell infection without termination or immunological control. Because CD4 T cells, which communicate with other immune cells to activate responses to foreign pathogens, are the targets for viral infection and production, their infection leads to reductions in healthy CD4 T cell production, causing the immune system to weaken. This reduction in the immune response becomes particularly concerning after remaining infected for longer periods of time, leading to acquired immune deficiency syndrome (AIDS).\nPerelson and coworkers have developed mathematical models to study HIV populations in the eukaryotic organisms. HIV-1 infects cells at a rate \\(k\\) and is produced from these infected T cells at a rate \\(p\\). On the other hand, the viruses are lost due to clearance by the immune system of drugs, which occurs at a rate \\(c\\), and infected cells die at a rate \\(\\delta\\) (Figure from Perelson, Nat. Rev. Immunol., 2, 28-36, 2002)\nThe above process can be written down as a system of differential equations.\n\\[\\begin{align}\n\\frac{dT^*}{dt} &= k V_I T - \\delta T^*\\\\[1em]\n\\frac{dV_I}{dt} &= -cV_I\\\\[1em]\n\\frac{dV_{NI}}{dt} &= N \\delta T^{*} - c V_{NI},\n\\end{align}\\]\nHere, \\(T(t)\\) is the number of uninfected T-cells at time \\(t\\), and \\(T^*\\) is the number of infected T cells. Furthermore, there is a concentration \\(V_I(t)\\) of infectious viruses that infect T cells at the rate \\(k\\). We also have a concentration \\(V_{NI}\\) of innocuous viruses. We define \\(N(t)\\) to be the number of newly produced viruses from one infected cell over this cell’s lifetime. We can measure the total viral load, \\(V(t) = V_I(t) + V_{NI}(t)\\). If we initially have a viral load of \\(V(0) = V_0\\), we can solve the system of differential equations to give\n\\[\\begin{align}\nV(t;V_0,c,\\delta) = V_0e^{-ct} + \\frac{cV_0}{c-\\delta}\\left[\\frac{c}{c-\\delta}(e^{-{\\delta}t} - e^{-ct}) - {\\delta}te^{-ct}\\right].\n\\end{align}\\]",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nonidentifiable models</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/nonidentifiable.html#the-theoretical-model",
    "href": "lessons/statistical_watchouts/nonidentifiable.html#the-theoretical-model",
    "title": "54  Nonidentifiable models",
    "section": "",
    "text": "Figure 54.1: Basic model of viral infection, taken from Perelson, Nat. Rev. Immunol., 2002",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nonidentifiable models</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/nonidentifiable.html#the-data-set",
    "href": "lessons/statistical_watchouts/nonidentifiable.html#the-data-set",
    "title": "54  Nonidentifiable models",
    "section": "54.2 The data set",
    "text": "54.2 The data set\nWe will take viral load data from a real patient (which you can download here). The patient was treated with the drug Ritonavir, a protease inhibitor that serves to clear the viruses; i.e., it modulates \\(c\\). So, in the context of the model, \\(c\\) is a good parameter to use to understand the efficacy of a drug.\nLet us take a quick look at the data set.\n\ndf = pl.read_csv(os.path.join(data_path, \"hiv_data.csv\"), comment_prefix=\"#\")\n\np = bokeh.plotting.figure(\n    frame_height=250,\n    frame_width=450,\n    x_axis_label=\"days after drug administration\",\n    y_axis_label=\"RNA copies per mL\",\n)\np.scatter(df[\"Days after administration\"], df[\"RNA copies per mL\"])\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nAfter some noisy viral RNA levels early on, the virus begins to be cleared after about a day, bringing the viral load down.",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nonidentifiable models</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/nonidentifiable.html#the-statistical-model",
    "href": "lessons/statistical_watchouts/nonidentifiable.html#the-statistical-model",
    "title": "54  Nonidentifiable models",
    "section": "54.3 The statistical model",
    "text": "54.3 The statistical model\nFor our statistical model (which we will later see is nonidentifiable), we will assume that residual \\(i\\) is Normally distributed with scale parameter \\(\\sigma_i = \\sigma_0 V(t_i;V_0, v, \\delta)\\). That is, we assume that the variability scales with the size of the measurement.\n\\[\\begin{align}\n&\\mu_i = V(t_i;V_0,c,\\delta) = V_0e^{-ct_i} + \\frac{cV_0}{c-\\delta}\\left[\\frac{c}{c-\\delta}(e^{-{\\delta}t_i} - e^{-ct_i}) - {\\delta}t_ie^{-ct_i}\\right] \\;\\forall i, \\\\[1em]\n&V_i \\sim \\text{Norm}(\\mu_i, \\sigma_0 \\mu_i) \\;\\forall i.\n\\end{align}\\]",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nonidentifiable models</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/nonidentifiable.html#performing-a-regression-to-determine-the-parameters-δ-and-c",
    "href": "lessons/statistical_watchouts/nonidentifiable.html#performing-a-regression-to-determine-the-parameters-δ-and-c",
    "title": "54  Nonidentifiable models",
    "section": "54.4 Performing a regression to determine the parameters δ and c",
    "text": "54.4 Performing a regression to determine the parameters δ and c\nThe most common procedure to characterize the parameters δ and c is to minimize the sum of the square of the residuals between the theoretical equation given above and the measured data. As we showed in one of the homeworks, this is equivalent to finding the MAP parameter values for a model with a homoscedastic Gaussian likelihood and Uniform priors. Let’s proceed to do this MAP calculation.\nIn the viral load model, we need to make sure that we don’t get a divide by zero error when \\(c \\approx \\delta\\). We will use the fact that\n\\[\\begin{align}\n\\lim_{c\\to\\delta} V(t;V_0,c,\\delta) = V_0 \\left(1 + \\delta t + \\frac{\\delta^2 t^2}{2}\\right)\\mathrm{e}^{-\\delta t}.\n\\end{align}\\]\nWe can now write our viral load model. Because we will be computing the posterior on a grid for plotting later, I am going to use Numba to speed things up.\n\n@numba.njit\ndef viral_load_model(V0, c, delta, t):\n    \"\"\"Perelman model for viral load\"\"\"\n    # If c and d are close, use limiting expression\n    if abs(c - delta) &lt; 1e-9:\n        return V0 * (1 + delta * t + (delta * t) ** 2 / 2) * np.exp(-delta * t)\n\n    # Proceed with typical calculation\n    bracket_term = c / (c - delta) * (\n        np.exp(-delta * t) - np.exp(-c * t)\n    ) - delta * t * np.exp(-c * t)\n\n    return V0 * (np.exp(-c * t) + c / (c - delta) * bracket_term)\n\nI will now code up the statistical model. I will not use scipy.stats, opting for hand-coding the Normal log PDF by hand for speed.\n\n@numba.njit\ndef log_like_no_bounds(V0, c, delta, sigma_0, t, V):    \n    V_theor = viral_load_model(V0, c, delta, t)\n    sigma = sigma_0 * V_theor\n    \n    return -np.sum(np.log(sigma)) - np.sum((V - V_theor)**2 / sigma**2) / 2\n\n\ndef log_likelihood(params, t, V):\n    \"\"\"Log likelihood for viral load model Gaussian.\"\"\"\n    if (params &lt; 0).any():\n        return -np.inf\n\n    V0, c, delta, sigma_0 = params\n    return log_like_no_bounds(V0, c, delta, sigma_0, t, V)\n\nBased on the plot of the data, we will guess \\(V_0 \\approx 150000\\) RNA copies/mL. We’ll guess that \\(c\\) is dominating the drop in load and choose \\(c \\approx 3\\) days\\(^{-1}\\). We’ll guess \\(\\delta\\) is slower, say 0.3 day\\(^{-1}\\). Now, we’ll perform the curve fit.\n\n# Unpack Numpy arrays for speed\nt = df['Days after administration'].to_numpy()\nV = df['RNA copies per mL'].to_numpy()\n\ndef mle(data):\n    \"\"\"Compute MLE for HIV model.\"\"\"\n    # Unpack\n    t = data[:, 0]\n    V = data[:, 1]\n    \n    # Initial guess\n    p0 = np.array([150000, 3, 0.3, 0.3])\n\n    # Find MLE\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        res = scipy.optimize.minimize(\n            fun=lambda p, t, V: -log_likelihood(p, t, V),\n            x0=p0,\n            args=(t, V),\n            method='powell'\n        )\n\n    if res.success:\n        return res.x\n    else:\n        raise RuntimeError('Convergence failed with message', res.message)\n\npopt = mle(df[['Days after administration', 'RNA copies per mL']].to_numpy())\n\n# Print results to screen\nprint(\"\"\"MLEs:\nV_0 = {0:.2e} copies of RNA/mL\n  c = {1:.2f} 1/days\n  δ = {2:.2f} 1/days\n σ0 = {3:.2f}\n\"\"\".format(*popt))\n\nMLEs:\nV_0 = 1.32e+05 copies of RNA/mL\n  c = 4.69 1/days\n  δ = 0.44 1/days\n σ0 = 0.21\n\n\n\nNow, we’ll plot the regression line with the data. This is what is commonly done, and indeed was done in the Perelson paper, though we will momentarily do a graphical model check, which is a better way to construct the plot.\n\n# Plot the regression line\nt_smooth = np.linspace(0, 8, 200)\np.line(t_smooth, viral_load_model(*popt[:-1], t_smooth), color=\"orange\", line_width=2)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nonidentifiable models</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/nonidentifiable.html#graphical-model-check",
    "href": "lessons/statistical_watchouts/nonidentifiable.html#graphical-model-check",
    "title": "54  Nonidentifiable models",
    "section": "54.5 Graphical model check",
    "text": "54.5 Graphical model check\nWe’ll now perform a quick graphical model check, using our generative model parametrized by the MLE to generate data sets. First, we’ll code up a function to generate samples and then generate them.\n\nrng = np.random.default_rng()\n\ndef sample_hiv(V0, c, delta, sigma_0, t, size=1, rng=rng):\n    \"\"\"Generate samples of of HIV viral load curves\"\"\"\n    samples = np.empty((size, len(t)))\n\n    for i in range(size):\n        mu = viral_load_model(V0, c, delta, t)\n        samples[i] = np.maximum(0, rng.normal(mu, sigma_0*mu))\n\n    return samples\n\nt_theor = np.linspace(0, 8, 200)\nsamples = sample_hiv(*popt, t_theor, size=10000)\n\nAnd now we can build the plot.\n\np = bebi103.viz.predictive_regression(\n    samples=samples,\n    samples_x=t_theor,\n    data=df[['Days after administration', 'RNA copies per mL']].to_numpy(),\n    x_axis_label='days after drug administraion',\n    y_axis_label='RNA copies per mL',\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis looks ok, with most measured points falling within the middle 95th percentile of the generated data sets.",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nonidentifiable models</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/nonidentifiable.html#confidence-intervals-of-parameters",
    "href": "lessons/statistical_watchouts/nonidentifiable.html#confidence-intervals-of-parameters",
    "title": "54  Nonidentifiable models",
    "section": "54.6 Confidence intervals of parameters",
    "text": "54.6 Confidence intervals of parameters\nWe can proceed to compute confidence intervals for the parameters. We will use our usual method of parametrically drawing new data sets out of the generative distribution parametrized by the MLE. WE first have to write a function to generate new data sets out of the model with the requisite call signature for bebi103.bootstrap.draw_bs_reps_mle().\n\ndef gen_hiv(params, t, size, rng):\n    return np.vstack((t, sample_hiv(*params, t, size=len(t), rng=rng))).transpose()\n\nbs_reps = bebi103.bootstrap.draw_bs_reps_mle(\n    mle,\n    gen_hiv,\n    data=df[[\"Days after administration\", \"RNA copies per mL\"]].to_numpy(),\n    mle_args=(),\n    gen_args=(t,),\n    size=10000,\n    n_jobs=3,\n    progress_bar=True,\n)\n\n# Compute confidence intervals and take a look\npl.DataFrame(\n    data=np.percentile(bs_reps, [2.5, 97.5], axis=0),\n    schema=[\"V0*\", \"c*\", \"δ*\", \"σ0*\"],\n)\n\n100%|██████████████████████████████████████████████████████| 3333/3333 [00:09&lt;00:00, 350.03it/s]\n100%|██████████████████████████████████████████████████████| 3333/3333 [00:09&lt;00:00, 345.85it/s]\n100%|██████████████████████████████████████████████████████| 3334/3334 [00:09&lt;00:00, 345.83it/s]\n/Users/bois/Dropbox/git/huji_stats/.pixi/envs/default/lib/python3.13/multiprocessing/resource_tracker.py:301: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown: {'/mp-krnbcbml'}\n  warnings.warn(\n/Users/bois/Dropbox/git/huji_stats/.pixi/envs/default/lib/python3.13/multiprocessing/resource_tracker.py:301: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown: {'/mp-mceb45yf'}\n  warnings.warn(\n/Users/bois/Dropbox/git/huji_stats/.pixi/envs/default/lib/python3.13/multiprocessing/resource_tracker.py:301: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown: {'/mp-3w3calix'}\n  warnings.warn(\n/Users/bois/Dropbox/git/huji_stats/.pixi/envs/default/lib/python3.13/multiprocessing/resource_tracker.py:317: UserWarning: resource_tracker: '/mp-3w3calix': [Errno 2] No such file or directory\n  warnings.warn('resource_tracker: %r: %s' % (name, e))\n\n\n\nshape: (2, 4)\n\n\n\nV0*\nc*\nδ*\nσ0*\n\n\nf64\nf64\nf64\nf64\n\n\n\n\n109762.350885\n1.56781\n0.390109\n0.115502\n\n\n155468.316137\n4.0624e12\n0.54608\n0.261255\n\n\n\n\n\n\nAll parameters look like they have reasonable confidence intervals for their MLE except for \\(c^*\\). There is something funky, here, with the end of the confidence interval being \\(10^{12}\\) days\\(^{-1}\\)! We can take a quick look at the ECDF of our samples of \\(c^*\\).\n\ndf_res = pl.DataFrame(data=bs_reps, schema=['V0*', 'c*', 'δ*', 'σ0*'])\n\np = iqplot.ecdf(\n    data=df_res,\n    q='c*',\n    x_axis_type='log',\n    x_axis_label='c* (1/days)',\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nMost samples are below 10, but we see 20% of them are well above 10, some positively enormous. This could be a solver issue, or it could be a lurking nonidentifiability. Let’s investigate.",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nonidentifiable models</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/nonidentifiable.html#plotting-the-log-likelihood-function",
    "href": "lessons/statistical_watchouts/nonidentifiable.html#plotting-the-log-likelihood-function",
    "title": "54  Nonidentifiable models",
    "section": "54.7 Plotting the log likelihood function",
    "text": "54.7 Plotting the log likelihood function\nTo diagnose the issue, we will look at the likelihood function. To start with, we will fix the values of \\(V_0\\), \\(\\delta\\), and \\(\\sigma_0\\) to their MLEs and plot the likelihood function as a function of \\(c\\).\n\n# c values for plotting\nc = np.linspace(0.01, 40, 300)\n\nlog_like = np.array(\n    [log_likelihood(np.array([popt[0], c_val, *popt[-2:]]), t, V) for c_val in c]\n)\nlike = np.exp(log_like - max(log_like))\n\np = bokeh.plotting.figure(\n    frame_height=250,\n    frame_width=450,\n    x_axis_label=\"c (1/days)\",\n    y_axis_label=\"likelihood\",\n)\np.line(c, like, line_width=2)\n\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis looks ok, though that tail looks a little troublesome. Let’s plot the log likelihood to better resolve it.\n\n# c values for plotting\nc = np.linspace(0.01, 100, 300)\n\nlog_like = np.array(\n    [log_likelihood(np.array([popt[0], c_val, *popt[-2:]]), t, V) for c_val in c]\n)\n\np = bokeh.plotting.figure(\n    frame_height=250,\n    frame_width=450,\n    x_axis_label=\"c (1/days)\",\n    y_axis_label=\"log likelihood\",\n)\np.line(c, log_like, line_width=2)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe log likelihood asymptotes to a constant value of \\(c\\), with all other parameters held at their MLE values.\nLet’s take another look, considering also \\(\\delta\\), plotting the likelihood function in the \\(c\\)-\\(\\delta\\) plane.\n\n# c and δ values for plotting\nc = np.linspace(0.01, 100, 200)\ndelta = np.linspace(0.2, 0.8, 200)\n\n# Compute log-likelihood for each value\nlog_like = np.empty((200, 200))\nfor j, c_val in enumerate(c):\n    for i, delta_val in enumerate(delta):\n        log_like[i, j] = log_likelihood(\n            np.array([popt[0], c_val, delta_val, popt[-1]]), t, V\n        )\n\nlike = np.exp(log_like - log_like.max())\n\n# Display the image\nbokeh.io.show(\n    bebi103.image.imshow(\n        like,\n        x_range=[c.min(), c.max()],\n        y_range=[delta.min(), delta.max()],\n        x_axis_label=\"c\",\n        y_axis_label=\"δ\",\n        frame_width=300,\n        frame_height=300,\n        flip=False,\n    )\n)\n\n\n  \n\n\n\n\n\nThere is the nonidentifiability! The faint tail at \\(\\delta \\approx 0.4\\) days\\(^{-1}\\) goes off to infinity. If we have \\(\\delta \\approx 0.4\\), then any value of \\(c\\) will give roughly the same likelihood. From our estimates, we expect \\(\\delta\\) to lie somewhere near 0.4 days\\(^{-1}\\), so this nonidentifiability is something we can likely encounter. It tells us that we cannot actually know \\(c\\) for sure, that is unless we had an independent measurement for \\(\\delta\\). We would have no way of knowing this if we just solved the least squares problem and reported our values.\nComputing the predictive regression and the confidence interval of the MLEs is the minimum you should do with your model. Next term, we will discuss principled modeling techniques that will help protect you from these pathologies that are encountered a lot more than we know.",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nonidentifiable models</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/nonidentifiable.html#computing-environment",
    "href": "lessons/statistical_watchouts/nonidentifiable.html#computing-environment",
    "title": "54  Nonidentifiable models",
    "section": "54.8 Computing environment",
    "text": "54.8 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,scipy,numba,bokeh,iqplot,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\npolars    : 1.31.0\nscipy     : 1.16.0\nnumba     : 0.61.2\nbokeh     : 3.7.3\niqplot    : 0.3.7\nbebi103   : 0.1.28\njupyterlab: 4.4.5",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Nonidentifiable models</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/dancing.html",
    "href": "lessons/statistical_watchouts/dancing.html",
    "title": "55  Dancing statistics",
    "section": "",
    "text": "55.1 Cohen’s d\n| Download notebook\nDataset download\nThis lesson was inspired by Geoff Cumming.\nIn this fun exercise, we will investigate how replicable certain statistical conclusions are. What I mean by replicability is best understood be working through this notebook.\nFor this lesson we will use the zebrafish embryo sleep data from the Prober lab. A description of their work on the genetic regulation of sleep can be found on the research page of the lab website. In particular, the movie below comes from their experiments watching moving/sleeping larvae over time.\nThe data we will use are processed from raw data published in Gandhi et al., 2015. In their experiment they were studying the effect of a deletion in the gene coding for arylalkylamine N-acetyltransferase (aanat), which is a key enzyme in the rhythmic production of melatonin. Melatonin is a hormone responsible for regulation of circadian rhythms. It is often taken as a drug to treat sleep disorders. The goal of this study is to investigate the effects of aanat deletion on sleep pattern in 5+ day old zebrafish larvae.\nAmong other sleep properties, they measured the mean rest bout length on the sixth night, comparing wild type larvae to the homozygous mutant. A rest bout is defined as a period of time in which the fish does not move. The length of a rest bout is just the amount of time the fish is still. We are primarily interested in the difference in the mean bout lengths between the two genotypes. The processed data are found here.\nLet’s load the data.\nLet’s look at these data with an ECDF.\nThe distribution for the mutant appears to be shifted to the left of the wild type, meaning that the mutant has shorted rest bouts. Let’s add 95% confidence intervals to the plot to see how the ECDFs might change if we did the experiment again.\nThese is strong overlap of the confidence intervals, so it may just be that the differences are due to the finite sample size. We will investigate further with some modeling.\nCohen’s d is a commonly used measure of effect size in comparison of two data sets. It is the ratio of the difference of means compared to a pooled standard deviation.\n\\[\\begin{align}\nd = \\frac{|\\bar{x} - \\bar{y}|}{\\sqrt{\\left.(n_x \\hat{\\sigma}_x^2 + n_y \\hat{\\sigma}_y^2) \\middle/ (n_x + n_y - 2)\\right.}}.\n\\end{align}\\]\nHere, \\(\\bar{x}\\) is the plug-in estimate for the mean of the data from sample \\(x\\), \\(\\hat{\\sigma}_x^2\\) is the plug-in estimate for the variance from sample \\(x\\), and \\(n_x\\) is the number of measurements in sample \\(x\\). The values for sample \\(y\\) are similarly defined.\nRoughly speaking, Cohen’s d tells us how different the means of the data sets are compared to the variability in the data. A large Cohen’s d means that the effect is large compared to the variability of the measurement.",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Dancing statistics</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/dancing.html#estimates-of-the-difference-of-means-and-cohens-d",
    "href": "lessons/statistical_watchouts/dancing.html#estimates-of-the-difference-of-means-and-cohens-d",
    "title": "55  Dancing statistics",
    "section": "55.2 Estimates of the difference of means and Cohen’s d",
    "text": "55.2 Estimates of the difference of means and Cohen’s d\nFirst, we will compute nonparametric estimates from the data. We will estimate the difference in the mean bout length and Cohen’s d. For speed, we save the two data sets as NumPy arrays.\n\nwt = df.filter(pl.col('genotype') == 'wt')['mean_rest_bout_length'].to_numpy()\nmut = df.filter(pl.col('genotype') == 'mut')['mean_rest_bout_length'].to_numpy()\n\nNext, we’ll write some functions to conveniently generate bootstrap replicates and do our hypothesis tests. These borrow heavily from the lessons on hacker stats.\n\n@numba.jit(nopython=True)\ndef cohen_d(x, y, return_abs=False):\n    \"\"\"Cohen's d for two data sets.\"\"\"\n    diff = x.mean() - y.mean()\n    pooled_variance = (len(x) * np.var(x) + len(y) * np.var(y)) / (len(x) + len(y) - 2)\n\n    if return_abs:\n        return np.abs(diff) / np.sqrt(pooled_variance)\n    return diff / np.sqrt(pooled_variance)\n\n\n@numba.jit(nopython=True)\ndef t_stat(x, y):\n    \"\"\"Welch's t-statistic.\"\"\"\n    return (np.mean(x) - np.mean(y)) / np.sqrt(\n        np.var(x) / (len(x) - 1) + np.var(y) / (len(y) - 1)\n    )\n\n\n@numba.jit(nopython=True)\ndef draw_perm_sample(x, y):\n    \"\"\"Generate a permutation sample.\"\"\"\n    concat_data = np.concatenate((x, y))\n    np.random.shuffle(concat_data)\n    return concat_data[: len(x)], concat_data[len(x) :]\n\n\n@numba.jit(nopython=True)\ndef draw_bs_sample(data):\n    \"\"\"Draw a single bootstrap sample.\"\"\"\n    return np.random.choice(data, size=len(data))\n\n\n@numba.jit(nopython=True)\ndef draw_perm_reps_t(x, y, size=10000):\n    out = np.empty(size)\n    for i in range(size):\n        x_perm, y_perm = draw_perm_sample(x, y)\n        out[i] = t_stat(x_perm, y_perm)\n    return out\n\n\n@numba.jit(nopython=True)\ndef draw_bs_reps_mean(data, size=10000):\n    out = np.empty(size)\n    for i in range(size):\n        out[i] = np.mean(draw_bs_sample(data))\n    return out\n\n\n@numba.jit(nopython=True)\ndef draw_bs_reps_diff_mean(x, y, size=10000):\n    out = np.empty(size)\n    for i in range(size):\n        out[i] = np.mean(draw_bs_sample(x)) - np.mean(draw_bs_sample(y))\n    return out\n\n\n@numba.jit(nopython=True)\ndef draw_bs_reps_cohen_d(x, y, size=10000, return_abs=False):\n    out = np.empty(size)\n    for i in range(size):\n        out[i] = cohen_d(draw_bs_sample(x), draw_bs_sample(y), return_abs)\n    return out\n\n\n@numba.jit(nopython=True)\ndef draw_bs_reps_t(x, y, size=10000):\n    \"\"\"\n    Bootstrap replicates using the Welch's t-statistic.\n    \"\"\"\n    out = np.empty(size)\n    for i in range(size):\n        out[i] = t_stat(draw_bs_sample(x), draw_bs_sample(y))\n    return out\n\nNow we can compute the replicates. First, let’s look at the means and their respective confidence intervals.\n\nwt_reps = draw_bs_reps_mean(wt)\nmut_reps = draw_bs_reps_mean(mut)\n\nAnd from these, compute the 95% confidence interval.\n\nwt_mean_conf_int = np.percentile(wt_reps, [2.5, 97.5])\nmut_mean_conf_int = np.percentile(mut_reps, [2.5, 97.5])\n\nsummaries = [\n    dict(estimate=est, conf_int=conf, label=name)\n    for est, conf, name in zip(\n        [wt.mean(), mut.mean()], [wt_mean_conf_int, mut_mean_conf_int], [\"WT\", \"mutant\"]\n    )\n]\n\np = bebi103.viz.confints(summaries, x_axis_label='mean rest bout lengths (min)')\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThere is some overlap in the confidence interval, though wild type tend to have longer rest bouts. Just looking at these numbers, we may not be all that certain that there is a discernible difference between wild type and mutant.\nNow, let’s look at the difference in the mean rest bout lengths, which we define as \\(\\delta = \\bar{x}_\\mathrm{wt} - \\bar{x}_\\mathrm{mut}\\).\n\nreps = draw_bs_reps_diff_mean(wt, mut)\ndiff_mean_conf_int = np.percentile(reps, [2.5, 97.5])\n\nprint(\n    \"δ = WT - MUT:  [{1:.2f}, {0:.2f}, {2:.2f}] minutes\".format(\n        np.mean(wt) - np.mean(mut), *tuple(diff_mean_conf_int)\n    )\n)\n\nδ = WT - MUT:  [-0.06, 0.31, 0.66] minutes\n\n\nAs we might expect, on the tail end of the confidence interval for the difference of means, we see that the mutant might actually have longer rest bouts that wild type.\nFinally, lets compute the Cohen’s d to check the effect size.\n\nreps = draw_bs_reps_cohen_d(wt, mut)\ncohen_d_conf_int = np.percentile(reps, [2.5, 97.5])\n\nprint(\n    \"WT - MUT Cohen's d:  [{1:.2f}, {0:.2f}, {2:.2f}]\".format(\n        cohen_d(wt, mut), *tuple(cohen_d_conf_int)\n    )\n)\n\nWT - MUT Cohen's d:  [-0.08, 0.54, 1.43]\n\n\nSo, the effect size is 0.54, meaning that the mutant tends to have rest bouts 0.5 standard deviations as large as wild type fix. Jacob Cohen would call this a “medium” sized effect.",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Dancing statistics</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/dancing.html#null-hypothesis-significance-testing",
    "href": "lessons/statistical_watchouts/dancing.html#null-hypothesis-significance-testing",
    "title": "55  Dancing statistics",
    "section": "55.3 Null hypothesis significance testing",
    "text": "55.3 Null hypothesis significance testing\nWe will now perform an NHST on these two data sets. We formulate the hypothesis as follows.\n\n\\(H_0\\): Wild type and mutant fish have the same mean rest about length.\nTest statistic: Cohen’s d.\nAt least as extreme as: Cohen’s d larger than what was observed.\n\nWe can then perform a bootstrap hypothesis test.\n\n@numba.jit(nopython=True)\ndef cohen_nhst(wt, mut, size=100000):\n    \"\"\"\n    Perform hypothesis test assuming equal means, using\n    Cohen-d as test statistic.\n    \"\"\"\n    # Shift data sets so that they have the same mean.\n    wt_shifted = wt - np.mean(wt) + np.mean(np.concatenate((wt, mut)))\n    mut_shifted = mut - np.mean(mut) + np.mean(np.concatenate((wt, mut)))\n\n    # Draw replicates of Cohen's d\n    reps = draw_bs_reps_cohen_d(wt_shifted, mut_shifted, size=size)\n\n    # Compute p-value\n    return np.sum(reps &gt;= cohen_d(wt, mut)) / len(reps)\n\nprint(\"Cohen's d p-value:\", cohen_nhst(wt, mut))\n\nCohen's d p-value: 0.06688\n\n\nWe get a p-value of about 0.07, which, if we use the typical bright line p-value for statistical significance, we would say that this difference is not statistically significant. We could also test what would happen if we used a different test statistic, like the difference of means.\n\n# Shift data sets so that they have the same mean.\nwt_shifted = wt - np.mean(wt) + np.mean(np.concatenate((wt, mut)))\nmut_shifted = mut - np.mean(mut) + np.mean(np.concatenate((wt, mut)))\n\n# Draw replicates of difference of means\nreps = draw_bs_reps_diff_mean(wt_shifted, mut_shifted, size=100000)\n\n# Compute p-value\np_val = np.sum(reps &gt;= np.mean(wt) - np.mean(mut)) / len(reps)\n\nprint('Difference of means p-value:', p_val)\n\nDifference of means p-value: 0.04077\n\n\nHere, we get a p-value of about 0.04. We would say that the result is statistically significant if we used a bright line value of 0.05.\nFinally, let’s try a canonical test for this circumstance, the Welch’s t-test. As a reminder, the test statistic for the Welch’s t-test is\n\\[\\begin{align}\nT = \\frac{\\bar{x}_w - \\bar{x}_m}{\\sqrt{\\hat{\\sigma}_w^2/n_w + \\hat{\\sigma}_m^2/n_m}},\n\\end{align}\\]\nwhere \\(\\hat{\\sigma}_w^2\\) and \\(\\hat{\\sigma}_m^2\\) are plug-in estimates for the variances. Importantly, when performing a Welch’s t-test, Normality of the two samples is assumed. So, the hypothesis test is defined as follows.\n\n\\(H_0\\): The two samples are both Normally distributed with equal means.\nTest statistic: t-statistic.\nAt least as extreme as: t-statistic (wild type minus mutant) greater than or equal to what was observed.\n\nThis is implemented as scipy.stats.ttest_ind() using the kwarg equal_var=False. We divide by two to get the one-tailed test. Note that Welch’s t-test is not exact, but is asymptotically exact for large sample sizes.\n\nprint(\"Welch's p-value:\", st.ttest_ind(wt, mut, equal_var=False)[1]/2)\n\nWelch's p-value: 0.05254200490883057\n\n\nHere, we are just above the bright line value of 0.05. We can perform a similar hypothesis test without the Normal assumption using the same test statistic as in the Welch’s test.\n\n# Draw replicates of t statistic\nreps = draw_bs_reps_t(wt_shifted, mut_shifted, size=100000)\n\n# Compute p-value\np_val = np.sum(reps &gt;= t_stat(wt, mut)) / len(reps)\n\nprint(\"Welch's t-test without Normal assumption p-value:\", p_val)\n\nWelch's t-test without Normal assumption p-value: 0.06416\n\n\nFinally, we will perform a permutation test. This test is specified as follows.\n\n\\(H_0\\): The sleep bout lengths of mutant and wild type fish are identically distributed.\nTest statistic: Welch’s t-statistic\nAt least as extreme as : difference of means is greater than what was observed.\n\n\n# Draw permutation replicates\nreps = draw_perm_reps_t(wt, mut, size=100000)\n\n# Compute p-value\np_val = np.sum(reps &gt;= t_stat(wt, mut)) / len(reps)\n\nprint(\"Permutation test p-value:\", p_val)\n\nPermutation test p-value: 0.04999\n\n\nSo, all of our tests give p-values that are close to each other, ranging from about 0.04 to 0.07. If we choose bright line p-values to deem something as significant or not, some similar hypothesis/test statistic pairs can give different results. So, my advice is do not use brightline p-values. You went through the trouble of computing the p-value, just report it and leave it at that. Don’t change a float to a bool.",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Dancing statistics</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/dancing.html#model-comparison",
    "href": "lessons/statistical_watchouts/dancing.html#model-comparison",
    "title": "55  Dancing statistics",
    "section": "55.4 Model comparison",
    "text": "55.4 Model comparison\nAs an alternative to NHST, we can ask a similar (but different) question. We can compare two generative models. In one model, wild type and mutant sleep mean bout lengths come from the same Normal distribution. In the other, they come from different Normal distributions. We can compute an AIC for each model and then the Akaike weight for the first model (that they come from the same Normal distribution). Again, we can use the convenient feature that for Normal distributions, the MLE is given by the plug-in estimates for the parameters.\n\ndef akaike_weight(wt, mut):\n    \"\"\"Compute the Akaike weight for model 1\"\"\"\n    x_concat = np.concatenate((wt, mut))\n    mu_1 = np.mean(x_concat)\n    sigma_1 = np.std(x_concat)\n    aic_1 = -2 * (st.norm.logpdf(x_concat, mu_1, sigma_1).sum() - 2)\n    \n    mu_wt = np.mean(wt)\n    sigma_wt = np.std(wt)\n    mu_mut = np.mean(mut)\n    sigma_mut = np.std(mut)\n    aic_2 = -2 * (\n        st.norm.logpdf(wt, mu_wt, sigma_wt).sum()\n        + st.norm.logpdf(mut, mu_mut, sigma_mut).sum()\n        - 4\n    )\n\n    aic_max = max(aic_1, aic_2)\n\n    return np.exp(-(aic_1 - aic_max) / 2) / (\n        np.exp(-(aic_1 - aic_max) / 2) + np.exp(-(aic_2 - aic_max) / 2)\n    )\n\nNow that we have this function, we can compute the Akaike weight for this data set.\n\nakaike_weight(wt, mut)\n\nnp.float64(0.598909687738875)\n\n\nThe Akaike weight says that we should slightly favor the model where the data points come from the same Normal distribution. This runs contrary to our hypothesis tests. The AIC is penalizing the model with more parameters.",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Dancing statistics</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/dancing.html#dancing",
    "href": "lessons/statistical_watchouts/dancing.html#dancing",
    "title": "55  Dancing statistics",
    "section": "55.5 Dancing",
    "text": "55.5 Dancing\nWe will now do a fun, instructive experiment. We will “re-acquire” the data by drawing random samples out of Normal distributions parametrized by the maximum likelihood estimates we obtain from the data. (Recall that the MLE for the mean and variance of a Normally distributed random variable is given by the plug-in estimates.) We will then compute the confidence interval and credible region for \\(\\delta\\) and see how they vary from experiment to experiment. We will later repeat this with p-values and odds ratios.\nThe idea here is that if the data are indeed Gaussian distributed, we are looking at data that could plausibly be generated in an identical experiment.\nFirst, we’ll write a function to generate new data and use it to generate 500 new data sets.\n\ndef new_data(mu, sigma, n):\n    \"\"\"Generate new data\"\"\"\n    return np.maximum(np.random.normal(mu, sigma, n), 0.01)\n\n# Values from real data\nmu_wt = np.mean(wt)\nmu_mut = np.mean(mut)\nsigma_wt = np.std(wt, ddof=0)\nsigma_mut = np.std(mut, ddof=0)\n\n# How many new data sets to generate\nn_new_data = 500\n\n# Generate new data\nnew_wt = [new_data(mu_wt, sigma_wt, len(wt)) for _ in range(n_new_data)]\nnew_mut = [new_data(mu_mut, sigma_mut, len(mut)) for _ in range(n_new_data)]\n\nNow we can do the calculations. First, we’ll compute the confidence intervals for \\(\\delta\\).\n\n# Set up arrays for storing results\nconf_int = np.empty((n_new_data, 2))\ndelta = np.empty(n_new_data)\n\n# Do calcs!\nfor i, (wt_data, mut_data) in enumerate(tqdm.tqdm(zip(new_wt, new_mut))):\n    # Compute confidence interval\n    bs_reps = draw_bs_reps_diff_mean(wt_data, mut_data)\n    conf_int[i, :] = np.percentile(bs_reps, (2.5, 97.5))\n\n    # Sample difference of means\n    delta[i] = wt_data.mean() - mut_data.mean()\n\n# Store the results conveniently\ndf_res = pl.DataFrame(\n    schema=[\"conf_low\", \"conf_high\", \"delta\"],\n    data=np.hstack((conf_int, delta.reshape(n_new_data, 1))),\n)\n\n500it [00:02, 198.75it/s]\n\n\nNext, we can do some null hypothesis significance testing. We will compute three p-values, our custom bootstraped p-value with Cohen’s d, the p-value from a permutaiton test, and a p-value from Welch’s t-test. Remember, with our custom bootstrapped p-value, the hypothesis is that the mutant and wild type sleep bout lengths were drawn out of distributions of the same mean (and no other assumptions). The test statistic is Cohen’s d. The hypothesis in the permutation test is that the two data sets are identically distributed. The hypothesis in Welch’s t-test is that the mutant and wild type were drawn from Normal distributions with the same mean, but with difference variances. The test statistic is a t-statistic, defined above.\n\n# Set up arrays for storing results\ncohen_p = np.empty(n_new_data)\nperm_test_p = np.empty(n_new_data)\nwelch_p = np.empty(n_new_data)\n\n\n@numba.jit(nopython=True)\ndef perm_test_t(wt, mut, size=100000):\n    reps = draw_perm_reps_t(wt, mut, size=size)\n    return np.sum(reps &gt;= t_stat(wt, mut)) / len(reps)\n\n\n# Do calcs!\nfor i, (wt_data, mut_data) in enumerate(tqdm.tqdm(zip(new_wt, new_mut))):\n    # Compute p-values\n    cohen_p[i] = cohen_nhst(wt_data, mut_data)\n    perm_test_p[i] = perm_test_t(wt_data, mut_data)\n    welch_p[i] = st.ttest_ind(wt_data, mut_data, equal_var=False)[1] / 2\n\ndf_res = df_res.with_columns(\n    pl.Series(cohen_p).alias(\"cohen_p\"),\n    pl.Series(perm_test_p).alias(\"perm_p\"),\n    pl.Series(welch_p).alias(\"welch_p\"),\n)\n\n500it [00:45, 11.11it/s]\n\n\nFinally, we can compute the Akaike weights for all of our generated data sets.\n\ndf_res = df_res.with_columns(\n    pl.Series(\n        [akaike_weight(wt_data, mut_data) for wt_data, mut_data in zip(new_wt, new_mut)]\n    ).alias(\"akaike_weight\")\n)\n\n\n55.5.1 Dancing confidence intervals(?)\nTo visualize the results, we’ll plot the confidence intervals. We’ll plot the confidence interval as a bar, and then the bounds of the credible region as dots. For ease of viewing, we will only plot 100 of these and will sort them by the plug-in estimate for δ.\n\n# Choose 100 and sort by delta for easy display\ndf_res_sorted = df_res.sample(100).sort(by='delta')\n\n# Set up figure\np = bokeh.plotting.figure(frame_height=250, frame_width=700, y_axis_label=\"δ (min)\")\n\n# Populate glyphs\np.scatter(np.arange(len(df_res_sorted)), df_res_sorted['delta'])\nx_conf = [[i, i] for i in range(len(df_res_sorted))]\ny_conf = [[r[\"conf_low\"], r[\"conf_high\"]] for r in df_res_sorted.iter_rows(named=True)]\np.multi_line(x_conf, y_conf, line_width=2, color=\"#1f77b4\")\n\n# Turn off axis ticks for x\np.xaxis.visible = False\np.xgrid.visible = False\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe confidence interval can vary from experiment to experiment, but not that much. That is, the confidence intervals “dance” around, but all within about a factor of 3 of the original observed values.\n\n\n55.5.2 Dancing: p-values\nNow, let’s look at the p-values and the Akaike weights.\n\np = bokeh.plotting.figure(\n    x_axis_type=\"log\",\n    y_axis_type=\"log\",\n    x_axis_label=\"Welch's p-value\",\n    frame_width=500,\n    frame_height=500,\n    x_range=[1e-6, 1],\n    y_range=[1e-6, 1],\n)\np.scatter(\n    df_res[\"welch_p\"],\n    df_res[\"cohen_p\"],\n    color=\"#1f77b4\",\n    alpha=0.5,\n    legend_label=\"Cohen's d p-value\",\n)\np.scatter(\n    df_res[\"welch_p\"],\n    df_res[\"perm_p\"],\n    color=\"#ffbb78\",\n    alpha=0.5,\n    legend_label=\"permuation test p-value\",\n)\np.scatter(\n    df_res[\"welch_p\"],\n    df_res[\"akaike_weight\"],\n    color=\"#2ca02c\",\n    alpha=0.5,\n    legend_label=\"Akaike weight\",\n)\n\np.legend.location = \"bottom_right\"\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe custom p-value computed with a Cohen’s d test statistic and the permutation test p-value are nearly equal to the Welch’s p-value.\nBut what is really striking here is the scale! Wow! In 500 repeats, we get p-values ranging over four or five orders of magnitude! That’s three exclamations in a row! Four, now. Those exclamation points are there to highlight that the p-value is not a reproducible statistic at all.\nThe Akaike weights are less variable, and more conservative. Not many of them dip below 0.1, and we would be unlikely to select one model against another for most of the values of the Akaike weights we calculated. However, they are still rather variable, and in 500 repeats the can var over many orders of magnitude as well. Though better than the p-values, they are still not terribly reproducible.\nConversely, both confidence intervals don’t really dance much, and p-values and odds ratios dance like this.\n\n\n55.5.3 The effect of sample size on dancing\nThe zebrafish sleep experiment had only about 20 samples, so maybe larger sample sizes will result in less extreme dancing of p-values. Let’s do a numerical experiment to look at that. We will take 15, 20, 50, and 100 samples for our experimental “repeats” and investigate how the p-value varies. For speed, we will only compute the p-value from Welch’s t-test, which we showed previously to track closely with our custom p-values.\n\nn_new_data = 1000\nn_samples = [15, 20, 50, 100]\np_vals = np.empty((n_new_data, len(n_samples)))\nakaike_weights = np.empty((n_new_data, len(n_samples)))\n\n# Do calcs!\nfor i in tqdm.tqdm(range(n_new_data)):\n    for j, n in enumerate(n_samples):\n        # Generate new data\n        new_wt = new_data(mu_wt, sigma_wt, n)\n        new_mut = new_data(mu_mut, sigma_mut, n)\n\n        # Compute p-values and Akaikie weights\n        p_vals[i,j] = st.ttest_ind(new_wt, new_mut, equal_var=False)[1]/2\n        akaike_weights[i, j] = akaike_weight(new_wt, new_mut)\n\n100%|██████████████████████████████████████████████████████| 1000/1000 [00:01&lt;00:00, 704.11it/s]\n\n\nLet’s look at the ECDFs of p-values and Akaike weights.\n\n# Make tidy data frames for convenient plotting\ndf_p = pl.DataFrame(data=p_vals, schema=[\"n = \" + str(n) for n in n_samples])\ndf_p = df_p.unpivot(variable_name=\"n\", value_name=\"p\")\ndf_akaike = pl.DataFrame(\n    data=akaike_weights, schema=[\"n = \" + str(n) for n in n_samples]\n)\ndf_akaike = df_akaike.unpivot(variable_name=\"n\", value_name=\"akaike_weight\")\n\n# Make plots\np1 = iqplot.ecdf(\n    df_p,\n    cats=[\"n\"],\n    q=\"p\",\n    x_axis_label=\"p-value\",\n    x_axis_type=\"log\",\n    order=[\"n = 15\", \"n = 20\", \"n = 50\", \"n = 100\"],\n    frame_width=500,\n    frame_height=150,\n    palette=bokeh.palettes.d3[\"Category20c\"][4],\n)\np2 = iqplot.ecdf(\n    df_akaike,\n    cats=[\"n\"],\n    q=\"akaike_weight\",\n    order=[\"n = 15\", \"n = 20\", \"n = 50\", \"n = 100\"],\n    x_axis_label=\"Akaike weight\",\n    x_axis_type=\"log\",\n    frame_width=500,\n    frame_height=150,\n    palette=bokeh.palettes.d3[\"Category20c\"][8][4:],\n)\np1.legend.location = \"top_left\"\np2.legend.location = \"top_left\"\np1.x_range = p2.x_range\n\nbokeh.io.show(bokeh.layouts.column(p1, p2))\n\n\n  \n\n\n\n\n\nWe see that even though the p-value and Akaike weight have large spreads as the number of samples increases, they also shift leftward. This is because small differences in samples can be discerned with large sample sizes. But notice that the p-value and the Akaike weight varies over orders of magnitude for similar data set.",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Dancing statistics</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/dancing.html#conclusion",
    "href": "lessons/statistical_watchouts/dancing.html#conclusion",
    "title": "55  Dancing statistics",
    "section": "55.6 Conclusion",
    "text": "55.6 Conclusion\nThis little exercise in reproducibility tells use that because the p-values “dance”, and to a lesser extent so do the Akaike weights, we had better be sure the dancefloor is far to the left. This suggests large \\(n\\) is needed.\nI would argue that you should do a similar “dancing” analysis of your data sets when you have a reasonable generative model in mind so that you can decide what constitutes a small p-value of Akaike weight.",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Dancing statistics</span>"
    ]
  },
  {
    "objectID": "lessons/statistical_watchouts/dancing.html#computing-environment",
    "href": "lessons/statistical_watchouts/dancing.html#computing-environment",
    "title": "55  Dancing statistics",
    "section": "55.7 Computing environment",
    "text": "55.7 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,polars,numba,tqdm,bokeh,iqplot,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\nscipy     : 1.16.0\npolars    : 1.31.0\nnumba     : 0.61.2\ntqdm      : 4.67.1\nbokeh     : 3.7.3\niqplot    : 0.3.7\nbebi103   : 0.1.28\njupyterlab: 4.4.5",
    "crumbs": [
      "Statistical watchouts",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Dancing statistics</span>"
    ]
  },
  {
    "objectID": "homework/plotting_an_elephant.html",
    "href": "homework/plotting_an_elephant.html",
    "title": "HW 1.1: Plotting an elephant",
    "section": "",
    "text": "This problem is worth 30 points.\nIn this problem, we will practice skills with manipulating Numpy arrays and making plots with a fun example.\na) Read this little gem of an article. It has a good lesson, but it is important to note, as you will learn if you take part (b) of this course, a model is not automatically invalid because it has a lot of parameters. More important are what the parameters are and to what physical quantities they relate. Nonetheless, it is often desirable to have simpler models for many reasons, including interpretability.\nb) Based on the anecdote about John von Neumann, Mayer, Khairy, and Howard worked out a scheme to draw an elephant with four complex parameters. The complex parameters are\n\\[\\begin{align}\n&p_1 = -60 - 12 i,\\\\[1em]\n&p_2 = -30 + 14 i,\\\\[1em]\n&p_3 = 8 - 50 i,\\\\[1em]\n&p_4 = -10 - 18 i.\n\\end{align}\\]\nFor notational ease, let \\(r_j\\) be the real part of parameter \\(j\\) and \\(i_j\\) by the imaginary part of parameter \\(j\\). For example, \\(r_1 = -60\\) and \\(i_3 = -50\\).\nUsing these parameters, Mayer, Khairy, and Howard worked out a parametric curve for the shape of an elephant based on a truncated Fourier series. You can write the \\(x\\) and \\(y\\) values of a smooth parametric curve as a Fourier series as\n\\[\\begin{align}\nx(t) = A_{0,x} + \\sum_{k = 1}^\\infty A_{k,x} \\cos kt + \\sum_{k = 1}^\\infty B_{k,x} \\sin kt, \\\\[1em]\ny(t) = A_{0,y} + \\sum_{k = 1}^\\infty A_{k,y} \\cos kt + \\sum_{k = 1}^\\infty B_{k,y} \\sin kt,\n\\end{align}\\]\nwhere \\(t\\) ranges from zero to \\(2\\pi\\). Mayer, Khairy, and Howard worked out that you can get an elephant using\n\\[\\begin{align}\n&A_{1,x} = r_1,\\;B_{1,x} = r_2, \\; B_{2,x} = r_3, \\; B_{3,x} = r_4,\\\\[1em]\n&A_{3,y} = i_1,\\;A_{5,y} = i_2, \\; B_{1,y} = i_3, \\; B_{2,y} = i_4,\n\\end{align}\\]\nwith all other Fourier coefficients being zero.\nCompute a smooth curve for an elephant using this formula and plot it. (This problem is not meant to teach you about Fourier series, but rather to practice creating and manipulating Numpy arrays and making plots.)\nc) For fun, you can make a little scene for your elephant by adding other glyphs to the plot. You can read Bokeh’s documentation to learn about what you can do. You may wish to investigate patches and box annotations, among others. (This part of the problem is not graded.)",
    "crumbs": [
      "Homework",
      "HW 1.1: Plotting an elephant"
    ]
  },
  {
    "objectID": "homework/penguins_split_apply_combine.html",
    "href": "homework/penguins_split_apply_combine.html",
    "title": "HW 1.2: Palmer penguins and split-apply-combine",
    "section": "",
    "text": "This problem is worth 30 points.\nData set download\n\nThe Palmer penguins data set is a nice data set with which to practice various data science skills. For this exercise, we will use as subset of it, which you can download here: https://s3.amazonaws.com/bebi103.caltech.edu/data/penguins_subset.csv. The data set consists of measurements of three different species of penguins acquired at the Palmer Station in Antarctica. The measurements were made between 2007 and 2009 by Kristen Gorman.\na) Take a look at the CSV file containing the data set. Is it in tidy format? Why or why not?\nb) You can convert the CSV file to a “tall” format using the bebi103.utils.unpivot_csv() function. You can do that with the following function call, where path_to_penguins is a string containing the path to the penguin_subset.csv file.\nbebi103.utils.unpivot_csv(\n    path_to_penguins,\n    \"penguins_tall.csv\",\n    n_header_rows=2,\n    header_names=[\"species\", \"quantity\"],\n    comment_prefix=\"#\",\n    retain_row_index=True,\n    row_index_name='penguin_id',\n)    \nAfter running that function, load in the data set stored in the penguins_tall.csv file and store it in a variable named df_tall. Is this a tidy data set?\nc) Perform the following operations to make a new DataFrame from the one you loaded in to generate a new DataFrame. You do not need to worry about what these operations do (that is the topic of next week, just do them to answer this question):\ndf = (\n    df_tall\n    .pivot(\n        index=['penguin_id', 'species'], on='quantity', values='value'\n    )\n    .select(pl.exclude('penguin_id'))\n)\nIs the resulting data frame df tidy? Why or why not?\nd) Using the data frame you created in part (c), slice out all of the bill lengths for Gentoo penguins.\ne) Make a new data frame containing the mean measured bill depth, bill length, body mass in kg, and flipper length for each species. You can use millimeters for all length measurements.\nf) Make a scatter plot of bill length versus flipper length with the glyphs colored by species.",
    "crumbs": [
      "Homework",
      "HW 1.2: Palmer penguins and split-apply-combine"
    ]
  },
  {
    "objectID": "homework/mt_ecdf.html",
    "href": "homework/mt_ecdf.html",
    "title": "HW 2.1: Microtubule catastrophe and ECDFs",
    "section": "",
    "text": "This problem is a team problem.\nThis problem is worth 35 points.\nData set download\n\nAn ECDF evaluated at point x is defined as\nECDF(x) = fraction of data points ≤ x.\nThe ECDF is defined on the entire real number line, with \\(\\mathrm{ECDF}(x\\to-\\infty) = 0\\) and \\(\\mathrm{ECDF}(x\\to\\infty) = 1\\). However, the ECDF is often plotted as discrete points, \\(\\{(x_i, y_i)\\}\\), where for point \\(i\\), \\(x_i\\) is the value of the measured quantity and \\(y_i\\) is \\(\\mathrm{ECDF}(x_i)\\). For example, if I have a set of measured data with values (1.1, –6.7, 2.3, 9.8, 2.3), the points on the ECDF plot are\n\n\n\nx\ny\n\n\n\n\n–6.7\n0.2\n\n\n1.1\n0.4\n\n\n2.3\n0.6\n\n\n2.3\n0.8\n\n\n9.8\n1.0\n\n\n\nIn this problem, you will use you newly acquired skills using Numpy and Bokeh to compute ECDFs from a real data set and plot them.\nGardner, Zanic, and coworkers investigated the dynamics of microtubule catastrophe, the switching of a microtubule from a growing to a shrinking state. In particular, they were interested in the time between the start of growth of a microtubule and the catastrophe event. They monitored microtubules by using tubulin (the monomer that comprises a microtubule) that was labeled with a fluorescent marker. As a control to make sure that fluorescent labels and exposure to laser light did not affect the microtubule dynamics, they performed a similar experiment using differential interference contrast (DIC) microscopy. They measured the time until catastrophe with labeled and unlabeled tubulin.\nWe will look at the data used to generate Fig. 2a of their paper. In the end, you will generate a plot similar to that figure.\na) Write a function with the call signature ecdfvals(data), which takes a one-dimensional Numpy array (or Polars Series; the same construction of your function should work for both) of data and returns the x and y values for plotting the ECDF in the “dots” style, as in Fig. 2a of the Gardner, Zanic, et al. paper. As a reminder,\n\nECDF(x) = fraction of data points ≤ x.\n\nWhen you write this function, you may only use base Python and the standard library, in addition to Numpy and Polars.\nb) Use the ecdfvals() function that you wrote to plot the ECDFs shown in Fig. 2a of the Gardner, Zanic, et al. paper. By looking this plot, do you think that the fluorescent labeling makes a difference in the onset of catastrophe? (We will do a more careful statistical inference later in the course, but for now, does it pass the eye test? Eye tests are an important part of EDA, but certainly not an end point.) You can access the data set here: https://s3.amazonaws.com/bebi103.caltech.edu/data/gardner_time_to_catastrophe_dic_tidy.csv",
    "crumbs": [
      "Homework",
      "HW 2.1: Microtubule catastrophe and ECDFs"
    ]
  },
  {
    "objectID": "homework/cool_gal4.html",
    "href": "homework/cool_gal4.html",
    "title": "HW 2.2: EDA for a temperature controlled Gal4-UAS system",
    "section": "",
    "text": "This problem is worth 65 points.\nDataset download\n\nOne of my former students, Han Wang, published (Wang, H., …, Sternberg, P.W. (2017). cGAL, a temperature-robust GAL4-UAS system for Caenorhabditis elegans, Nat. Methods, 14(2), 145-148) an improved Gal4/UAS system in C. elegans. Briefly, the Gal4-UAS system was hijacked from budding yeast and incorporated into the genomes of other organisms, Drosophila being the first. The idea is to insert the Gal4 gene into the genome such that it is under control of a driver that is native to the organism. When Gal4 is expressed, it binds to UAS (upstream activation sequence) and it is activating, leading to expression of the UAS target gene.\nIn the paper, Han used the system with UAS activating production of green fluorescent protein (GFP). The Gal4 production is driven by Pmyo-2, which is only expressed in the pharynx of the worm.\nThe Gal4/UAS system typically works only at high temperatures. This does not work as well in worms that are stored at lower temperatures. Han therefore engineered a “cool” Gal4, which works at lower temperatures. To test how the new system worked, he measured the GFP fluorescence signal in the pharynx of worms.\nHe generously donated his data set for us to work with. He sent me a MS Excel file with the data, along with some comments via email. Here is what he said in that email about the data set (verbatim):\n\nSC (orignal Gal4)\nSK (cool Gal4)\nm3 Pmyo-2::GFP fusion (control; measure of driver expression)\n15 20 and 25 at the end for the name of each column shows the experimental temperature.\n\nYou can download the MS Excel file here: https://s3.amazonaws.com/bebi103.caltech.edu/data/wang_cool_gal4.xlsx.\na) Load and tidy the DataFrame. Be sure to remove any NaNs.\nb) Do some exploratory data analysis of the data set. That is, make some instructive plots. Discuss why you chose to visualize the data set the way(s) you did. What can you say about Han’s cool Gal4 just by looking at the plots?",
    "crumbs": [
      "Homework",
      "HW 2.2: EDA for a temperature controlled Gal4-UAS system"
    ]
  },
  {
    "objectID": "homework/beetle_hypnotists.html",
    "href": "homework/beetle_hypnotists.html",
    "title": "HW 3.1: Beetle hypnotists",
    "section": "",
    "text": "This problem is worth 70 points.\nData set download\n\nThe Parker lab at Caltech studies rove beetles that can infiltrate ant colonies. In one of their experiments, they place a rove beetle and an ant in a circular area and track the movements of the ants. They do this by using a deep learning algorithm to identify the head, thorax, abdomen, and right and left antennae. While deep learning applied to biological images is a beautiful and useful topic, we will not cover it in this course (be on the lookout for future courses that do!). We will instead work with a data set that is the output of the deep learning algorithm.\nFor the experiment you are considering in this problem, an ant and a beetle were placed in a circular arena and recorded with video at a frame rate of 28 frames per second. The positions of the body parts of the ant were tracked throughout the video recording. You can download the data set here. Hint: As of October 2025, Polars does not natively open ZIP files. In order to open them, you can use built-in utilities in the Python standard library and send bytes directly to pl.read_csv(). For this file, it would look something like this:\nimport io, os, zipfile\nwith zipfile.ZipFile(os.path.join(data_path, 'ant_joint_locations.zip')) as fz:\n    with fz.open('ant_joint_locations.csv') as f:\n        df = pl.read_csv(io.BytesIO(f.read()), comment_prefix='#')\nTo save you from having to unzip and read the comments for the data file, here they are:\n# This data set was kindly donated by Julian Wagner from Joe Parker's lab at \n# Caltech. In the experiment, an ant and a beetle were placed in a circular\n# arena and recorded with video at a frame rate of 28 frames per second. \n# The positions of the body parts the ant are tracked throughout the video\n# recording.\n#\n# The experiment aims to distinguish the ant behavior in the presence of\n# a beetle from the genus Sceptobius, which secretes a chemical that modifies\n# the behavior of the ant, versus in the presence of a beetle from the species\n# Dalotia, which does not.\n#\n# The data set has the following columns.\n#  frame : frame number from the video acquisition\n#  beetle_treatment : Either dalotia or sceptobius\n#  ID : The unique integer identifier of the ant in the experiment\n#  bodypart : The body part being tracked in the experiment. Possible values\n#             are head, thorax, abdomen, antenna_left, antenna_right.\n#  x_coord : x-coordinate of the body part in units of pixels\n#  y_coord : y-coordinate of the body part in units of pixels\n#  likelihood : A rating, ranging from zero to one, given by the deep learning\n#               algorithm that approximately quantifies confidence that the\n#               body part was correctly identified.\n#\n# The interpixel distance for this experiment was 0.8 millimeters.\nYour task in this problem is to extract records of interest out of the tidy data frame containing the data from the experiment, perform calculations on the data, and make informative plots.\na) The columns x_coord and y_coord give the coordinates of the ant’s body parts in units of pixels. Create a column 'x (mm)' and a column 'y (mm)' in the data frame that has the coordinates in units of millimeters. Also create a column 'time (sec)' that gives the time since recording started in seconds.\nb) Make a plot displaying the position over time of the thorax of an ant or ants placed in an arena with a Dalotia beetle and position over time of an ant or ants with a Sceptobius beetle. I am intentionally not giving more specification for your plot. You need to make decisions about how to effectively extract and display the data. Think carefully about your visualizations. This is in many ways how you let your data speak.\nc) From this quick graphical exploratory analysis, what would you say about the relative activities of ants with Dalotia versus Sceptobius rove beetles?",
    "crumbs": [
      "Homework",
      "HW 3.1: Beetle hypnotists"
    ]
  },
  {
    "objectID": "homework/marginal_binomial.html",
    "href": "homework/marginal_binomial.html",
    "title": "HW 3.2: A marginal distribution",
    "section": "",
    "text": "This problem is worth 30 points.\n\nYou may submit this problem as a PDF, either with clearly typed mathematics or with neat handwriting, if you like.\nSay we are doing an experiment measuring the sex of the progeny of fruit flies. We randomly select one male fruit fly and one female fruit fly. We then mate them, select \\(N\\) of their progeny, and count the number \\(n\\) of males. We do this experiment many times to get an idea about the distribution describing \\(n\\).\na) We initially choose a Binomial distribution for \\(n\\). That is,\n\\[\\begin{align}\nn \\sim \\text{Binom}(N, \\theta).\n\\end{align}\\]\nHere, \\(\\theta\\) is the probability that offspring from flies is male. Explain why this is a reasonable model.\nb) Upon analyzing the data using some of the techniques we will learn later in the class, we decide we need to update the model. We suspect that each male-female pair of flies may not have the same propensity for male offspring. Therefore, we decide not to take \\(\\theta\\) as a fixed parameter for all pairs of flies, but rather to model \\(\\theta\\) as coming from its own Beta distribution. Our new model is\n\\[\\begin{align}\n&\\theta \\sim \\text{Beta}(\\alpha, \\beta),\\\\[1em]\n&n \\sim \\text{Binom}(N, \\theta).\n\\end{align}\\]\nWrite down the joint probability density/mass function, \\(f(n, \\theta ; N, \\alpha, \\beta)\\).\nc) Both \\(n\\) and \\(\\theta\\) are random variables (they vary meaningfully from experiment to experiment), but we can only observe \\(n\\). We therefore we want to know the probability mass function for \\(n\\). Show that\n\\[\\begin{align}\nf(n ; N,\\alpha, \\beta) = \\binom{N}{n}\\,\\frac{B(n+\\alpha, N-n+\\beta)}{B(\\alpha, \\beta)}.\n\\end{align}\\]\nwhere \\(B(x, y)\\) denotes a Beta function.\nd) As you may note in the Distribution Explorer, the Beta distribution may also be parametrized with \\(\\phi = \\alpha / (\\alpha + \\beta)\\) and \\(\\kappa = \\alpha + \\beta\\). Show that in the limit of \\(\\kappa \\to \\infty\\),\n\\[\\begin{align}\nf(n ; N, \\phi) = \\binom{N}{n}\\,\\phi^n\\,(1-\\phi)^{N-n},\n\\end{align}\\]\nwhich is again a Binomial PMF. You do not need to explictly take any limits or perform any integrals. You can use stories.",
    "crumbs": [
      "Homework",
      "HW 3.2: A marginal distribution"
    ]
  },
  {
    "objectID": "homework/normal_approximations.html",
    "href": "homework/normal_approximations.html",
    "title": "HW 4.1: Normal approximations",
    "section": "",
    "text": "This problem is worth 40 points.\n\na) Imagine I have a univariate continuous distribution with PDF \\(f(y)\\) that has a maximum at \\(y^*\\). Assume that the first and second derivatives of \\(f(y)\\) are defined and continuous near \\(y^*\\). Show by expanding the log PDF of this distribution in a Taylor series about \\(y^*\\) that the distribution is locally Normal near the maximum.\nIn performing the Taylor series, how is the scale parameter \\(\\sigma\\) of the Normal approximation of the distribution related to the log PDF of the distribution it is approximating?\nb) Another way you can approximate a distribution as Normal is to use its mean and variance as the parameters as the approximate Normal. We will call this technique “equating moments.” Can you do this if the distribution you are approximating has heavy tails, say like a Cauchy distribution? Why or why not?\nc) Make plots of the PDF and CDF of the following distributions with their Normal approximations as derived from the Taylor series and by equating moments. Do you have any comments about the approximations?\n\n\nBeta with α = β = 10\nGamma with α = 5 and β = 2\n\nd) Discrete distributions are also often approximated as Normal. In fact, early studies of the Normal distributions arose from it being used to approximate a Binomial distribution. Use the method of equating moments to make a plot of the PMF of the Binomial distribution and the PDF of the Normal approximation of the Binomial distribution for:\n\nBinomial with N = 100 and θ = 0.1.\nBinomial with N = 10 and θ = 0.1.\n\nComment on what you see.",
    "crumbs": [
      "Homework",
      "HW 4.1: Normal approximations"
    ]
  },
  {
    "objectID": "homework/simulating_distributions.html",
    "href": "homework/simulating_distributions.html",
    "title": "HW 4.2: Simulating distributions",
    "section": "",
    "text": "This problem is a team problem.\nThis problem is worth 60 points.\nIn a previous problem, you worked with a data set of measured times for microtubule catastrophe. In this problem, we will develop a model for microtubule catastrophe.\na) In the Gardner, Zanic, et al. paper, the authors assumed that the microtubule catatrophe times are Gamma distributed. Discuss how the story behind the Gamma distribution might work for modeling microtubule catastrophe.\nb) As an alternative model, we assert that two biochemical processes have to happen in succession to trigger catastrophe. That is, the first process happens, and only after the first process happens can the second one happen. We model each of the two process as a Poisson process (as is very often done with (bio)chemical dynamics). The rate of arrivals for the first one is \\(\\beta_1\\) and the rate of arrivals for the second one is \\(\\beta_2\\).\nIn a typical experiment, Gardner and Zanic measured about 150 catastrophe events. Use random number generation to simulate one of these experiments with this successive Poisson process model and plot the ECDF of times to catastrophe. That is, generate 150 random numbers that are distributed according to the story of the model. You are simulating the story to do this. You can plot the time axis of the ECDF in units of \\(\\beta_1^{-1}\\). Do this for several values of \\(\\beta_2/\\beta_1\\).\nc) By using random number generation, you have shown how you might expect the experimental results to be distributed. You can derive the distribution for the time \\(t\\) to catastrophe analytically. Show analytically that the PDF of the distribution matching this story is\n\\[\\begin{align}\nf(t;\\beta_1, \\beta_2) = \\frac{\\beta_1 \\beta_2}{\\beta_2 - \\beta_1}\\left(\\mathrm{e}^{-\\beta_1 t} - \\mathrm{e}^{-\\beta_2 t}\\right)\n\\end{align}\\]\nfor \\(\\beta_1 \\ne \\beta_2\\). The CDF for this distribution is\n\\[\\begin{align}\nF(t; \\beta_1, \\beta_2) =\n\\frac{\\beta_1 \\beta_2}{\\beta_2-\\beta_1}\\left[\n\\frac{1}{\\beta_1}\\left(1-\\mathrm{e}^{- \\beta_1 t}\\right)- \\frac{1}{\\beta_2}\\left(1-\\mathrm{e}^{-\\beta_2 t}\\right)\n\\right].\n\\end{align}\\]\nOverlay this analytical CDF with an ECDF from your simulation to verify that they match.\nd) Without formally doing any integrals, computing any derivatives, or taking limits, show that the PDF of the distribution for \\(\\beta_1 = \\beta_2 \\equiv \\beta\\) is\n\\[\\begin{align}\nf(t;\\beta) = \\beta^2\\,t\\,\\mathrm{e}^{-\\beta t}.\n\\end{align}\\]",
    "crumbs": [
      "Homework",
      "HW 4.2: Simulating distributions"
    ]
  },
  {
    "objectID": "appendices/notation.html",
    "href": "appendices/notation.html",
    "title": "Appendix A — Notation",
    "section": "",
    "text": "Below are mathematical notational rules used throughout the course.\n\nScalar quantities as denoted as italicized symbols, such as \\(x\\), \\(y\\), \\(\\mu\\), and \\(\\sigma\\).\nVector quantities (first-rank tensors) are denoted in bold, such as \\(\\mathbf{x}\\), \\(\\mathbf{y}\\), \\(\\boldsymbol{\\mu}\\), and \\(\\boldsymbol{\\sigma}\\).\nMatrix quantities (second-rank tensors) are denoted with sans serif capital letters, such as \\(\\mathsf{A}\\), \\(\\mathsf{W}\\), and \\(\\mathsf{\\sigma}\\).\nThe one exception to the boldface and sans serif convention is when we denote a generic set of data or parameters. In that case, we use standard italicized symbols like \\(\\theta\\) (typically for a set of parameters) or \\(z\\) (typically for a set of latent variables).\nSubscripts typically denote an element of a vector, such as \\(x_i\\), or an element of a matrix, such as \\(A_{ij}\\). They can also denote an entry in a non-ordered collection, such as \\(M_i\\).\nTransposes are denoted with a superscript \\(\\mathsf{T}\\).\nVector dot products result in a scalar and are denoted with a dot, such as \\(\\mathbf{x}\\cdot\\mathbf{y}\\). Note that this is denoted as \\(\\mathbf{x}^\\mathsf{T}\\mathbf{x}\\) in some texts, but we will not use that notation. Writing out the sum, this is\n\n\\[\\begin{aligned}\n\\mathbf{x}\\cdot\\mathbf{y} = \\sum_{i}x_i\\, y_i.\n\\end{aligned}\n\\tag{A.1}\\]\n\nMatrix-vector products result in a vector are also denoted with a dot, such as \\(\\mathsf{A}\\cdot\\mathbf{x}\\). Writing out the sum, this is\n\n\\[\\begin{aligned}\n\\mathsf{A}\\cdot\\mathbf{x} = \\begin{pmatrix}\\sum_{i}A_{i1} x_i \\\\ \\sum_{i}A_{i2} x_i \\\\ \\vdots \\end{pmatrix}\n\\end{aligned}\n\\tag{A.2}\\]\n\nMatrix-matrix multiplication results in a matrix and is also denoted with a dot, such as \\(\\mathsf{A}\\cdot\\mathsf{B}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/setting_up_python_computing_environment.html",
    "href": "appendices/python_basics/setting_up_python_computing_environment.html",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "",
    "text": "B.1 Why Python?\n| Download notebook\nThere are plenty of programming languages that are widely used in data science and in scientific computing more generally. Some of these, in addition to Python, are Matlab/Octave, Mathematica, R, Julia, Java, JavaScript, Rust, and C++.\nI have chosen to use Python. I believe language wars are counterproductive and welcome anyone to port the code we use to any language of their choice, I nonetheless feel we should explain this choice.\nPython is a flexible programming language that is widely used in many applications. This is in contrast to more domain-specific languages like R and Julia. It is easily extendable, which is in many ways responsible for its breadth of use. We find that there is a decent Python-based tool for many applications we can dream up, certainly in data science. However, the Python-based tool is often not the very best for the particular task at hand, but it is almost always pretty good. Thus, knowing Python is like having a Swiss Army knife; you can wield it to effectively accomplish myriad tasks. Finally, we also find that it has a shallow learning curve with most students.\nFurthermore, Python is widely used in machine learning and AI. The development of packages like TensorFlow, PyTorch, JAX, Keras, and scikit-learn have led to very widespread adoption of Python.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/setting_up_python_computing_environment.html#jupyter-notebooks",
    "href": "appendices/python_basics/setting_up_python_computing_environment.html#jupyter-notebooks",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.2 Jupyter notebooks",
    "text": "B.2 Jupyter notebooks\nThe materials of this course are constructed from Jupyter notebooks. To quote Jupyter’s documentation,\n\nJupyter Notebook and its flexible interface extends the notebook beyond code to visualization, multimedia, collaboration, and more. In addition to running your code, it stores code and output, together with markdown notes, in an editable document called a notebook.\n\nThis allows for executable documents that have code, but also richly formatted text and graphics, enabling the reader to interact with the material as they read it.\nSpecifically, notebooks are comprised of cells, where each cell contains either executable Python code or text.\nWhile you read the materials, you can read the HTML-rendered versions of the notebooks. To execute (and even edit!) code in the notebooks, you will need to run them. There are many options available to run Jupyter notebooks. Here are a few we have found useful.\n\nJupyterLab: This is a browser-based interface to Jupyter notebooks and more (including a terminal application, text editor, file manager, etc.). As of March 2025, Chrome, Firefox, Safari, and Edge are supported.\nVSCode: This is an excellent source code editor that supports Jupyter notebooks. Be sure to read the documentation on how to use Jupyter notebooks in VSCode. This may be an especially good option for Windows users.\nGoogle Colab: Google offers this service to run notebooks in the cloud on their machines. There are a few caveats, though. First, not all packages and updates are available in Colab. Furthermore, not all interactivity that will work natively in Jupyter notebooks works with Colab. If a notebook sits idle for too long, you will be disconnected from Colab. Finally, there is a limit to resources that are available for free, and as of March 2025, that limit is unpublished and can vary. All of the notebooks in the HTML rendering of this book have an “Open in Colab” button at the upper right that allows you to launch the notebook in Colab. This is a quick-and-easy way to execute the book’s contents.\n\nFor our work in this programming bootcamp, I encourage you to use either JupyterLab in the browser or VSCode, with Colab as a backup if you’re having trouble.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/setting_up_python_computing_environment.html#marimo",
    "href": "appendices/python_basics/setting_up_python_computing_environment.html#marimo",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.3 Marimo",
    "text": "B.3 Marimo\nMarimo offers a very nice notebook interface that is a departure from Jupyter notebooks in its structure. The biggest departure is that Marimo notebooks are specifically for Python, as opposed to being language agnostic like Jupyter. As a result, Marimo notebooks can offer many features not seen in Jupyter notebooks (without add-ons). The two most compelling, at least to me, are\n\nMarimo notebooks are simple .py files which allow for easier version control and simple execution as scripts.\nMarimo notebooks are reactive, meaning that the ordering of the cells is irrelevant and the notebook runs all cells that need to be rerun as a result of a change of value of a variable in any given cell.\n\nIn the course, we will use Jupyter notebooks, but you are welcome to play with Marimo notebooks. Upon completing the installation instructions in this notebook, Marimo will be installed.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/setting_up_python_computing_environment.html#installing-python-tools",
    "href": "appendices/python_basics/setting_up_python_computing_environment.html#installing-python-tools",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.4 Installing Python tools",
    "text": "B.4 Installing Python tools\nPrior to embarking on your journey into data analysis, you need to have a functioning Python distribution installed on your computer. We will use pixi, a relatively new package manager that I have found very effective.\nImportantly, it does so in a project-based way. That is, for each project, you use Pixi to create and manage the packages needed for that project. Our “project” here is our course!\nPixi is a package management tool that allows installation of packages. Importantly, it does so in a project-based way. That is, for each project, you use Pixi to create and manage the packages needed for that project. Our “project” here is our data analysis/statistical inference course.\nStep 1: Install Pixi. To install Pixi, you need access to the command line. For macOS users, hit Command-space, type in “terminal” and open the Terminal app. In Windows, open PowerShell by opening the Start Menu, typing “PowerShell” in the search bar, and selecting “Windows PowerShell.” I assume you know how to get access to the command line if you are using Linux.\nOn the command line, do the following.\nmacOS or Linux\ncurl -fsSL https://pixi.sh/install.sh | sh\nWindows\npowershell -ExecutionPolicy ByPass -c \"irm -useb https://pixi.sh/install.ps1 | iex\"\nStep 2: Create a directory for your work in the course. You might want to name the directory bebi103a/, which is what I have named it. You can do this either with the command line of your graphical file management program (e.g., Finder for macOS).\nStep 3 Navigate to the directory you created on the command line. For example, if the directory is bebi103/ in your home directory and you are in your home directory, you can do\ncd bebi103\non the command line.\nStep 4 Download the requisite Pixi files: pixi.toml, pixi.lock. These files need to be stored in the directory you created in step 3. You may download them by right-clicking those links, or by doing the following on the command line.\nmacOS or Linux\ncurl -fsSL https://raw.githubusercontent.com/bebi103a/bebi103a.github.io/refs/heads/main/pixi.toml -o ./pixi.toml\ncurl -fsSL https://raw.githubusercontent.com/bebi103a/bebi103a.github.io/refs/heads/main/pixi.lock -o ./pixi.lock\nWindows\nirm -useb https://raw.githubusercontent.com/bebi103a/bebi103a.github.io/refs/heads/main/pixi.toml -OutFile pixi.toml\n\nirm -useb https://raw.githubusercontent.com/bebi103a/bebi103a.github.io/refs/heads/main/pixi.lock -OutFile pixi.lock\nStep 5 Install the environment! Do the following on the command line.\npixi install\nStep 6 To be able to use all of the packages, you need to invoke a Pixi shell. To do so, execute the following on the command line.\npixi shell\nYou are now good to go! After you are done working, to exit the Pixi shell, hit Control-D.\nFor doing work for this class, you will need to cd into the directory you created in step 2 and execute pixi shell every time you open a new terminal (or PowerShell) window.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/setting_up_python_computing_environment.html#launching-jupyterlab",
    "href": "appendices/python_basics/setting_up_python_computing_environment.html#launching-jupyterlab",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.5 Launching JupyterLab",
    "text": "B.5 Launching JupyterLab\nOnce you have invoked a Pixi shell, you can launch JupyterLab via your operating system’s terminal program (Terminal on macOS and PowerShell on Windows). To do so, enter the following on the command line (after having run pixi shell).\njupyter lab\nYou will have an instance of JupyterLab running in your default browser. If you want to specify the browser, you can, for example, type\njupyter lab --browser=firefox\non the command line.\nAlternatively, if you are using VSCode, you can use its menu system to open .ipynb files. Make sure you select the Python kernel corresponding to your environment. You can read the documentation here. Hint: You may need to restart VSCode after doing the above installations so it is aware of your pixi environment.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/setting_up_python_computing_environment.html#checking-your-distribution",
    "href": "appendices/python_basics/setting_up_python_computing_environment.html#checking-your-distribution",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.6 Checking your distribution",
    "text": "B.6 Checking your distribution\nLet’s now run a quick test to make sure things are working properly. We will make a quick plot that requires some of the scientific libraries we will use.\nLaunch a Jupyter notebook in JupyterLab. In the first cell (the box next to the [ ]: prompt), paste the code below. To run the code, press Shift+Enter while the cursor is active inside the cell. You should see a plot that looks like the one below. If you do, you have a functioning Python environment for scientific computing!\n\nimport numpy as np\nimport bokeh.plotting\nimport bokeh.io\n\nbokeh.io.output_notebook()\n\n# Generate plotting values\nt = np.linspace(0, 2*np.pi, 200)\nx = 16 * np.sin(t)**3\ny = 13 * np.cos(t) - 5 * np.cos(2*t) - 2 * np.cos(3*t) - np.cos(4*t)\n\np = bokeh.plotting.figure(height=250, width=275)\np.line(x, y, color='red', line_width=3)\ntext = bokeh.models.Label(x=0, y=0, text='BE/Bi 103 a', text_align='center')\np.add_layout(text)\n\nbokeh.io.show(p)\n\n    \n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/setting_up_python_computing_environment.html#computing-environment",
    "href": "appendices/python_basics/setting_up_python_computing_environment.html#computing-environment",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "Computing environment",
    "text": "Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\nbokeh     : 3.7.3\njupyterlab: 4.4.5",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_jupyterlab.html",
    "href": "appendices/notebooks_and_latex/intro_to_jupyterlab.html",
    "title": "Appendix C — Introduction to JupyterLab",
    "section": "",
    "text": "C.1 The Python interpreter\n| Download notebook\nIn this lesson, you will learn about different ways of interacting with the Python interpreter and importantly the basics on how to use JupyterLab. All of your homework will be submitted as Jupyter notebooks, so this is something you will need to master. It will be useful for you to go over the intro to LaTeX to learn how to use \\(\\LaTeX\\) in your Jupyter notebooks.\nYou should, of course, read the official JupyterLab documentation as well.\nWe will start by introducing the Python interpreter.\nBefore diving into the Python interpreter, I pause here to remind you that this course is not meant to teach Python syntax (though you will learn that). The things you learn here are meant to help you understand how to use your computer for data analysis more generally. Think of it this way: part of the mission of this course is to help you unleash the power of your computer on your biological problems. Python is just the language of instruction. That said, let’s start talking about how Python works.\nPython is an interpreted language, which means that each line of code you write is translated, or interpreted, into a set of instructions that your machine can understand by the Python interpreter. This stands in contrast to compiled languages. For these languages (the dominant ones being Fortran, C, and C++), your entire code is translated into machine language before you ever run it. When you execute your program, it is already in machine language.\nSo, whenever you want your Python code to run, you give it to the Python interpreter.\nThere are many ways to launch the Python interpreter. One way is to type\non the command line. This launches the vanilla Python interpreter. We will never really use this in the class. Rather, we will have a greatly enhanced Python experience, either using IPython, a feature-rich, enhanced interactive Python available through JupyterLab’s console, or using a notebook, also launchable in JupyterLab.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to JupyterLab</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#the-python-interpreter",
    "href": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#the-python-interpreter",
    "title": "Appendix C — Introduction to JupyterLab",
    "section": "",
    "text": "python",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to JupyterLab</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#hello-world.-and-the-print-function",
    "href": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#hello-world.-and-the-print-function",
    "title": "Appendix C — Introduction to JupyterLab",
    "section": "C.2 “Hello, world.” and the print() function",
    "text": "C.2 “Hello, world.” and the print() function\nTraditionally, the first program anyone writes when learning a new language is called “Hello, world.” In this program, the words “Hello, world.” are printed on the screen. The original Hello, world. was likely written by Brian Kernighan, one of the inventors of Unix, and the author of the classic and authoritative book on the C programming language. In his original, the printed text was “hello, world” (no period nor capital H), but people use lots of variants.\nWe will first write and run this little program using a JupyterLab console. After launching JupyterLab, you probably already have the Launcher in your JupyterLab window. If you do not, you can expand the Files tab at the left of your JupyterLab window (if it is not already expanded) by clicking on that tab, or alternatively hit ctrl+b (or cmd+b on macOS). At the top of the Files tab is a + sign, which gives you a Jupyter Launcher.\nIn the Jupyter Launcher, click the Python 3 icon under Console. This will launch a console, which has a large white space above a prompt that says In []:. You can enter Python code in this prompt, and it will be executed.\nTo print Hello, world., enter the code below. To execute the code, hit shift+enter.\n\nprint('Hello, world.')\n\nHello, world.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to JupyterLab</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#py-files",
    "href": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#py-files",
    "title": "Appendix C — Introduction to JupyterLab",
    "section": "C.3 .py files",
    "text": "C.3 .py files\nNow let’s use our new knowledge of the print() function to have our computer say a bit more than just Hello, world. Type these lines in at the prompt, hitting enter each time you need a new line. After you’ve typed them all in, hit shift+enter to run them.\n\n# The first few lines from The Zen of Python by Tim Peters\nprint('Beautiful is better than ugly.')\nprint('Explicit is better than implicit.')\nprint('Simple is better than complex.')\nprint('Complex is better than complicated.')\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\n\n\nNote that the first line is preceded with a # sign, and the Python interpreter ignored it. The # sign denotes a comment, which is ignored by the interpreter, but very important for the human!\nWhile the console prompt was nice entering all of this, a better option is to store them in a file, and then have the Python interpreter run the lines in the file. This is how you typically store Python code, and the suffix of such files is .py.\nSo, let’s create a .py file. To do this, use the JupyterLab Launcher to launch a text editor. Once it is launched, you can right click on the tab of the text editor window to change the name. We will call this file zen.py. Within this file, enter the four lines of code you previously entered in the console prompt. Be sure to save it.\nTo run the code in this file, you can invoke the Python interpreter at the command line, followed by the file name. I.e., enter\npython zen.py\nat the command line. Note that when you run code this way, the interpreter exits after completion of running the code, and you do not get a prompt.\nTo run the code in this file using the Jupyter console, you can use the %run magic function.\n%run zen.py\nTo shut down the console, you can click on the Running tab at the left of the JupyterLab window and click on SHUTDOWN next to the console.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to JupyterLab</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#jupyter",
    "href": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#jupyter",
    "title": "Appendix C — Introduction to JupyterLab",
    "section": "C.4 Jupyter",
    "text": "C.4 Jupyter\nAt this point, we have introduced JupyterLab, its text editor, and the console, as well as the Python interpreter itself. You might be asking….\n\nC.4.1 What is Jupyter?\nFrom the Project Jupyter website: &gt;Project Jupyter is an open source project was born out of the IPython Project in 2014 as it evolved to support interactive data science and scientific computing across all programming languages.\nSo, Jupyter is an extension of IPython the pushes interactive computing further. It is language agnostic as its name suggests. The name “Jupyter” is a combination of Julia (a newer excellent language for scientific computing), Python (which you know and love), and R (the dominant tool for statistical computation). However, you can run over 40 different languages in a JupyterLab, not just Julia, Python, and R.\nCentral to Jupyter/JupyterLab are Jupyter notebooks. In fact, the document you are reading right now was generated from a Jupyter notebook. We will use Jupyter notebooks extensively in the course, along with .py files.\n\n\nC.4.2 Why Jupyter notebooks?\nWhen writing code you will reuse, you should develop fully tested modules using .py files. You can always import those modules when you are using a Jupyter notebook. So, a Jupyter notebook is not good for an application where you are building reusable code or scripts. However, Jupyter notebooks are very useful in the following applications.\n\nExploring data/analysis. Jupyter notebooks are great for trying things out with code, or exploring a data set. This is an important part of the research process. The layout of Jupyter notebooks is great for organizing thoughts as you synthesize them.\nDeveloping image processing pipelines. This is really just a special case of (1), but it worth mentioning separately because Jupyter notebooks are especially useful when figuring out what steps are best for extracting useful data from images, which happens all-too-often in biology. Using the Jupyter notebook, you can write down what you hope to accomplish in each step of processing and then graphically show the results as images as you go through the analysis.\nSharing your thinking in your analysis. Because you can combine nicely formatted text and executable code, Jupyter notebooks are great for sharing how you go about doing your calculations with collaborators and with readers of your publications. Famously, LIGO used a Jupyter notebook to explain the signal processing involved in their first discovery of a gravitational wave.\nPedagogy. All of the content in this class, including this lesson, was developed using Jupyter notebooks!\n\nNow that we know what Jupyter notebooks are and what the motivation is for using them, let’s start!\n\n\nC.4.3 Launching a Jupyter notebook\nTo launch a Jupyter notebook, click on the Notebook icon of the JupyterLab launcher. If you want to open an existing notebook, click on it in the Files tab of the JupyterLab window and open it.\n\n\nC.4.4 Cells\nA Jupyter notebook consists of cells. The two main types of cells you will use are code cells and markdown cells, and we will go into their properties in depth momentarily. First, an overview.\nA code cell contains actual code that you want to run. You can specify a cell as a code cell using the pulldown menu in the toolbar of your Jupyter notebook. Otherwise, you can can hit esc and then y (denoted “esc, y”) while a cell is selected to specify that it is a code cell. Note that you will have to hit enter after doing this to start editing it.\nIf you want to execute the code in a code cell, hit “shift + enter.” Note that code cells are executed in the order you shift-enter them. That is to say, the ordering of the cells for which you hit “Shift + Enter” is the order in which the code is executed. If you did not explicitly execute a cell early in the document, its results are not known to the Python interpreter. This is a very important point and is often a source of confusion and frustration for students.\nMarkdown cells contain text. The text is written in markdown, a lightweight markup language. You can read about its syntax here. Note that you can also insert HTML into markdown cells, and this will be rendered properly. As you are typing the contents of these cells, the results appear as text. Hitting “Shift + Enter” renders the text in the formatting you specify.\nYou can specify a cell as being a markdown cell in the Jupyter toolbar, or by hitting “esc, m” in the cell. Again, you have to hit enter after using the quick keys to bring the cell into edit mode.\nIn general, when you want to add a new cell, you can click the + icon on the notebook toolbar. The shortcut to insert a cell below is “esc, b” and to insert a cell above is “esc, a.” Alternatively, you can execute a cell and automatically add a new one below it by hitting “alt + enter.”\n\n\nC.4.5 Code cells\nBelow is an example of a code cell printing hello, world. Notice that the output of the print statement appears in the same cell, though separate from the code block.\n\n# Say hello to the world.\nprint('Hello, world.')\n\nHello, world.\n\n\nIf you evaluate a Python expression that returns a value, that value is displayed as output of the code cell. This only happens, however, for the last line of the code cell.\n\n# Would show 9 if this were the last line, but it is not, so shows nothing\n4 + 5\n\n# I hope we see 11.\n5 + 6\n\n11\n\n\nNote that if the last line does not return a value, such as if we assigned a variable, there is no visible output from the code cell.\n\n# Variable assignment, so no visible output.\na = 5 + 6\n\n\n# However, now if we ask for a, its value will be displayed\na\n\n11\n\n\n\n\nC.4.6 Display of graphics\nWe will be using Bokeh almost exclusively during the course. To make sure the Bokeh plots get shown in the notebook, you should execute\nbokeh.io.output_notebook()\nin your notebook. It is good practice to execute this in the first cell of the notebook. Let us now make a plot using Bokeh.\n\n# Generate data to plot\nx = np.linspace(0, 2 * np.pi, 200)\ny = np.exp(np.sin(np.sin(x)))\n\n# Set up plot\np = bokeh.plotting.figure(\n    frame_height=200,\n    frame_width=250,\n    x_axis_label='x',\n    y_axis_label='y',\n    x_range=[0, 2 * np.pi],\n)\n\n# Populate glyph\np.line(x, y, line_width=2)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\n\n\nC.4.7 Proper formatting of cells\nGenerally, it is a good idea to keep cells simple. You can define one function, or maybe two or three closely related functions, in a single cell, and that’s about it. When you define a function, you should make sure it is properly commented with a descriptive doc string. Below is an example of how I might generate a plot of the Lorenz attractor (which I choose just because it is fun) with code cells and markdown cells with discussion of what I am doing. (The doc string in this function is nice, but longer than that is necessary for submitted homework in class. At least something akin to the first line of the doc string must appear in function definitions in your submitted notebooks.)\nBetween cells, you should explain with text what you are doing. Let’s look at a fun example.\nWe will use scipy.integrate.odeint() to numerically integrate the Lorenz attractor. We therefore first define a function that returns the right hand side of the system of ODEs that define the Lorentz attractor.\n\ndef lorenz_attractor(r, t, p):\n    \"\"\"\n    Compute the right hand side of system of ODEs for Lorenz attractor.\n    \n    Parameters\n    ----------\n    r : array_like, shape (3,)\n        (x, y, z) position of trajectory.\n    t : dummy_argument\n        Dummy argument, necessary to pass function into \n        scipy.integrate.odeint\n    p : array_like, shape (3,)\n        Parameters (s, k, b) for the attractor.\n        \n    Returns\n    -------\n    output : ndarray, shape (3,)\n        Time derivatives of Lorenz attractor.\n        \n    Notes\n    -----\n    .. Returns the right hand side of the system of ODEs describing\n       the Lorenz attractor.\n        x' = s * (y - x)\n        y' = x * (k - z) - y\n        z' = x * y - b * z\n    \"\"\"\n    # Unpack variables and parameters\n    x, y, z = r\n    s, p, b = p\n    \n    return np.array([s * (y - x), \n                     x * (p - z) - y, \n                     x * y - b * z])\n\nWith this function in hand, we just have to pick our initial conditions and time points and run the numerical integration.\n\n# Parameters to use\np = np.array([10.0, 28.0, 8.0 / 3.0])\n\n# Initial condition\nr0 = np.array([0.1, 0.0, 0.0])\n\n# Time points to sample\nt = np.linspace(0.0, 30.0, 4000)\n\n# Use scipy.integrate.odeint to integrate Lorentz attractor\nr = scipy.integrate.odeint(lorenz_attractor, r0, t, args=(p,))\n\n# Unpack results into x, y, z.\nx, y, z = r.transpose()\n\nNow, we’ll construct a plot of the trajectory using Bokeh.\n\n# Set up plot\np = bokeh.plotting.figure(\n    frame_height=200,\n    frame_width=200,\n    x_axis_label='x',\n    y_axis_label='z',\n)\n\n# Populate glyph\np.line(x, z)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\n\n\nC.4.8 Best practices for code cells\nHere is a summary of some general rules for composing and formatting your code cells.\n\nKeep the width of code in cells below 80 characters. This is not a hard limit, but you should strive for it and consider 88 characters a hard limit.\nKeep your code cells short. If you find yourself having one massive code cell, break it up.\nProvide complete doc strings for any functions you define. You can and should have comments in your code, but you really should not need much because your markdown cells around the code cells should clearly describe what you are trying to do.\nDo all of your imports in the first code cell at the top of the notebook. With the exception of “from ... import ...” imports, import one module per line. You should also include bokeh.io.output_notebook() in the top cell as well when using Bokeh.\nFor submitting assignments, always display your graphics in the notebook.\n\n\n\nC.4.9 Markdown cells\nMarkdown cells contain text. The text is written in markdown, a lightweight markup language. The list of syntactical constructions at this link are pretty much all you need to know for standard markdown. Note that you can also insert HTML into markdown cells, and this will be rendered properly. As you are typing the contents of these cells, the results appear as text. Hitting “shift + enter” renders the text in the formatting you specify.\nYou can specify a cell as being a markdown cell in the Jupyter tool bar, or by hitting “esc, m” in the cell. Again, you have to hit enter after using the quick keys to bring the cell into edit mode.\nIn addition to HTML, some \\(\\LaTeX\\) expressions may be inserted into markdown cells. \\(\\LaTeX\\) (pronounced “lay-tech”) is a document markup language that uses the \\(\\TeX\\) typesetting software. It is particularly well-suited for beautiful typesetting of mathematical expressions. In Jupyter notebooks, the \\(\\LaTeX\\) mathematical input is rendered using software called MathJax. This is usually run off of a remote server, so if you are not connected to the internet, your equations may not be rendered. You will use \\(\\LaTeX\\) extensively in preparation of your assignments. There are plenty of resources on the internet for getting started with \\(\\LaTeX\\), but you will only need a tiny subset of its functionality in your assignments, and the next part of this lesson, plus cheat sheets you may find by Google (such as this one) are useful.\n\n\nC.4.10 Quick keys\nThere are some keyboard shortcuts that are convenient to use in JupyterLab. (They do not all work in Colab.) We already encountered Shift + Enter to run a code cell. Importantly, pressing Esc brings you into command mode in which you are not editing the contents of a single cell, but are doing things like adding cells. Below are some useful quick keys. If two keys are separated by a + sign, they are pressed simultaneously, and if they are separated by a - sign, they are pressed in succession.\n\n\n\nQuick keys\nmode\naction\n\n\n\n\nEsc - m\ncommand\nswitch cell to Markdown cell\n\n\nEsc - y\ncommand\nswitch cell to code cell\n\n\nEsc - a\ncommand\ninsert cell above\n\n\nEsc - b\ncommand\ninsert cell below\n\n\nEsc - d - d\ncommand\ndelete cell\n\n\nAlt + Enter\nedit\nexecute cell and insert a cell below\n\n\n\nThere are many others (and they are shown in the pulldown menus within JupyterLab), but these are the ones I seem to encounter most often.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to JupyterLab</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#rendering-of-notebooks-as-html",
    "href": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#rendering-of-notebooks-as-html",
    "title": "Appendix C — Introduction to JupyterLab",
    "section": "C.5 Rendering of notebooks as HTML",
    "text": "C.5 Rendering of notebooks as HTML\nWhen you submit homework, you will also submit an HTML rendering of your notebooks. To save a notebook as HTML, you can click File → Export Notebook As... → Export Notebook to HTML.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to JupyterLab</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#computing-environment",
    "href": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#computing-environment",
    "title": "Appendix C — Introduction to JupyterLab",
    "section": "C.6 Computing environment",
    "text": "C.6 Computing environment\nAt the end of every lesson, and indeed at the end (or beginning) of any notebook you make, you should include information about the computing environment including the version numbers of all packages you use. This helps reproducibility. The watermark package is quite useful for this. The watermark package is an IPython magic extension. These extensions allow convenient functionality within IPython or Jupyter notebooks. In general, to use magic functions, you precede them with a % sign (or a double %%) in a cell. We use the built-in %load_ext magic function to load watermark, and then we use %watermark to invoke it.\nWe use the -v flag to ask watermark to give us the Python and IPython verison numbers and the -p flag to give us version numbers on specified packages we’ve used. We can also use a -m flag to give information about the machine running the notebook, and you should do that, but I will not do that for this course to avoid clutter.\nYour versions might not always match (especially if you are using Colab), but doing this is good practice and can help with debugging.\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\nscipy     : 1.16.0\nbokeh     : 3.7.3\njupyterlab: 4.4.5",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to JupyterLab</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_latex.html",
    "href": "appendices/notebooks_and_latex/intro_to_latex.html",
    "title": "Appendix D — Introduction to LaTeX",
    "section": "",
    "text": "D.1 Basic inline LaTeX\n| Download notebook\nIn this tutorial, you will learn some of the basics on how to use \\(\\LaTeX\\) to display equations in Jupyter notebooks. For looking up symbols you may need, you can use any of the many cheat sheets you can find by asking Google. I have provided a few that will come up often in this course at the end of this lesson.\nIn this lesson, whenever a LaTeX expression is shown, the raw Markdown/LaTeX is shown beneath it. (The word LaTeX is generally stylized as \\(\\LaTeX\\), but I get tired of reading that, so going forward, I will just write “LaTeX.”)\nTo embed LaTeX within text, simply encapsulate the LaTeX portions in dollar signs ($). MathJax takes care of the rest. As an example, consider the sentence below and the markdown/LaTeX code to render it.\nEinstein told us that \\(E = mc^2\\).\nNotice how the equation is properly rendered, with mathematical variables in italics. Note also how ^2 was used to raise to a power. If the power has more than one character in it, it should be enclosed in braces ({}). In fact, braces are used to generally group symbols in LaTeX.\nEuler told us that \\(\\mathrm{e}^{i \\pi} - 1 = 0\\).\nAside from the grouping braces, there are several other syntactical items of note. First, notice that I made the special character \\(\\pi\\) with \\pi. In general, a backward slash precedes special symbols or commands in LaTeX. If we want another Greek letter, like \\(\\theta\\), we use \\theta. Now, also note that I used “\\mathrm{e}” for the base of the natural logarithm. I was signaling to LaTeX that I wanted the character written in Roman font, and not italics, so I used \\mathrm. Anything in the braces following the function \\mathrm is rendered in Roman font. Note the difference.\nThis is \\(e\\). This is \\(\\mathrm{e}\\)\nNow, back to grouping things in braces. We can do similar groupings using braces with with subscripts.\nThe dot product of two \\(n\\)-vectors is \\(\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i\\).\nHere, I have used $\\mathbf{a}$ to make the character a boldface, denoting a vector. Note that we denote subscripts with an underscore. Notice also that the bounds of the sum use the same underscore and caret notation as for subscripts and superscripts.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Introduction to LaTeX</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_latex.html#basic-inline-latex",
    "href": "appendices/notebooks_and_latex/intro_to_latex.html#basic-inline-latex",
    "title": "Appendix D — Introduction to LaTeX",
    "section": "",
    "text": "Einstein told us that $E = mc^2$.\n\n\nEuler told us that $\\mathrm{e}^{i \\pi} - 1 = 0$.\n\n\nThis is $e$. This is $\\mathrm{e}$.\n\n\nThe dot product of two $n$-vectors is $\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i$.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Introduction to LaTeX</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_latex.html#displaying-equations-on-separate-lines",
    "href": "appendices/notebooks_and_latex/intro_to_latex.html#displaying-equations-on-separate-lines",
    "title": "Appendix D — Introduction to LaTeX",
    "section": "D.2 Displaying equations on separate lines",
    "text": "D.2 Displaying equations on separate lines\nThe bounds on the summation in the above example may look a little funny to you because they are not above and below the summation symbol. This is because this particular equation is written inline. If we had separated it from the text, it renders differently.\nWe can make an equation appear centered on a new line, like\n\\[\\begin{align}\n\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i.\n\\end{align}\\]\nWe can make an equation appear centered on a new line, like\n\n\\begin{align}\n    \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i.\n\\end{align}\nThe align environment in LaTeX specifies that you want centered equations, separated from the text. It is called align because it allows you to align the equations. You separate lines in the equations with a double backslash (//). Insert an ampersand (&) in each line at the alignment point. All equations will be aligned at the location of the ampersand symbols (and, of course, the ampersands will not appear in the rendered equations).\nFor a three-vector consisting of \\(x\\), \\(y\\), and \\(z\\) components,\n\\[\\begin{align}\n\\mathbf{a} \\cdot \\mathbf{b} &= \\sum_{i=1}^n a_i b_i \\\\\n&= a_x b_x + a_y b_y + a_z b_z.\n\\end{align}\\]\nFor a three-vector consisting of $x$, $y$, and $z$ components,\n\n\\begin{align}\n    \\mathbf{a} \\cdot \\mathbf{b} &= \\sum_{i=1}^n a_i b_i \\\\\n    &= a_x b_x + a_y b_y + a_z b_z.\n\\end{align}\nNote that I always put an extra blank line before the \\begin{align} statement. This is not necessary, but I think things look better with the extra space.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Introduction to LaTeX</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_latex.html#fractions-and-an-example-of-fine-tuning",
    "href": "appendices/notebooks_and_latex/intro_to_latex.html#fractions-and-an-example-of-fine-tuning",
    "title": "Appendix D — Introduction to LaTeX",
    "section": "D.3 Fractions (and an example of fine-tuning)",
    "text": "D.3 Fractions (and an example of fine-tuning)\nTo display fractional quantities, we use the \\frac{}{} command. \\frac is always followed by two sets of braces; the numerator is contained in the first, and the denominator is contained in the second. As an example, we can write an equation you will become intimately familiar with if you take the second term of this course,\n\\[\\begin{align}\nP(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)}\n\\end{align}\\]\n\\begin{align}\n    P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)}\n\\end{align}\nThe right hand side has a nicely-formatted fraction. I did a little extra fine-tuning in this equation. I’ll show the equation again without the fine-tuning, which used the \\mid and \\, commands.\n\\[\\begin{align}\nP(A | B) = \\frac{P(B | A) P(A)}{P(B)}.\n\\end{align}\\]\n\\begin{align}\n    P(A | B) = \\frac{P(B | A) P(A)}{P(B)}.\n\\end{align}\nFirst, the \\mid command should be used in conditional probabilities. Just using a vertical bar (|) results in crowding. Similarly, I used the \\, command to insert a little extra space between the two probabilities in the numerator. This makes the equation a bit easier to read. This \\, operator is especially important when defining integrals. We can put a little space between the \\(\\mathrm{d}x\\) and the integrand.\n\\[\\begin{align}\n\\text{good: } &\\int_0^{2\\pi} \\mathrm{d}x \\, \\sin x. \\\\[1em]\n\\text{bad: } &\\int_0^{2\\pi} \\mathrm{d}x \\sin x.\n\\end{align}\\]\n\\begin{align}\n    \\text{good: } &\\int_0^{2\\pi} \\mathrm{d}x \\, \\sin x. \\\\[1em]\n    \\text{bad: } &\\int_0^{2\\pi} \\mathrm{d}x \\sin x.\n\\end{align}\nNote that I inserted extra space after the new line. Specifically, \\\\[1em] instructs LaTeX to insert a space equation to the width of an M character between the equations. I often do this to keep things clear.\nIt is also very important to note that I used \\(\\sin\\) and not \\(sin\\). Mathematical functions should be in Roman font and are invoked with a backslash. Otherwise, the characters are interpreted as separate variables. To be clear:\n\\[\\begin{align}\n\\text{good: } &\\sin x. \\\\[1em]\n\\text{bad: } & sin x.\n\\end{align}\\]\n\\begin{align}\n    \\text{good: } &\\sin x. \\\\[1em]\n    \\text{bad: } & sin x.\n\\end{align}\nFinally, notice that I was able to put text in the equation like this: \\text{good: }.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Introduction to LaTeX</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_latex.html#grouping-operators-and-more-fine-tuning",
    "href": "appendices/notebooks_and_latex/intro_to_latex.html#grouping-operators-and-more-fine-tuning",
    "title": "Appendix D — Introduction to LaTeX",
    "section": "D.4 Grouping operators (and more fine-tuning)",
    "text": "D.4 Grouping operators (and more fine-tuning)\nCompare the following equations.\n\\[\\begin{align}\n\\text{good: } &\\sum_{i=1}^n i^3 = \\left(\\sum_{i=1}^n i\\right)^2. \\\\[1em]\n\\text{bad: }  &\\sum_{i=1}^n i^3 = (\\sum_{i=1}^n i)^2.\n\\end{align}\\]\n\\begin{align}\n    \\text{good: } &\\sum_{i=1}^n i^3 = \\left(\\sum_{i=1}^n i\\right)^2. \\\\[1em]\n    \\text{bad: }  &\\sum_{i=1}^n i^3 = (\\sum_{i=1}^n i)^2.\n\\end{align}\nIn the second equation, I did not use the \\left( and \\right) construction for parentheses and the result looks pretty awful. In LaTeX, the height of anything that is encapsulated by \\left( and \\right) scales the parentheses appropriately. You can use \\left and \\right with many symbols. An important example is \\left\\{. Note that to display braces in an equation, you have to use \\{ because just a plain brace ({) has a different meaning.\n(By the way, that equation is true, and pretty amazing. It says that the sum of the first \\(n\\) cubes of integers is equal to the sum of the first \\(n\\) integers squared!)\nFinally, if you use \\left. or \\right., LaTeX will simply scale the opposite symbol to match the height of the text, but will suppress printing the other. For example,\n\\[\\begin{align}\n\\left. \\frac{1}{x + 2} \\right|_0^2 = -\\frac{1}{4}.\n\\end{align}\\]\n\\begin{align}\n    \\left. \\frac{1}{x + 2} \\right|_0^2 = -\\frac{1}{4}.\n\\end{align}\nThis is also useful if you are going to use / for a division operation. Compare the following.\n\\[\\begin{align}\n\\text{good: } & \\left. x^2 \\middle/ y^2 \\right. \\\\[1em]\n\\text{bad: } & x^2 / y^2\n\\end{align}\\]\n\\begin{align}\n    \\text{good: } & \\left. x^2 \\middle/ y^2 \\right. \\\\[1em]\n    \\text{bad: } & x^2 / y^2\n\\end{align}\nHere, we used the \\middle operator to scale the length of the division sign.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Introduction to LaTeX</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_latex.html#matrices-and-arrays",
    "href": "appendices/notebooks_and_latex/intro_to_latex.html#matrices-and-arrays",
    "title": "Appendix D — Introduction to LaTeX",
    "section": "D.5 Matrices and arrays",
    "text": "D.5 Matrices and arrays\nOn occasion, you’ll need to express matrices. This is most easily done using the pmatrix environment. For example, a covariance matrix for two variables might be written as\n\\[\\begin{align}\n\\sigma^2 = \\begin{pmatrix}\n\\sigma_1^2 & \\sigma_{12} \\\\\n\\sigma_{12} & \\sigma_2^2\n\\end{pmatrix}.\n\\end{align}\\]\n\\begin{align}\n    \\sigma^2 = \\begin{pmatrix}\n    \\sigma_1^2 & \\sigma_{12} \\\\\n    \\sigma_{12} & \\sigma_2^2 \n    \\end{pmatrix}.\n\\end{align}\nOnce in the pmatrix environment, each row has entries separated by an ampersand. The row ends with a \\\\. Each row must have the same number of entries.\nYou may also need to represent an values stacked on top of each other. For example, we might specify a piecewise linear function like this.\n\\[\\begin{align}\n\\text{rectifier}(x) = \\left\\{\n\\begin{array}{cl}\n0 & x \\le 0 \\\\\nx & x &gt; 0.\n\\end{array}\n\\right.\n\\end{align}\\]\n\\begin{align}\n    \\text{rectifier}(x) = \\left\\{\n    \\begin{array}{cl}\n    0 & x \\le 0 \\\\\n    x & x &gt; 0.\n    \\end{array}\n    \\right.\n\\end{align}\nThe array environment allows arrays of text. The {cl} after \\begin{array} indicates that two columns are wanted, with the first column being centered and the second being left-aligned. If we chose instead {lr}, the first column is left-aligned and the second column is right-aligned.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Introduction to LaTeX</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_latex.html#useful-latex-symbols-for-bebi-103",
    "href": "appendices/notebooks_and_latex/intro_to_latex.html#useful-latex-symbols-for-bebi-103",
    "title": "Appendix D — Introduction to LaTeX",
    "section": "D.6 Useful LaTeX symbols for BE/Bi 103",
    "text": "D.6 Useful LaTeX symbols for BE/Bi 103\nFollowing is a list of some symbols you may find useful in this class.\n\n\n\n\n\n\n\nLaTeX\nsymbol\n\n\n\n\n\\approx\n\\(\\approx\\)\n\n\n\\sim\n\\(\\sim\\)\n\n\n\\propto\n\\(\\propto\\)\n\n\n\\le\n\\(\\le\\)\n\n\nge\n\\(\\ge\\)\n\n\n\\pm\n\\(\\pm\\)\n\n\n\\in\n\\(\\in\\)\n\n\n\\ln\n\\(\\ln\\)\n\n\n\\exp\n\\(\\exp\\)\n\n\n\\prod_{i\\in D}\n\\({\\displaystyle \\prod_{i\\in D}}\\)\n\n\n\\sum_{i\\in D}\n\\({\\displaystyle \\sum_{i\\in D}}\\)\n\n\n\\frac{\\partial f}{\\partial x}\n\\({\\displaystyle \\frac{\\partial f}{\\partial x}}\\)\n\n\n\\sqrt{x}\n\\(\\sqrt{x}\\)\n\n\n\\bar{x}\n\\(\\bar{x}\\)\n\n\n\\langle x \\rangle\n\\(\\langle x \\rangle\\)\n\n\n\\left\\langle \\frac{x}{y} \\right\\rangle\n\\(\\left\\langle \\frac{x}{y} \\right\\rangle\\)",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Introduction to LaTeX</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html",
    "href": "appendices/python_basics/hello_world.html",
    "title": "Appendix E — Hello, world.",
    "section": "",
    "text": "E.1 The Python interpreter\n| Download notebook\nIn this appendix, we introduce some of the basic syntax and ideas behind the Python programming language and the Jupyter interface.\nPython is an interpreted language, which means that each line of code you write is translated, or interpreted, into a set of instructions that your machine can understand by the Python interpreter. This stands in contrast to compiled languages. For these languages (the dominant ones being Fortran, C, and C++), your entire code is translated into machine language before you ever run it. When you execute your program, it is already in machine language.\nSo, whenever you want your Python code to run, you give it to the Python interpreter.\nThere are many ways to launch the Python interpreter. One way is to type\non the command line of a terminal. This launches the vanilla Python interpreter. Because we are using Python code to explore biological circuit design, we will never really use this. Rather, we will have a greatly enhanced Python experience using Jupyter notebooks. Nevertheless, as you go beyond notebooks and do more sophisticated computing in your adventures with biological circuits, it is good to know about running Python outside of notebooks, so we will do that now.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html#the-python-interpreter",
    "href": "appendices/python_basics/hello_world.html#the-python-interpreter",
    "title": "Appendix E — Hello, world.",
    "section": "",
    "text": "python",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html#hello-world.-and-the-print-function",
    "href": "appendices/python_basics/hello_world.html#hello-world.-and-the-print-function",
    "title": "Appendix E — Hello, world.",
    "section": "E.2 Hello, world. and the print() function",
    "text": "E.2 Hello, world. and the print() function\nTraditionally, the first program anyone writes when learning a new language is called “Hello, world.” In this program, the words “Hello, world.” are printed on the screen. The original Hello, world. was likely written by Brian Kernighan, one of the inventors of Unix, and the author of the classic and authoritative book on the C programming language. In his original, the printed text was “hello, world” (no period or capital H), but people use lots of variants.\nWe will first write and run this little program using a JupyterLab console. After launching JupyterLab, you probably already have the Launcher in your JupyterLab window. If you do not, you can expand the Files tab at the left of your JupyterLab window (if it is not already expanded) by clicking on that tab, or alternatively hit ctrl+b (or cmd+b on macOS). At the top of the Files tab is a + sign, which gives you a Jupyter Launcher.\nIn the Jupyter Launcher, click the Python 3 icon under Console. This will launch a console, which has a large white space above a prompt that says In []:. You can enter Python code in this prompt, and it will be executed.\nTo print Hello, world., enter the code below. To execute the code, hit shift+enter.\n\nprint('Hello, world.')\n\nHello, world.\n\n\nHooray! We just printed Hello, world. to the screen. To do this, we used Python’s built-in print() function. The print() function takes a string as an argument. It then prints that string to the screen. We will learn more about function syntax later, but we can already see the rough syntax with the print() function.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html#py-files",
    "href": "appendices/python_basics/hello_world.html#py-files",
    "title": "Appendix E — Hello, world.",
    "section": "E.3 .py files",
    "text": "E.3 .py files\nNow let’s use our new knowledge of the print() function to have our computer say a bit more than just Hello, world. Type these lines in at the prompt, hitting enter each time you need a new line. After you’ve typed them all in, hit shift+enter to run them.\n\n# The first few lines from The Zen of Python by Tim Peters\nprint('Beautiful is better than ugly.')\nprint('Explicit is better than implicit.')\nprint('Simple is better than complex.')\nprint('Complex is better than complicated.')\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\n\n\nNote that the first line is preceded with a # sign, and the Python interpreter ignored it. The # sign denotes a comment, which is ignored by the interpreter, but very very important for the human!\nWhile the console prompt was nice for entering all of this, a better option is to store them in a file, and then have the Python interpreter run the lines in the file. This is how you typically store Python code, and the suffix of such files is .py.\nSo, let’s create a .py file. To do this, use the JupyterLab Launcher to launch a text editor. Once it is launched, you can right click on the tab of the text editor window to change the name. We will call this file zen.py. Within this file, enter the four lines of code you previously entered in the console prompt. Be sure to save it.\nTo run the code in this file, you can invoke the Python interpreter at the command line, followed by the file name. I.e., enter\npython zen.py\nat the command line. Note that when you run code this way, the interpreter exits after completion of running the code, and you do not get a prompt.\nTo run the code in this file using the Jupyter console, you can use the %run magic function.\n\n%run zen.py\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\n\n\nTo shut down the console, you can click on the Running tab at the left of the JupyterLab window and click on SHUTDOWN next to the console.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html#jupyter",
    "href": "appendices/python_basics/hello_world.html#jupyter",
    "title": "Appendix E — Hello, world.",
    "section": "E.4 Jupyter",
    "text": "E.4 Jupyter\nAt this point, we have introduced JupyterLab, its text editor, and the console, as well as the Python interpreter itself. You might be asking….\n\nE.4.1 What is Jupyter?\nFrom the Project Jupyter website: &gt;Project Jupyter is an open source project was born out of the IPython Project in 2014 as it evolved to support interactive data science and scientific computing across all programming languages.\nSo, Jupyter is an extension of IPython that pushes interactive computing further. It is language agnostic as its name suggests. The name “Jupyter” is a combination of Julia (a newer language for scientific computing), Python (which you know and love), and R (the dominant tool for statistical computation). However, you can run over 40 different languages in a JupyterLab, not just Julia, Python, and R.\nCentral to Jupyter/JupyterLab are Jupyter notebooks. In fact, the document you are reading right now was generated from a Jupyter notebook.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html#why-jupyter-notebooks",
    "href": "appendices/python_basics/hello_world.html#why-jupyter-notebooks",
    "title": "Appendix E — Hello, world.",
    "section": "E.5 Why Jupyter notebooks?",
    "text": "E.5 Why Jupyter notebooks?\nWhen writing code you will reuse, you should develop fully tested modules using .py files. You can always import those modules when you are using a Jupyter notebook (more on modules and importing them later in this appendix). So, a Jupyter notebook is not good for an application where you are building reusable code or scripts. However, Jupyter notebooks are very useful in the following applications.\n\nExploring data/analysis. Jupyter notebooks are great for trying things out with code, or exploring a data set. This is an important part of the research process. The layout of Jupyter notebooks is great for organizing thoughts as you synthesize them.\nSharing your thinking in your analysis. Because you can combine nicely formatted text and executable code, Jupyter notebooks are great for sharing how you go about doing your calculations with collaborators and with readers of your publications. Famously, LIGO used a Jupyter notebook to explain the signal processing involved in their first discovery of a gravitational wave.\nPedagogy. All of the content in this book, including this appendix, was developed using Jupyter notebooks!\n\nNow that we know what Jupyter notebooks are and what the motivation is for using them, let’s start!\n\nE.5.1 Launching a Jupyter notebook\nYou can launch JupyterLab via your operating system’s terminal program (Terminal on macOS and PowerShell on Windows). If you are on a Mac, open the Terminal program. You can do this hitting Command + space bar and searching for “terminal.” Using Windows, you should launch PowerShell. You can do this by hitting Windows + R and typing “powershell” in the text box.\nYou need to make sure you are using the caltech_datasai environment whenever you launch JupyterLab, so you should do conda activate caltech_datasai each time you open a terminal.\nNow that you have activated the caltech_datasai environment, you can launch JupyterLab by typing\njupyter lab\non the command line. You will have an instance of JupyterLab running in your default browser. If you want to specify the browser, you can, for example, type\njupyter lab --browser=firefox\non the command line.\nAlternatively, if you are using VSCode, you can use its menu system to open .ipynb files. Within the JupyterLab window, you will have the option to launch a notebook, a console, a terminal, or a text editor.\n\n\nE.5.2 Cells\nA Jupyter notebook consists of cells. The two main types of cells you will use are code cells and markdown cells, and we will go into their properties in depth momentarily. First, an overview.\nA code cell contains actual code that you want to run. You can specify a cell as a code cell using the pulldown menu in the toolbar of your Jupyter notebook. Otherwise, you can can press Esc and then y (denoted Esc - y“) while a cell is selected to specify that it is a code cell. Note that you will have to hit enter after doing this to start editing it.\nIf you want to execute the code in a code cell, hit Enter while holding down the Shift key (denoted Shift + Enter). Note that code cells are executed in the order you shift-enter them. That is to say, the ordering of the cells for which you hit Shift + Enter is the order in which the code is executed. If you did not explicitly execute a cell early in the document, its results are not known to the Python interpreter. This is a very important point and is often a source of confusion and frustration for students.\nMarkdown cells contain text. The text is written in markdown, a lightweight markup language. You can read about its syntax here. Note that you can also insert HTML into markdown cells, and this will be rendered properly. As you are typing the contents of these cells, the results appear as text. Hitting Shift + Enter renders the text in the formatting you specify.\nMarkdown cells can also render LaTeX for mathematics. For example,\nThe FitzHugh-Nagumo model is a simplification of the Hodgkin-Huxley model with two ordinaray differential equations,\n\n\\begin{align}\n\\frac{\\mathrm{d}v}{\\mathrm{d}t} &= v - \\frac{v^3}{3} - w + RI,\\\\[1em]\n\\tau\\,\\frac{\\mathrm{d}w}{\\mathrm{d}t} &= v + a + b w.\n\\end{align}\nrenders as:\nThe FitzHugh-Nagumo model is a simplification of the Hodgkin-Huxley model with two ordinaray differential equations,\n\\[\\begin{align}\n\\frac{\\mathrm{d}v}{\\mathrm{d}t} &= v - \\frac{v^3}{3} - w + RI,\\\\[1em]\n\\tau\\,\\frac{\\mathrm{d}w}{\\mathrm{d}t} &= v + a + b w.\n\\end{align}\\]\nYou can specify a cell as being a markdown cell in the Jupyter toolbar, or by hitting Esc - m in the cell. Again, you have to hit enter after using the quick keys to bring the cell into edit mode.\nIn general, when you want to add a new cell, you can click the + icon on the notebook toolbar. The shortcut to insert a cell below is Esc - b and to insert a cell above is Esc - a. Alternatively, you can execute a cell and automatically add a new one below it by hitting Alt + Enter.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html#code-cells",
    "href": "appendices/python_basics/hello_world.html#code-cells",
    "title": "Appendix E — Hello, world.",
    "section": "E.6 Code cells",
    "text": "E.6 Code cells\nBelow is an example of a code cell printing hello, world. Notice that the output of the print statement appears in the same cell, though separate from the code block.\n\n# Say hello to the world.\nprint('hello, world.')\n\nhello, world.\n\n\nIf you evaluate a Python expression that returns a value, that value is displayed as output of the code cell. This only happens for the last line of the code cell.\n\n# Would show 9 if this were the last line, but it is not, so shows nothing\n4 + 5\n\n# I hope we see 11.\n5 + 6\n\n11\n\n\nNote, however, if the last line does not return a value, such as if we assigned value to a variable, there is no visible output from the code cell.\n\n# Variable assignment, so no visible output.\na = 5 + 6\n\n\n# However, now if we ask for a, its value will be displayed\na\n\n11\n\n\n\nE.6.1 Display of graphics\nWhen we discuss plotting with Bokeh, you will learn about displaying graphics in Jupyter notebooks.\n\n\nE.6.2 Quick keys\nThere are some keyboard shortcuts that are convenient to use in JupyterLab. We already encountered many of them. Importantly, pressing Esc brings you into command mode in which you are not editing the contents of a single cell, but are doing things like adding cells. Below are some useful quick keys. If two keys are separated by a + sign, they are pressed simultaneously, and if they are separated by a - sign, they are pressed in succession.\n\n\n\nQuick keys\nmode\naction\n\n\n\n\nEsc - m\ncommand\nswitch cell to Markdown cell\n\n\nEsc - y\ncommand\nswitch cell to code cell\n\n\nEsc - a\ncommand\ninsert cell above\n\n\nEsc - b\ncommand\ninsert cell below\n\n\nEsc - d - d\ncommand\ndelete cell\n\n\nAlt + Enter\nedit\nexecute cell and insert a cell below\n\n\n\nThere are many others (and they are shown in the pulldown menus within JupyterLab), but these are the ones I seem to encounter most often.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html#computing-environment",
    "href": "appendices/python_basics/hello_world.html#computing-environment",
    "title": "Appendix E — Hello, world.",
    "section": "Computing environment",
    "text": "Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 8.30.0\n\njupyterlab: 4.3.6",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html",
    "href": "appendices/python_basics/variables_operators_types.html",
    "title": "Appendix F — Variables, operators, and types",
    "section": "",
    "text": "F.1 Determining the type\n| Download notebook\nWhether you are programming in Python or pretty much any other language, you will be working with variables. While the precise definition of a variable will vary from language to language, we’ll focus on Python variables here.\nWe will talk more about objects later, but a variable, like everything in Python, is an object. For now, you can think of it in the following way. The following can be properties of a variable: 1. The type of variable. E.g., is it an integer, like 2, or a string, like 'Hello, world.'? 2. The value of the variable.\nDepending on the type of the variable, you can do different things to it and other variables of similar type. This, as with most things, is best explored by example. We will go through some of the properties of variables and things you can do to them.\nFirst, we will use Python’s built-in type() function to determine the type of some variables.\ntype(2)\n\nint\ntype(2.3)\n\nfloat\ntype('Hello, world.')\n\nstr\nThe type function told us that 2 is an int (short for integer), 2.3 is a float (short for floating point number, basically a real number that is not an integer), and 'Hello, world.' is a str (short for string). Note that the single quotes around the characters indicate that it is a string. So, '1' is a string, but 1 is an integer.\nNote that we can also express floats using scientific notation; \\(4.5\\times 10^{-7}\\) is expressed as 4.5e-7.\ntype(4.5e-7)\n\nfloat",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#determining-the-type",
    "href": "appendices/python_basics/variables_operators_types.html#determining-the-type",
    "title": "Appendix F — Variables, operators, and types",
    "section": "",
    "text": "F.1.1 A note on strings\nWe just saw that strings can be enclosed in single quotes. In Python, we can equivalently enclose them in double quotes. E.g.,\n'my string'\nand\n\"my string\"\nare the same thing. We can also denote a string with triple quotes. So,\n\"\"\"my string\"\"\"\n'''my string'''\n\"my string\"\n'my string'\nare all the same thing. The difference with triple quotes is that it allows a string to extend over multiple lines.\n\n# A multi-line string\nmy_str = \"\"\"It was the best of times,\nit was the worst of times...\"\"\"\n\nprint(my_str)\n\nIt was the best of times,\nit was the worst of times...\n\n\nNote, though, we cannot do this with single quotes.\n\n# This is a SyntaxError\nmy_str = 'It was the best of times,\nit was the worst of times...'\n\n\n  Cell In[6], line 2\n    my_str = 'It was the best of times,\n             ^\nSyntaxError: unterminated string literal (detected at line 2)",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#operators",
    "href": "appendices/python_basics/variables_operators_types.html#operators",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.2 Operators",
    "text": "F.2 Operators\nOperators allow you to do things with variables, like add them. They are represented by special symbols, like + and *. For now, we will focus on arithmetic operators. Python’s arithmetic operators are\n\n\n\naction\noperator\n\n\n\n\naddition\n+\n\n\nsubtraction\n-\n\n\nmultiplication\n*\n\n\ndivision\n/\n\n\nraise to power\n**\n\n\nmodulo\n%\n\n\nfloor division\n//\n\n\n\nWarning: Do not use the ^ operator to raise to a power. That is actually the operator for bitwise XOR, which we will not really use. Observe firey death if you use these inappropriately:\n\n10^200\n\n194\n\n\nInstead of raising 10 to the 200th power, Python performed a bitwise XOR as illustrated below:\n\n\n\na\nBinary\nDecimal\n\n\n\n\nInput 1\n00001010\n10\n\n\nInput 2\n11001000\n200\n\n\nOutput\n11000010\n194\n\n\n\nNote: if you want to see how a decimal number is represented in binary, you can use the following:\n\n'{0:b}'.format(194)\n\n'11000010'\n\n\n\nF.2.1 Operations on integers\nLet’s see how these operators work on integers.\n\n2 + 3\n\n5\n\n\n\n2 - 3\n\n-1\n\n\n\n2 * 3\n\n6\n\n\n\n2 / 3\n\n0.6666666666666666\n\n\n\n2 ** 3\n\n8\n\n\n\n2 % 3\n\n2\n\n\n\n2 // 3\n\n0\n\n\nThis is what we would expect. An import note, though. If you are using Python 2, division of integers defaults to floor division. Some legacy code is written in Python 2, though it officially sunset on New Years Day 2020.\n\n\nF.2.2 Operations on floats\nLet’s try floats.\n\n2.1 + 3.2\n\n5.300000000000001\n\n\nWait a minute! We know 2.1 + 3.2 = 5.3, but Python gives 5.300000000000001. This is due to the fact that floating point numbers are stored with a finite number of binary bits. There will always be some rounding errors. This means that as far as the computer is concerned, it cannot tell you that 2.1 + 3.2 and 5.3 are equal. This is important to remember when dealing with floats, as we will see in the next lesson.\n\n2.1 - 3.2\n\n-1.1\n\n\n\n# Very very close to zero because of finite precision\n5.3 - (2.1 + 3.2)\n\n-8.881784197001252e-16\n\n\n\n2.1 * 3.2\n\n6.720000000000001\n\n\n\n2.1 / 3.2\n\n0.65625\n\n\n\n2.1 ** 3.2\n\n10.74241047739471\n\n\n\n2.1 % 3.2\n\n2.1\n\n\n\n2.1 // 3.2\n\n0.0\n\n\nAside from the floating point precision issue I already pointed out, everything is like we would expect. Note, though, that we cannot divide by zero.\n\n2.1 / 0.0\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[24], line 1\n----&gt; 1 2.1 / 0.0\n\nZeroDivisionError: float division by zero\n\n\n\nWe cannot do it with ints, either.\n\n2 / 0\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 2 / 0\n\nZeroDivisionError: division by zero\n\n\n\n\n\nF.2.3 Operations on integers and floats\nThis proceeds as we think it should.\n\n2.1 + 3\n\n5.1\n\n\n\n2.1 - 3\n\n-0.8999999999999999\n\n\n\n2.1 * 3\n\n6.300000000000001\n\n\n\n2.1 / 3\n\n0.7000000000000001\n\n\n\n2.1 ** 3\n\n9.261000000000001\n\n\n\n2.1 % 3\n\n2.1\n\n\n\n2.1 ** 3\n\n9.261000000000001\n\n\nAnd again we have the rounding errors, but everything is otherwise intuitive.\n\n\nF.2.4 Operations on strings\nNow let’s try some of these operations on strings. This idea of applying mathematical operations to strings seems strange, but let’s just mess around and see what we get.\n\n'Hello, ' + 'world.'\n\n'Hello, world.'\n\n\nAdding strings together concatenates them! This is also intuitive. How about subtracting strings?\n\n'Hello, ' - 'world'\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[34], line 1\n----&gt; 1 'Hello, ' - 'world'\n\nTypeError: unsupported operand type(s) for -: 'str' and 'str'\n\n\n\nThat stands to reason. Subtracting strings does not make sense. The Python interpreter was kind enough to give us a nice error message saying that we cannot have a str and a str operand type for the subtraction operation. It also makes sense that we can’t do multiplication, raising of power, etc., with two strings. How about multiplying a string by an integer?\n\n'Hello, world.' * 3\n\n'Hello, world.Hello, world.Hello, world.'\n\n\nYes, this makes sense! Multiplication by an integer is the same thing as just adding multiple times, so the Python interpreter concatenates the string several times.\nAs a final note on operators with strings, watch out for this:\n\n'4' + '2'\n\n'42'\n\n\nThe result is not 6, but it is a string containing the characters '4' and '2'.\n\n\nF.2.5 Order of operations\nThe order of operations is also as we would expect. Exponentiation comes first, followed by multiplication and division, floor division, and modulo. Next comes addition and subtraction. In order of precedence, our arithmetic operator table is\n\n\n\nprecedence\noperators\n\n\n\n\n1\n**\n\n\n2\n*, /, //, %\n\n\n3\n+, -\n\n\n\nYou can also group operations with parentheses. Operations within parentheses is are always evaluated first. As a watchout, do not use excessive parentheses. So often, we see students not trusting the order of operations and polluting their code with lots of parentheses, making it unreadable. This has been the source of countless bugs we have encountered in student code through the years.\nLet’s practice order-of-operations.\n\n1 + 4**2\n\n17\n\n\n\n1 + 4/2\n\n3.0\n\n\n\n1**3 + 2**3 + 3**3 + 4**3\n\n100\n\n\n\n(1 + 2 + 3 + 4)**2\n\n100\n\n\nInterestingly, we also demonstrated that the sum of the first \\(n\\) cubes is equal to the sum of the first \\(n\\) integers squared. Fun!",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#variables-and-assignment-operators",
    "href": "appendices/python_basics/variables_operators_types.html#variables-and-assignment-operators",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.3 Variables and assignment operators",
    "text": "F.3 Variables and assignment operators\nSo far, we have essentially just used Python as an oversized desktop calculator. We would really like to be able to think about our computational problems symbolically. We mentioned variables at the beginning of the tutorial, but in practice we were just using numbers and strings directly. We would like to say that a variable, a, represents an integer and another variable b represents another integer. Then, we could do things like add a and b. So, we see immediately that the variables have to have a type associated with them so the Python interpreter knows what to do when we use operators with them. A variable should also have a value associated with it, so the interpreter knows, e.g., what to add.\nIn order to create, or instantiate, a variable, we can use an assignment operator. This operator is the equals sign. So, let’s make variables a and b and add them.\n\na = 2\nb = 3\na + b\n\n5\n\n\nGreat! We get what we expect! And we still have a and b.\n\na, b\n\n(2, 3)\n\n\nNow, we might be tempted to say, “a is two.” No. a is not two. a is a variable that has a value of 2. A variable in Python is not just its value. A variable also carries with it a type. It also has more associated with it under the hood of the interpreter that we will not get into. So, you can think about a variable as a map to an address in RAM (called a pointer in computer-speak) that stores information, including a type and a value.\n\nF.3.1 Assignment/increment operators\nNow, let’s say we wanted to update the value of a by adding 4.1 to it. Python will do some magic for us.\n\nprint(type(a), a)\n\na = a + 4.1\n\nprint(type(a), a)\n\n&lt;class 'int'&gt; 2\n&lt;class 'float'&gt; 6.1\n\n\nWe see that a was initially an integer with a value of 2. But we added 4.1 to it, so the Python interpreter knew to change its type to a float and update its value.\nThis operation of updating a value can also be accomplished with an increment operator.\n\na = 2\na += 4.1\na\n\n6.1\n\n\nThe += operator told the interpreter to take the value of a and add 4.1 to it, changing the type of a in the intuitive way if need be. The other six arithmetic operators have similar constructions, -=, *=, /=, //=, %=, and **=.\n\na = 2\na **= 3\na\n\n8",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#type-conversion",
    "href": "appendices/python_basics/variables_operators_types.html#type-conversion",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.4 Type conversion",
    "text": "F.4 Type conversion\nSuppose you have a variable of one type, and you want to convert it to another. For example, say you have a string, '42', and you want to convert it to an integer. This would happen if you were reading information from a text file, which by definition is full of strings, and you wanted to convert some string to a number. This is done as follows.\n\nmy_str = '42'\nmy_int = int(my_str)\nprint(my_int, type(my_int))\n\n42 &lt;class 'int'&gt;\n\n\nConversely, we can convert an int back to a str.\n\nstr(my_int)\n\n'42'\n\n\nWhen converting a float to an int, the interpreter does not round the result, but gives the floor.\n\nint(2.9)\n\n2\n\n\nAlso consider our string concatenation warning/example from above:\n\nprint('4' + '2')\nprint(int('4') + int('2'))\n\n42\n6",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#relational-operators",
    "href": "appendices/python_basics/variables_operators_types.html#relational-operators",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.5 Relational operators",
    "text": "F.5 Relational operators\nSuppose we want to compare the values of two numbers. We may want to know if they are equal for example. The operator used to test for equality is ==, an example of a relational operator (also called a comparison operator).\n\nF.5.1 The equality relational operator\nLet’s test out the == to see how it works.\n\n5 == 5\n\nTrue\n\n\n\n5 == 4\n\nFalse\n\n\nNotice that using the operator gives either True or False. These are important keywords in Python that indicate truth. True and False have a special type, called bool, short for Boolean.\n\ntype(True)\n\nbool\n\n\n\ntype(False)\n\nbool\n\n\nThe equality operator, like all relational operators in Python, also works with variables, testing for equality of their values. Equality of the variables themselves uses identity operators, described below.\n\na = 4\nb = 5\nc = 4\n\na == b\n\nFalse\n\n\n\na == c\n\nTrue\n\n\nNow, let’s try it out with some floats.\n\n5.3 == 5.3\n\nTrue\n\n\n\n2.1 + 3.2 == 5.3\n\nFalse\n\n\nYikes! Python is telling us that 2.1 + 3.2 is not 5.3. This is floating point arithmetic haunting us. Note that floating point numbers that can be exactly represented with binary numbers do not have this problem.\n\n2.2 + 3.2 == 5.4\n\nTrue\n\n\nThis behavior is unpredictable, so here is a rule.\n\nNever use the == operator with floats.\n\n\n\nF.5.2 Other relational operators\nAs you might expect, there are other relational operators. The relational operators are\n\n\n\nEnglish\nPython\n\n\n\n\nis equal to\n==\n\n\nis not equal to\n!=\n\n\nis greater than\n&gt;\n\n\nis less than\n&lt;\n\n\nis greater than or equal to\n&gt;=\n\n\nis less than or equal to\n&lt;=\n\n\n\nWe can try some of them out!\n\n4 &lt; 5\n\nTrue\n\n\n\n5.7 &lt;= 3\n\nFalse\n\n\n\n'michael jordan' &gt; 'lebron james'\n\nTrue\n\n\nWhoa. What happened on that last one? The Python interpreter has weighed in on the debate about the greatest basketball player of all time. It clearly thinks Michael Jordan is better than LeBron James, but that seems kind of subjective. To understand what the interpreter is doing, we need to understand how it compares strings.\n\n\nF.5.3 A brief aside on Unicode\nIn Python, characters are encoded with Unicode. This is a standardized library of characters from many languages around the world that contains over 100,000 characters. Each character has a unique number associated with it. We can access what number is assigned to a character using Python’s built-in ord() function.\n\nord('a')\n\n97\n\n\n\nord('λ')\n\n955\n\n\nThe relational operators on characters compare the values that the ord function returns. So, using a relational operator on 'a' and 'b' means you are comparing ord('a') and ord('b'). When comparing strings, the interpreter first compares the first character of each string. If they are equal, it compares the second character, and so on. So, the reason that 'michael jordan' &gt; 'lebron james' gives a value of True is because ord('m') &gt; ord('l').\nNote that a result of this scheme is that testing for equality of strings means that all characters must be equal. This is the most common use case for relational operators with strings.\n\n'lebron' == 'lebron james'\n\nFalse\n\n\n\n'lebron' == 'LeBron'\n\nFalse\n\n\n\n'LeBron James' == 'LeBron James'\n\nTrue\n\n\n\n'AGTCACAGTA' == 'AGTCACAGCA'\n\nFalse\n\n\n\n\nF.5.4 Chaining relational operators\nPython allow chaining of relational operators.\n\n4 &lt; 6 &lt; 6.1 &lt; 9.3\n\nTrue\n\n\n\n4 &lt; 6.1 &lt; 6 &lt; 9.3\n\nFalse\n\n\nThis is convenient do to. However, it is important not to do the following, even though it is legal.\n\n4 &lt; 6.1 &gt; 5\n\nTrue\n\n\nIn other words, do not mix the direction of the relational operators. You could run into trouble because, in this case, 5 and 4 are never compared. An expression with different relations among all three numbers also returns True.\n\n4 &lt; 6.1 &gt; 3\n\nTrue\n\n\nSo, I issue a warning.\n\nDo not mix the directions of chained relational operators.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#identity-operators",
    "href": "appendices/python_basics/variables_operators_types.html#identity-operators",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.6 Identity operators",
    "text": "F.6 Identity operators\nIdentity operators check to see if two variables occupy the same space in memory; i.e., they are the same object (we’ll learn more about objects as we go along). This is different that the equality relational operator, ==, which checks to see if two variables have the same value. The two identity operators are in the table below.\n\n\n\nEnglish\nPython\n\n\n\n\nis the same object\nis\n\n\nis not the same object\nis not\n\n\n\nThat’s right. The operators are pretty much the same as English! Let’s see these operators in action and get at the difference between == and is. Let’s use the is operator to investigate how Python stored variables in memory, starting with floats.\n\na = 5.6\nb = 5.6\n\na == b, a is b\n\n(True, False)\n\n\nEven though a and b have the same value, they are stored in different places in memory. They can occupy the same place in memory if we do a b = a assignment.\n\na = 5.6\nb = a\n\na == b, a is b\n\n(True, True)\n\n\nBecause we assigned b = a, they necessarily have the same (immutable) value. So, the two variables occupy the same place in memory for efficiency.\n\na = 5.6\nb = a\na = 6.1\n\na == b, a is b\n\n(False, False)\n\n\nIn the last two examples, we see that assigning b = a, where a is a float in this case, means that a and b occupy the same memory. However, reassigning the value of a resulted in the interpreter placing a in a new space in memory. We can double check the values.\nIntegers sometimes do not behave the same way, however.\n\na = 5\nb = 5\n\na == b, a is b\n\n(True, True)\n\n\nEven though we assigned a and b separately, they occupy the same place in memory. This is because Python employs integer caching for all integers between -5 and 256. This caching does not happen for more negative or larger integers.\n\na = 350\nb = 350\n\na is b\n\nFalse\n\n\nNow, let’s look at strings.\n\na = 'Hello, world.'\nb = 'Hello, world.'\n\na == b, a is b\n\n(True, False)\n\n\nSo, even though a and b have the same value, they do not occupy the same place in memory. If we do a b = a assignment, we get similar results as with floats.\n\na = 'Hello, world.'\nb = a\n\na == b, a is b\n\n(True, True)\n\n\nLet’s try string assignment again with a different string.\n\na = 'python'\nb = 'python'\n\na == b, a is b\n\n(True, True)\n\n\nWait a minute! If we choose a string 'python', it occupies the same place in memory as another variable with the same value, but that was not the case for 'Hello, world.'. This is a result of Python also doing string interning which allows for (sometimes very) efficient string processing. Whether two strings occupy the same place in memory depends on what the strings are.\nThe caching and interning might be a problem, but you generally do not need to worry about it for immutable variables. Being immutable means that once the variables are created, their values cannot be changed. If we do change the value the variable gets a new place in memory. All variables we’ve encountered so far, ints, floats, and strs, are immutable. We will see encounter mutable data types in future lesson, in which case it really does matter practically to you as a programmer whether or not two variables are in the same location in memory.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#logical-operators",
    "href": "appendices/python_basics/variables_operators_types.html#logical-operators",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.7 Logical operators",
    "text": "F.7 Logical operators\nLogical operators can be used to connect relational and identity operators. Python has three logical operators.\n\n\n\nLogic\nPython\n\n\n\n\nAND\nand\n\n\nOR\nor\n\n\nNOT\nnot\n\n\n\nThe and operator means that if both operands are True, return True. The or operator gives True if either of the operands are True. Finally, the not operator negates the logical result.\nThat might be as clear as mud to you. It is easier to learn this, as usual, by example.\n\nTrue and True\n\nTrue\n\n\n\nTrue and False\n\nFalse\n\n\n\nTrue or False\n\nTrue\n\n\n\nTrue or True\n\nTrue\n\n\n\nnot False and True\n\nTrue\n\n\n\nnot(False and True)\n\nTrue\n\n\n\nnot False or True\n\nTrue\n\n\n\nnot (False or True)\n\nFalse\n\n\n\n7 == 7 or 7.6 == 9.1\n\nTrue\n\n\n\n7 == 7 and 7.6 == 9.1\n\nFalse\n\n\nI think these examples will help you get the hang of it. Note that it is important to specify the ordering of your operations, particularly when using the not operator.\nNote also that\na &lt; b &lt; c\nis equivalent to\n(a &lt; b) and (b &lt; c)\nWith these new types of operators in hand, we can construct a more complete table of operator precedence.\n\n\n\nprecedence\noperators\n\n\n\n\n1\n**\n\n\n2\n*, /, //, %\n\n\n3\n+, -\n\n\n4\n&lt;, &gt;, &lt;=, &gt;=\n\n\n5\n==, !=\n\n\n6\n=, +=, -=, *=, /=, **=, %=, //=\n\n\n7\nis, is not\n\n\n8\nand, or, not",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#operators-we-left-out",
    "href": "appendices/python_basics/variables_operators_types.html#operators-we-left-out",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.8 Operators we left out",
    "text": "F.8 Operators we left out\nWe have left out a few operators in Python. Two that we left out are the membership operators, in and not in, which we will visit in forthcoming sections of this appendix. The others we left out are bitwise operators and operators on sets, which we will not be covering.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#the-numerical-values-of-true-and-false",
    "href": "appendices/python_basics/variables_operators_types.html#the-numerical-values-of-true-and-false",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.9 The numerical values of True and False",
    "text": "F.9 The numerical values of True and False\nAs we move to conditionals, it is important to take a moment to evaluate the numerical values of the keywords True and False. They have numerical values of 1 and 0, respectively.\n\nTrue == 1\n\nTrue\n\n\n\nFalse == 0\n\nTrue\n\n\nYou can do arithmetic on True and False, but you will get implicit type conversion.\n\nTrue + False\n\n1\n\n\n\ntype(True + False)\n\nint",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#conditionals",
    "href": "appendices/python_basics/variables_operators_types.html#conditionals",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.10 Conditionals",
    "text": "F.10 Conditionals\nConditionals are used to tell your computer to do a set of instructions depending on whether or not a Boolean is True. In other words, we are telling the computer:\nif something is true:\n    do task a\notherwise:\n    do task b\nIn fact, the syntax in Python is almost exactly the same. As an example, let’s ask whether or not a codon is the canonical start codon (AUG).\n\ncodon = 'AUG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\n\nThis codon is the start codon.\n\n\nThe syntax of the if statement is apparent in the above example. The Boolean expression, codon == 'AUG', is called the condition. If it is True, the indented statement below it is executed. This brings up a very important aspect of Python syntax.\n\nIndentation matters.\n\nAny lines with the same level of indentation will be evaluated together.\n\ncodon = 'AUG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\n    print('Same level of intentation, so still printed!')\n\nThis codon is the start codon.\nSame level of intentation, so still printed!\n\n\nWhat happens if our codon is not the start codon?\n\ncodon = 'AGG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\n\nNothing is printed. This is because we did not tell Python what to do if the Boolean expression codon == 'AUG' evaluated False. We can add that with an else clause in the conditional.\n\ncodon = 'AGG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\nelse:\n    print('This codon is not the start codon.')\n\nThis codon is not the start codon.\n\n\nGreat! Now, we have a construction that can choose which action to take depending on a value. So, if we’re zooming along an RNA sequence, we could pick out the start codon and infer where translation would start. Now, what if we want to know if we hit a canonical stop codon (UAA, UAG, or UGA)? We can nest the conditionals!\n\ncodon = 'UAG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\nelse:\n    if codon == 'UAA' or codon == 'UAG' or codon == 'UGA':\n        print('This codon is a stop codon.')\n    else:\n        print('This codon is neither a start nor stop codon.')\n\nThis codon is a stop codon.\n\n\nNotice that the indentation defines which clause the statement belongs to. E.g., the second if statement is executed as part of the first else clause.\nWhile this nesting is very nice, we can be more concise by using an elif clause.\n\ncodon = 'UGG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\nelif codon == 'UAA' or codon == 'UAG' or codon == 'UGA':\n    print('This codon is a stop codon.')\nelse:\n    print('This codon is neither a start nor stop codon.')\n\nThis codon is neither a start nor stop codon.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#computing-environment",
    "href": "appendices/python_basics/variables_operators_types.html#computing-environment",
    "title": "Appendix F — Variables, operators, and types",
    "section": "Computing environment",
    "text": "Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.10.9\nIPython version      : 8.10.0\n\njupyterlab: 3.5.3",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/lists_and_tuples.html",
    "href": "appendices/python_basics/lists_and_tuples.html",
    "title": "Appendix G — Lists and tuples",
    "section": "",
    "text": "G.1 Lists\n| Download notebook\nIn this tutorial, we will explore two important data types in Python, lists and tuples. They are both sequences of objects. Just like a string is a sequence (that is, an ordered collection) of characters, lists and tuples are sequences of arbitrary objects, called items or elements. They are a way to make a single object that contains many other objects. We will start our discussion with lists.\nAs usual, it is easiest to explore new topics by example. We’ll start by creating a list.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/lists_and_tuples.html#lists",
    "href": "appendices/python_basics/lists_and_tuples.html#lists",
    "title": "Appendix G — Lists and tuples",
    "section": "",
    "text": "G.1.1 List creation\nWe create lists by putting Python values or expressions inside square brackets, separated by commas. For example:\n\nmy_list_1 = [1, 2, 3, 4]\ntype(my_list_1)\n\nlist\n\n\nWe observe here that although the elements of the list are ints, the type of the list is list. Actually, any Python expression can be inside a list (including another list!):\n\nmy_list_2 = [1, 2.4, 'a string', ['a string in another list', 5]]\nmy_list_2\n\n[1, 2.4, 'a string', ['a string in another list', 5]]\n\n\n\nmy_list_3 = [2+3, 5*3, 4**2]\nmy_list_3\n\n[5, 15, 16]\n\n\nmy_list_2 contains ints, a float, a string and another list. And our third list contains expressions that get evaluated when the list as a whole gets created.\nWe can also create a list by type conversion. For example, we can convert a string into a list of characters.\n\nmy_str = 'A string.'\nlist(my_str)\n\n['A', ' ', 's', 't', 'r', 'i', 'n', 'g', '.']\n\n\n\n\nG.1.2 List operators\nOperators on lists behave much like operators on strings. The + operator on lists means list concatenation.\n\n[1, 2, 3] + [4, 5, 6]\n\n[1, 2, 3, 4, 5, 6]\n\n\nThe * operator on lists means list replication and concatenation.\n\n[1, 2, 3] * 3\n\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n\n\n\nG.1.2.1 Membership operators\nMembership operators are used to determine if an item is in a list. The two membership operators are:\n\n\n\nEnglish\noperator\n\n\n\n\nis a member of\nin\n\n\nis not a member of\nnot in\n\n\n\n\nThe result of the operator is True or False. Let’s look at my_list_2 again:\n\nmy_list_2 = [1, 2.4, 'a string', ['a string in another list', 5]]\n1 in my_list_2\n\nTrue\n\n\n\n['a string in another list', 5] in my_list_2\n\nTrue\n\n\n\n'a string in another list' in my_list_2\n\nFalse\n\n\n\n7 not in my_list_2\n\nTrue\n\n\nImportantly, we see that the string 'a string in another list' is not in my_list_2. This is because that string itself is not one of the four items of my_list_2. The string 'a string in another list' is in a list that is an item in my_list_2.\nNow, these membership operators offer a great convenience for conditionals. Remember our example about stop codons?\n\ncodon = 'UGG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\nelif codon == 'UAA' or codon == 'UAG' or codon == 'UGA':\n    print('This codon is a stop codon.')\nelse:\n    print('This codon is neither a start nor stop codon.')\n\nThis codon is neither a start nor stop codon.\n\n\nWe can rewrite this much more cleanly, and with a lower chance of bugs, using a list and the in operator.\n\n# Make a list of stop codons\nstop_codons = ['UAA', 'UAG', 'UGA']\n\n# Specify codon\ncodon = 'UGG'\n\n# Check to see if it is a start or stop codon\nif codon == 'AUG':\n    print('This codon is the start codon.')\nelif codon in stop_codons:\n    print('This codon is a stop codon.')\nelse:\n    print('This codon is neither a start nor stop codon.')\n\nThis codon is neither a start nor stop codon.\n\n\nThe simple expression\ncodon in stop_codons\nreplaced the more verbose\ncodon == 'UAA' or codon == 'UAG' or codon == 'UGA'\nMuch nicer!\n\n\n\nG.1.3 List indexing\nImagine that we would like to access an item in a list. Because a list is ordered, we can ask for the first item, the second item, the nth item, the last item, etc. This is done using a bracket notation. We first write the name of our list and then enclosed in square brackets we write the location (index) of the desired element:\n\nmy_list = [1, 2.4, 'a string', ['a string in another list', 5]]\n\nmy_list[1]\n\n2.4\n\n\nWait a minute! Shouldn’t my_list[1] give the first item in the list? It seems to give the second. This is because indexing in Python starts at zero. This is very important. (Historical note: Why Python uses 0-based indexing It is also worth reading Edsgar Dijkstra’s thoughts on the matter.)\n\nIndexing in Python starts at zero.\n\nNow that we know that, let’s look at the items in the list.\n\nprint(my_list[0])\nprint(my_list[1])\nprint(my_list[2])\nprint(my_list[3])\n\n1\n2.4\na string\n['a string in another list', 5]\n\n\nWe can also index the list that is within my_list by adding another set of brackets.\n\nmy_list[3][0]\n\n'a string in another list'\n\n\nSo, now we have the basics of list indexing. There are more ways to specify items in a list. We’ll look at some of these now, but in order to do it, it helps to have a simpler list. We’ll therefore create a list that goes from zero to ten.\n\nmy_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nmy_list[4]\n\n4\n\n\nWe already knew that would be the result. We can use negative indexing as well! This just means we start indexing from the last entry, starting at -1.\n\nmy_list[-1]\n\n10\n\n\n\nmy_list[-3]\n\n8\n\n\nThis is very convenient for indexing in reverse. Now make it more clear, here are the forward and backward indices for the list:\n\n\n\nValues\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nForward indices\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nReverse indices\n-11\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n\n\n\n\n\nG.1.4 List slicing\nNow, what if we want to pull out multiple items in a list, called slicing? We can use colons (:) for that.\n\nmy_list[0:5]\n\n[0, 1, 2, 3, 4]\n\n\nWe got elements 0 through 4. When using the colon indexing, my_list[i:j], we get items i through j-1. I.e., the range is inclusive of the first index and exclusive of the last. If the slice’s final index is larger than the length of the sequence, the slice ends at the last element.\n\nmy_list[3:1000]\n\n[3, 4, 5, 6, 7, 8, 9, 10]\n\n\nNow, we can also use negative indices with colons.\n\nmy_list[0:-3]\n\n[0, 1, 2, 3, 4, 5, 6, 7]\n\n\nAgain, note that we only went to index -4.\nWe can also specify a stride. The stride comes after a second colon. For example, if we only wanted the even numbers, we could do the following.\n\nmy_list[0::2]\n\n[0, 2, 4, 6, 8, 10]\n\n\nNotice that we did not enter anything for the end value of the slice. If the end is left blank, the default is to include the entire string. Similarly, we can leave out the start index, as its default is zero.\n\nmy_list[::2]\n\n[0, 2, 4, 6, 8, 10]\n\n\nSo, in general, the indexing scheme is:\n    my_list[start:end:stride]\n\nIf there are no colons, a single element is returned.\nIf there are any colons, we are slicing the list, and a list is returned.\nIf there is one colon, stride is assumed to be 1.\nIf start is not specified, it is assumed to be zero.\nIf end is not specified, the interpreted assumed you want the entire list.\nIf stride is not specified, it is assumed to be 1.\n\nWith this in hand, we do lots of crazy slicing. We can even use a negative stride, which results in reversing the list.\n\nmy_list[::-1]\n\n[10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n\n\nNote that the meaning of the “start” and “end” index is a bit ambiguous when you have a negative stride. When the stride is negative, we still slice from start to end, but then reverse the order.\nNow, let’s look at a few examples (inspired by Brett Slatkin).\n\nprint(my_list[2::2])\nprint(my_list[2:-1:2])\nprint(my_list[-2::-2])\nprint(my_list[-2:2:-2])\nprint(my_list[2:2:-2])\n\n[2, 4, 6, 8, 10]\n[2, 4, 6, 8]\n[9, 7, 5, 3, 1]\n[9, 7, 5, 3]\n[]\n\n\nYou can see that it takes a lot of thought to understand what the slices actually are. So, here is some good advice: Do not use start, end, and slice all at the same time (even though you can). Do the stride first and then the slice, on separate lines. For example, if we wanted just the even numbers, but not the first and last (this was the my_list[2:-1:2] example we just did), we would do\n\n# Extract evens\nevens = my_list[::2]\n\n# Cut off end values\nevens_without_end_values = evens[1:-1]\n\nevens_without_end_values\n\n[2, 4, 6, 8]\n\n\nThis is more verbose, but much easier to read and understand.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/lists_and_tuples.html#mutability",
    "href": "appendices/python_basics/lists_and_tuples.html#mutability",
    "title": "Appendix G — Lists and tuples",
    "section": "G.2 Mutability",
    "text": "G.2 Mutability\nLists are mutable. That means that you can change their values without creating a new list. (You cannot change the data type or identity.) Let’s see this by example.\n\nmy_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nmy_list[3] = 'four'\n\nmy_list\n\n[1, 2, 3, 'four', 5, 6, 7, 8, 9, 10]\n\n\nThe other data types we have encountered so far, ints, floats, and strs, are immutable. You cannot change their values without reassigning them. To see this, we’ll use the id() function, which tells us where in memory that the variable is stored. (Note: this identity is unique to the Python interpreter, and should not be considered an actual physical address in memory.)\n\na = 689\nprint(id(a))\n\na = 690\nprint(id(a))\n\n4400362064\n4400362832\n\n\nSo, we see that the identity of a, an integer, changed when we tried to change its value. So, we didn’t actually change its value; we made a new variable. With lists, though, this is not the case.\n\nprint(id(my_list))\n\nmy_list[0] = 'zero'\nprint(id(my_list))\n\n4402226560\n4402226560\n\n\nIt is still the same list! This is very important to consider when we do assignments.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/lists_and_tuples.html#pitfall-aliasing",
    "href": "appendices/python_basics/lists_and_tuples.html#pitfall-aliasing",
    "title": "Appendix G — Lists and tuples",
    "section": "G.3 Pitfall: Aliasing",
    "text": "G.3 Pitfall: Aliasing\nAliasing is a subtle issue which can come up when assigning lists to variables. Let’s look at an example. We will make a list, then assign a new variable to the list (which we will momentarily erroneously think of as making a copy of the list) and then change a value of an entry in the “copied” list.\n\nmy_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nmy_list_2 = my_list     # copy of my_list?\nmy_list_2[0] = 'a'\n\nmy_list_2\n\n['a', 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nNow, let’s look at our original list to see what it looks like.\n\nmy_list\n\n['a', 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nSo we see that assigning a list to a variable does not copy the list! Instead, you just get a new reference to the same value. This has the real potential to introduce a nasty bug that will bite you!\nThere is a way we can avoid this problem by using list slices. If both the slice’s starting index and the slice’s ending index of a list are left out, the slice is a copy of the entire list in a new hunk of memory.\n\nmy_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nmy_list_2 = my_list[:]\nmy_list_2[0] = 'a'\n\nmy_list_2\n\n['a', 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\nmy_list\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nAnother option is to use a data type that is very much like a list, except it is immutable.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/lists_and_tuples.html#tuples",
    "href": "appendices/python_basics/lists_and_tuples.html#tuples",
    "title": "Appendix G — Lists and tuples",
    "section": "G.4 Tuples",
    "text": "G.4 Tuples\nA tuple is just like a list, except it is immutable (basically a read-only list). (What I just said there is explosive, as described in this blog post. Tuples do have many other capabilities beyond what you would expect from just bring “a read-only list,” but for us just beginning now, we can think of it that way.) A tuple is created just like a list, except we use parentheses instead of brackets. The only watch-out is that a tuple with a single item needs to include a comma after the item.\n\nmy_tuple = (0,)\n\nnot_a_tuple = (0) # this is just the number 0 (normal use of parantheses)\n\ntype(my_tuple), type(not_a_tuple)\n\n(tuple, int)\n\n\nWe can also create a tuple by doing a type conversion. We can convert our list to a tuple.\n\nmy_list = [1, 2.4, 'a string', ['a sting in another list', 5]]\n\nmy_tuple = tuple(my_list)\n\nmy_tuple\n\n(1, 2.4, 'a string', ['a sting in another list', 5])\n\n\nNote that the list within my_list did not get converted to a tuple. It is still a list, and it is mutable.\n\nmy_tuple[3][0] = 'a string in a list in a tuple'\n\nmy_tuple\n\n(1, 2.4, 'a string', ['a string in a list in a tuple', 5])\n\n\nHowever, if we try to change an item in a tuple, we get an error.\n\nmy_tuple[1] = 7\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[37], line 1\n----&gt; 1 my_tuple[1] = 7\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\nEven though the list within the tuple is mutable, we still cannot change the identity of that list.\n\nmy_tuple[3] = ['a', 'new', 'list']\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[38], line 1\n----&gt; 1 my_tuple[3] = ['a', 'new', 'list']\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\nG.4.1 Slicing of tuples\nSlicing of tuples is the same as lists, except a tuple is returned from the slicing operation, not a list.\n\nmy_tuple = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n# Reverse\nmy_tuple[::-1]\n\n(10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)\n\n\n\n# Odd numbers\nmy_tuple[1::2]\n\n(1, 3, 5, 7, 9)\n\n\n\n\nG.4.2 The + operator with tuples\nAs with lists we can concatenate tuples with the + operator.\n\nmy_tuple + (11, 12, 13, 14, 15)\n\n(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)\n\n\n\n\nG.4.3 Membership operators with tuples\nMembership operators work the same as with lists.\n\n5 in my_tuple\n\nTrue\n\n\n\n'LeBron James' not in my_tuple\n\nTrue\n\n\n\n\nG.4.4 Tuple unpacking\nIt is like a multiple assignment statement that is best seen through example.\n\nmy_tuple = (1, 2, 3)\n(a, b, c) = my_tuple\n\na\n\n1\n\n\n\nb\n\n2\n\n\n\nc\n\n3\n\n\nThis is useful when we want to return more than one value from a function and further using the values as stored in different variables. We will make use of this as the bootcamp goes on; we’ll be writing functions in just a couple lessons!\nNote that the parentheses are dispensable.\n\na, b, c = my_tuple\n\nprint(a, b, c)\n\n1 2 3",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/lists_and_tuples.html#wisdom-on-tuples-and-lists",
    "href": "appendices/python_basics/lists_and_tuples.html#wisdom-on-tuples-and-lists",
    "title": "Appendix G — Lists and tuples",
    "section": "G.5 Wisdom on tuples and lists",
    "text": "G.5 Wisdom on tuples and lists\nAt face, tuples and lists are very similar, differing essentially only in mutability. The differences are actually more profound, as described in the aforementioned blog post. We will make extensive use of them in our programs.\n“When should I use a tuple and when should I use a list?” you ask. Here is my advice.\n\nAlways use tuples instead of lists unless you need mutability.\n\nThis keeps you out of trouble. It is very easy to inadvertently change one list, and then another list (that is actually the same, but with a different variable name) gets mangled. That said, mutability is often very useful, so you can use it to make your list and adjust it as you need. However, after you have finalized your list, you should convert it to a tuple so it cannot get mangled. We’ll come back to this later in the bootcamp.\nSo, I ask you, which is better?\n\n# Should it be a list?\nstop_codons = ['UAA', 'UAG', 'UGA']\n\n# or a tuple?\nstop_codons = ('UAA', 'UAG', 'UGA')",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/lists_and_tuples.html#computing-environment",
    "href": "appendices/python_basics/lists_and_tuples.html#computing-environment",
    "title": "Appendix G — Lists and tuples",
    "section": "G.6 Computing environment",
    "text": "G.6 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/iteration.html",
    "href": "appendices/python_basics/iteration.html",
    "title": "Appendix H — Iteration",
    "section": "",
    "text": "H.1 Introducing the for loop\n| Download notebook\nWe often want a computer program to do a similar operation many times. For example, we may want to analyze many codons, one after another, and find the start and stop codons in order to determine the length of the open reading frame. Or, as a simpler example, we may wish to find the GC content of a specific sequence. We check each base to see if it is G or C and keep a count. Doing these repeated operations in a program is called iteration.\nThe first method of iteration we will discuss is the for loop. As an example of its use, we compute the GC content of an RNA sequence.\n# The sequence we want to analyze\nseq = 'GACAGACUCCAUGCACGUGGGUAUCUGUC'\n\n# Initialize GC counter\nn_gc = 0\n\n# Initialize sequence length\nlen_seq = 0\n\n# Loop through sequence and count G's and C's\nfor base in seq:\n    len_seq += 1\n    if base in 'GCgc':\n        n_gc += 1\n        \n# Divide to get GC content\nn_gc / len_seq\n\n0.5517241379310345\nLet’s look carefully at what we did here. We took a string containing a sequence of nucleotides and then we did something for each character (base) in that string (nucleic acid sequence). A string is a sequence in the sense of the programming language as well; just like a list or tuple, the string is an ordered collection of characters. (So as not to confuse between biological sequences and sequences as a part of the Python language, we will always write the latter in italics.)\nNow, let’s translate the new syntax in the above code to English.\nPython: for base in seq:\nEnglish: for each character in the string whose variable name is seq, do the following, with that character taking the name base\nThis exposes a general way of doing things repeatedly in Python. For every item in a sequence, we do something. That something follows the for clause and is contained in an indentation block. When we do this, we say are “looping over a sequence.” In the context of a for clause, the membership operator, in, means that we consider in order each item in the sequence or iterator (we’ll talk about iterators in a moment).\nNow, looking within the loop, the first thing we do is increment the length of the sequence. For each base we encounter in the loop, we add one to the sequence length. Makes sense!\nNext, we have an if statement. We use the membership operator again. We ask if the current base is a G or a C. To be safe, we also included lower case characters in case the sequence was entered that way. If the base is a G or a C, we increment the counter of GC bases by one.\nFinally, we get the fractional GC content by dividing the number of GC’s by the total length of the sequence.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/iteration.html#introducing-the-for-loop",
    "href": "appendices/python_basics/iteration.html#introducing-the-for-loop",
    "title": "Appendix H — Iteration",
    "section": "",
    "text": "H.1.1 Aside: the len() function\nNote in the last example that we determined the length of the RNA sequence by iterating the len_seq counter within the for loop. This works, but Python has a built-in function (like print() is a built-in function) to compute the length of a string (or list, tuple, or other sequence). To find out the length of a sequence, simply use it as an argument to the len() function.\n\nlen(seq)\n\n29\n\n\n\n\nH.1.2 Example and watchout: modifying a list\nLet’s look at another example of iterating through a list. Say we have a list of integers, and we want to change it by doubling each one. Let’s just do it.\n\n# We'll do one through 5\nmy_integers = [1, 2, 3, 4, 5]\n\n# Double each one\nfor n in my_integers:\n    n *= 2\n    \n# Check out the result\nmy_integers\n\n[1, 2, 3, 4, 5]\n\n\nWhoa! It didn’t seem to double any of the integers! This is because my_integers was converted to an iterator in the for clause, and the iterator returns a copy of the item in a list, not a reference to it. Therefore, the n inside the for block is not a view into the original list, and doubling it does nothing meaningful.\nWe’ve seen how to change individual list elements with indexing:\n\n# Don't do things this way\nmy_integers[0] *= 2\nmy_integers[1] *= 2\nmy_integers[2] *= 2\nmy_integers[3] *= 2\nmy_integers[4] *= 2\n\nmy_integers\n\n[2, 4, 6, 8, 10]\n\n\nBut we’d obviously like a better way to do this, with less typing and without knowing ahead of time the length of the list. Let’s look at a new concept that will help with this example.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/iteration.html#iterators",
    "href": "appendices/python_basics/iteration.html#iterators",
    "title": "Appendix H — Iteration",
    "section": "H.2 Iterators",
    "text": "H.2 Iterators\nIn the previous example, we iterated over a sequence. A sequence is one of many iterable objects, called iterables. Under the hood, the Python interpreter actually converts an iterable to an iterator. An iterator is a special object that gives values in succession. A major difference between a sequence and an iterator is that you cannot index an iterator. This seems like a trivial difference, but iterators make for more efficient computing than directly using a sequence with indexing.\nWe can explicitly convert a sequence to an iterator using the built-in function iter(), but we will not bother with that here because the Python interpreter automatically does this for you when you use a sequence in a loop. (This is incidentally why the previous example didn’t work; when the list is converted to an iterator, a copy is made of each element so the original list is unchanged.)\nInstead, we will now explore how we can create useful iterators using the range(), enumerate(), and zip() functions. I know we have not yet covered functions, but the syntax should not be so complicated that you cannot understand what these functions are doing, just like with the print() and len() functions.\n\nH.2.1 The range() function\nThe range() function gives an iterable that enables counting. Let’s look at an example.\n\nfor i in range(10):\n    print(i, end='  ')\n\n0  1  2  3  4  5  6  7  8  9  \n\n\nWe see that range(10) gives us ten numbers, from 0 to 9. As with indexing, range() inclusively starts at zero by default, and the ending is exclusive.\nIt turns out that the arguments of the range() function work much like indexing. If you have a single argument, you get that many integers, starting at 0 and incrementing by one. If you give two arguments, you start inclusively at the first and increment by one ending exclusively at the second argument. Finally, you can specify a stride with the third argument.\n\n# Print numbers 2 through 9\nfor i in range(2, 10):\n    print(i, end='  ')\n\n# Print a newline\nprint()\n    \n# Print even numbers, 2 through 9\nfor i in range(2, 10, 2):\n    print(i, end='     ')\n\n2  3  4  5  6  7  8  9  \n2     4     6     8     \n\n\nIt is often useful to make a list or tuple that has the same entries that a corresponding range object would have. We can do this with type conversion.\n\nlist(range(10))\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nWe can use the range() function along with the len() function on lists to double the elements of our list from a bit ago:\n\nmy_integers = [1, 2, 3, 4, 5]\n\n# Since len(my_integers) = 5, this takes i from 0 to 4, \n# exactly the indices of my_integers\nfor i in range(len(my_integers)):\n    my_integers[i] *= 2\n    \nmy_integers\n\n[2, 4, 6, 8, 10]\n\n\nThis works, but there is an even better way to do this, with the next function.\n\n\nH.2.2 The enumerate() function\nLet’s say we want to print the indices of all G bases in a DNA sequence. We could do this by modifying our previous program.\n\n# Initialize GC counter\nn_gc = 0\n\n# Initialized sequence length\nlen_seq = 0\n\n# Loop through sequence and print index of G's\nfor base in seq:\n    if base in 'Gg':\n        print(len_seq, end='  ')\n    len_seq += 1\n\n0  4  12  16  18  19  20  26  \n\n\nThis is not so bad, but there is an easier way to do this. The enumerate() function gives an iterator that provides both the index and the item of a sequence. Again, this is best demonstrated in practice.\n\n# Loop through sequence and print index of G's\nfor i, base in enumerate(seq):\n    if base in 'Gg':\n        print(i, end='  ')\n\n0  4  12  16  18  19  20  26  \n\n\nThe enumerate() function allowed us to use an index and a base at the same time. To make it more clear, we can print the index and base type for each base in the sequence.\n\n# Print index and identity of bases\nfor i, base in enumerate(seq):\n    print(i, base)\n\n0 G\n1 A\n2 C\n3 A\n4 G\n5 A\n6 C\n7 U\n8 C\n9 C\n10 A\n11 U\n12 G\n13 C\n14 A\n15 C\n16 G\n17 U\n18 G\n19 G\n20 G\n21 U\n22 A\n23 U\n24 C\n25 U\n26 G\n27 U\n28 C\n\n\nThe enumerate() function is really useful and should be used in favor of just doing indexing. For example, many programmers, especially those first trained in lower-level languages, would write the above code similar to how we did the list doubling, with the range() and len() functions, but this is not good practice in Python.\nUsing enumerate(), the list doubling code looks like this:\n\nmy_integers = [1, 2, 3, 4, 5]\n\n# Double each one\nfor i, _ in enumerate(my_integers):\n    my_integers[i] *= 2\n    \n# Check out the result\nmy_integers\n\n[2, 4, 6, 8, 10]\n\n\nenumerate() is more generic and the overhead for returning a reference to an object isn’t an issue. The range(len()) construct will break on an object without support for len(). In addition, you are more likely to introduce bugs by imposing indexing on objects that are iterable but not unambiguously indexable. It is better to use the enumerate() function.\nNote that we used the underscore, _, as a throwaway variable that we do not use. There is no rule for this, but this is generally accepted Python syntax and helps signal that you are not going to use the variable.\nOne last gotcha: if we tried to do a similar technique with a string, we get a TypeError because a string is immutable. We’ll revisit examples like this in the lesson on string methods.\n\n# Try to convert capital G to lower g\nfor i, base in enumerate(seq):\n    if base == 'G':\n        seq[i] = 'g'\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[13], line 4\n      2 for i, base in enumerate(seq):\n      3     if base == 'G':\n----&gt; 4         seq[i] = 'g'\n\nTypeError: 'str' object does not support item assignment\n\n\n\n\n\nH.2.3 The zip() function\nThe zip() function enables us to iterate over several iterables at once. In the example below we iterate over the jersey numbers, names, and positions of the LAFC players who scored in the 2022 MLS Cup Final.\n\nnames = ('Acosta', 'Murillo', 'Bale')\npositions = ('MF', 'D', 'F')\nnumbers = (23, 3, 11)\n\nfor num, pos, name in zip(numbers, positions, names):\n    print(num, name, pos)\n\n23 Acosta MF\n3 Murillo D\n11 Bale F\n\n\n\n\nH.2.4 The reversed() function\nThis function is useful for giving an iterator that goes in the reverse direction. We’ll see that this can be convenient in the next lesson. For now, let’s pretend we’re NASA and need to count down.\n\ncount_up = ('ignition', 1, 2, 3, 4, 5, 6, 7, 8 ,9, 10)\n\nfor count in reversed(count_up):\n    print(count)\n\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\nignition",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/iteration.html#the-while-loop",
    "href": "appendices/python_basics/iteration.html#the-while-loop",
    "title": "Appendix H — Iteration",
    "section": "H.3 The while loop",
    "text": "H.3 The while loop\nThe for loop is very powerful and allows us to construct iterative calculations. When we use a for loop, we need to set up an iterator. A while loop, on the other hand, allows iteration until a conditional expression evaluates False.\nAs an example of a while loop, we will calculate the length of a sequence before hitting a start codon.\n\n# Define start codon\nstart_codon = 'AUG'\n\n# Initialize sequence index\ni = 0\n\n# Scan sequence until we hit the start codon\nwhile seq[i:i+3] != start_codon:\n    i += 1\n    \n# Show the result\nprint('The start codon starts at index', i)\n\nThe start codon starts at index 10\n\n\nLet’s walk through the while loop. The value of i is changing with each iteration, being incremented by one. Each time we consider doing another iteration, the conditional is checked: do the next three bases match the start codon? We set up the conditional to evaluate to True when the bases are not the start codon, so the iteration continues. In other words, iteration continues in a while loop until the conditional returns False.\nNotice that we sliced the string the same way we sliced lists and tuples. In the case of strings, a slice gives another string, i.e., a sequential collection of characters.\nLet’s try looking for another codon…. But, let’s actually not do that. If you run the code below, it will run forever and nothing will get printed to the screen.\n# Define codon of interest\ncodon = 'GCC'\n\n# Initialize sequence index\ni = 0\n\n# Scan sequence until we hit the start codon, but DON'T DO THIS!!!!!\nwhile seq[i:i+3] != codon:\n    i += 1\n    \n# Show the result\nprint('The codon starts at index', i)\nThe reason this runs forever is that the conditional expression in the while statement never returns False. If we slice a string beyond the length of the string we get an empty string result.\n\nseq[100:103]\n\n''\n\n\nThis does not equal the codon we’re interested in, so the while loop keeps going. Forever. This is called an infinite loop, and you definitely to not want these in your code! We can fix it by making a conditional that will evaluate to False if we reach the end of the string.\n\n# Define codon of interest\ncodon = 'GCC'\n\n# Initialize sequence index\ni = 0\n\n# Scan sequence until we hit the start codon or the end of the sequence\nwhile seq[i:i+3] != codon and i &lt; len(seq):\n    i += 1\n    \n# Show the result\nif i == len(seq):\n    print('Codon not found in sequence.')\nelse:\n    print('The codon starts at index', i)\n\nCodon not found in sequence.\n\n\n\nH.3.1 for vs while\nMost anything that requires a loop can be done with either a for loop or a while loop, but there’s a general rule of thumb for which type of loop to use. If you know how many times you have to do something (or if your program knows), use a for loop. If you don’t know how many times the loop needs to run until you run it, use a while loop. For example, when we want to do something with each character in a string or each entry in a list, the program knows how long the sequence is and a for loop is more appropriate. In the previous examples, we don’t know how long it will be before we hit the start codon; it depends on the sequence you put into the program. That makes it more suited to a while loop.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/iteration.html#the-break-and-else-keywords",
    "href": "appendices/python_basics/iteration.html#the-break-and-else-keywords",
    "title": "Appendix H — Iteration",
    "section": "H.4 The break and else keywords",
    "text": "H.4 The break and else keywords\nIteration stops in a for loop when the iterator is exhausted. It stops in a while loop when the conditional evaluates to False. These is another way to stop iteration: the break keyword. Whenever break is encountered in a for or while loop, the iteration halts and execution continues outside the loop. As an example, we’ll do the calculation above with a for loop with a break instead of a while loop.\n\n# Define start codon\nstart_codon = 'AUG'\n\n# Scan sequence until we hit the start codon\nfor i in range(len(seq)):\n    if seq[i:i+3] == start_codon:\n        print('The start codon starts at index', i)\n        break\nelse:\n    print('Codon not found in sequence.')\n\nThe start codon starts at index 10\n\n\nNotice that we have an else block after the for loop. In Python, for and while loops can have an else statement after the code block to be evaluated in the loop. The contents of the else block are evaluated if the loop completes without encountering a break.\n\nH.4.1 A note about use of the word “codon”\nThe astute biologists among you will note that we have not really been using the word “codon” properly here. We are taking it to mean any three consecutive bases, but the more precise definition of a codon means that it is three consecutive bases that code for an amino acid. That means that for a three-base sequence to be a codon, it must be in-register with the start codon.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/iteration.html#computing-environment",
    "href": "appendices/python_basics/iteration.html#computing-environment",
    "title": "Appendix H — Iteration",
    "section": "H.5 Computing environment",
    "text": "H.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html",
    "href": "appendices/python_basics/intro_to_functions.html",
    "title": "Appendix I — Introduction to functions",
    "section": "",
    "text": "I.1 Basic function syntax\n| Download notebook\nA function is a key element in writing programs. You can think of a function in a computing language in much the same way you think of a mathematical function. The function takes in arguments, performs some operation based on the identities of the arguments, and then returns a result. For example, the mathematical function\n\\[\\begin{align}\nf(x, y) = \\frac{x}{y}\n\\end{align}\\]\ntakes arguments \\(x\\) and \\(y\\) and then returns the ratio between the two, \\(x/y\\). In this lesson, we will learn how to construct functions in Python.\nFor our first example, we will translate the above function into Python. A function is defined using the def keyword. This is best seen by example.\ndef ratio(x, y):\n    \"\"\"The ratio of `x` to `y`.\"\"\"\n    return x / y\nFollowing the def keyword is a function signature which indicates the function’s name and its arguments. Just like in mathematics, the arguments are separated by commas and enclosed in parentheses. The indentation following the def line specifies what is part of the function. As soon as the indentation goes to the left again, aligned with def, the contents of the functions are complete.\nImmediately following the function definition is the doc string (short for documentation string), a brief description of the function. The first string after the function definition is always defined as the doc string. Usually, it is in triple quotes, as doc strings often span multiple lines.\nDoc strings are more than just comments for your code, the doc string is what is returned by the native python function help() when someone is looking to learn more about your function. For example:\nhelp(ratio)\n\nHelp on function ratio in module __main__:\n\nratio(x, y)\n    The ratio of `x` to `y`.\nThey are also printed out when you use the ? in a Jupyter notebook or JupyterLab console.\nratio?\n\n\nSignature: ratio(x, y)\nDocstring: The ratio of `x` to `y`.\nFile:      /var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/ipykernel_7590/1007532851.py\nType:      function\nYou are free to type whatever you like in doc strings, or even omit them, but you should always have a doc string with some information about what your function is doing. True, this example of a function is kind of silly, since it is easier to type x / y than ratio(x, y), but it is still good form to have a doc string. This is worth saying explicitly.\nIn the next line of the function, we see a return keyword. Whatever is after the return statement is, you guessed it, returned by the function. Any code after the return is not executed because the function has already returned!",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#basic-function-syntax",
    "href": "appendices/python_basics/intro_to_functions.html#basic-function-syntax",
    "title": "Appendix I — Introduction to functions",
    "section": "",
    "text": "All functions should have doc strings.\n\n\n\nI.1.1 Calling a function\nNow that we have defined our function, we can call it.\n\nratio(5, 4)\n\n1.25\n\n\n\nratio(4, 2)\n\n2.0\n\n\n\nratio(90.0, 8.4)\n\n10.714285714285714\n\n\nIn each case, the function returns a float with the ratio of its arguments.\n\n\nI.1.2 Functions need not have arguments\nA function does not need arguments. As a silly example, let’s consider a function that just returns 42 every time. Of course, it does not matter what its arguments are, so we can define a function without arguments.\n\ndef answer_to_the_ultimate_question_of_life_the_universe_and_everything():\n    \"\"\"Simpler program than Deep Thought's, I bet.\"\"\"\n    return 42\n\nWe still needed the open and closed parentheses at the end of the function name. Similarly, even though it has no arguments, we still have to call it with parentheses.\n\nanswer_to_the_ultimate_question_of_life_the_universe_and_everything()\n\n42\n\n\n\n\nI.1.3 Functions need not return anything\nJust like they do not necessarily need arguments, functions also do not need to return anything. If a function does not have a return statement (or it is never encountered in the execution of the function), the function runs to completion and returns None by default. None is a special Python keyword which basically means “nothing.” For example, a function could simply print something to the screen.\n\ndef think_too_much():\n    \"\"\"Express Caesar's skepticism about Cassius\"\"\"\n    print(\"\"\"Yond Cassius has a lean and hungry look,\nHe thinks too much; such men are dangerous.\"\"\")\n\nWe call this function as all others, but we can show that the result it returns is None.\n\nreturn_val = think_too_much()\n\n# Print a blank line\nprint()\n\n# Print the return value\nprint(return_val)\n\nYond Cassius has a lean and hungry look,\nHe thinks too much; such men are dangerous.\n\nNone",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#built-in-functions-in-python",
    "href": "appendices/python_basics/intro_to_functions.html#built-in-functions-in-python",
    "title": "Appendix I — Introduction to functions",
    "section": "I.2 Built-in functions in Python",
    "text": "I.2 Built-in functions in Python\nThe Python programming language has several built-in functions. We have already encountered print(), id(), ord(), len(), range(), enumerate(), zip(), and reversed(), in addition to type conversions such as list(). The complete set of built-in functions can be found here. A word of warning about these functions and naming your own.\n\nNever define a function or variable with the same name as a built-in function.\n\nAdditionally, Python has keywords (such as def, for, in, if, True, None, etc.), many of which we have already encountered. A complete list of them is here. The interpreter will throw an error if you try to define a function or variable with the same name as a keyword.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#an-example-function-reverse-complement",
    "href": "appendices/python_basics/intro_to_functions.html#an-example-function-reverse-complement",
    "title": "Appendix I — Introduction to functions",
    "section": "I.3 An example function: reverse complement",
    "text": "I.3 An example function: reverse complement\nLet’s write a function that does not do something so trivial as computing ratios or giving us the Answer to the Ultimate Question of Life, the Universe, and Everything. We’ll write a function to compute the reverse complement of a sequence of DNA. Within the function, we’ll use some of our newly acquired iteration skills.\n\ndef complement_base(base):\n    \"\"\"Returns the Watson-Crick complement of a base.\"\"\"\n    if base in 'Aa':\n        return 'T'\n    elif base in 'Tt':\n        return 'A'\n    elif base in 'Gg':\n        return 'C'\n    else:\n        return 'G'\n\n\ndef reverse_complement(seq):\n    \"\"\"Compute reverse complement of a sequence.\"\"\"\n    # Initialize reverse complement\n    rev_seq = ''\n    \n    # Loop through and populate list with reverse complement\n    for base in reversed(seq):\n        rev_seq += complement_base(base)\n        \n    return rev_seq\n\nNote that we do not have error checking here, which we should definitely do, but we’ll cover that in a future lesson. For now, let’s test it to see if it works.\n\nreverse_complement('GCAGTTGCA')\n\n'TGCAACTGC'\n\n\nIt looks good, but we might want to write yet another function to display the template strand (from 5\\('\\) to 3\\('\\)) above its reverse complement (from 3\\('\\) to 5\\('\\)). This makes it easier to verify.\n\ndef display_complements(seq):\n    \"\"\"Print sequence above its reverse complement.\"\"\"\n    # Compute the reverse complement\n    rev_comp = reverse_complement(seq)\n    \n    # Print template\n    print(seq)\n    \n    # Print \"base pairs\"\n    for base in seq:\n        print('|', end='')\n    \n    # Print final newline character after base pairs\n    print()\n            \n    # Print reverse complement\n    for base in reversed(rev_comp):\n        print(base, end='')\n        \n    # Print final newline character\n    print()\n\nLet’s call this function and display the input sequence and the reverse complement returned by the function.\n\nseq = 'GCAGTTGCA'\ndisplay_complements(seq)\n\nGCAGTTGCA\n|||||||||\nCGTCAACGT\n\n\nOk, now it’s clear that the result looks good! This example demonstrates an important programming principle regarding functions. We used three functions to compute and display the reverse complement.\n\ncomplement_base() gives the Watson-Crick complement of a given base.\nreverse_complement() computes the reverse complement.\ndisplay_complements() displays the sequence and the reverse complement.\n\nWe could very well have written a single function to compute the reverse complement with the if statements included within the for loop. Instead, we split this larger operation up into smaller functions. This is an example of modular programming, in which the desired functionality is split up into small, independent, interchangeable modules. This is a very, very important concept.\n\nWrite small functions that do single, simple tasks.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#pause-and-think-about-testing",
    "href": "appendices/python_basics/intro_to_functions.html#pause-and-think-about-testing",
    "title": "Appendix I — Introduction to functions",
    "section": "I.4 Pause and think about testing",
    "text": "I.4 Pause and think about testing\nLet’s pause for a moment and think about what the complement_base() and reverse_complement() functions do. They do a well-defined operation on string inputs. If we’re doing some bioinformatics, we might use these functions over and over again. We should therefore thoroughly test the functions. For example, we should test that reverse_complement('GCAGTTGCA') returns 'TGCAACTGC'. For now, we will proceed without writing tests, but we will soon cover test-driven development, in which your functions are built around tests. For now, I will tell you this: If your functions are not thoroughly tested, you are entering a world of pain. A world of pain. Test your functions.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#keyword-arguments",
    "href": "appendices/python_basics/intro_to_functions.html#keyword-arguments",
    "title": "Appendix I — Introduction to functions",
    "section": "I.5 Keyword arguments",
    "text": "I.5 Keyword arguments\nNow let’s say that instead of the reverse DNA complement, we want the reverse RNA complement. We could re-write the complement_base() function to do this. Better yet, let’s modify it.\n\ndef complement_base(base, material='DNA'):\n    \"\"\"Returns the Watson-Crick complement of a base.\"\"\"\n    if base in 'Aa':\n        if material == 'DNA':\n            return 'T'\n        elif material == 'RNA':\n            return 'U'\n    elif base in 'TtUu':\n        return 'A'\n    elif base in 'Gg':\n        return 'C'\n    else:\n        return 'G'\n    \ndef reverse_complement(seq, material='DNA'):\n    \"\"\"Compute reverse complement of a sequence.\"\"\"\n    # Initialize reverse complement\n    rev_seq = ''\n    \n    # Loop through and populate list with reverse complement\n    for base in reversed(seq):\n        rev_seq += complement_base(base, material=material)\n        \n    return rev_seq\n\nWe have added a named keyword argument, also known as a named kwarg. The syntax for a named kwarg is\nkwarg_name=default_value\nin the def clause of the function definition. In this case, we say that the default material is DNA, but we could call the function with another material (RNA). Conveniently, when you call the function and omit the kwargs, they take on the default value within the function. So, if we wanted to use the default material of DNA, we don’t have to do anything different in the function call.\n\nreverse_complement('GCAGTTGCA')\n\n'TGCAACTGC'\n\n\nBut, if we want RNA, we can use the kwarg. We use the same syntax to call it that we did when defining it.\n\nreverse_complement('GCAGTTGCA', material='RNA')\n\n'UGCAACUGC'",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#calling-a-function-with-a-splat",
    "href": "appendices/python_basics/intro_to_functions.html#calling-a-function-with-a-splat",
    "title": "Appendix I — Introduction to functions",
    "section": "I.6 Calling a function with a splat",
    "text": "I.6 Calling a function with a splat\nPython offers another convenient way to call functions. Say a function takes three arguments, a, b, and c, taken to be the sides of a triangle, and determines whether or not the triangle is a right triangle. I.e., it checks to see if \\(a^2 + b^2 = c^2\\).\n\ndef is_almost_right(a, b, c):\n    \"\"\"\n    Checks to see if a triangle with side lengths\n    `a`, `b`, and `c` is right.\n    \"\"\"\n    # Use sorted(), which gives a sorted list\n    a, b, c = sorted([a, b, c])\n    \n    # Check to see if it is almost a right triangle\n    if abs(a**2 + b**2 - c**2) &lt; 1e-12:\n        return True\n    else:\n        return False\n\nRemember our warning from before: never use equality checks with floats. We therefore just check to see if the Pythagorean theorem almost holds. The function works as expected.\n\nis_almost_right(13, 5, 12)\n\nTrue\n\n\n\nis_almost_right(1, 1, 1.4)\n\nFalse\n\n\nNow, let’s say we had a tuple with the triangle side lengths in it.\n\nside_lengths = (13, 5, 12)\n\nWe can pass these all in separately by splitting the tuple but putting a * in front of it. A * before a tuple used in this way is referred an unpacking operator, and is referred to by some programmers as a “splat.”\n\nis_almost_right(*side_lengths)\n\nTrue\n\n\nThis can be very convenient, and we will definitely use this feature later in the bootcamp when we do some string formatting.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#anonymous-a.k.a.-lambda-functions",
    "href": "appendices/python_basics/intro_to_functions.html#anonymous-a.k.a.-lambda-functions",
    "title": "Appendix I — Introduction to functions",
    "section": "I.7 Anonymous (a.k.a. lambda) functions",
    "text": "I.7 Anonymous (a.k.a. lambda) functions\nSo far, we have written functions using a def statement, followed by an indented block of code defining what will be executed when the function is called. Sometimes the functions are very short, like the ratio() function at the beginning of this lesson. We might wish to more succinctly define a function like this. Python’s lambda keyword enables this. As an example, let’s look at how we could define the ratio() function from before.\n\nf = lambda x, y: x / y\n\nf(3, 5)\n\n0.6\n\n\nThe syntax for defining an anonymous function, which must be on one line, is as follows.\n\nThe keyword lambda.\nThe arguments of the anonymous function, separated by commas.\nAn expression that is the return value of the function.\n\nYou may be thinking that anonymous functions run contrary to the idea that all functions should have doc strings. You’re right. Anonymous functions are typically only used when another function requires a function as an argument. In the is_almost_right() function above, we employed the sorted() function is used to sort a list. The sorted() function takes a key keyword argument that gives a function that is applied to each entry in the list and then the values returned by that function are used in the sorting. As an example, we can sort a list of names of goalscorers for LAFC in the epic 2022 MLS Cup final.\n\nsorted(['Kellyn Acosta', 'Jesus Murillo', 'Gareth Bale'])\n\n['Gareth Bale', 'Jesus Murillo', 'Kellyn Acosta']\n\n\nThis sorted by their first names, but we want to sort for their last names. As we will learn in the lesson on string methods, we can find the index of a space in a string using my_string.find(' '), so that the letter at the start of a last name for a player with string x is x[x.find(' ')+1]. We can use a lambda function to give this as the key and get a nicely sorted list.\n\nsorted(['Kellyn Acosta', 'Jesus Murillo', 'Gareth Bale'], key=lambda x: x[x.find(' ')+1])\n\n['Kellyn Acosta', 'Gareth Bale', 'Jesus Murillo']",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#computing-environment",
    "href": "appendices/python_basics/intro_to_functions.html#computing-environment",
    "title": "Appendix I — Introduction to functions",
    "section": "I.8 Computing environment",
    "text": "I.8 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/string_methods.html",
    "href": "appendices/python_basics/string_methods.html",
    "title": "Appendix J — String methods",
    "section": "",
    "text": "J.1 Indexing and slicing of strings\n| Download notebook\nIn the last lesson, we wrote some functions to parse strings and compute things like reverse complements. This helped us practice using functions and the iteration skills we learned.\nYou might think, “Hey, replacing characters in strings sounds like it may be pretty common.” You would be right. You might also think, “I bet someone, possibly someone who is a really good programmer, already has written code to do this.” You would again be right.\nFor common tasks, there are often already methods written by someone smart, and working with strings is no different. In this lesson, we will explore some of the string processing tools that come with Python’s standard library.\nBefore getting into string methods, we pause to note that indexing and slicing of strings works just as it does for lists and tuples.\nmy_str = 'The Dude abides.'\n\nprint(my_str[5])\nprint(my_str[:6])\nprint(my_str[::2])\nprint(my_str[::-1])\n\nu\nThe Du\nTeDd bds\n.sediba eduD ehT",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/string_methods.html#revisiting-previous-examples-using-string-methods",
    "href": "appendices/python_basics/string_methods.html#revisiting-previous-examples-using-string-methods",
    "title": "Appendix J — String methods",
    "section": "J.2 Revisiting previous examples using string methods",
    "text": "J.2 Revisiting previous examples using string methods\nWe’ll start by revisiting some of the examples we’ve seen so far.\n\nJ.2.1 Computing GC content\nIf you remember from the iteration lesson, we started by computing the GC content of a nucleic acid sequence. We counted the occurrences of 'G' and 'C' in the string using a for loop. We can use the count() string method do to this.\n\n# Define sequence\nseq = 'GACAGACUCCAUGCACGUGGGUAUCAUGUC'\n\n# Count G's and C's\nseq.count('G') + seq.count('C')\n\n16\n\n\nThe seq.count() method enabled us to count the number times G and C occurred in the string seq. This notation is new. We have a variable, followed by a dot (.), and then a function. These functions are called methods in the language of object-oriented programming (OOP). If you have a string my_str, and you want to execute one of Python’s built-in string methods on it, the syntax is\nmy_str.string_method_of_choice(*args)\nIn general, the count method gives the number of times a substring appears in a string. We can learn more about its behavior by playing with it.\n\n# Substrings of more than one characater\nseq.count('GAC')\n\n2\n\n\n\n# Substrings cannot overlap\n'AAAAAAA'.count('AA')\n\n3\n\n\n\n# Something that's not there.\nseq.count('nonsense')\n\n0\n\n\n\n\nJ.2.2 Finding the index of a start codon\nAnother task in the iteration lesson was to find the index of the start codon in an RNA sequence. Let’s do it with another string method.\n\nseq.find('AUG')\n\n10\n\n\nWow, that was easy. The find() method gives the index where the substring argument first appears. But, what if a substring is not in the string?\n\nseq.find('nonsense')\n\n-1\n\n\nIn this case, find() returns -1. This is not to be interpreted as index -1! find() always returns positive indices if it finds a substring. Note that you should not use find() to test if a substring is present. Use the in operator we already learned about.\n\n'AUG' in seq\n\nTrue\n\n\n\n\nJ.2.3 Finding the last index of a substring\nLet’s say we wanted to find the last instance of the start codon. We basically want to search from the right. This is exactly what the rfind() method does.\n\nseq.rfind('AUG')\n\n25\n\n\n\n\nJ.2.4 Finding the complementary base\nIn our lesson on functions, we wrote a function to compute a complementary base comparing against both the capital and lowercase letter. Here is that function implemented with some handy string methods.\n\ndef complement_base(base):\n    \"\"\"Returns the Watson-Crick complement of a base.\"\"\"\n    # Convert to lowercase\n    base = base.lower()\n    \n    if base == 'a':\n        return 'T'\n    elif base == 't':\n        return 'A'\n    elif base == 'g':\n        return 'C'\n    else:\n        return 'G'\n\nWe were able to avoid all the “base in 'Tt'”-style operations by just converting the base to lowercase using the lower() method. In general, the lower() method takes a string and converts any capital letters to lower case. The upper() function works analogously.\n\n'LeBron James'.lower()\n\n'lebron james'\n\n\n\n'Make me aLl caPS.'.upper()\n\n'MAKE ME ALL CAPS.'\n\n\n\n\nJ.2.5 Converting RNA to DNA\nWe also updated the complementary base function to account for RNA or DNA. Perhaps an easier way is just to replace all Us in an RNA sequence with Ts to get a DNA sequence. The replace() method makes this easy.\n\nseq.replace('U', 'T')\n\n'GACAGACTCCATGCACGTGGGTATCATGTC'\n\n\nNote that seq did not change. Remember, strings are immutable, so the replace() method returns a new string, as does lower(), upper(), and any other string method that returns a string. So, the characters stored in the variable seq are unchanged.\n\nseq\n\n'GACAGACUCCAUGCACGUGGGUAUCAUGUC'",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/string_methods.html#the-join-method",
    "href": "appendices/python_basics/string_methods.html#the-join-method",
    "title": "Appendix J — String methods",
    "section": "J.3 The join() method",
    "text": "J.3 The join() method\nOne of the most useful string methods is the join() method. Say we have a list of words that we want to craft into a sentence.\n\nword_tuple = ('The', 'Dude', 'abides.')\n\nNow, we would like to concatenate them into a single string. (This is sort of like the opposite of taking a string and making a list of its characters by doing a list() type conversion.) We need to know what we want to put between each word. In this case, we want a space. Here’s the nifty syntax to do that.\n\n' '.join(word_tuple)\n\n'The Dude abides.'\n\n\nWe now have a single string with the elements of the tuple, separated by spaces. The string before the dot (.) specifies what goes between the strings in the list or tuple (or other iterable). If we wanted “*” between each word, we could do that, too.\n\n' * '.join(word_tuple)\n\n'The * Dude * abides.'",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/string_methods.html#the-format-method",
    "href": "appendices/python_basics/string_methods.html#the-format-method",
    "title": "Appendix J — String methods",
    "section": "J.4 The format() method",
    "text": "J.4 The format() method\nThe format() method is very powerful. We not go over all use cases here, but I’ll show you what I think is most intuitive and commonly used. Again, this is best learned by example.\n\nmy_str = \"\"\"\nLet's do a Mad Lib!\nDuring this bootcamp, I feel {adjective}.\nThe instructors give us {plural_noun}.\n\"\"\".format(adjective='truculent', plural_noun='haircuts')\n\nprint(my_str)\n\n\nLet's do a Mad Lib!\nDuring this bootcamp, I feel truculent.\nThe instructors give us haircuts.\n\n\n\nSee the pattern? Given a string, the format() method takes kwargs that are themselves strings. Within the string, the name of the kwargs are given in braces. Then, the arguments in the format() method inserts the strings at the places delimited by braces.\nNow, what if we want to insert a number into a string? We could convert it to a string, but we should instead use string conversions. These are short directives that specify how the number should be represented in a string. A complete list is here. The table below shows some that are commonly used.\n\n\n\nconversion\ndescription\n\n\n\n\nd\ninteger\n\n\n04d\ninteger with four digits, possibly with leading zeros\n\n\nf\nfloat, default to six digits after decimal\n\n\n.8f\nfloat with 8 digits after the decimal\n\n\ne\nscientific notation, default to six digits after decimal\n\n\n.16e\nscientific notation with 16 digits after the decimal\n\n\ns\ndisplay as a string\n\n\n\nBelow are examples of all of these.\n\nprint('There are {n:d} states in the US.'.format(n=50))\nprint('Your file number is {n:d}.'.format(n=23))\nprint('π is approximately {pi:f}.'.format(pi=3.14))\nprint('e is approximately {e:.8f}.'.format(e=2.7182818284590451))\nprint(\"Avogadro's number is approximately {N_A:e}.\".format(N_A=6.022e23))\nprint('ε₀ is approximately {eps_0:.16e} F/m.'.format(eps_0=8.854187817e-12))\nprint('That {thing:s} really tied the room together.'.format(thing='rug'))\n\nThere are 50 states in the US.\nYour file number is 23.\nπ is approximately 3.140000.\ne is approximately 2.71828183.\nAvogadro's number is approximately 6.022000e+23.\nε₀ is approximately 8.8541878170000005e-12 F/m.\nThat rug really tied the room together.\n\n\nNote the syntax. In the braces, we specify the name of the kwarg, and then we put a colon followed by the string conversion. Note also that I used double quotes on the outside of the string containing Avogadro’s number so that I could include an apostrophe in the string. Finally, note that we got a subscript zero using the Unicode character, ₀.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/string_methods.html#f-strings",
    "href": "appendices/python_basics/string_methods.html#f-strings",
    "title": "Appendix J — String methods",
    "section": "J.5 f-strings",
    "text": "J.5 f-strings\nf-strings are strings that are prefixed with an f or F that allow convenient insertion of entries into strings. Here are some examples.\n\nn_states = 50\nfile_number = 23\npi = 3.14\ne = 2.7182818284590451\nN_A = 6.022e23\neps_0=8.854187817e-12\nthing = 'rug'\n\nprint(f'There are {n_states} states in the US.')\nprint(f'Your file number is {file_number}.')\nprint(f'π is approximately {pi}.')\nprint(f'e is approximately {e:.8f}.')\nprint(f\"Avogadro's number is approximately {N_A}.\")\nprint(f'ε₀ is approximately {eps_0} F/m.')\nprint(f'That {thing} really tied the room together.')\n\nThere are 50 states in the US.\nYour file number is 23.\nπ is approximately 3.14.\ne is approximately 2.71828183.\nAvogadro's number is approximately 6.022e+23.\nε₀ is approximately 8.854187817e-12 F/m.\nThat rug really tied the room together.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/string_methods.html#there-are-many-more-string-methods",
    "href": "appendices/python_basics/string_methods.html#there-are-many-more-string-methods",
    "title": "Appendix J — String methods",
    "section": "J.6 There are many more string methods",
    "text": "J.6 There are many more string methods\nYou can find a complete list of string methods from the Python doc pages. Various methods will come in handy when parsing strings going forward.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/string_methods.html#computing-environment",
    "href": "appendices/python_basics/string_methods.html#computing-environment",
    "title": "Appendix J — String methods",
    "section": "J.7 Computing environment",
    "text": "J.7 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html",
    "href": "appendices/python_basics/dictionaries.html",
    "title": "Appendix K — Dictionaries",
    "section": "",
    "text": "K.1 Mapping objects and dictionaries\n| Download notebook\nA mapping object allows an arbitrary collection of objects to be indexed by an arbitrary collection of values. That’s a mouthful. It is easier to understand instead by comparing to a sequence.\nLet’s take a sequence of two strings, say a tuple containing a first and last name.\nWe are restricted on how we reference the sequence. I.e., the first name is name[0], and the last name is name[1]. A mapping object could instead be indexed like name['first name'] and name['last name']. You can imagine this would be very useful! A classic example in biology might be looking up amino acids that are coded for by given codons. E.g., you might want\nto give you 'Leucine'.\nPython’s build-in mapping type is a dictionary. You might imagine that the Oxford English Dictionary might conveniently be stored as a dictionary (obviously). I.e., you would not want to store definitions that have to be referenced like\nRather, you would like to get definitions like this:\nImportantly, note that in Python 3.5 and older dictionaries have no sense of order. In Python 3.6, dictionaries were stored in insertion order as an implementation improvement. In Python 3.7 and beyond, dictionaries are guaranteed to be ordered in the order in which their entries were created. It is therefore advisable be cautious when relying on ordering in dictionaries. For safety’s sake, you may be better off assuming there is no sense of order.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#mapping-objects-and-dictionaries",
    "href": "appendices/python_basics/dictionaries.html#mapping-objects-and-dictionaries",
    "title": "Appendix K — Dictionaries",
    "section": "",
    "text": "name = ('jeffrey', 'lebowski')\n\naa['CTT']\n\n\noed[103829]\n\noed['computer']",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#dictionary-syntax",
    "href": "appendices/python_basics/dictionaries.html#dictionary-syntax",
    "title": "Appendix K — Dictionaries",
    "section": "K.2 Dictionary syntax",
    "text": "K.2 Dictionary syntax\nThe syntax for creating a dictionary, as usual, is best seen through example.\n\nmy_dict = {'a': 6, 'b': 7, 'c': 27.6}\nmy_dict\n\n{'a': 6, 'b': 7, 'c': 27.6}\n\n\nA dictionary is created using curly braces ({}). Each entry has a key, followed by a colon, and then the value associated with the key. In the example above, the keys are all strings, which is the most common use case. Note that the items can be of any type; in the above example, they are ints and a float.\nWe could also create the dictionary using the built-in dict() function, which can take a tuple of 2-tuples, each one containing a key-value pair.\n\ndict((('a', 6), ('b', 7), ('c', 27.6)))\n\n{'a': 6, 'b': 7, 'c': 27.6}\n\n\nFinally, we can make a dictionary with keyword arguments to the dict() function.\n\ndict(a='yes', b='no', c='maybe')\n\n{'a': 'yes', 'b': 'no', 'c': 'maybe'}\n\n\nWe do not need to have strings as the keys. In fact, any immutable object can be a key.\n\nmy_dict = {\n    0: 'zero',\n    1.7: [1, 2, 3],\n    (5, 6, 'dummy string'): 3.14,\n    'strings are immutable': 42\n}\n\nmy_dict\n\n{0: 'zero',\n 1.7: [1, 2, 3],\n (5, 6, 'dummy string'): 3.14,\n 'strings are immutable': 42}\n\n\nHowever, mutable objects cannot be used as keys.\n\nmy_dict = {\n    \"immutable is ok\": 1, \n    [\"mutable\", \"not\", \"ok\"]: 5\n}\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 my_dict = {\n      2     \"immutable is ok\": 1, \n      3     [\"mutable\", \"not\", \"ok\"]: 5\n      4 }\n\nTypeError: unhashable type: 'list'",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#indexing-dictionaries",
    "href": "appendices/python_basics/dictionaries.html#indexing-dictionaries",
    "title": "Appendix K — Dictionaries",
    "section": "K.3 Indexing dictionaries",
    "text": "K.3 Indexing dictionaries\nAs mentioned at the beginning of the lesson, we index dictionaries by key.\n\n# Make a dictionary\nmy_dict = dict(a='yes', b='no', c='maybe')\n\n# Pull out an entry\nmy_dict['b']\n\n'no'\n\n\nBecause the indexing of dictionaries is by key and not by sequential integers, they cannot be sliced; they must be accessed element-by-element. (Actually there are ways to slice keys of dictionaries using itertools, but we will not cover that.)",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#useful-dictionaries-in-bioinformatics",
    "href": "appendices/python_basics/dictionaries.html#useful-dictionaries-in-bioinformatics",
    "title": "Appendix K — Dictionaries",
    "section": "K.4 Useful dictionaries in bioinformatics",
    "text": "K.4 Useful dictionaries in bioinformatics\nIt might be useful to quickly look up 3-letter amino acid codes. Dictionaries are particularly useful for this.\n\naa_dict = {\n    \"A\": \"Ala\",\n    \"R\": \"Arg\",\n    \"N\": \"Asn\",\n    \"D\": \"Asp\",\n    \"C\": \"Cys\",\n    \"Q\": \"Gln\",\n    \"E\": \"Glu\",\n    \"G\": \"Gly\",\n    \"H\": \"His\",\n    \"I\": \"Ile\",\n    \"L\": \"Leu\",\n    \"K\": \"Lys\",\n    \"M\": \"Met\",\n    \"F\": \"Phe\",\n    \"P\": \"Pro\",\n    \"S\": \"Ser\",\n    \"T\": \"Thr\",\n    \"W\": \"Trp\",\n    \"Y\": \"Tyr\",\n    \"V\": \"Val\",\n}\n\nAnother useful dictionary would contain the set of codons and the amino acids they code for. This is built in the code below using the zip() function we learned before. To see the logic on how this is constructed, see the codon table here.\n\n# The set of DNA bases\nbases = ['T', 'C', 'A', 'G']\n\n# Build list of codons\ncodon_list = []\nfor first_base in bases:\n    for second_base in bases:\n        for third_base in bases:\n            codon_list += [first_base + second_base + third_base]\n\n# The amino acids that are coded for (* = STOP codon)\namino_acids = 'FFLLSSSSYY**CC*WLLLLPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG'\n\n# Build dictionary from tuple of 2-tuples (technically an iterator, but it works)\ncodons = dict(zip(codon_list, amino_acids))\n\n# Show that we did it\nprint(codons)\n\n{'TTT': 'F', 'TTC': 'F', 'TTA': 'L', 'TTG': 'L', 'TCT': 'S', 'TCC': 'S', 'TCA': 'S', 'TCG': 'S', 'TAT': 'Y', 'TAC': 'Y', 'TAA': '*', 'TAG': '*', 'TGT': 'C', 'TGC': 'C', 'TGA': '*', 'TGG': 'W', 'CTT': 'L', 'CTC': 'L', 'CTA': 'L', 'CTG': 'L', 'CCT': 'P', 'CCC': 'P', 'CCA': 'P', 'CCG': 'P', 'CAT': 'H', 'CAC': 'H', 'CAA': 'Q', 'CAG': 'Q', 'CGT': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R', 'ATT': 'I', 'ATC': 'I', 'ATA': 'I', 'ATG': 'M', 'ACT': 'T', 'ACC': 'T', 'ACA': 'T', 'ACG': 'T', 'AAT': 'N', 'AAC': 'N', 'AAA': 'K', 'AAG': 'K', 'AGT': 'S', 'AGC': 'S', 'AGA': 'R', 'AGG': 'R', 'GTT': 'V', 'GTC': 'V', 'GTA': 'V', 'GTG': 'V', 'GCT': 'A', 'GCC': 'A', 'GCA': 'A', 'GCG': 'A', 'GAT': 'D', 'GAC': 'D', 'GAA': 'E', 'GAG': 'E', 'GGT': 'G', 'GGC': 'G', 'GGA': 'G', 'GGG': 'G'}\n\n\nThese two dictionaries are particularly useful, so I put them in a little module which we will discuss in the lesson on packages and modules.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#a-dictionary-is-an-implementation-of-a-hash-table",
    "href": "appendices/python_basics/dictionaries.html#a-dictionary-is-an-implementation-of-a-hash-table",
    "title": "Appendix K — Dictionaries",
    "section": "K.5 A dictionary is an implementation of a hash table",
    "text": "K.5 A dictionary is an implementation of a hash table\nIt is useful to stop and think about how a dictionary works. Let’s create a dictionary and look at where the values are stored in memory.\n\n# Create dictionary\nmy_dict = {'a': 6, 'b': 7, 'c':12.6}\n\n# Find where they are stored\nprint(id(my_dict))\nprint(id(my_dict['a']))\nprint(id(my_dict['b']))\nprint(id(my_dict['c']))\n\n4570286848\n4343739664\n4343739696\n4600140880\n\n\nSo, each entry in the dictionary is stored at a different location in memory. The dictionary itself also has its own address. So, when I index a dictionary with a key, how does the dictionary know which address in memory to use to fetch the value I am interested in?\nDictionaries use a hash function to do this. A hash function converts its input to an integer. For example, we can use Python’s built-in hash function to convert the keys to integers.\n\nhash('a'), hash('b'), hash('c')\n\n(199680081378453410, -6901413122848462881, -5789435826028719999)\n\n\nUnder the hood, Python then converts these integers to integers that could correspond to locations in memory. A collection of elements that can be indexed this way is called a hash table. This is a very common data structure in computing. Wikipedia has a pretty good discussion on them.\nGiven what you know about how dictionaries work, why do you think mutable objects are not acceptable as keys?",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#dictionaries-are-mutable",
    "href": "appendices/python_basics/dictionaries.html#dictionaries-are-mutable",
    "title": "Appendix K — Dictionaries",
    "section": "K.6 Dictionaries are mutable",
    "text": "K.6 Dictionaries are mutable\nDictionaries are mutable. This means that they can be changed in place. For example, if we want to add an element to a dictionary, we use simple syntax.\n\n# Remind ourselves what the dictionary is\nprint(my_dict)\n\n# Add an entry\nmy_dict['d'] = 'Bootcamp is so much fun!'\n\n# Look at dictionary again\nprint(my_dict)\n\n# Change an entry\nmy_dict['a'] = 'I was not satisfied with entry a.'\n\n# Look at it again\nprint(my_dict)\n\n{'a': 6, 'b': 7, 'c': 12.6}\n{'a': 6, 'b': 7, 'c': 12.6, 'd': 'Bootcamp is so much fun!'}\n{'a': 'I was not satisfied with entry a.', 'b': 7, 'c': 12.6, 'd': 'Bootcamp is so much fun!'}",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#membership-operators-with-dictionaries",
    "href": "appendices/python_basics/dictionaries.html#membership-operators-with-dictionaries",
    "title": "Appendix K — Dictionaries",
    "section": "K.7 Membership operators with dictionaries",
    "text": "K.7 Membership operators with dictionaries\nThe in and not in operators work with dictionaries, but both only query keys and not values. We see this again by example.\n\n# Make a fresh my_dict\nmy_dict = {'a': 1, 'b': 2, 'c': 3}\n\n# in works with keys\n'b' in my_dict, 'd' in my_dict, 'e' not in my_dict\n\n(True, False, True)\n\n\n\n# Try it with values\n2 in my_dict\n\nFalse\n\n\nYup! We get False. Why? Because 2 is not a key in my_dict. We can also iterate over the keys in a dictionary.\n\nfor key in my_dict:\n    print(key, ':', my_dict[key])\n\na : 1\nb : 2\nc : 3\n\n\nThe best, and preferred, method, is to iterate over key,value pairs in a dictionary using the items() method of a dictionary.\n\nfor key, value in my_dict.items():\n    print(key, ':', value)\n\na : 1\nb : 2\nc : 3\n\n\nNote, however, that like lists, the items that come out of the my_dict.items() iterator are not items in the dictionary, but copies of them. If you make changes within the for loop, you will not change entries in the dictionary.\n\nfor key, value in my_dict.items():\n    value = 'this string will not be in dictionary.'\n    \nmy_dict\n\n{'a': 1, 'b': 2, 'c': 3}\n\n\nYou will, though, if you use the keys.\n\nfor key, _ in my_dict.items():\n    my_dict[key] = 'this will be in the dictionary.'\n    \nmy_dict\n\n{'a': 'this will be in the dictionary.',\n 'b': 'this will be in the dictionary.',\n 'c': 'this will be in the dictionary.'}",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#built-in-functions-for-dictionaries",
    "href": "appendices/python_basics/dictionaries.html#built-in-functions-for-dictionaries",
    "title": "Appendix K — Dictionaries",
    "section": "K.8 Built-in functions for dictionaries",
    "text": "K.8 Built-in functions for dictionaries\nThe built-in len() function and del operation work on dictionaries.\n\nlen(d) gives the number of entries in dictionary d\ndel d[k] deletes entry with key k from dictionary d\n\nThis is the first time we’ve encountered the del keyword. This keyword is used to delete variables and their values from memory. The del keyword can also be to delete items from lists. Let’s see things in practice.\n\n# Create my_list and my_dict for reference\nmy_dict = dict(a=1, b=2, c=3, d=4)\nmy_list = [1, 2, 3, 4]\n\n# Print them\nprint('my_dict:', my_dict)\nprint('my_list:', my_list)\n\n# Get lengths\nprint('length of my_dict:', len(my_dict))\nprint('length of my_list:', len(my_list))\n\n# Delete a key from my_dict\ndel my_dict['b']\n\n# Delete entry from my_list\ndel my_list[1]\n\n# Show post-deleted objects\nprint('post-deleted my_dict:', my_dict)\nprint('post-deleted my_list:', my_list)\n\nmy_dict: {'a': 1, 'b': 2, 'c': 3, 'd': 4}\nmy_list: [1, 2, 3, 4]\nlength of my_dict: 4\nlength of my_list: 4\npost-deleted my_dict: {'a': 1, 'c': 3, 'd': 4}\npost-deleted my_list: [1, 3, 4]\n\n\nNote, though, that you cannot delete an item from a tuple, since it’s immutable.\n\nmy_tuple = (1, 2, 3, 4)\ndel my_tuple[1]\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[19], line 2\n      1 my_tuple = (1, 2, 3, 4)\n----&gt; 2 del my_tuple[1]\n\nTypeError: 'tuple' object doesn't support item deletion",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#dictionary-methods",
    "href": "appendices/python_basics/dictionaries.html#dictionary-methods",
    "title": "Appendix K — Dictionaries",
    "section": "K.9 Dictionary methods",
    "text": "K.9 Dictionary methods\nDictionaries have several built-in methods in addition to the items() you have already seen. Following are a few of them, assuming the dictionary is d.\n\n\n\n\n\n\n\nmethod\neffect\n\n\n\n\nd.keys()\nreturn keys\n\n\nd.pop(key)\nreturn value associated with key and delete key from d\n\n\nd.values()\nreturn the values in d\n\n\nd.get(key, None)\nFetch a value in d by key giving a default value (the second argument) if key is missing\n\n\n\nLet’s try these out.\n\nmy_dict = dict(a=1, b=2, c=3, d=4)\n\nmy_dict.keys()\n\ndict_keys(['a', 'b', 'c', 'd'])\n\n\nNote that this is a dict_keys object. We cannot index it. If, say, we wanted to sort the keys and have them index-able, we would have to convert them to a list.\n\nsorted(list(my_dict.keys()))\n\n['a', 'b', 'c', 'd']\n\n\nThis is not a usual use case, though, and be warned that doing then when this is not explicitly what you want can lead to bugs. Now let’s try popping an entry out of the dictionary.\n\nmy_dict.pop('c')\n\n3\n\n\n\nmy_dict\n\n{'a': 1, 'b': 2, 'd': 4}\n\n\n…and, as we expect, key 'c' is now deleted, and its value was returned in the call to my_dict.pop('c'). Now, let’s look at the values.\n\nmy_dict.values()\n\ndict_values([1, 2, 4])\n\n\nWe get a dict_values object, similar to the dict_keys object we got with the my_dict.keys() method. Finally, let’s consider get().\n\nmy_dict.get('d')\n\n4\n\n\nThis is the same as my_dict['d'], except that if the key 'd' is not there, it will return a default value. Let’s try using my_dict.get() with the deleted entry 'c'.\n\nmy_dict.get('c')\n\nNote that there was no error (there would be if we did my_dict['c']), and we got None. We could specify a default value.\n\nmy_dict.get('c', 3)\n\n3\n\n\nYou should think about what behavior you want when you attempt to get a value out of a dictionary by key. Do you want an error when the key is missing? Then use indexing. Do you want to have a (possibly None) default if the key is missing and no error? Then use my_dict.get().\nYou can get more information about build-in methods from the Python documentation.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#list-methods",
    "href": "appendices/python_basics/dictionaries.html#list-methods",
    "title": "Appendix K — Dictionaries",
    "section": "K.10 List methods",
    "text": "K.10 List methods\nAs you may guess, the dictionary method pop() has an analog that works for lists. (Why don’t the dictionary key() and values() methods work for lists?) We take this opportunity to introduce a few more useful list methods. Imagine the list is called s.\n\n\n\nmethod\neffect\n\n\n\n\ns.pop(i)\nreturn value at index i and delete it from the list\n\n\ns.append(x)\nPut x at the end of the list\n\n\ns.insert(i, x)\nInsert x at index i in the list\n\n\ns.remove(x)\nRemove the first occurrence of x from the list\n\n\ns.reverse()\nReverse the order of items in the list",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#using-dictionaries-as-kwargs",
    "href": "appendices/python_basics/dictionaries.html#using-dictionaries-as-kwargs",
    "title": "Appendix K — Dictionaries",
    "section": "K.11 Using dictionaries as kwargs",
    "text": "K.11 Using dictionaries as kwargs\nA nifty feature of dictionaries is that they can be passed into functions as keyword arguments. We covered named keyword arguments in the lesson on functions. In addition to the named keyword arguments, a function can take in arbitrary keyword arguments (not arbitrary non-keyword arguments). This is specified in the function definition by including a last argument with a double-asterisk, **. The kwargs with the double-asterisk get passed in as a dictionary.\n\ndef concatenate_sequences(a, b, **kwargs):\n    \"\"\"Concatenate (combine) 2 or more sequences.\"\"\"\n    seq = a + b\n\n    for key in kwargs:\n        seq += kwargs[key]\n        \n    return seq\n\nLet’s try it!\n\nconcatenate_sequences('TGACAC', 'CAGGGA', c='GGGGGGGGG', d='AAAATTTTT')\n\n'TGACACCAGGGAGGGGGGGGGAAAATTTTT'\n\n\nNow, imagine we have a dictionary that contains our values.\n\nmy_dict = {\"a\": \"TGACAC\", \"b\": \"CAGGGA\", \"c\": \"GGGGGGGGG\", \"d\": \"AAAATTTTT\"}\n\nWe can now pass this directly into the function by preceding it with a double asterisk.\n\nconcatenate_sequences(**my_dict)\n\n'TGACACCAGGGAGGGGGGGGGAAAATTTTT'\n\n\nBeautiful! This example is kind of trivial, but you can imagine that it can come in handy, e.g. with large sets of sequence fragments that you read in from a file. We will use **kwargs later in the bootcamp.\nQuestion: What is the risk in using a dictionary in this way to concatenate sequences?",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#merging-dictionaries",
    "href": "appendices/python_basics/dictionaries.html#merging-dictionaries",
    "title": "Appendix K — Dictionaries",
    "section": "K.12 Merging dictionaries",
    "text": "K.12 Merging dictionaries\nSaw I have two dictionaries that have no like keys and I want to merge them together. This might be like considering two volumes of an encyclopedia; they do not have and like keys, and we might like to consider them as a single volume. How can we accomplish this?\nThe dict() function, combined with the ** operator in function calls allows for this. We simple call dict() with ** before each dictionary argument.\n\nrestriction_dict = {\"KpnI\": \"GGTACC\", \"HindII\": \"AAGCTT\", \"ecoRI\": \"GAATTC\"}\n\ndict(**my_dict, **restriction_dict, another_seq=\"AGTGTAGTG\")\n\n{'a': 'TGACAC',\n 'b': 'CAGGGA',\n 'c': 'GGGGGGGGG',\n 'd': 'AAAATTTTT',\n 'KpnI': 'GGTACC',\n 'HindII': 'AAGCTT',\n 'ecoRI': 'GAATTC',\n 'another_seq': 'AGTGTAGTG'}",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#computing-environment",
    "href": "appendices/python_basics/dictionaries.html#computing-environment",
    "title": "Appendix K — Dictionaries",
    "section": "K.13 Computing environment",
    "text": "K.13 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/comprehensions.html",
    "href": "appendices/python_basics/comprehensions.html",
    "title": "Appendix L — Comprehensions",
    "section": "",
    "text": "L.1 List comprehensions\n| Download notebook\nWe have learned how to build lists, tuples, arrays, etc. by constructing them directly. E.g., list(range(10)) gives us a list of all integers between 0 and 9 inclusive. But what if we want to build a list or array by iterating the contains something a bit more complicated. For example, let’s say we want to get a list of all prime numbers less than 1000. This could be a bit cumbersome, even with sympy’s lovely isprime() function.\nBecause we do not know a priori how many entries there are going to be, we have to keep appending to a list. Under the hood, this means that the Python interpreter has to keep allocating memory as it creates and grows lists. So, in addition to being syntactically clunky, the above way of creating a list is inefficient. It would be nice to have a more convenient way of doing this.\nEnter list comprehensions.\nAs is often the case, this is best seen by example. We will create the same Numpy array of primes using a list comprehension.\nprimes = [x for x in range(n_max) if sympy.isprime(x)]\n\n# Take a look\nprint(primes)\n\n[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997]\nIn one line, we have made our list of primes! The list comprehension is enclosed in brackets. The first part, x, is an expression that will be inserted into the list. Next comes a for statement to produce the iterator. Finally, there is a conditional; if the conditional evaluates True, then the expression expression is included in the list.\nIf a condition is absent, all entries are put in the list. For example, if we didn’t want to just do list(range(100)) to get integers, we could use a list comprehension without a conditional.\n# Give same result as list(range(100))\nmy_list_of_ints = [i for i in range(100)]\n\nprint(my_list_of_ints)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Comprehensions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/comprehensions.html#list-comprehensions",
    "href": "appendices/python_basics/comprehensions.html#list-comprehensions",
    "title": "Appendix L — Comprehensions",
    "section": "",
    "text": "L.1.1 Another example list comprehension\nLet’s say we wanted to build a list containing the information about the 2018 Nobel laureates. We have, in three separate arrays, their names, nationalities, and category for the prize.\n\nnames = (\n    \"Frances Arnold\",\n    \"George Smith\",\n    \"Gregory Winter\",\n    \"postponed\",\n    \"Denis Mukwege\",\n    \"Nadia Murad\",\n    \"Arthur Ashkin\",\n    \"Gérard Mourou\",\n    \"Donna Strickland\",\n    \"James Allison\",\n    \"Tasuku Honjo\",\n    \"William Nordhaus\",\n    \"Paul Romer\",\n)\n\nnationalities = (\n    \"USA\",\n    \"USA\",\n    \"UK\",\n    \"---\",\n    \"DRC\",\n    \"Iraq\",\n    \"USA\",\n    \"France\",\n    \"Canada\",\n    \"USA\",\n    \"Japan\",\n    \"USA\",\n    \"USA\",\n)\n\ncategories = (\n    \"Chemistry\",\n    \"Chemistry\",\n    \"Chemistry\",\n    \"Literature\",\n    \"Peace\",\n    \"Peace\",\n    \"Physics\",\n    \"Physics\",\n    \"Physics\",\n    \"Physiology or Medicine\",\n    \"Physiology or Medicine\",\n    \"Economics\",\n    \"Economics\",\n)\n\nWith these tuples in hand, we can use a list comprehension to build a nice list of tuples containing the information about the laureates.\n\n[(cat, name, nat) for name, nat, cat in zip(names, nationalities, categories)]\n\n[('Chemistry', 'Frances Arnold', 'USA'),\n ('Chemistry', 'George Smith', 'USA'),\n ('Chemistry', 'Gregory Winter', 'UK'),\n ('Literature', 'postponed', '---'),\n ('Peace', 'Denis Mukwege', 'DRC'),\n ('Peace', 'Nadia Murad', 'Iraq'),\n ('Physics', 'Arthur Ashkin', 'USA'),\n ('Physics', 'Gérard Mourou', 'France'),\n ('Physics', 'Donna Strickland', 'Canada'),\n ('Physiology or Medicine', 'James Allison', 'USA'),\n ('Physiology or Medicine', 'Tasuku Honjo', 'Japan'),\n ('Economics', 'William Nordhaus', 'USA'),\n ('Economics', 'Paul Romer', 'USA')]\n\n\nNotice that I do not have to use range(); I can use any iterator, including one that puts out multiple values using zip().\nNow, let’s say we are really interested in the prize in chemistry. We can add an if statement to the comprehension like we did in the prime number example.\n\n[\n    (cat, name, nat)\n    for name, nat, cat in zip(names, nationalities, categories)\n    if cat == \"Chemistry\"\n]\n\n[('Chemistry', 'Frances Arnold', 'USA'),\n ('Chemistry', 'George Smith', 'USA'),\n ('Chemistry', 'Gregory Winter', 'UK')]\n\n\n(Note here that we split the list comprehension over many lines for readability, which is perfectly legal.) We can also nest iterators. For example, let’s say the the chemistry and medicine prize winners got together in Sweden and wanted to play against each other in basketball. There are three chemistry winners, but only two medicine winners. So, to play 2-on-2, we would have to choose only two chemistry laureates. So, let’s make a list of all possible pairs of chemistry winners.\n\n# First get list of chemistry laureates\nchem_names = [name for name, cat in zip(names, categories) if cat == \"Chemistry\"]\n\n# List of all possible pairs of chemistry laureates\n[\n    (n1, n2)\n    for i, n1 in enumerate(chem_names)\n    for j, n2 in enumerate(chem_names)\n    if i &lt; j\n]\n\n[('Frances Arnold', 'George Smith'),\n ('Frances Arnold', 'Gregory Winter'),\n ('George Smith', 'Gregory Winter')]\n\n\nTo summarize this structure of list comprehensions, borrowing from Dave Beazley’s explanation in Python Essential Reference, a list comprehension has the following structure.\n[expression_to_put_in_list for i_1 in iterable_1 if condition_1\n                           for i_2 in iterable_2 if condition_2\n                                     ...\n                           for i_n in iterable_n if condition_n]\nwhich is roughly equivalent to\nmy_list = []\nfor i_1 in iterable_1:\n    if condition_1:\n        for i_2 in iterable_2:\n            if condition_2:\n                ...\n                for i_n in iterable_n:\n                    if condition_n:\n                        my_list += [expression_to_put_in_list]\n\n\nL.1.2 What if you want an else statement in a list comprehension?\nNow, let’s say that we deem “Physiology or Medicine” to be too long of a title for the category of the prize. We instead want to substitute that phrase with “Medicine” for brevity. We might construct the list like this:\n\n[\n    (\"Medicine\", name, nat)\n    for name, nat, cat in zip(names, nationalities, categories)\n    if cat == \"Physiology or Medicine\"\n]\n\n[('Medicine', 'James Allison', 'USA'), ('Medicine', 'Tasuku Honjo', 'Japan')]\n\n\nThis leaves out all of the other prizes. So, we need an else statement. To include all prizes, we might try it like this.\n\n[\n    (\"Medicine\", name, nat)\n    for name, nat, cat in zip(names, nationalities, categories)\n    if cat == \"Physiology or Medicine\" else (cat, name, nat)\n]\n\n\n  Cell In[10], line 4\n    if cat == \"Physiology or Medicine\" else (cat, name, nat)\n                                       ^\nSyntaxError: invalid syntax\n\n\n\n\nSyntax error! This structure of a list comprehension does not match the template shown above. In the conditional expression of list comprehensions, you cannot have an else block.\nHowever, the expression_to_put_in_list can be any valid Python expression. The following is a valid Python expression:\n(\"Medicine\", name, nat) if cat == \"Physiology or Medicine\" else (cat, name, nat)\nSo, we can still use a list comprehension to build the list.\n\n[\n    (\"Medicine\", name, nat) if cat == \"Physiology or Medicine\" else (cat, name, nat)\n    for name, nat, cat in zip(names, nationalities, categories)\n]\n\n[('Chemistry', 'Frances Arnold', 'USA'),\n ('Chemistry', 'George Smith', 'USA'),\n ('Chemistry', 'Gregory Winter', 'UK'),\n ('Literature', 'postponed', '---'),\n ('Peace', 'Denis Mukwege', 'DRC'),\n ('Peace', 'Nadia Murad', 'Iraq'),\n ('Physics', 'Arthur Ashkin', 'USA'),\n ('Physics', 'Gérard Mourou', 'France'),\n ('Physics', 'Donna Strickland', 'Canada'),\n ('Medicine', 'James Allison', 'USA'),\n ('Medicine', 'Tasuku Honjo', 'Japan'),\n ('Economics', 'William Nordhaus', 'USA'),\n ('Economics', 'Paul Romer', 'USA')]\n\n\nTo be clear here, there is no conditional in the list comprehension; the conditional is in the expression to be added to the list, which we have called expression_to_put_in_list.\nList comprehensions will prove very useful, and most Pythonistas use them extensively.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Comprehensions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/comprehensions.html#dictionary-comprehensions",
    "href": "appendices/python_basics/comprehensions.html#dictionary-comprehensions",
    "title": "Appendix L — Comprehensions",
    "section": "L.2 Dictionary comprehensions",
    "text": "L.2 Dictionary comprehensions\nIn addition to list comprehensions, Python also allows for dictionary comprehensions (and set comprehensions, but we will not discuss sets in the bootcamp). To demonstrate a dictionary comprehension, let’s use the name of the laureate as a key and the values in the dictionary are their nationality and category.\n\n{name: (cat, nat) for name, nat, cat in zip(names, nationalities, categories)}\n\n{'Frances Arnold': ('Chemistry', 'USA'),\n 'George Smith': ('Chemistry', 'USA'),\n 'Gregory Winter': ('Chemistry', 'UK'),\n 'postponed': ('Literature', '---'),\n 'Denis Mukwege': ('Peace', 'DRC'),\n 'Nadia Murad': ('Peace', 'Iraq'),\n 'Arthur Ashkin': ('Physics', 'USA'),\n 'Gérard Mourou': ('Physics', 'France'),\n 'Donna Strickland': ('Physics', 'Canada'),\n 'James Allison': ('Physiology or Medicine', 'USA'),\n 'Tasuku Honjo': ('Physiology or Medicine', 'Japan'),\n 'William Nordhaus': ('Economics', 'USA'),\n 'Paul Romer': ('Economics', 'USA')}\n\n\nAaaand we have our dictionary! This is quite a powerful way to construct this, and you may find dictionary comprehensions quite useful. I use them in specifying **kwargs and in creating dictionaries I want to convert to data frames.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Comprehensions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/comprehensions.html#paul-romer-and-jupyter-and-open-source-software",
    "href": "appendices/python_basics/comprehensions.html#paul-romer-and-jupyter-and-open-source-software",
    "title": "Appendix L — Comprehensions",
    "section": "L.3 Paul Romer and Jupyter and open source software",
    "text": "L.3 Paul Romer and Jupyter and open source software\nCoincidentally, one of the laureates featured in this lesson, Paul Romer, is a big fan of Jupyter notebooks. I love this quote from this blog post of his:\n\nIn the larger contest between open and proprietary models, Mathematica versus Jupyter would be a draw if the only concern were their technical accomplishments. In the 1990s, Mathematica opened up an undeniable lead. Now, Jupyter is the unambiguous technical leader.\nThe tie-breaker is social, not technical. The more I learn about the open source community, the more I trust its members. The more I learn about proprietary software, the more I worry that objective truth might perish from the earth.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Comprehensions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/comprehensions.html#computing-environment",
    "href": "appendices/python_basics/comprehensions.html#computing-environment",
    "title": "Appendix L — Comprehensions",
    "section": "L.4 Computing environment",
    "text": "L.4 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\nnumpy     : 1.26.4\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Comprehensions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html",
    "href": "appendices/python_basics/packages_and_modules.html",
    "title": "Appendix M — Packages and modules",
    "section": "",
    "text": "M.1 Example: I want to compute the mean and median of a list of numbers\n| Download notebook\nThe Python Standard Library has lots of built-in modules that contain useful functions and data types for doing specific tasks. You can also use modules from outside the standard library. And you will undoubtedly write your own modules!\nA module is contained in a file that ends with .py. This file can have classes, functions, and other objects. We will not discuss defining your own classes until much later in the bootcamp, so your modules will essentially just contain functions for now.\nA package contains several related modules that are all grouped together under one name. We will extensively use the NumPy, SciPy, Pandas, and Bokeh packages, among others, in the bootcamp, and I’m sure you will also use them beyond. As such, the first module we will consider is NumPy. We will talk a lot more about NumPy later in the bootcamp.\nSay I have a list of numbers and I want to compute the mean. This happens all the time; you repeat a measurement multiple times and you want to compute the mean. We could write a function to do this.\ndef mean(values):\n    \"\"\"Compute the mean of a sequence of numbers.\"\"\"\n    return sum(values) / len(values)\nAnd it works as expected.\nprint(mean([1, 2, 3, 4, 5]))\nprint(mean((4.5, 1.2, -1.6, 9.0)))\n\n3.0\n3.275\nIn addition to the mean, we might also want to compute the median, the standard deviation, etc. These seem like really common tasks. Remember my advice: if you want to do something that seems really common, a good programmer (or a team of them) probably already wrote something to do that. Means, medians, standard deviations, and lots and lots and lots of other numerical things are included in the Numpy module. To get access to it, we have to import it.\nimport numpy\nThat’s it! We now have the numpy module available for use. Remember, in Python everything is an object, so if we want to access the methods and attributes available in the numpy module, we use dot syntax. In a Jupyter notebook or in the JupyterLab console, you can type\n(note the dot) and hit tab, and we will see what is available. For Numpy, there is a huge number of options!\nSo, let’s try to use Numpy’s numpy.mean() function to compute a mean.\nprint(numpy.mean([1, 2, 3, 4, 5]))\nprint(numpy.mean((4.5, 1.2, -1.6, 9.0)))\n\n3.0\n3.275\nGreat! We get the same values! Now, we can use the numpy.median() function to compute the median.\nprint(numpy.median([1, 2, 3, 4, 5]))\nprint(numpy.median((4.5, 1.2, -1.6, 9.0)))\n\n3.0\n2.85\nThis is nice. It gives the median, including when we have an even number of elements in the sequence of numbers, in which case it automatically interpolates. It is really important to know that it does this interpolation, since if you are not expecting it, it can give unexpected results. So, here is an important piece of advice:\nWe can access the doc string of the numpy.median() function in JupyterLab by typing\nand looking at the output. An important part of that output:\nThis is where the documentation tells you that the median will be reported as the average of two middle values when the number of elements is even. Note that you could also read the documentation here, which is a bit easier to read.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#example-i-want-to-compute-the-mean-and-median-of-a-list-of-numbers",
    "href": "appendices/python_basics/packages_and_modules.html#example-i-want-to-compute-the-mean-and-median-of-a-list-of-numbers",
    "title": "Appendix M — Packages and modules",
    "section": "",
    "text": "numpy.\n\n\n\n\n\n\n\nAlways check the doc strings of functions.\n\n\nnumpy.median?\n\nNotes\n-----\nGiven a vector ``V`` of length ``N``, the median of ``V`` is the\nmiddle value of a sorted copy of ``V``, ``V_sorted`` - i\ne., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the\ntwo middle values of ``V_sorted`` when ``N`` is even.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#the-as-keyword",
    "href": "appendices/python_basics/packages_and_modules.html#the-as-keyword",
    "title": "Appendix M — Packages and modules",
    "section": "M.2 The as keyword",
    "text": "M.2 The as keyword\nWe use Numpy all the time. Typing numpy over and over again can get annoying. So, it is common practice to use the as keyword to import a module with an alias. Numpy’s alias is traditionally np, and this is the only alias you should ever use for Numpy.\n\nimport numpy as np\n\nnp.median((4.5, 1.2, -1.6, 9.0))\n\nnp.float64(2.85)\n\n\nI prefer to do things this way, though some purists differ. We will use traditional aliases for major packages like Numpy and Pandas throughout the bootcamp.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#third-party-packages",
    "href": "appendices/python_basics/packages_and_modules.html#third-party-packages",
    "title": "Appendix M — Packages and modules",
    "section": "M.3 Third party packages",
    "text": "M.3 Third party packages\nStandard Python installations come with the standard library. Numpy and other useful packages are not in the standard library. Outside of the standard library, there are several packages available. Several. Ha! There are currently (September, 2025) about 680,000 packages available through the Python Package Index, PyPI. Usually, you can ask Google about what you are trying to do, and there is often a third party module to help you do it.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#writing-your-own-module",
    "href": "appendices/python_basics/packages_and_modules.html#writing-your-own-module",
    "title": "Appendix M — Packages and modules",
    "section": "M.4 Writing your own module",
    "text": "M.4 Writing your own module\nTo write your own module, you need to create a .py file and save it. You can do this using the text editor in JupyterLab. Let’s call our module na_utils, for “nucleic acid utilities.” So, we create a file called na_utils.py. We’ll build this module to have two functions, based on things we’ve already written. We’ll have a function dna_to_rna(), which converts a DNA sequence to an RNA sequence (just changes T to U), and another function reverse_rna_complement(), which returns the reverse RNA complement of a DNA template. The contents of na_utils.py should look as follows.\n\"\"\"\nUtilities for parsing nucleic acid sequences.\n\"\"\"\n\ndef dna_to_rna(seq):\n    \"\"\"\n    Convert a DNA sequence to RNA.\n    \"\"\"\n    # Determine if original sequence was uppercase\n    seq_upper = seq.isupper()\n\n    # Convert to lowercase\n    seq = seq.lower()\n\n    # Swap out 't' for 'u'\n    seq = seq.replace('t', 'u')\n\n    # Return upper or lower case RNA sequence\n    if seq_upper:\n        return seq.upper()\n    else:\n        return seq\n\n\ndef reverse_rna_complement(seq):\n    \"\"\"\n    Convert a DNA sequence into its reverse complement as RNA.\n    \"\"\"\n    # Determine if original was uppercase\n    seq_upper = seq.isupper()\n\n    # Reverse sequence\n    seq = seq[::-1]\n\n    # Convert to upper\n    seq = seq.upper()\n\n    # Compute complement\n    seq = seq.replace('A', 'u')\n    seq = seq.replace('T', 'a')\n    seq = seq.replace('G', 'c')\n    seq = seq.replace('C', 'g')\n\n    # Return result\n    if seq_upper:\n        return seq.upper()\n    else:\n        return seq\nNote that the file starts with a doc string saying what the module contains.\nI then have my two functions, each with doc strings. We will now import the module and then use these functions. In order for the import to work, the file na_utils.py must be in your present working directory, since this is where the Python interpreter will look for your module. In general, if you execute the code\nimport my_module\nthe Python interpreter will look first in the pwd to find my_module.py.\n\nimport na_utils\n\n# Sequence\nseq = 'GACGATCTAGGCGACCGACTGGCATCG'\n\n# Convert to RNA\nna_utils.dna_to_rna(seq)\n\n'GACGAUCUAGGCGACCGACUGGCAUCG'\n\n\nWe can also compute the reverse RNA complement.\n\nna_utils.reverse_rna_complement(seq)\n\n'CGAUGCCAGUCGGUCGCCUAGAUCGUC'\n\n\nWonderful! You now have your own functioning module!\n\nM.4.1 A quick note on error checking\nThese functions have minimal error checking of the input. For example, the dna_to_rna() function will take gibberish in and give jibberish out.\n\nna_utils.dna_to_rna('You can observe a lot by just watching.')\n\n'you can observe a lou by jusu wauching.'\n\n\nIn general, checking input and handling errors is an essential part of writing functions, and we will cover that in a later lesson.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#importing-modules-in-your-.py-files-and-notebooks",
    "href": "appendices/python_basics/packages_and_modules.html#importing-modules-in-your-.py-files-and-notebooks",
    "title": "Appendix M — Packages and modules",
    "section": "M.5 Importing modules in your .py files and notebooks",
    "text": "M.5 Importing modules in your .py files and notebooks\nThe Python style guide (PEP 8) says:\n\nImports are always put at the top of the file, just after any module comments and docstrings, and before module globals and constants.\nImports should be grouped in the following order:\n\nstandard library imports\nrelated third party imports\nlocal application/library specific imports\n\nYou should put a blank line between each group of imports.\n\nYou should follow this guide. I generally do it for Jupyter notebooks as well, with my first code cell having all of the imports I need. Therefore, going forward all of our lessons will have all necessary imports at the top of the document. The only exception is when we are explicitly demonstrating a concept that requires an import.\n\nM.5.1 Imports and updates\nOnce you have imported a module or package, the interpreter stores its contents in memory. You cannot update the contents of the package and expect the interpreter to know about the changes. You will need to restart the kernel and then import the package again in a fresh instance.\nThis can seem annoying, but it is good design. It ensures that code you are running does not change as you go through executing a notebook. However, when developing modules, it is sometimes convenient to have an imported module be updated as you run through the notebook as you are editing. To enable this, you can use the autoreload extension. To activate it, run the following in a code cell.\n%load_ext autoreload\n%autoreload 2\nWhenever you run a cell, imported packages and modules will be automatically reloaded.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#shareable-reusable-packages",
    "href": "appendices/python_basics/packages_and_modules.html#shareable-reusable-packages",
    "title": "Appendix M — Packages and modules",
    "section": "M.6 Shareable, reusable packages",
    "text": "M.6 Shareable, reusable packages\nWhen we wrote the na_utils module, we stored it in the directory that we were working in, or the pwd. But what if you write a module that you want to use regardless of what directory your are in? To allow this kind of usage, you can use the setuptools module of the standard library to manage your packages. You should read the documentation on Python packages and modules to understand the details of how this is done, but what we present here is sufficient to get simple packages running and installed.\n\nM.6.1 Package architecture\nIn order for the tools in setuptools to effectively install your modules for widespread use, you need to follow a specific architecture for your package. I made an example jb_bootcamp package for a computing bootcamp that I teach.\nThe file structure is of the package is\n/jb_bootcamp\n  /jb_bootcamp\n    __init__.py\n    na_utils.py\n    bioinfo_dicts.py\n    ...\nsetup.py\nREADME.md\nThe ellipsis above signifies that there are other files in there that we are not going to use yet. I am trying to keep it simple for now to show how package management works.\nIt is essential that the name of the root directory be the name of the package, and that there be a subdirectory with the same name. That subdirectory must contain a file __init__.py. This file contains information about the package and how the modules of the package are imported, but it may be empty for simple modules. In this case, I included a string with the name and version of the package, as well as instructions to import appropriate modules. Here are the contents of __init__.py. The first two lines of code tell the interpreter what to import when running import jb_bootcamp.\n\"\"\"Top-level package for utilities for bootcamp.\"\"\"\n\nfrom .na_utils import *\nfrom .bioinfo_dicts import *\n\n__author__ = 'Justin Bois'\n__email__ = 'bois@caltech.edu'\n__version__ = '0.0.1'\nAlso within the subdirectory are the .py files containing the code of the package. In our case, we have, na_utils.py and bioinfo_dicts.py.\nIt is also good practice to have a README file (which I suggest you write in Markdown) that has information about the package and what it does. Since this little demo package is kind of trivial, the README is quite short. Here are the contents I made for README.md (shown in unrendered raw Markdown).\n# jb_bootcamp\n\nUtilities for use in the Introduction to Programming in the Biological Sciences Bootcamp.\nFinally, in the main directory, we need to have a file called setup.py, which contains the instructions for setuptools to install the package. We use the setuptools.setup() function to do the installation.\nimport setuptools\n\nwith open(\"README.md\", \"r\") as f:\n    long_description = f.read()\n\nsetuptools.setup(\n    name='jb_bootcamp',\n    version='0.0.1',\n    author='Justin Bois',\n    author_email='bois@caltech.edu',\n    description='Utilities for use in bootcamp.',\n    long_description=long_description,\n    long_description_content_type='ext/markdown',\n    packages=setuptools.find_packages(),\n    classifiers=(\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: OS Independent\",\n    ),\n)\nThis is a minimal setup.py function, but will be sufficient for most packages you write for your own use. For your use, you make obvious changes to the name, author, etc., fields.\n\n\nM.6.2 Installing your package\nYou can install your package locally using\npip install -e path_to_my_package\nThe -e flag means it is editable, such that pip installs the package locally and is aware of edits you make to the files of the package. Once your package is mature, you should deposit it on the Python Package Index, in which case you can directly install it using\npip install my_package_name\n…Though in both cases, you should probably use a package manager to do so.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#package-management",
    "href": "appendices/python_basics/packages_and_modules.html#package-management",
    "title": "Appendix M — Packages and modules",
    "section": "M.7 Package management",
    "text": "M.7 Package management\nYour workflows may require many packages. These packages depend on each other in various ways. For example, Pandas requires NumPy, python-dateutil, and pytz, plus loads of optional dependencies. To make matters complicated, different versions of various packages depend on specific versions of their dependencies, which can form a tangled web of requirements. How can we handle this mess?\nPackage management systems solve this problem. The package management system we are using is Pixi. When you want to install a new package, you can do so with a command like\npixi add the-package-i-want\nand Pixi will make sure that all of the version numbers line up, updating or downgrading already installed packages to accommodate the new one. Unless you set it up otherwise, Pixi uses packages in the conda-forge channel. Pixi also plays nicely with pip, which can also be used to install packages. To do this, you need to include the --pypi flag.\npixi add --pypi the-pypi-package-i-want\nIf you want to add an editable package you are working on, you should use Pixi to handle the project. The Pixi documentation goes over how to do this.\nThe smaller the set of packages you need to manage, the better. Therefore, Pixi allows you to set up environments. (The Pixi developers call environments “projects” to emphasize that the environments in Pixi are more granular than typical of other package management systems.) Each environment contains a set of packages with versions in them. In the lesson in which you set up your computer, you set up an environment for this course that we have been using. Using environments for your projects, as opposed to a single monolithic base environment that has tons and tons of packages, is advantageous for several reasons.\n\nBy keeping the number of packages limited to those you need, you can avoid version clashes.\nProjects may require specific versions of packages, which can be explicitly installed. In other environments, you can use different versions.\nYou can encode your environment in a pixi.toml file that you can share. This allows your collaborators to readily set up environments mirroring yours and more easily share packages.\n\nI will not go through the details of how to use Pixi here, but rather refer you to Pixi’s extensive documentation.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#computing-environment",
    "href": "appendices/python_basics/packages_and_modules.html#computing-environment",
    "title": "Appendix M — Packages and modules",
    "section": "M.8 Computing environment",
    "text": "M.8 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\njupyterlab: 4.4.7",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/exceptions_and_error_handling.html",
    "href": "appendices/python_basics/exceptions_and_error_handling.html",
    "title": "Appendix N — Errors and exception handling",
    "section": "",
    "text": "N.1 Kinds of errors\n| Download notebook\nAnd now we’ll proceed to discussing errors and exception handling.\nSo far, we have encountered errors when we did something wrong. For example, when we tried to change a character in a string, we got a TypeError.\nIn this case, the TypeError indicates that we tried to do something that is legal in Python for some types, but we tried to do it to a type for which it is illegal (strings are immutable). In Python, an error detected during execution is called an exception. We say that the interpreter “raised an exception.” There are many kinds of built-in exceptions, and you can find a list of them, with descriptions here. You can write your own kinds of exceptions, but we will not cover that in bootcamp.\nIn this lesson, we will investigate how to handle errors in your code. Importantly, we will also touch on the different kinds of errors and how to avoid them. Or, more specifically, you will learn how to use exceptions to help you write better, more bug-free code.\nIn computer programs, we can break down errors into three types.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/exceptions_and_error_handling.html#kinds-of-errors",
    "href": "appendices/python_basics/exceptions_and_error_handling.html#kinds-of-errors",
    "title": "Appendix N — Errors and exception handling",
    "section": "",
    "text": "N.1.1 Syntax errors\nA syntax error means you wrote something nonsensical, something the Python interpreter cannot understand. An example of a syntax error in English would be the following.\n\nSir Tristram, violer d’amores, fr’over the short sea, had passen-core rearrived from North Armorica on this side the scraggy isthmus of Europe Minor to wielderfight his penisolate war: nor had topsawyer’s rocks by the stream Oconee exaggerated themselse to Laurens County’s gorgios while they went doublin their mumper all the time: nor avoice from afire bellowsed mishe mishe to tauftauf thuartpeatrick: not yet, though venissoon after, had a kidscad buttended a bland old isaac: not yet, though all’s fair in vanessy, were sosie sesthers wroth with twone nathandjoe.\n\nThis is recognizable as English. In fact, it is the second sentence of a very famous novel (Finnegans Wake by James Joyce). Clearly, many spelling and punctuation rules of English are violated here. To many of us, it is nonsensical, but I do know of some people who have read the book and understand it. So, English is fairly tolerant of syntax errors. A simpler example would be\n\nBoootcamp is fun!\n\nThis has a syntax error (“Boootcamp” is not in the English language), but we understand what it means. A syntax error in Python would be this:\nmy_list = [1, 2, 3\nWe know what this means. We are trying to create a list with three items, 1, 2, and 3. However, we forgot the closing bracket. Unlike users of the English language, the Python interpreter is not forgiving; it will raise a SyntaxError exception.\n\nmy_list = [1, 2, 3\n\n\n  Cell In[3], line 1\n    my_list = [1, 2, 3\n                      ^\nSyntaxError: incomplete input\n\n\n\n\nSyntax errors are often the easiest to deal with, since the program will not run at all if any are present.\n\n\nN.1.2 Runtime errors\nRuntime errors occur when a program is syntactically correct, so it can run, but the interpreter encountered something wrong. The example at the start of the tutorial, trying to change a character in a string, is an example of a runtime error. This particular one was a TypeError, which is a more specific type of runtime error. Python does have a RuntimeError, which just indicates a generic runtime (non-syntax) error.\nRuntime errors are more difficult to spot than syntax errors because it is possible that a program could run all the way through without encountering the error for some inputs, but for other inputs, you get an error. Let’s consider the example of a simple function meant to add two numbers.\n\ndef add_two_things(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\nSyntactically, this function is just fine. We can use it and it works.\n\nadd_two_things(6, 7)\n\n13\n\n\nWe can even add strings, even though it was meant to add two numbers.\n\nadd_two_things('Hello, ', 'world.')\n\n'Hello, world.'\n\n\nHowever, when we try to add a string and a number, we get a TypeError, the kind of runtime error we saw before.\n\nadd_two_things('a string', 5.7)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 add_two_things('a string', 5.7)\n\nCell In[4], line 3, in add_two_things(a, b)\n      1 def add_two_things(a, b):\n      2     \"\"\"Add two numbers.\"\"\"\n----&gt; 3     return a + b\n\nTypeError: can only concatenate str (not \"float\") to str\n\n\n\n\n\nN.1.3 Semantic errors\nSemantic errors are perhaps the most nefarious. They occur when your program is syntactically correct, executes without runtime errors, and then produces the wrong result. These errors are the hardest to find and can do the most damage. After all, when your program does not do what you designed it to do, you want it to scream out with an exception!\nFollowing is a common example of a semantic error in which we change a mutable object within a function and then try to reuse it.\n\n# A function to append a list onto itself, with the intention of \n# returning a new list, but leaving the input unaltered\ndef double_list(in_list):\n    \"\"\"Append a list to itself.\"\"\"\n    in_list += in_list\n    return in_list\n\n# Make a list\nmy_list = [3, 2, 1]\n\n# Double it\nmy_list_double = double_list(my_list)\n\n# Later on in our program, we want a sorted my_list\nmy_list.sort()\n\n# Let's look at my_list:\nprint('We expect [1, 2, 3]')\nprint('We get   ', my_list)\n\nWe expect [1, 2, 3]\nWe get    [1, 1, 2, 2, 3, 3]\n\n\nYikes! We changed my_list within the function unintentionally. Question: How would you re-rewrite double_list() to avoid this issue?",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/exceptions_and_error_handling.html#handling-errors-in-your-code",
    "href": "appendices/python_basics/exceptions_and_error_handling.html#handling-errors-in-your-code",
    "title": "Appendix N — Errors and exception handling",
    "section": "N.2 Handling errors in your code",
    "text": "N.2 Handling errors in your code\nIf you have a syntax error, your code will not even run. So, we will assume we are without syntax errors in this discussion on how to handle errors. So, how can we handle runtime errors? In most use cases, we just write our code and let the Python interpreter tell us about these exceptions. However, sometimes we want to use the fact that we know we might encounter a runtime error within our code. A common example of this is when importing modules that are convenient, but not essential, for your code to run. Errors are handled in your code using a try statement.\nLet’s try importing a module that computes GC content. This doesn’t exist, so we will get an ImportError.\n\nimport gc_content\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 import gc_content\n\nModuleNotFoundError: No module named 'gc_content'\n\n\n\nNow, if we had the gc_content module, we would like to use it. But if not, we will just hand-code a calculation of the GC content of a sequence. We use a try statement.\n\n# Try to get the gc_content module\ntry:\n    import gc_content\n    have_gc = True\nexcept ImportError as e:\n    have_gc = False\nfinally:\n    # Do whatever is necessary here, like close files\n    pass\n\nseq = 'ACGATCTACGATCAGCTGCGCGCATCG'\n    \nif have_gc:\n    print(gc_content(seq))\nelse:\n    print(seq.count('G') + seq.count('C'))\n\n16\n\n\nThe program now runs just fine! The try statement consists of an initial try clause. Everything under the try clause is attempted to be executed. If it succeeds, the rest of the try statement is skipped, and the interpreter goes to the seq = ... line.\nIf, however, there is an ImportError, the code within the except ImportError as e clause is executed. The exception does not halt the program. If there is some other kind of error other than an ImportError, the interpreter will raise an exception after it does whatever code is in the finally clause. The finally clause is useful to tidy things up, like closing open file handles. While it is possible for a try statement to handle any generic exception by not specifying ImportError as e, it is good practice to explicitly specify the exception(s) that you anticipate in try statements as shown here. In this case, we only want to have control over ImportErrors. We want the interpreter to scream at us for any other, unanticipated errors.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/exceptions_and_error_handling.html#issuing-warnings",
    "href": "appendices/python_basics/exceptions_and_error_handling.html#issuing-warnings",
    "title": "Appendix N — Errors and exception handling",
    "section": "N.3 Issuing warnings",
    "text": "N.3 Issuing warnings\nWe may want to issue a warning instead of silently continuing. For this, the warnings module from the standard library is useful. We use the warnings.warn() method to issue the warning.\n\n# Try to get the gc_content module\ntry:\n    import gc_content\n\n    have_gc = True\nexcept ImportError as e:\n    have_gc = False\n    warnings.warn(\n        \"Failed to load gc_content. Using custom function.\", UserWarning\n    )\nfinally:\n    pass\n\nseq = \"ACGATCTACGATCAGCTGCGCGCATCG\"\n\nif have_gc:\n    print(gc_content(seq))\nelse:\n    print(seq.count(\"G\") + seq.count(\"C\"))\n\n16\n\n\n/var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/ipykernel_16676/3620265156.py:8: UserWarning: Failed to load gc_content. Using custom function.\n  warnings.warn(\n\n\nNormally, we would use an ImportWarning, but those are ignored by default, so we have used a UserWarning.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/exceptions_and_error_handling.html#checking-input",
    "href": "appendices/python_basics/exceptions_and_error_handling.html#checking-input",
    "title": "Appendix N — Errors and exception handling",
    "section": "N.4 Checking input",
    "text": "N.4 Checking input\nIt is often the case that you want to check the input of a function to ensure that it works properly. In other words, you want to anticipate errors that the user (or you) might make in running your function, and you want to give descriptive error messages. For example, let’s say you are writing a code that processes protein sequences that contain only the 20 naturally occurring amino acids represented by their one-letter abbreviation. You may wish to check that the amino acid sequence is legitimate. In particular, the letters B, J, O, U, X, and Z, are not valid abbreviations for standard amino acids. (We will not use the ambiguity code, e.g. B for aspartic acid or asparagine, Z for glutamine or glutamic acid, or X for any amino acid.)\nTo illustrate the point, we will write a simple function that converts the sequence of one-letter amino acids to the three-letter abbreviation. We’ll use the dictionary that converts single-letter amino acid codes to triple letter that we encountered in the lesson on dictionaries that is now included in the bootcamp_utils package.\n\ndef one_to_three(seq):\n    \"\"\"\n    Converts a protein sequence using one-letter abbreviations\n    to one using three-letter abbreviations.\n    \"\"\"\n    # Convert seq to upper case\n    seq = seq.upper()\n\n    aa_list = []\n    for amino_acid in seq:\n        # Check if the `amino_acid` is in our dictionary `bootcamp_utils.aa`\n        if amino_acid not in bootcamp_utils.aa.keys():\n            raise RuntimeError(f\"{amino_acid} is not a valid amino acid\")\n        # Add the `amino_acid` to our aa_list\n        aa_list.append(bootcamp_utils.aa[amino_acid])\n\n    # Return the amino acids, joined together, with a dash as a separator.\n    return \"-\".join(aa_list)\n\nSo, if we put in a legitimate amino acid sequence, the function works as expected.\n\none_to_three('waeifnsdfklnsae')\n\n'Trp-Ala-Glu-Ile-Phe-Asn-Ser-Asp-Phe-Lys-Leu-Asn-Ser-Ala-Glu'\n\n\nBut, it we put in an improper amino acid, we will get a descriptive error.\n\none_to_three('waeifnsdfzklnsae')\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[14], line 1\n----&gt; 1 one_to_three('waeifnsdfzklnsae')\n\nCell In[12], line 13, in one_to_three(seq)\n     10 for amino_acid in seq:\n     11     # Check if the `amino_acid` is in our dictionary `bootcamp_utils.aa`\n     12     if amino_acid not in bootcamp_utils.aa.keys():\n---&gt; 13         raise RuntimeError(f\"{amino_acid} is not a valid amino acid\")\n     14     # Add the `amino_acid` to our aa_list\n     15     aa_list.append(bootcamp_utils.aa[amino_acid])\n\nRuntimeError: Z is not a valid amino acid\n\n\n\nGood code checks for errors and gives useful error messages. We will use exception handling extensively when we go over test driven development in future lessons.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/exceptions_and_error_handling.html#computing-environment",
    "href": "appendices/python_basics/exceptions_and_error_handling.html#computing-environment",
    "title": "Appendix N — Errors and exception handling",
    "section": "N.5 Computing environment",
    "text": "N.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p bootcamp_utils,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\nbootcamp_utils: 0.0.7\njupyterlab    : 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/file_io.html",
    "href": "appendices/python_basics/file_io.html",
    "title": "Appendix O — File I/O",
    "section": "",
    "text": "O.1 File objects\n| Download notebook\nReading data in from files and then writing your results out again is one of the most common practices in scientific computing. In this tutorial, we will learn about some of Python’s File I/O capabilities. We will use a PDB file as an example. The PDB file contains the crystal structure for the tetramerization domain of p53.It is stored in the file ~/git/bootcamp/data/1OLG.pdb. (Make sure you launch your notebook from the ~/git/bootcamp/ directory.) Note that 1OLG is its unique Protein Databank identifier.\nTo open a file, we use the built-in open() function. When opening files, we should do this using context management. I will demonstrate how to open a file and then describe the syntax.\nwith open('data/1OLG.pdb', 'r') as f:\n    print(type(f))\n\n&lt;class '_io.TextIOWrapper'&gt;\nPython has a wonderful keyword, with. This keyword enables context management. Upon entry into a with block, variables have certain meaning. In this case, the variable f has the meaning of an open file, an instance of the _io.TextIOWrapper class. Upon exit, certain operations take place. For file objects created by opening them, the file is automatically closed upon exit, even if there is an error. This is important. If your program raises an exception before you have a chance to close the file, it won’t get closed and you could be in trouble. If you use context management, the file will still get closed. So here is an important tip:\nLet’s focus for a moment on the variable f in the above code cell. It is a Python file object, which has methods and attributes, just like any other object. We’ll explore those in a moment, but first, let’s look at how we opened the file. The first argument to open() is a string that has the name of the file, with the full path if necessary. The second argument is a string that says what we will be doing with the file. I.e., are we reading or writing to the file? The possible strings for this second argument are\nWe will mostly be working with text files in the bootcamp, so the first three are the most useful. A big warning, though….",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>O</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/file_io.html#file-objects",
    "href": "appendices/python_basics/file_io.html#file-objects",
    "title": "Appendix O — File I/O",
    "section": "",
    "text": "Use context management using with when working with files.\n\n\n\n\n\n\n\n\n\nstring\nmeaning\n\n\n\n\n'r'\nopen a text file for reading\n\n\n'w'\ncreate and open a text file for writing\n\n\n'a'\nappend an existing text file\n\n\n'r+'\nopen a text file for reading and writing\n\n\nappend 'b' to any of the above\nsame as above, except for binary files\n\n\n\n\n\nTrying to open an existing file with ‘w’ will wipe it out and create a new file.\n\n\nO.1.1 Reading data out of the file with file object methods\nWe will focus on the methods f.read() and f.readlines(). What do they do?\n\n\n\n\n\n\n\nmethod\ntask\n\n\n\n\nf.read()\nRead the entire contents of the file into a string\n\n\nf.readlines()\nRead the entire file into a list with each item being a string representing a line\n\n\n\nFirst, we’ll try using the first method to get a single string with the entire contents of the file.\n\n# Read file into string\nwith open('data/1OLG.pdb', 'r') as f:\n    f_str = f.read()\n\n# Let's look at the first 1000 characters\nf_str[:1000]\n\n'HEADER    ANTI-ONCOGENE                           13-JUN-94   1OLG              \\nTITLE     HIGH-RESOLUTION SOLUTION STRUCTURE OF THE OLIGOMERIZATION             \\nTITLE    2 DOMAIN OF P53 BY MULTI-DIMENSIONAL NMR                               \\nCOMPND    MOL_ID: 1;                                                            \\nCOMPND   2 MOLECULE: TUMOR SUPPRESSOR P53 (OLIGOMERIZATION DOMAIN);             \\nCOMPND   3 CHAIN: A, B, C, D;                                                   \\nCOMPND   4 ENGINEERED: YES                                                      \\nSOURCE    MOL_ID: 1;                                                            \\nSOURCE   2 ORGANISM_SCIENTIFIC: HOMO SAPIENS;                                   \\nSOURCE   3 ORGANISM_COMMON: HUMAN;                                              \\nSOURCE   4 ORGANISM_TAXID: 9606                                                 \\nKEYWDS    ANTI-ONCOGENE                                                         \\nEXPDTA    SOLUTION NMR      '\n\n\nWe see lots of \\n, which signifies a new line. The backslash is known as an escape character, meaning that the n after it does not signify the letter n, but that \\n together means a new line.\nNow, let’s try reading it in as a list.\n\n# Read contents of the file in as a list\nwith open('data/1OLG.pdb', 'r') as f:\n    f_list = f.readlines()\n\n# Look at the list (first ten entries)\nf_list[:10]\n\n['HEADER    ANTI-ONCOGENE                           13-JUN-94   1OLG              \\n',\n 'TITLE     HIGH-RESOLUTION SOLUTION STRUCTURE OF THE OLIGOMERIZATION             \\n',\n 'TITLE    2 DOMAIN OF P53 BY MULTI-DIMENSIONAL NMR                               \\n',\n 'COMPND    MOL_ID: 1;                                                            \\n',\n 'COMPND   2 MOLECULE: TUMOR SUPPRESSOR P53 (OLIGOMERIZATION DOMAIN);             \\n',\n 'COMPND   3 CHAIN: A, B, C, D;                                                   \\n',\n 'COMPND   4 ENGINEERED: YES                                                      \\n',\n 'SOURCE    MOL_ID: 1;                                                            \\n',\n 'SOURCE   2 ORGANISM_SCIENTIFIC: HOMO SAPIENS;                                   \\n',\n 'SOURCE   3 ORGANISM_COMMON: HUMAN;                                              \\n']\n\n\nWe see that each entry is a line, including the newline character. To look at lines in files, the rstrip() method for strings can come it handy. It strips all whitespace, including newlines, from the end of a string.\n\nf_list[0].rstrip()\n\n'HEADER    ANTI-ONCOGENE                           13-JUN-94   1OLG'\n\n\n\n\nO.1.2 Reading line-by-line\nWhat if we do not want to read the entire file into a list? For example, if a file is several gigabytes, we do not want to spend all of our RAM storing a list. Instead, we can read it line-by-line. Conveniently, the file object can be used as an iterator.\n\n# Print the first ten lines of the file\nwith open('data/1OLG.pdb', 'r') as f:\n    for i, line in enumerate(f):\n        print(line.rstrip())\n        if i &gt;= 10:\n            break\n\nHEADER    ANTI-ONCOGENE                           13-JUN-94   1OLG\nTITLE     HIGH-RESOLUTION SOLUTION STRUCTURE OF THE OLIGOMERIZATION\nTITLE    2 DOMAIN OF P53 BY MULTI-DIMENSIONAL NMR\nCOMPND    MOL_ID: 1;\nCOMPND   2 MOLECULE: TUMOR SUPPRESSOR P53 (OLIGOMERIZATION DOMAIN);\nCOMPND   3 CHAIN: A, B, C, D;\nCOMPND   4 ENGINEERED: YES\nSOURCE    MOL_ID: 1;\nSOURCE   2 ORGANISM_SCIENTIFIC: HOMO SAPIENS;\nSOURCE   3 ORGANISM_COMMON: HUMAN;\nSOURCE   4 ORGANISM_TAXID: 9606\n\n\nAlternatively, we can use the method f.readline() to read a single line in the file and return it as a string.\n\n# Print the first ten lines of the file\nwith open('data/1OLG.pdb', 'r') as f:\n    i = 0\n    while i &lt; 10:\n        print(f.readline().rstrip())\n        i += 1\n\nHEADER    ANTI-ONCOGENE                           13-JUN-94   1OLG\nTITLE     HIGH-RESOLUTION SOLUTION STRUCTURE OF THE OLIGOMERIZATION\nTITLE    2 DOMAIN OF P53 BY MULTI-DIMENSIONAL NMR\nCOMPND    MOL_ID: 1;\nCOMPND   2 MOLECULE: TUMOR SUPPRESSOR P53 (OLIGOMERIZATION DOMAIN);\nCOMPND   3 CHAIN: A, B, C, D;\nCOMPND   4 ENGINEERED: YES\nSOURCE    MOL_ID: 1;\nSOURCE   2 ORGANISM_SCIENTIFIC: HOMO SAPIENS;\nSOURCE   3 ORGANISM_COMMON: HUMAN;\n\n\nEach subsequent call to f.readline() reads in the next line of the file. (As we read through a file, we keep moving forward in the bytes of the file and we have to use f.seek() to rewind.)",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>O</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/file_io.html#writing-to-a-file",
    "href": "appendices/python_basics/file_io.html#writing-to-a-file",
    "title": "Appendix O — File I/O",
    "section": "O.2 Writing to a file",
    "text": "O.2 Writing to a file\nWriting to a file has similar syntax. We already saw how to open a file for writing. Again, context management is useful. However, before trying to open a file, we should check to make sure a file of the same name does not exist before opening it. The os.path module is useful. The function os.path.isfile() function checks to see if a file exists.\n\nos.path.isfile('data/1OLG.pdb')\n\nTrue\n\n\nNow that we know how to check existence of a file so we do not overwrite it, we can open and write a file.\n\nif os.path.isfile('yogi.txt'):\n    raise RuntimeError('File yogi.txt already exists.')\n\nwith open('yogi.txt', 'w') as f:\n    f.write('When you come to a fork in the road, take it.')\n    f.write('You can observe a lot by just watching.')\n    f.write('I never said most of the things I said.')\n\nNote that we can use the f.write() method to write strings to a file. Let’s look at the file contents.\n\n!cat yogi.txt\n\nWhen you come to a fork in the road, take it.You can observe a lot by just watching.I never said most of the things I said.\n\n\nAh! There are no newlines! When writing to a file, unlike when you use the print() function, you must include the newline characters. Let’s try again, intentionally obliterating our first attempt.\n\nwith open('yogi.txt', 'w') as f:\n    f.write('When you come to a fork in the road, take it.\\n')\n    f.write('You can observe a lot by just watching.\\n')\n    f.write('I never said most of the things I said.\\n')\n    \n!cat yogi.txt\n\nWhen you come to a fork in the road, take it.\nYou can observe a lot by just watching.\nI never said most of the things I said.\n\n\nThat’s better. Note also that f.write() only takes strings as arguments. You cannot pass numbers. They must be converted to strings first.\n\n# This will result in an exception\nwith open('gimme_phi.txt', 'w') as f:\n    f.write('The golden ratio is φ = ')\n    f.write(1.61803398875)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[12], line 4\n      2 with open('gimme_phi.txt', 'w') as f:\n      3     f.write('The golden ratio is φ = ')\n----&gt; 4     f.write(1.61803398875)\n\nTypeError: write() argument must be str, not float\n\n\n\nYup. It must be a string. Let’s try again.\n\nwith open('gimme_phi.txt', 'w') as f:\n    f.write('The golden ratio is φ = ')\n    f.write('{phi:.8f}'.format(phi=1.61803398875))\n\n!cat gimme_phi.txt\n\nThe golden ratio is φ = 1.61803399\n\n\nThat works!",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>O</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/file_io.html#an-exercise-extract-atomic-coordinates-for-first-chain-in-tetramer",
    "href": "appendices/python_basics/file_io.html#an-exercise-extract-atomic-coordinates-for-first-chain-in-tetramer",
    "title": "Appendix O — File I/O",
    "section": "O.3 An exercise: extract atomic coordinates for first chain in tetramer",
    "text": "O.3 An exercise: extract atomic coordinates for first chain in tetramer\nAs an example on how to do file I/O, we will take the PDB file and extract only the ATOM records for the first chain of the tetramer and write only those entries to a new file.\nIt is useful to know that according to the PDB format specification, column 21 in the ATOM entry gives the ID of the chain.\nWe also conveniently use the fact that we can have multiple files open in our with block, separating them with commas.\n\nwith open('data/1OLG.pdb', 'r') as f, open('atoms_chain_A.txt', 'w') as f_out:\n    # Put the ATOM lines from chain A in new file\n    for line in f:\n        if len(line) &gt; 21 and line[:4] == 'ATOM' and line[21] == 'A':\n            f_out.write(line)\n\nLet’s see how we did!\n\n!head -10 atoms_chain_A.txt\n\nATOM      1  N   LYS A 319      18.634  25.437  10.685  1.00  4.81           N  \nATOM      2  CA  LYS A 319      17.984  25.295   9.354  1.00  4.32           C  \nATOM      3  C   LYS A 319      18.160  23.876   8.818  1.00  3.74           C  \nATOM      4  O   LYS A 319      19.259  23.441   8.537  1.00  3.67           O  \nATOM      5  CB  LYS A 319      18.609  26.282   8.371  1.00  4.67           C  \nATOM      6  CG  LYS A 319      18.003  26.056   6.986  1.00  5.15           C  \nATOM      7  CD  LYS A 319      16.476  26.057   7.091  1.00  5.90           C  \nATOM      8  CE  LYS A 319      16.014  27.341   7.784  1.00  6.51           C  \nATOM      9  NZ  LYS A 319      16.388  28.518   6.952  1.00  7.33           N  \nATOM     10  H1  LYS A 319      18.414  24.606  11.281  1.00  5.09           H  \n\n\n\n!tail -10 atoms_chain_A.txt\n\nATOM    689  HD2 PRO A 359       0.183  25.663  13.542  1.00  4.71           H  \nATOM    690  HD3 PRO A 359       0.246  23.956  13.062  1.00  4.53           H  \nATOM    691  N   GLY A 360      -3.984  26.791  10.832  1.00  5.45           N  \nATOM    692  CA  GLY A 360      -4.489  28.138  10.445  1.00  5.95           C  \nATOM    693  C   GLY A 360      -5.981  28.236  10.765  1.00  6.77           C  \nATOM    694  O   GLY A 360      -6.401  27.621  11.732  1.00  7.24           O  \nATOM    695  OXT GLY A 360      -6.679  28.924  10.039  1.00  7.15           O  \nATOM    696  H   GLY A 360      -4.589  26.020  10.828  1.00  5.72           H  \nATOM    697  HA2 GLY A 360      -3.950  28.896  10.995  1.00  5.99           H  \nATOM    698  HA3 GLY A 360      -4.341  28.288   9.386  1.00  6.05           H  \n\n\nNice!",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>O</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/file_io.html#finding-files-and-with-glob",
    "href": "appendices/python_basics/file_io.html#finding-files-and-with-glob",
    "title": "Appendix O — File I/O",
    "section": "O.4 Finding files and with glob",
    "text": "O.4 Finding files and with glob\nIn the above snippet of code, we extracted all atom records from a PDB file. We might want to do this (or some other operation) for many files. For example, the directory ~/git/data/ has four PDB files in it. For the present discussion, let’s say we want to pull the sequence of chain A out of each PDB file.\nThe glob module from the standard library enables us to get a list of all files that match a pattern. In our case, we want all files matching data/*.pdb, where * is a wild card character, meaning that any matches of characters where * appears are allowed. Let’s see what glob.glob() gives us.\n\nfile_list = glob.glob('data/*.pdb')\n\nfile_list\n\n['data/1OLG.pdb', 'data/1J6Z.pdb', 'data/1FAG.pdb', 'data/2ERK.pdb']\n\n\nWe have the four PDB files. We can now loop over them and pull out the sequences.\n\n# Dictionary to hold sequences\nseqs = {}\n\n# Loop through all matching files\nfor file_name in file_list:\n    # Extract PDB ID\n    pdb_id = file_name[file_name.find('/')+1:file_name.rfind('.')]\n    \n    # Initialize sequence string, which we build as we go along\n    seq = ''\n    with open(file_name, 'r') as f:\n        for line in f:\n            if len(line) &gt; 11 and line[:6] == 'SEQRES' and line[11] == 'A':\n                seq += line[19:].rstrip() + ' '\n\n    # Build sequence with dash-joined three letter codes\n    seq = '-'.join(seq.split())\n\n    # Store in the dictionary\n    seqs[pdb_id] = seq\n\nLet’s take a look at what we got. We’ll look at actin.\n\nseqs['1J6Z']\n\n'ASP-GLU-ASP-GLU-THR-THR-ALA-LEU-VAL-CYS-ASP-ASN-GLY-SER-GLY-LEU-VAL-LYS-ALA-GLY-PHE-ALA-GLY-ASP-ASP-ALA-PRO-ARG-ALA-VAL-PHE-PRO-SER-ILE-VAL-GLY-ARG-PRO-ARG-HIS-GLN-GLY-VAL-MET-VAL-GLY-MET-GLY-GLN-LYS-ASP-SER-TYR-VAL-GLY-ASP-GLU-ALA-GLN-SER-LYS-ARG-GLY-ILE-LEU-THR-LEU-LYS-TYR-PRO-ILE-GLU-HIC-GLY-ILE-ILE-THR-ASN-TRP-ASP-ASP-MET-GLU-LYS-ILE-TRP-HIS-HIS-THR-PHE-TYR-ASN-GLU-LEU-ARG-VAL-ALA-PRO-GLU-GLU-HIS-PRO-THR-LEU-LEU-THR-GLU-ALA-PRO-LEU-ASN-PRO-LYS-ALA-ASN-ARG-GLU-LYS-MET-THR-GLN-ILE-MET-PHE-GLU-THR-PHE-ASN-VAL-PRO-ALA-MET-TYR-VAL-ALA-ILE-GLN-ALA-VAL-LEU-SER-LEU-TYR-ALA-SER-GLY-ARG-THR-THR-GLY-ILE-VAL-LEU-ASP-SER-GLY-ASP-GLY-VAL-THR-HIS-ASN-VAL-PRO-ILE-TYR-GLU-GLY-TYR-ALA-LEU-PRO-HIS-ALA-ILE-MET-ARG-LEU-ASP-LEU-ALA-GLY-ARG-ASP-LEU-THR-ASP-TYR-LEU-MET-LYS-ILE-LEU-THR-GLU-ARG-GLY-TYR-SER-PHE-VAL-THR-THR-ALA-GLU-ARG-GLU-ILE-VAL-ARG-ASP-ILE-LYS-GLU-LYS-LEU-CYS-TYR-VAL-ALA-LEU-ASP-PHE-GLU-ASN-GLU-MET-ALA-THR-ALA-ALA-SER-SER-SER-SER-LEU-GLU-LYS-SER-TYR-GLU-LEU-PRO-ASP-GLY-GLN-VAL-ILE-THR-ILE-GLY-ASN-GLU-ARG-PHE-ARG-CYS-PRO-GLU-THR-LEU-PHE-GLN-PRO-SER-PHE-ILE-GLY-MET-GLU-SER-ALA-GLY-ILE-HIS-GLU-THR-THR-TYR-ASN-SER-ILE-MET-LYS-CYS-ASP-ILE-ASP-ILE-ARG-LYS-ASP-LEU-TYR-ALA-ASN-ASN-VAL-MET-SER-GLY-GLY-THR-THR-MET-TYR-PRO-GLY-ILE-ALA-ASP-ARG-MET-GLN-LYS-GLU-ILE-THR-ALA-LEU-ALA-PRO-SER-THR-MET-LYS-ILE-LYS-ILE-ILE-ALA-PRO-PRO-GLU-ARG-LYS-TYR-SER-VAL-TRP-ILE-GLY-GLY-SER-ILE-LEU-ALA-SER-LEU-SER-THR-PHE-GLN-GLN-MET-TRP-ILE-THR-LYS-GLN-GLU-TYR-ASP-GLU-ALA-GLY-PRO-SER-ILE-VAL-HIS-ARG-LYS-CYS-PHE'\n\n\nExcellent! Our function worked, and we now have the protein sequence.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>O</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/file_io.html#computing-environment",
    "href": "appendices/python_basics/file_io.html#computing-environment",
    "title": "Appendix O — File I/O",
    "section": "O.5 Computing environment",
    "text": "O.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>O</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "",
    "text": "P.1 A very brief introduction to NumPy arrays\n| Download notebook\nHere you will learn about NumPy, arguably the most important package for scientific computing, and SciPy, a package containing lots of goodies for scientific computing, like special functions and numerical integrators.\nThe central object for NumPy and SciPy is the ndarray, commonly referred to as a “NumPy array.” This is an array object that is convenient for scientific computing. We will go over it in depth in the next lesson, but for now, let’s just create some NumPy arrays and see how operators work on them.\nJust like with type conversions with lists, tuples, and other data types we’ve looked at, we can convert a list to a NumPy array using\nNote that above we imported the NumPy package with the np alias. This is for convenience; it allow us to use np as a prefix instead of numpy. NumPy is in very widespread use, and the convention is to use the np abbreviation.\n# Create a NumPy array from a list\nmy_ar = np.array([1, 2, 3, 4])\n\n# Look at it\nmy_ar\n\narray([1, 2, 3, 4])\nWe see that the list has been converted, and it is explicitly shown as an array. It has several attributes and lots of methods. The most important attributes are probably the data type of its elements and the shape of the array.\n# The data type of stored entries\nmy_ar.dtype\n\ndtype('int64')\n# The shape of the array\nmy_ar.shape\n\n(4,)\nThere are also lots of methods. The one we use most often is astype(), which converts the data type of the array.\nmy_ar.astype(float)\n\narray([1., 2., 3., 4.])\nThere are many others. For example, we can compute summary statistics about the entries in the array, very similar to what we have see with Pandas.\nprint(my_ar.max())\nprint(my_ar.min())\nprint(my_ar.sum())\nprint(my_ar.mean())\nprint(my_ar.std())\n\n4\n1\n10\n2.5\n1.118033988749895\nImportantly, NumPy arrays can be arguments to NumPy functions. In this case, these functions do the same operations as the methods we just looked at.\nprint(np.max(my_ar))\nprint(np.min(my_ar))\nprint(np.sum(my_ar))\nprint(np.mean(my_ar))\nprint(np.std(my_ar))\n\n4\n1\n10\n2.5\n1.118033988749895",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#a-very-brief-introduction-to-numpy-arrays",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#a-very-brief-introduction-to-numpy-arrays",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "",
    "text": "np.array()",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#other-ways-to-make-numpy-arrays",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#other-ways-to-make-numpy-arrays",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.2 Other ways to make NumPy arrays",
    "text": "P.2 Other ways to make NumPy arrays\nThere are many other ways to make NumPy arrays besides just converting lists or tuples. Below are some examples.\n\n# How long our arrays will be\nn = 10\n\n# Make a NumPy array of length n filled with zeros\nnp.zeros(n)\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\n# Make a NumPy array of length n filled with ones\nnp.ones(n)\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\n# Make an empty NumPy array of length n without initializing entries\n# (while it initially holds whatever values were previously in the memory\n# locations assigned, ones will be displayed)\nnp.empty(n)\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\n# Make a NumPy array filled with zeros the same shape as another NumPy array\nmy_ar = np.array([[1, 2], [3, 4]])\nnp.zeros_like(my_ar)\n\narray([[0, 0],\n       [0, 0]])\n\n\nAs we work through the rest of this exercise, we will use more interesting arrays (not just zeroes, ones, and counting).\n\nx = np.array(\n    [1683, 2061, 1792, 1852, 2091, 1781, 1912, 1802, 1751, 1731, 1892,\n     1951, 1809, 1683, 1787, 1840, 1821, 1910, 1930, 1800, 1833, 1683,\n     1671, 1680, 1692, 1800, 1821, 1882, 1642, 1749, 1712, 1661, 1701,\n     2141, 1863, 1752, 1740, 1721, 1660, 1930, 2030, 1851, 2131, 1828])\n\ny = np.array(\n    [1840, 2090, 2169, 1988, 2212, 2339, 1989, 2144, 2290, 1920, 2280,\n     1809, 2158, 1800, 2133, 2060, 2160, 2001, 2030, 2088, 1951, 2460])",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#slicing-numpy-arrays",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#slicing-numpy-arrays",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.3 Slicing NumPy arrays",
    "text": "P.3 Slicing NumPy arrays\nWe can slice NumPy arrays like lists and tuples. Here are a few examples.\n\n# Reversed array\nx[::-1]\n\narray([1828, 2131, 1851, 2030, 1930, 1660, 1721, 1740, 1752, 1863, 2141,\n       1701, 1661, 1712, 1749, 1642, 1882, 1821, 1800, 1692, 1680, 1671,\n       1683, 1833, 1800, 1930, 1910, 1821, 1840, 1787, 1683, 1809, 1951,\n       1892, 1731, 1751, 1802, 1912, 1781, 2091, 1852, 1792, 2061, 1683])\n\n\n\n# Every 5th element, starting at index 3\nx[3::5]\n\narray([1852, 1751, 1683, 1930, 1680, 1642, 2141, 1660, 1828])\n\n\n\n# Entries 10 to 20\nx[10:21]\n\narray([1892, 1951, 1809, 1683, 1787, 1840, 1821, 1910, 1930, 1800, 1833])\n\n\n\nP.3.1 Fancy indexing\nNumPy arrays also allow fancy indexing, where we can slice out specific values. For example, say we wanted indices 1, 19, and 6 (in that order) from x. We just index with a list of the indices we want.\n\nx[[1, 19, 6]]\n\narray([2061, 1800, 1912])\n\n\nInstead of a list, we could also use a NumPy array.\n\nx[np.array([1, 19, 6])]\n\narray([2061, 1800, 1912])\n\n\nAs a very nice feature, we can use Boolean indexing with Numpy arrays. Say we only want entries greater than 2000.\n\n# Just slice out the big ones\nx[x &gt; 2000]\n\narray([2061, 2091, 2141, 2030, 2131])\n\n\nIf we want to know the indices where the values are high, we can use the np.where() function.\n\nnp.where(x &gt; 2000)\n\n(array([ 1,  4, 33, 40, 42]),)",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#numpy-arrays-are-mutable",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#numpy-arrays-are-mutable",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.4 NumPy arrays are mutable",
    "text": "P.4 NumPy arrays are mutable\nYes, NumPy arrays are mutable. Let’s look at some consequences.\n\n# Make an array\nmy_ar = np.array([1, 2, 3, 4])\n\n# Change an element\nmy_ar[2] = 6\n\n# See the result\nmy_ar\n\narray([1, 2, 6, 4])\n\n\nNow, let’s try attaching another variable to the NumPy array.\n\n# Attach a new variable\nmy_ar2 = my_ar\n\n# Set an entry using the new variable\nmy_ar2[3] = 9\n\n# Does the original change? (yes.)\nmy_ar\n\narray([1, 2, 6, 9])\n\n\nLet’s see how messing with NumPy in functions affects things.\n\n# Re-instantiate my_ar\nmy_ar = np.array([1, 2, 3, 4]).astype(float)\n\n# Function to normalize x (note that /= works with mutable objects)\ndef normalize(x):\n    x /= np.sum(x)\n\n# Pass it through a function\nnormalize(my_ar)\n\n# Is it normalized even though we didn't return anything? (Yes.)\nmy_ar\n\narray([0.1, 0.2, 0.3, 0.4])\n\n\nSo, be careful when writing functions. What you do to your NumPy array inside the function will happen outside of the function as well. Always remember that:\n\nNumPy arrays are mutable.\n\n\nP.4.1 Slices of NumPy arrays are views, not copies\nA very important distinction between NumPy arrays and lists is that slices of NumPy arrays are views into the original NumPy array, NOT copies. To illustrate this, we will again use out 1, 2, 3, 4 array for simplicity and clarity.\n\n# Make list and array\nmy_list = [1, 2, 3, 4]\nmy_ar = np.array(my_list)\n\n# Slice out of each\nmy_list_slice = my_list[1:-1]\nmy_ar_slice = my_ar[1:-1]\n\n# Mess with the slices\nmy_list_slice[0] = 9\nmy_ar_slice[0] = 9\n\n# Look at originals\nprint(my_list)\nprint(my_ar)\n\n[1, 2, 3, 4]\n[1 9 3 4]\n\n\nMessing with an element of a slice of a NumPy array messes with that element in the original! This is not the case with lists. Let’s issue a warning.\n\nSlices of NumPy arrays are views, not copies.\n\nFortunately, you can make a copy of an array using the np.copy() function.\n\n# Make a copy\nx_copy = np.copy(x)\n\n# Mess with an entry\nx_copy[10] = 2000\n\n# Check equality\nnp.allclose(x, x_copy)\n\nFalse\n\n\nSo, messing with an entry in the copy did not affect the original.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#mathematical-operations-with-arrays",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#mathematical-operations-with-arrays",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.5 Mathematical operations with arrays",
    "text": "P.5 Mathematical operations with arrays\nMathematical operations on arrays are done elementwise to all elements.\n\n# Divide one array be another\nnp.array([5, 6, 7, 8]) / np.array([1, 2, 3, 4])\n\narray([5.        , 3.        , 2.33333333, 2.        ])\n\n\n\n# Multiply by scalar\n-4 * x\n\narray([-6732, -8244, -7168, -7408, -8364, -7124, -7648, -7208, -7004,\n       -6924, -7568, -7804, -7236, -6732, -7148, -7360, -7284, -7640,\n       -7720, -7200, -7332, -6732, -6684, -6720, -6768, -7200, -7284,\n       -7528, -6568, -6996, -6848, -6644, -6804, -8564, -7452, -7008,\n       -6960, -6884, -6640, -7720, -8120, -7404, -8524, -7312])\n\n\n\n# Raise to power\nx**2\n\narray([2832489, 4247721, 3211264, 3429904, 4372281, 3171961, 3655744,\n       3247204, 3066001, 2996361, 3579664, 3806401, 3272481, 2832489,\n       3193369, 3385600, 3316041, 3648100, 3724900, 3240000, 3359889,\n       2832489, 2792241, 2822400, 2862864, 3240000, 3316041, 3541924,\n       2696164, 3059001, 2930944, 2758921, 2893401, 4583881, 3470769,\n       3069504, 3027600, 2961841, 2755600, 3724900, 4120900, 3426201,\n       4541161, 3341584])",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#indexing-2d-numpy-arrays",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#indexing-2d-numpy-arrays",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.6 Indexing 2D NumPy arrays",
    "text": "P.6 Indexing 2D NumPy arrays\nNumPy arrays need not be one-dimensional. We’ll create a two-dimensional NumPy array by reshaping our x array from having shape (44,) to having shape (11, 4). That is, it will become an array with 11 rows and 4 columns. (The 2D nature of this array has no meaning in this case; it’s just meant for demonstration.)\n\n# New 2D array using the reshape() method\nmy_ar = x.reshape((11, 4))\n\n# Look at it\nmy_ar\n\narray([[1683, 2061, 1792, 1852],\n       [2091, 1781, 1912, 1802],\n       [1751, 1731, 1892, 1951],\n       [1809, 1683, 1787, 1840],\n       [1821, 1910, 1930, 1800],\n       [1833, 1683, 1671, 1680],\n       [1692, 1800, 1821, 1882],\n       [1642, 1749, 1712, 1661],\n       [1701, 2141, 1863, 1752],\n       [1740, 1721, 1660, 1930],\n       [2030, 1851, 2131, 1828]])\n\n\nNotice that it is represented as an array made out of a list of lists. If we had a list of lists, we would index it like this:\nlist_of_lists[i][j]\n\n# Make list of lists\nlist_of_lists = [[1, 2], [3, 4]]\n\n# Pull out value in first row, second column\nlist_of_lists[0][1]\n\n2\n\n\nThough this will work with NumPy arrays, this is not how NumPy arrays are indexed. They are indexed much more conveniently.\n\nmy_ar[0, 1]\n\n2061\n\n\nWe essentially have a tuple in the indexing brackets. Now, say we wanted the second row (indexing starting at 0).\n\nmy_ar[2, :]\n\narray([1751, 1731, 1892, 1951])\n\n\nWe can use Boolean indexing as before.\n\nmy_ar[my_ar &gt; 2000]\n\narray([2061, 2091, 2141, 2030, 2131])\n\n\nNote that this gives a one-dimensional array of the entries greater than 2000. If we wanted indices where this is the case, we can again use np.where().\n\nnp.where(my_ar &gt; 2000)\n\n(array([ 0,  1,  8, 10, 10]), array([1, 0, 1, 0, 2]))\n\n\nThis tuple of NumPy arrays is how we would index using fancy indexing to pull those values out using fancy indexing.\n\nmy_ar[(np.array([ 0,  1,  8, 10, 10]), np.array([1, 0, 1, 0, 2]))]\n\narray([2061, 2091, 2141, 2030, 2131])\n\n\nNumPy arrays can be of arbitrary integer dimension, and these principles extrapolate to 3D, 4D, etc., arrays.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#concatenating-arrays",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#concatenating-arrays",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.7 Concatenating arrays",
    "text": "P.7 Concatenating arrays\nLet’s say we want to study all cross sectional areas and don’t care if the mother was well-fed or not. We would want to concatenate our arrays. The np.concatenate() function accomplishes this. We simply have to pass it a tuple containing the NumPy arrays we want to concatenate.\n\ncombined = np.concatenate((x, y))\n\n# Look at it\ncombined\n\narray([1683, 2061, 1792, 1852, 2091, 1781, 1912, 1802, 1751, 1731, 1892,\n       1951, 1809, 1683, 1787, 1840, 1821, 1910, 1930, 1800, 1833, 1683,\n       1671, 1680, 1692, 1800, 1821, 1882, 1642, 1749, 1712, 1661, 1701,\n       2141, 1863, 1752, 1740, 1721, 1660, 1930, 2030, 1851, 2131, 1828,\n       1840, 2090, 2169, 1988, 2212, 2339, 1989, 2144, 2290, 1920, 2280,\n       1809, 2158, 1800, 2133, 2060, 2160, 2001, 2030, 2088, 1951, 2460])",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#numpy-has-useful-mathematical-functions",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#numpy-has-useful-mathematical-functions",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.8 NumPy has useful mathematical functions",
    "text": "P.8 NumPy has useful mathematical functions\nSo far, we have not done much mathematics with Python. We have done some adding and division, but nothing like computing a logarithm or cosine. The NumPy functions also work elementwise on the arrays when it is intuitive to do so. That is, they apply the function to each entry in the array. Check it out.\n\n# Exponential\nnp.exp(x / 1000)\n\narray([5.38167681, 7.8538197 , 6.00144336, 6.37255189, 8.09300412,\n       5.93578924, 6.76660849, 6.06175887, 5.76036016, 5.64629738,\n       6.63262067, 7.03571978, 6.10434004, 5.38167681, 5.97151103,\n       6.29653826, 6.1780334 , 6.7530888 , 6.88951024, 6.04964746,\n       6.2526164 , 5.38167681, 5.31748262, 5.36555597, 5.43033051,\n       6.04964746, 6.1780334 , 6.56662499, 5.16549017, 5.74885095,\n       5.54003047, 5.26457279, 5.47942408, 8.50794132, 6.44303692,\n       5.7661234 , 5.69734342, 5.59011579, 5.25931084, 6.88951024,\n       7.61408636, 6.36618252, 8.42328589, 6.22143134])\n\n\n\n# Cosine\nnp.cos(x)\n\narray([ 0.62656192,  0.9933696 ,  0.27501843,  0.03112568,  0.26681725,\n       -0.96021239, -0.33430744,  0.29228295, -0.42404251, -0.99984597,\n        0.72399324, -0.99748325,  0.84865001,  0.62656192, -0.84393482,\n        0.56257847,  0.43231386,  0.99610114,  0.48702972, -0.99122275,\n       -0.11903049,  0.62656192,  0.94691648, -0.73027654, -0.24968607,\n       -0.99122275,  0.43231386, -0.98275172, -0.49500319, -0.64703425,\n       -0.98592179, -0.61963892, -0.17156886,  0.00460656, -0.99936794,\n        0.53296056,  0.90375673,  0.82939405,  0.3256673 ,  0.48702972,\n        0.86222727, -0.824246  ,  0.5401501 ,  0.91834245])\n\n\n\n# Square root\nnp.sqrt(x)\n\narray([41.02438299, 45.39823785, 42.33202098, 43.03486958, 45.72745346,\n       42.20189569, 43.72642222, 42.44997055, 41.84495191, 41.60528813,\n       43.49712634, 44.17012565, 42.53234064, 41.02438299, 42.27292278,\n       42.89522118, 42.67317659, 43.70354677, 43.93176527, 42.42640687,\n       42.81354926, 41.02438299, 40.87786687, 40.98780306, 41.1339276 ,\n       42.42640687, 42.67317659, 43.38202393, 40.52159918, 41.82104733,\n       41.37632173, 40.75536774, 41.24318125, 46.27094121, 43.16248371,\n       41.85689907, 41.71330723, 41.48493703, 40.74309757, 43.93176527,\n       45.0555213 , 43.02324953, 46.16275555, 42.75511665])\n\n\nWe can even do some matrix operations (which are obviously not done elementwise), like dot products.\n\nnp.dot(x, x)\n\n146360195\n\n\nNumPy also has useful attributes, like np.pi.\n\nnp.pi\n\n3.141592653589793",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#scipy-has-even-more-useful-functions-in-modules",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#scipy-has-even-more-useful-functions-in-modules",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.9 SciPy has even more useful functions (in modules)",
    "text": "P.9 SciPy has even more useful functions (in modules)\nSciPy actually began life as a library of special functions that operate on NumPy arrays. For example, we can compute an error function using the scipy.special module, which contains lots of special functions. Note that you often have to individually import the SciPy module you want to use, for example with\nimport scipy.special\n\nscipy.special.erf(x / 2000)\n\narray([0.76597747, 0.8549794 , 0.7948931 , 0.80965587, 0.86074212,\n       0.79209865, 0.8236209 , 0.79740973, 0.78433732, 0.77904847,\n       0.81905337, 0.83227948, 0.79915793, 0.76597747, 0.7936263 ,\n       0.80676772, 0.8021292 , 0.82316805, 0.8276577 , 0.79690821,\n       0.80506817, 0.76597747, 0.76262579, 0.76514271, 0.76846912,\n       0.79690821, 0.8021292 , 0.81673693, 0.7543863 , 0.78381257,\n       0.77393853, 0.75980693, 0.77094188, 0.86995276, 0.81227529,\n       0.78459935, 0.78143985, 0.77636944, 0.75952376, 0.8276577 ,\n       0.84883448, 0.80941641, 0.86814949, 0.80384751])\n\n\nThere are many SciPy submodules which give plenty or rich functionality for scientific computing. You can check out the SciPy docs to learn about all of the functionality. Particularly useful modules that have come up in our work in systems biology include:\n\nscipy.special: Special functions.\nscipy.stats: Functions for statistical analysis.\nscipy.optimize: Numerical optimization.\nscipy.integrate: Numerical solutions to differential equations.\nscipy.interpolate: Smooth interpolation of functions.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#numpy-and-scipy-are-highly-optimized",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#numpy-and-scipy-are-highly-optimized",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.10 NumPy and SciPy are highly optimized",
    "text": "P.10 NumPy and SciPy are highly optimized\nImportantly, NumPy and SciPy routines are often fast. To understand why, we need to think a bit about how your computer actually runs code you write.\n\nP.10.1 Interpreted and compiled languages\nWe have touched on the fact that Python is an interpreted language. This means that the Python interpreter reads through your code, line by line, translates the commands into instructions that your computer’s processor can execute, and then these are executed. It also does garbage collection, which manages memory usage in your programs for you. As an interpreted language, code is often much easier to write, and development time is much shorter. It is often easier to debug. By contrast, with compiled languages (the dominant ones being Fortran, C, and C++), your entire source code is translated into machine code before you ever run it. When you execute your program, it is already in machine code. As a result, compiled code is often much faster than interpreted code. The speed difference depends largely on the task at hand, but there is often over a 100-fold difference.\nFirst, we’ll demonstrate the difference between compiled and interpreted languages by looking at a function to sum the elements of an array. Note that Python is dynamically typed, so the function below works for multiple data types, but the C function works only for double precision floating point numbers.\n\n# Python code to sum an array and print the result to the screen\nprint(sum(my_ar))\n\n[19793 20111 20171 19978]\n\n\n/* C code to sum an array and print the result to the screen */\n\n#include &lt;stdio.h&gt;\n\nvoid sum_array(double a[], int n);\n\nvoid sum_array(double a[], int n) {\n   int i; \n   double sum=0;\n   for (i = 0; i &lt; n; i++){\n       sum += a[i];\n   }\n   printf(\"%g\\n\", sum);\n}\nThe C code won’t even execute without another function called main to call it. You should notice the difference in complexity of the code. Interpreted code is very often much easier to write!\n\n\nP.10.2 NumPy and SciPy use compiled code!\nUnder the hood, when you call a NumPy or SciPy function, or use one of the methods, the Python interpreter passes the arrays into pre-compiled functions. (They are usually C or Fortran functions.) That means that you get to use an interpreted language with near-compiled speed! We can demonstrate the speed by comparing an explicit sum of elements of an array using a Python for loop versus NumPy. We will use the np.random module to generate a large array of random numbers (we will visit random number generation in a coming section). We then use the %timeit magic function of IPython to time the execution of the sum of the elements of the array.\n\n# Make array of 10,000 random numbers\nrng = np.random.default_rng()\nx = rng.random(10000)\n\n# Sum with Python for loop\ndef python_sum(x):\n    x_sum = 0.0\n    for y in x:\n        x_sum += y\n    return x_sum\n\n# Test speed\n%timeit python_sum(x)\n\n877 µs ± 9.01 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nNow we’ll do the same test with the NumPy implementation.\n\n%timeit np.sum(x)\n\n7.84 µs ± 99.1 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nWow! We went from a millisecond to microseconds!",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#word-of-advice-use-numpy-and-scipy",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#word-of-advice-use-numpy-and-scipy",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.11 Word of advice: use NumPy and SciPy",
    "text": "P.11 Word of advice: use NumPy and SciPy\nIf you are writing code and you think to yourself, “This seems like a pretty common things to do,” there is a good chance the someone really smart has written code to do it. If it’s something numerical, there is a good chance it is in NumPy or SciPy. Use these packages. Do not reinvent the wheel. It is very rare you can beat them for performance, error checking, etc.\nFurthermore, NumPy and SciPy are very well tested. In general, you do not need to write unit tests for well-established packages. Obviously, if you use NumPy or SciPy within your own functions, you still need to test what you wrote.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#computing-environment",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#computing-environment",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.12 Computing environment",
    "text": "P.12 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.10.9\nIPython version      : 8.10.0\n\nnumpy     : 1.23.5\nscipy     : 1.10.0\njupyterlab: 3.5.3",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  }
]