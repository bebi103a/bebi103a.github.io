[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BE/Bi 103 a 2025",
    "section": "",
    "text": "About the course\nModern biology is a quantitative science, and biological scientists need to be equipped with tools to analyze quantitative data. This course takes a hands-on approach to developing these tools. Together, we will analyze real data. We will learn how to organize, preserve, and share data sets, create informative interactive graphical displays of data, process images to extract actionable data, and perform basic resampling-based statistical inferences.\nImportantly, biological data is often “messy” and there is no one right way to perform an analysis or make a plot. As we work with data, we will discuss various approaches to get a feel for the art of biological data analysis.\nThe sequel to this course goes deeper into statistical modeling, mostly from a Bayesian perspective. This course is foundational for that and further studies in analysis of biological data.\nIf you are enrolled in the course, please read the Course policies. We will not go over them in detail in class, and it is your responsibility to understand them.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#useful-links",
    "href": "index.html#useful-links",
    "title": "BE/Bi 103 a 2025",
    "section": "Useful links",
    "text": "Useful links\n\nEd (used for course communications)\nCanvas (used for assignment submission/return)\nHomework solutions (password protected)",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#personnel",
    "href": "index.html#personnel",
    "title": "BE/Bi 103 a 2025",
    "section": "Personnel",
    "text": "Personnel\n\nCourse instructor\n\nJustin Bois\n\nTeaching assistants\n\nDillan Lau\nZach Martinez\nEmily Meehan\nMatthew Plazola",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "index.html#copyright-and-license",
    "href": "index.html#copyright-and-license",
    "title": "BE/Bi 103 a 2025",
    "section": "Copyright and License",
    "text": "Copyright and License\nCopyright 2025, Justin Bois.\nWith the exception of pasted graphics, where the source is noted, this work is licensed under a Creative Commons Attribution License CC BY-NC-SA 4.0. All code contained herein is licensed under an MIT license.",
    "crumbs": [
      "About the course"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule overview",
    "section": "",
    "text": "The schedule information on this page is subject to changes. All times are Pacific.\n\n\nLab\n\n\nSection 1: Mondays 9 am–noon, Chen 130\nSection 2: Mondays 1–4 pm, Chen 130\n\n\n\n\nLecture\n\n\nWednesdays 9–9:50 am, Chen 100\n\n\n\nInstructor office hours: Wednesdays 3–4 pm, Kerckhoff B123\nTA session: Thursdays 7–10 pm, Chen 130\n\n\n\nHomework due dates\n\nHomework 1: due 5 pm, October 3\nHomework 2: due 5 pm, October 10\nHomework 3: due 5 pm, October 17\nHomework 4: due 5 pm, October 24\nHomework 5: due 5 pm, October 31\nHomework 6: due 5 pm, November 7\nHomework 7: due 5 pm, November 14\nHomework 8: due 5 pm, November 21\nHomework 9: due 5 pm, December 5\nHomework 10: not graded\n\n\n\n\nExam dates\n\nMidterm: In class, November 3\nFinal: 9 am–noon, December 10\n\n\n\n\nLesson exercise due dates\n\nEDA lesson exercise: due noon, October 5\nProbability and sampling lesson exercise: due noon, October 12\nNonparametric stats lesson exercise: due noon, October 26\nNHST lesson exercise: due noon, November 9\nMLE lesson exercise: due noon, November 9\nVariate-covariate models lesson exercise: due noon, November 16\nModel assessment lesson exercise: due noon, November 23\n\n\n\n\nWeekly schedule\nThe notes for each Monday lesson must be read ahead of time and associated lesson exercises submitted by noon on the Sunday before the lesson. For example, the lesson exercises about exploratory data analysis must be submitted by noon on Sunday, October 5.\nIf one were reading through the lessons, the numbering of the lessons represents the most logical order. However, due to the constraints of class meeting times, some of the lessons are presented out of order. This is not a problem, though, as no lesson that strictly depends on another are presented out of order and the order shown in the schedule below is also a reasonable ordering of the lessons.\nWeek 0\n\nAppendix B: Configuring your computer\n\nWeek 1\n\nM 09/29: Course welcome and team set-up\nM 09/29: Lessons 2–9: Exploratory data analysis I\nW 10/01: Lesson 1: What are we doing? (lecture)\n\nWeek 2\n\nM 10/06: Lessons 10–16: Exploratory data analysis II\nW 10/08: Lessons 18 and 19: Introduction to probability (lecture)\n\nWeek 3\n\nM 10/13: Lessons 20–22: Random number generation and its uses\nW 10/15: Guest lecture by Tom Morrell: Good data storage and sharing practices\n\nWeek 4\n\nM 10/20: No reading\nW 10/22: Lessons 24–26: Plug-in estimates and confidence intervals (lecture)\n\nWeek 5\n\nM 10/27: Lessons ###: Nonparametric inference with hacker stats\nW 10/29: Lessons ###: Null hypothesis significance testing (lecture)\n\nWeek 6\n\nM 11/03: Midterm exam\nM 11/03: Lessons ###: NHST with hacker stats\nW 11/05: Lessons ###: Parametric inference (lecture)\n\nWeek 7\n\nM 11/10: Lessons ###: Numerical maximum likelihood estimation\nW 11/12: Lessons ###: Variate-covariate models (lecture)\n\nWeek 8\n\nM 11/17: Lessons ###: Confidence intervals of MLEs\nM 11/17: Lessons ###: Implementation of variate-covariate models\nW 11/19: Lessons ###: Model assessment and information criteria (lecture)\n\nWeek 9\n\nM 11/24: Lessons ###: Mixture models\nM 11/24: Lessons ###: Implementation of model assessment\nW 11/26: Lessons ###: Principal component analysis\n\nWeek 10\n\nM 12/01: No reading\nW 12/03: Lessons ###: Statistical watchouts (lecture)\n\nWeek 11\n\nW 12/10: Final exam, 9 am - noon",
    "crumbs": [
      "Course logistics",
      "Schedule overview"
    ]
  },
  {
    "objectID": "policies.html",
    "href": "policies.html",
    "title": "Course policies",
    "section": "",
    "text": "Meetings\nThese are tentative policies that may change before the start of the course.\nAll times are Pacific. Attendance at lectures and lab sessions is required. Weekly lectures are Wednesday mornings in Chen 100, 9-9:50 am. On Mondays, you may attend one of the two lab sessions in Chen 130: 9 am-noon or 1-4 pm. We encourage you to attend the lab session for which you registered.\nTA-led sessions are held 7–10 pm on Thursdays, also in Chen 130. For roughly the first thirty minutes, the TAs will highlight and/or clarify pertinent topics. The remainder of the time will be spent helping individual students.\nInstructor office hours are 3–4 pm on Wednesdays in Kerckhoff B123.\nThere will be no TA session nor instructor office hours the week of Thanksgiving.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#lab-sessions",
    "href": "policies.html#lab-sessions",
    "title": "Course policies",
    "section": "Lab sessions",
    "text": "Lab sessions\nThe lab sessions are spent working on the week’s homework, which typically include working with real data sets with the help of course staff and possibly in collaboration with classmates. You are expected to be working diligently during this time, and it is a golden opportunity to do so.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#submission-of-assignments",
    "href": "policies.html#submission-of-assignments",
    "title": "Course policies",
    "section": "Submission of assignments",
    "text": "Submission of assignments\nAll assignments are submitted (and graded assignments handed back) via Canvas. Lesson exercises are submitted as a single .ipynb file, and each homework problem is submitted separately. (See the Homework section below for more details.)",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#lessons-and-lesson-exercises",
    "href": "policies.html#lessons-and-lesson-exercises",
    "title": "Course policies",
    "section": "Lessons and lesson exercises",
    "text": "Lessons and lesson exercises\nPrior to each lab session, you must go through the lessons listed on the schedule page for the week. These will give you the requisite skills you need to work on the homework problems of the week. To verify completion of the lessons and to identify points of confusion ahead of time, you will submit a small exercise. The file name must be l##_lastname_firstname.ipynb, where ## is the number of the lesson exercise. Lesson exercises are due at noon Pacific time on the Sunday before the lab session.\nThe lesson exercises are not graded for correctness, but for thoughtfulness. A perfectly reasonable answer to a problem in a lesson exercise is, “I am really having trouble with this concept. Can you please go over it in class?”",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#homework",
    "href": "policies.html#homework",
    "title": "Course policies",
    "section": "Homework",
    "text": "Homework\nThere are weekly homework assignments. These consist almost entirely of working up real data sets.\n\nEach homework has a defined due date and time. For most homeworks, this is Friday at 5 pm Pacific time.\nEach homework problem must be submitted as a single Jupyter notebook with file name hw#.#_lastname_firstname.ipynb. All cells in the notebook must be pre-run in order prior to submission. (A tip: You should use Jupyter’s “Restart and Run All” option under the Run pulldown menu to make sure your notebook is in runnable, submittable form.) Additionally, any other files that are necessary for your homework should also be submitted. Note that these are all submitted as separate files, but the TAs will only add graded comments to the Jupyter notebook. There are two exceptions to this rule. Sometimes the notebook must also be submitted as an HTML file, meaning that you submit two files, hw#.#_lastname_firstname.ipynb and hw#.#_lastname_firstname.html, the latter of which is the notebook converted to HTML. This is done for problems that take a long time to run so the TAs do not need to necessarily rerun all cells of the notebook. Such problems will be clearly indicated in the problem statement. The other exception is problems where the instructions specifically say the problem may be submitted as a PDF, in which case a single PDF with file name hw#.#_lastname_firstname.pdf (or, if you prefer, a Jupyter notebook hw#.#_lastname_firstname.ipynb) is submitted.\nSome problems are marked as team problems. For these, all members of the team submit exactly the same notebook (except with different file names containing the name of the submitter), with the names of all team members at the top of the notebook.\nAll code you wrote to do your assignment must be included in the notebook. Code from imported packages that you did not write (e.g., modules distributed by the class instructors) need not be displayed in the notebook. We will run the code in your notebook; all code must run to get credit.\nSince we are running your code to check it, you must have the data sets be accessed in the standard way for the class. That is to say, the following code (or something similar that sets up the correct directory structure) must be at the top of each submitted notebook that uses a data set.\nimport os, sys\nif \"google.colab\" in sys.modules:\n    data_path = \"https://s3.amazonaws.com/bebi103.caltech.edu/data/\"\nelse:\n    data_path = \"../data/\"\nWhen accessing files within your notebooks, do it with something like this: filename = os.path.join(data_path, 'name_of_datafile.csv').\nAll of your results must be clearly explained and all graphics clearly presented and embedded in the Jupyter notebook.\nAny mathematics in your homework must render clearly and properly with MathJax. This essentially means that your equations must be written in correct LaTeX.\nWhere appropriate, you need to give detailed discussion of analysis choices you have made. As an example, you may choose to display data as a jittered strip plot as opposed to, say, a collection of ECDFs. You need to justify that choice.\nTo give a better guideline on how to construct your assignments (and this is good practice in general in your own workflows), you should follow these guidelines.\n\nEach code cell should do only one task or define only one, simple function.\nDo not have any adjacent code cells. Thus, you should have explanatory text describing what is happening in the code cells. This text should not just explain the code, but your reasoning behind why you are doing the calculation.\nShow all equations.\nUse Markdown headers to delineate sections of your notebook. In this class, this at least means using headers to delineate parts of the problem.\n\nBecause this is important to make your work clear, the TAs will deduct points if these rules are not followed.\nThere is seldom a single right way to analyze a set of data. You are encouraged to try different approaches to analysis. If you perform an analysis and find problems with it, clearly write what the problems are and how they came about. Even if your analysis does not completely work, but you demonstrate that you thought carefully about it and understand its difficulties, you will get nearly full credit.\nYou are expected to submit all assignments on time. Late homeworks are accepted up to three days after the due time, in which case 50% of credit is awarded. No assignments are accepted after three days. There are a few exceptions to this rule:\n\nYou have a total of two “grace days” you can use throughout the term. If you use a grace day, your homework may be submitted late without penalty. A grace day is spent for each 24 hours, or portion thereof, that a given homework is late. For example, if a homework is due at 5 pm on Friday, but you turn it in at 7 pm on Sunday, you spend both of your grace days, the first one being spent at 5 pm on Saturday, the second for the remaining two hours on Sunday.\nGrace days may not be applied to lesson exercises or exams. No late lesson exercises will be accepted.\nIf you have a CASS accommodation, you need to communicate it to the instructor within the first week of class. If your accommodation allows for extra time on coursework, you need to let the instructor know you will be exercising that accommodation at least 24 hours before the homework is due.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#exams",
    "href": "policies.html#exams",
    "title": "Course policies",
    "section": "Exams",
    "text": "Exams\nThere will be a midterm and a final exam. The exam format may change, but we expect each exam to consist of a closed, handwritten section, followed by a computational section. The midterm will be completed in class, and the final in class during a specified time during finals week. They will also be submitted via Canvas and have the same file naming and formatting guidelines as for homework, namely midterm_lastname_firstname.ipynb and final_lastname_firstname.ipynb.\nAs you work through the term, it will help to know that, as described below in the AI policies, no AI usage is allowed on the exams. This means also that you will need to be running JupyterLab locally on your machine through the browser (not VS Code or Google Colab) for the exam.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#grading",
    "href": "policies.html#grading",
    "title": "Course policies",
    "section": "Grading",
    "text": "Grading\nYour grade is determined as follows.\n\n5% lesson exercises\n25% homework\n30% midterm exam\n40% final exam\n\nDuring the lab sessions, you are expected to work alone, with your classmates, and/or with course instructors with your full attention. Each unexcused absence from a lab session, or not working on BE/Bi 103 a coursework during a lab session, will result in a 1% deduction from your final grade.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#collaboration-and-ai-policy-and-honor-code",
    "href": "policies.html#collaboration-and-ai-policy-and-honor-code",
    "title": "Course policies",
    "section": "Collaboration and AI policy and Honor Code",
    "text": "Collaboration and AI policy and Honor Code\nSome of the data we will use in this course is unpublished, generously given to us by researchers both from Caltech and from other institutions. They have given us their data in good faith that it will be used only in this class. It is therefore imperative that you do not disseminate data sets that I ask you not to distribute anywhere outside of this class.\nYou may work on your homework assignments with classmates, but each submission must be your own. You must indicate with whom you collaborated on the top of each problem. The exceptions are team problems; you and your teammate(s) submit exactly the same assignment. You may not collaborate on exams.\nYou must complete lesson exercises on your own without any use of large language models (LLMs) or other artificial intelligence (AI).\nYou may not consult solutions of homework or exam problems from previous editions of this course.\nYou are free to consult references, literature, websites, blogs, etc., outside of the materials presented in class. (The exceptions are places where homework problems are completely worked out, such as previous editions of this or other courses, or other students’ work.) If you do, you must properly cite the sources.\nYou may use LLMs or other AI tools to complete your homework. If you use an LLM, at the end of the notebook of your homework submission, you must include which LLM you used (including version number), your prompts, and the response of the AI in readable, nicely formatted text. That said, I strongly encourage you not to use AI at all. I am not trying to deny you an important tool; quite the contrary. In order to effectively use LLMs, you need to have a basal competence to be able to understand what the LLM gives you, and even what to ask of it. In this course, we are building that competence, so using LLMs can result in confusion and are best avoided.\nExams are to be completed on your own with allowed resources. As you plan your term, note that LLMs and other AI tools may not be used on exams. For the computational portion of the exams, there will be a set of internet domains you may use in addition to the course website, mostly consisting of package documentation. You may also use any materials you make yourself. Warning: If you rely heavily on LLMs to complete your homework, it will be very difficult for you to perform well on the exams.\nUse of disallowed resources or other Honor Code violations on either exam may result in failure of the course.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#excused-absences-and-extensions",
    "href": "policies.html#excused-absences-and-extensions",
    "title": "Course policies",
    "section": "Excused absences and extensions",
    "text": "Excused absences and extensions\nUnder very special circumstances, missed lab or lecture sessions will be excused and extensions given on the homework without costing grace days. They must be requested from the course instructor.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#course-communications",
    "href": "policies.html#course-communications",
    "title": "Course policies",
    "section": "Course communications",
    "text": "Course communications\nYou should use the course Ed page for all questions related to the course. Note that you may post private messages to course staff via Ed if you wish. If you need to communicate with course staff about a private matter, you should email the instructor.\nMost of our mass communication with you will be through Ed, so be sure to set your Ed account to give you email alerts if necessary.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "policies.html#ediquette",
    "href": "policies.html#ediquette",
    "title": "Course policies",
    "section": "“Ediquette”",
    "text": "“Ediquette”\nWhen posting on Ed, please follow these guidelines:\n\nIf you have a question about a coding bug make every attempt to provide a minimal example that demonstrates the problem. A minimal example strips out all other details beyond what is necessary to reproduce the problem or bug. Posting error messages without code is seldom helpful.\nWhen posting code, do not post screen shots. You can post code in Ed by selecting Code instead of Paragraph at the top left of the text entry window when you are making a post.\nIf you feel that posting a minimal example will result in showing too much of your answer to your classmates, you can post your question on Ed privately so that only the course staff can see it.\nWhile you are free to post anonymously to your classmates (course staff will always know who posts), we encourage you to post with your real name. This can spur discussions among students, which can be productive.\nCourse staff strives to answer questions quickly, but students should answer when they can.\n\nThis also spurs more conversation and results in faster answers to questions.",
    "crumbs": [
      "Course logistics",
      "Course policies"
    ]
  },
  {
    "objectID": "lessons/what_are_we_doing/what_are_we_doing.html",
    "href": "lessons/what_are_we_doing/what_are_we_doing.html",
    "title": "1  What are we doing?",
    "section": "",
    "text": "1.1 Data analysis pipelines\nIt is always good to start a course thinking about exactly what it is we are doing. Toward that end, we start with a question. What is the goal of doing (biological) experiments? There are many answers you may have for this. Some examples:\nMore obnoxious answers are\nThis question might be better addressed if we zoom out a bit and think about the scientific process as a whole. Below, we have a sketch of the scientific processes. This cycle repeats itself as we explore nature and learn more. In the boxes are milestones, and along the arrows in orange text are the tasks that get us to these milestones.\nWe start to the left with a hypothesis of how a biological system works, often expressed as a sketch, or cartoon, which is something we can have intuition about. Indeed, it is often a cartoon of how a system functions that we have in mind. From these cartoons, we can use deductive inference to move our model from a cartoon to a prediction. We then perform experiments to test our predictions, that is to assault our hypotheses, and the output of experiments is data. Once we have our data, we make inferences from them that enable us to update our hypothesis, thereby pushing human knowledge forward.\nLet’s consider the tasks and their milestones in a little more depth. We start in the lower left.\nLet’s consider the lower right arrow in the figure above, going from a data set to updated hypotheses and parameter estimates, the heart of this course. We zoom in on that arrow, there are several steps, which we will call a data analysis pipeline, depicted below.\nLet’s look at each step.\nIn this course, we will learn about all of these steps. We will spend roughly a third of the class on data validation and wrangling and exploratory data analysis, and the rest of the class on statistical inference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What are we doing?</span>"
    ]
  },
  {
    "objectID": "lessons/what_are_we_doing/what_are_we_doing.html#data-analysis-pipelines",
    "href": "lessons/what_are_we_doing/what_are_we_doing.html#data-analysis-pipelines",
    "title": "1  What are we doing?",
    "section": "",
    "text": "Figure 1.2: Data analysis pipeline. Each block represents a milestone along the pipeline and each arrow a task to bring you there.\n\n\n\n\n\nData validation. We always have assumptions about data coming from a data source into the pipeline. We have assumptions about file formats, data organization, etc. We also have assumptions about the data themselves. For example, fluorescence measurements must all be nonnegative. Before proceeding through the pipeline, we need to make sure the data comply with all of these assumptions, lest we have issues down the pipeline. This process is called data validation.\nData wrangling. This is the process of converting the format of the validated data from the source to a format that is easier to work with. This could involve restructuring tabular data or extracting useful information out of images, etc. There are countless examples.\nExploratory data analysis. Once data sets are tidy and easy to work with, we can start exploring them. Generally, this involves making informative graphics looking at the data set from different angles. Sometimes, this is sufficient to learn from the data, and we can proceed directly to updated hypotheses and publication, but we often also need to perform statistical inference, which is in the next two steps of the pipeline.\nParameter estimation. This is the practice of quantifying the observed effects in the data, and in particular quantifying the uncertainty in our estimates of the effect sizes. This falls under the umbrella of statistical inference.\nHypothesis testing. Also a technique of statistical inference, hypothesis testing involves evaluation of hypotheses about how the data were generated. This usually gives some quantitative measure about the hypotheses, but there are many issues with quantifying the “truth” of a hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What are we doing?</span>"
    ]
  },
  {
    "objectID": "lessons/eda/exploratory_data_analysis.html",
    "href": "lessons/eda/exploratory_data_analysis.html",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "Useful EDA advice from John Tukey\nIn 1977, John Tukey, one of the great statisticians and mathematicians of all time, published a book entitled Exploratory Data Analysis. In it, he laid out general principles on how researchers should handle their first encounters with their data, before formal statistical inference. Most of us spend a lot of time doing exploratory data analysis, or EDA, without really knowing it. Mostly, EDA involves a graphical exploration of a data set.\nWe start off with a few wise words from John Tukey himself.",
    "crumbs": [
      "Exploratory data analysis"
    ]
  },
  {
    "objectID": "lessons/eda/exploratory_data_analysis.html#useful-eda-advice-from-john-tukey",
    "href": "lessons/eda/exploratory_data_analysis.html#useful-eda-advice-from-john-tukey",
    "title": "Exploratory data analysis",
    "section": "",
    "text": "“Exploratory data analysis can never be the whole story, but nothing else can serve as a foundation stone—as the first step.”\n“In exploratory data analysis there can be no substitute for flexibility; for adapting what is calculated—and what we hope plotted—both to the needs of the situation and the clues that the data have already provided.”\n“There is no excuse for failing to plot and look.”\n“There is often no substitute for the detective’s microscope—or for the enlarging graphs.”\n“Graphs force us to note the unexpected; nothing could be more important.”\n“‘Exploratory data analysis’ is an attitude, a state of flexibility, a willingness to look for those things that we believe are not there, as well as those we believe to be there.”",
    "crumbs": [
      "Exploratory data analysis"
    ]
  },
  {
    "objectID": "lessons/eda/exploratory_data_analysis.html#the-tools-of-eda",
    "href": "lessons/eda/exploratory_data_analysis.html#the-tools-of-eda",
    "title": "Exploratory data analysis",
    "section": "The tools of EDA",
    "text": "The tools of EDA\nBeing able to load in a data set and quickly start exploring it graphically enables you to think about your data set instead being mired in the mechanics of producing a plot. In the notebooks that follow in this lesson, we will learn how to use the Python-based tools for EDA. In particular, we will learn how to use Polars to keep the data set organized and accessible and Bokeh to make interactive graphics.\nAlong the way, we will learn key concepts of data organization and display. Importantly, we will learn about tidy data, split-apply-combine, and how to plot all of your data.",
    "crumbs": [
      "Exploratory data analysis"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html",
    "href": "lessons/eda/polars/intro_to_polars.html",
    "title": "2  Introduction to data frames and Polars",
    "section": "",
    "text": "2.1 The data set\n| Download notebook\nData set download\nThroughout your research career, you will undoubtedly need to handle data, possibly lots of data. Once in a usable form, you are empowered to rapidly make graphics and perform statistical inference. Tidy data is an important format and we will discuss that in subsequent sections of this lesson. In an ideal world, data sets would be stored in tidy format and be ready to use. The data comes in lots of formats, and you may have to spend much of your time wrangling the data to get it into a usable format. Wrangling is the topic of the next lesson; for now all data sets will be in tidy format from the get-go.\nWe will explore using data frames with a real data set. We will use a data set published in Beattie, et al., Perceptual impairment in face identification with poor sleep, Royal Society Open Science, 3, 160321, 2016. In this paper, researchers used the Glasgow Facial Matching Test (GMFT) to investigate how sleep deprivation affects a subject’s ability to match faces, as well as the confidence the subject has in those matches. Briefly, the test works by having subjects look at a pair of faces. Two such pairs are shown below.\nThe top two pictures are the same person, the bottom two pictures are different people. For each pair of faces, the subject gets as much time as he or she needs and then says whether or not they are the same person. The subject then rates his or her confidence in the choice.\nIn this study, subjects also took surveys to determine properties about their sleep. The Sleep Condition Indicator (SCI) is a measure of insomnia disorder over the past month (scores of 16 and below indicate insomnia). The Pittsburgh Sleep Quality Index (PSQI) quantifies how well a subject sleeps in terms of interruptions, latency, etc. A higher score indicates poorer sleep. The Epworth Sleepiness Scale (ESS) assesses daytime drowsiness.\nThe data set can be downloaded here. The contents of this file were adapted from the Excel file posted on the public Dryad repository. (Note this: if you want other people to use and explore your data, make it publicly available.) I’ll say it more boldly.\nThe data file is a CSV file, where CSV stands for comma-separated value. This is a text file that is easily read into data structures in many programming languages. You should generally always store your data in such a format, not necessarily CSV, but a format that is open, has a well-defined specification, and is readable in many contexts. Excel files do not meet these criteria. Neither do .mat files. There are other good ways to store data, such as JSON, but we will almost exclusively use CSV files in this class.\nLet’s take a look at the CSV file. We will use the command line program head to look at the first 20 lines of the file.\nfname = os.path.join(data_path, \"gfmt_sleep.csv\")\n\n# This will not work in Colab because the file is not local\n!head {fname}\n\n﻿participant number,gender,age,correct hit percentage,correct reject percentage,percent correct,confidence when correct hit,confidence incorrect hit,confidence correct reject,confidence incorrect reject,confidence when correct,confidence when incorrect,sci,psqi,ess\n8,f,39,65,80,72.5,91,90,93,83.5,93,90,9,13,2\n16,m,42,90,90,90,75.5,55.5,70.5,50,75,50,4,11,7\n18,f,31,90,95,92.5,89.5,90,86,81,89,88,10,9,3\n22,f,35,100,75,87.5,89.5,*,71,80,88,80,13,8,20\n27,f,74,60,65,62.5,68.5,49,61,49,65,49,13,9,12\n28,f,61,80,20,50,71,63,31,72.5,64.5,70.5,15,14,2\n30,m,32,90,75,82.5,67,56.5,66,65,66,64,16,9,3\n33,m,62,45,90,67.5,54,37,65,81.5,62,61,14,9,9\n34,f,33,80,100,90,70.5,76.5,64.5,*,68,76.5,14,12,10\nThe first line contains the headers for each column. They are participant number, gender, age, etc. The data follow. There are two important things to note here. First, notice that the gender column has string data (m or f), while the rest of the data are numeric. Note also that there are some missing data, denoted by the *s in the file.\nGiven the file I/O skills you recently learned, you could write some functions to parse this file and extract the data you want. You can imagine that this might be kind of painful. However, if the file format is nice and clean, like we more or less have here, we can use pre-built tools to read in the data from the file and put it in a convenient data structure. Those structures are data frames.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#the-data-set",
    "href": "lessons/eda/polars/intro_to_polars.html#the-data-set",
    "title": "2  Introduction to data frames and Polars",
    "section": "",
    "text": "Figure 2.1: Two pairs of faces from the Glasgow Facial Matching Test (GFMT).\n\n\n\n\n\n\n\nIf at all possible, share your data freely.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#data-frames",
    "href": "lessons/eda/polars/intro_to_polars.html#data-frames",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.2 Data frames",
    "text": "2.2 Data frames\nThough we will use the word “data frame” over and over again, what a data frame is is actually a bit nebulous. Our working definition of a data frame is that it is a representation of two-dimensional tabular data where each column has a label. We will restrict ourselves to the case where each column has a specific data type (e.g., strings, ints, floats, or even lists).\nOne can think of a data frame as a collection of labeled columns, each one called a series. (A series may be thought of as a single column of data.) Alternatively, it is sometimes convenient to think of a data frame as a collection of rows, where each entry in the row is labeled with the column heading.\nFor more reading on the history of data frames and an attempt (in my opinion a very good attempt) at clearly defining them see section 4 of this paper by Petersohn, et al.\n\n2.2.1 Pandas\nPandas is one of the most widely used tools in the Python ecosystem for handling data. It is worth knowing about. We will, however, not be using Pandas, but instead will use Polars. I prefer Polars because its API is cleaner, in my opinion, but it has the added benefit of generally being much faster than Pandas.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#loading-the-data",
    "href": "lessons/eda/polars/intro_to_polars.html#loading-the-data",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.3 Loading the data",
    "text": "2.3 Loading the data\n\n2.3.1 Using polars.read_csv() to read in data\nWe have imported Polars with the alias pl as is customary. We will use pl.read_csv() to load the data set. The data are stored in a data frame (data type DataFrame), which is one of the data types that makes Polars so convenient for use in data analysis. Data frames offer mixed data types, including incomplete columns, and convenient slicing, among many, many other convenient features. We will use the data frame to look at the data, at the same time demonstrating some of the power of data frames. They are like spreadsheets, only a lot better.\n\ndf = pl.read_csv(fname, null_values=\"*\")\n\nNotice that we used the kwarg null_values=* to specify that entries marked with a * are missing. The resulting data frame is populated with null, wherever this character is present in the file. In this case, we want null_values='*'. So, let’s load in the data set.\nIf you check out the doc string for pl.read_csv(), you will see there are lots of options for reading in the data.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#exploring-the-dataframe",
    "href": "lessons/eda/polars/intro_to_polars.html#exploring-the-dataframe",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.4 Exploring the DataFrame",
    "text": "2.4 Exploring the DataFrame\nLet’s jump right in and look at the contents of the DataFrame. We can look at the first several rows using the df.head() method.\n\n# Look at the contents (first 5 rows)\ndf.head()\n\n\nshape: (5, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\n\n\n\n\n\n\nWe see that the column headings were automatically assigned, as have the data types of the columns, where i64, f64, and str respectively denote integers, floats and strings. Also note (in row 3) that the missing data are denoted as null.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#indexing-data-frames",
    "href": "lessons/eda/polars/intro_to_polars.html#indexing-data-frames",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.5 Indexing data frames",
    "text": "2.5 Indexing data frames\nThe data frame is a convenient data structure for many reasons that will become clear as we start exploring. Let’s start by looking at how data frames are indexed. The rows in Polars data frames are indexed by integers, starting with zero as usual for Python. So, the first row of the data frame may be accessed as follows.\n\ndf[0]\n\n\nshape: (1, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\n\n\n\n\n\n\nIn practice you will almost never use row indices, but rather use Boolean indexing, which is accomplished using Polars’s filter() method.\nBecause row indices in Polars data frames are always integers and column indices are not allowed to be integers (they must be strings), columns are accessed in the same way. If you choose to index with a string, Polars knows you are asking for a column.\n\ndf['percent correct']\n\n\nshape: (102,)\n\n\n\npercent correct\n\n\nf64\n\n\n\n\n72.5\n\n\n90.0\n\n\n92.5\n\n\n87.5\n\n\n62.5\n\n\n…\n\n\n77.5\n\n\n87.5\n\n\n75.0\n\n\n70.0\n\n\n62.5\n\n\n\n\n\n\nFor accessing a single column, I prefer the get_column() method.\n\ndf.get_column('percent correct')\n\n\nshape: (102,)\n\n\n\npercent correct\n\n\nf64\n\n\n\n\n72.5\n\n\n90.0\n\n\n92.5\n\n\n87.5\n\n\n62.5\n\n\n…\n\n\n77.5\n\n\n87.5\n\n\n75.0\n\n\n70.0\n\n\n62.5",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#boolean-indexing-of-data-frames",
    "href": "lessons/eda/polars/intro_to_polars.html#boolean-indexing-of-data-frames",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.6 Boolean indexing of data frames",
    "text": "2.6 Boolean indexing of data frames\nLet’s say I wanted the record for participant number 42. I can use Boolean indexing to specify the row. This is accomplished using the filter() method.\n\ndf.filter(pl.col(\"participant number\") == 42)\n\n\nshape: (1, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n42\n\"m\"\n29\n100\n70\n85.0\n75.0\nnull\n64.5\n43.0\n74.0\n43.0\n32\n1\n6\n\n\n\n\n\n\nThe argument of the filter() method is an expression, pl.col('participant number') == 42, which gives the rows (in this case, one row) for which the value of the 'participant number' column is 42.\nIf I just wanted the percent correct, I can first filter to get the row I want, then extract the 'percent correct' column, and then use the item() method to extract the scalar value.\n\n(\n    df\n    .filter(pl.col('participant number') == 42)\n    .get_column('percent correct')\n    .item()\n)\n\n85.0\n\n\nNote how I expressed this code snippet stylistically. I am doing method chaining, and having each method on its own line adds readability.\nNow, let’s pull out all records of females under the age of 21. We can again use Boolean indexing, but we need to use an & operator, taken to mean logical AND. We did not cover this bitwise operator before, but the syntax is self-explanatory in the example below. Note that it is important that each Boolean operation you are doing is in parentheses because of the precedence of the operators involved. The other bitwise operators you may wish to use for Boolean indexing in data frames are | for OR and ~ for NOT.\n\ndf.filter((pl.col('gender') == 'f') & (pl.col('age') &lt; 21))\n\n\nshape: (5, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n3\n\"f\"\n16\n70\n80\n75.0\n70.0\n57.0\n54.0\n53.0\n57.0\n54.5\n23\n1\n3\n\n\n5\n\"f\"\n18\n90\n100\n95.0\n76.5\n83.0\n80.0\nnull\n80.0\n83.0\n21\n7\n5\n\n\n58\n\"f\"\n16\n85\n85\n85.0\n55.0\n30.0\n50.0\n40.0\n52.5\n35.0\n29\n2\n11\n\n\n72\n\"f\"\n18\n80\n75\n77.5\n67.5\n51.5\n66.0\n57.0\n67.0\n53.0\n29\n4\n6\n\n\n85\n\"f\"\n18\n85\n85\n85.0\n93.0\n92.0\n91.0\n89.0\n91.5\n91.0\n25\n4\n21\n\n\n\n\n\n\nWe can do something even more complicated, like pull out all females under 30 who got more than 85% of the face matching tasks correct.\n\ndf.filter(\n      (pl.col('gender') == 'f') \n    & (pl.col('age') &lt; 30) \n    & (pl.col('percent correct') &gt; 85.0)\n)\n\n\nshape: (8, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n93\n\"f\"\n28\n100\n75\n87.5\n89.5\nnull\n67.0\n60.0\n80.0\n60.0\n16\n7\n4\n\n\n5\n\"f\"\n18\n90\n100\n95.0\n76.5\n83.0\n80.0\nnull\n80.0\n83.0\n21\n7\n5\n\n\n6\n\"f\"\n28\n95\n80\n87.5\n100.0\n85.0\n94.0\n61.0\n99.0\n65.0\n19\n7\n12\n\n\n10\n\"f\"\n25\n100\n100\n100.0\n90.0\nnull\n85.0\nnull\n90.0\nnull\n17\n10\n11\n\n\n44\n\"f\"\n21\n85\n90\n87.5\n66.0\n29.0\n70.0\n29.0\n67.0\n29.0\n26\n7\n18\n\n\n48\n\"f\"\n23\n90\n85\n87.5\n67.0\n47.0\n69.0\n40.0\n67.0\n40.0\n18\n6\n8\n\n\n51\n\"f\"\n24\n85\n95\n90.0\n97.0\n41.0\n74.0\n73.0\n83.0\n55.5\n29\n1\n7\n\n\n67\n\"f\"\n25\n100\n100\n100.0\n61.5\nnull\n58.5\nnull\n60.5\nnull\n28\n8\n9\n\n\n\n\n\n\nOf interest in this exercise in Boolean indexing is that we never had to write a loop. To produce our indices, we could have done the following.\n\n# Initialize array of Boolean indices\ninds = []\n\n# Iterate over the rows of the DataFrame to check if the row should be included\nfor row in df.iter_rows(named=True):\n    inds.append(    \n            row[\"age\"] &lt; 30 \n        and row[\"gender\"] == \"f\" \n        and row[\"percent correct\"] &gt; 85\n    )\n\n# Look at inds\nprint(inds)\n\n[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n\n\nIf we wanted, we could use this inds list of Trues and Falses to filter our values.\n\ndf.filter(inds)\n\n\nshape: (8, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n93\n\"f\"\n28\n100\n75\n87.5\n89.5\nnull\n67.0\n60.0\n80.0\n60.0\n16\n7\n4\n\n\n5\n\"f\"\n18\n90\n100\n95.0\n76.5\n83.0\n80.0\nnull\n80.0\n83.0\n21\n7\n5\n\n\n6\n\"f\"\n28\n95\n80\n87.5\n100.0\n85.0\n94.0\n61.0\n99.0\n65.0\n19\n7\n12\n\n\n10\n\"f\"\n25\n100\n100\n100.0\n90.0\nnull\n85.0\nnull\n90.0\nnull\n17\n10\n11\n\n\n44\n\"f\"\n21\n85\n90\n87.5\n66.0\n29.0\n70.0\n29.0\n67.0\n29.0\n26\n7\n18\n\n\n48\n\"f\"\n23\n90\n85\n87.5\n67.0\n47.0\n69.0\n40.0\n67.0\n40.0\n18\n6\n8\n\n\n51\n\"f\"\n24\n85\n95\n90.0\n97.0\n41.0\n74.0\n73.0\n83.0\n55.5\n29\n1\n7\n\n\n67\n\"f\"\n25\n100\n100\n100.0\n61.5\nnull\n58.5\nnull\n60.5\nnull\n28\n8\n9\n\n\n\n\n\n\nThis feature, where the looping is done automatically on Polars objects like data frames, is very powerful and saves us writing lots of lines of code. This example also showed how to use the iter_rows() method of a data frame. It is actually rare that you will need to do that, and you should generally avoid it, since it is also slow.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#contexts-and-expressions",
    "href": "lessons/eda/polars/intro_to_polars.html#contexts-and-expressions",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.7 Contexts and expressions",
    "text": "2.7 Contexts and expressions\nWe will now be a bit more formal in discussing how to work with Polars data frames. Specifically, Polars features the concepts of expressions and contexts.\n\n2.7.1 The filter context\nAs an example, let us consider our above task of filtering the data frame to extract females under the age of 21. The syntax was\ndf.filter((pl.col('gender') == 'f') & (pl.col('age') &lt; 21))\nConsider first pl.col('gender') == 'f'. This is an example of an expression. An expression consists of a calculation or transformation that can be applied to a series and returns a series. In this case, we are taking a column called 'gender' and we are evaluating whether each element in that column is 'f'. Indeed, if we ask the Python interpreter to tell us the type of the above expression, it is a Polars Expr.\n\ntype(pl.col('gender') == 'f')\n\npolars.expr.expr.Expr\n\n\nSimilarly, pl.col('age') &lt; 21 is also an expression, as is the result when we apply the & bitwise operator.\n\ntype((pl.col('gender') == 'f') & (pl.col('age') &lt; 21))\n\npolars.expr.expr.Expr\n\n\nSo, an expression says what we want to do to data. But now we ask, in what way, i.e., in what context, do we want to use the result of the expression? One way we may wish to use the above expression is to filter the rows in a data frame. The filter context is established by df.filter(). The argument of df.filter() is an expression (or expressions) that evaluate to Booleans. That is how we got our result; in the context of filtering, the expression is evaluated and only entries where the expression gives True are retained.\n\ndf.filter((pl.col('gender') == 'f') & (pl.col('age') &lt; 21))\n\n\nshape: (5, 15)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n3\n\"f\"\n16\n70\n80\n75.0\n70.0\n57.0\n54.0\n53.0\n57.0\n54.5\n23\n1\n3\n\n\n5\n\"f\"\n18\n90\n100\n95.0\n76.5\n83.0\n80.0\nnull\n80.0\n83.0\n21\n7\n5\n\n\n58\n\"f\"\n16\n85\n85\n85.0\n55.0\n30.0\n50.0\n40.0\n52.5\n35.0\n29\n2\n11\n\n\n72\n\"f\"\n18\n80\n75\n77.5\n67.5\n51.5\n66.0\n57.0\n67.0\n53.0\n29\n4\n6\n\n\n85\n\"f\"\n18\n85\n85\n85.0\n93.0\n92.0\n91.0\n89.0\n91.5\n91.0\n25\n4\n21\n\n\n\n\n\n\n\n\n2.7.2 The selection context\nThe simplest way we can use an expression is simply to evaluate the expression and give its result as a new data frame. This is the selection context, in which we get the output of the expression. It can be invoked with df.select().\n\ndf.select((pl.col('gender') == 'f') & (pl.col('age') &lt; 21))\n\n\nshape: (102, 1)\n\n\n\ngender\n\n\nbool\n\n\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n…\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n\n\n\n\nNote that this is a data frame and not a series; it is a data frame containing one column. In this case, the column is named after the first column used in our Boolean expression. We can adjust the column label by applying the alias() method, which does a renaming transformation.\n\nf_under_21 = (pl.col('gender') == 'f') & (pl.col('age') &lt; 21)\n\ndf.select(f_under_21.alias('female under 21'))\n\n\nshape: (102, 1)\n\n\n\nfemale under 21\n\n\nbool\n\n\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n…\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n\n\n\n\nIf we wanted a series instead of a new data frame, we can apply the get_column() method to the data frame returned by df.select().\n\n# Result of expression as a series\ndf.select(f_under_21.alias('female under 21')).get_column('female under 21')\n\n\nshape: (102,)\n\n\n\nfemale under 21\n\n\nbool\n\n\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n…\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\nfalse\n\n\n\n\n\n\nWe can also select with multiple expressions. For example, let’s say we additionally wanted to compute the ratio of confidence when correct to confidence when incorrect. First, we can make an expression for that.\n\nconf_ratio = pl.col('confidence when correct') / pl.col('confidence when incorrect')\n\nNow, we can select that as well as the 'female under 21' column.\n\ndf.select(\n    f_under_21.alias('female under 21'),\n    conf_ratio.alias('confidence ratio')\n)\n\n\nshape: (102, 2)\n\n\n\nfemale under 21\nconfidence ratio\n\n\nbool\nf64\n\n\n\n\nfalse\n1.033333\n\n\nfalse\n1.5\n\n\nfalse\n1.011364\n\n\nfalse\n1.1\n\n\nfalse\n1.326531\n\n\n…\n…\n\n\nfalse\n1.040541\n\n\nfalse\n0.925\n\n\nfalse\n0.802469\n\n\nfalse\n1.588235\n\n\nfalse\n1.109589\n\n\n\n\n\n\nNotice that df.select() returns a new data frame containing only the columns that are given by the expressions and the original data frame is discarded. If we want the results of the expressions to instead be added to the data frame (keeping all of its original columns), we use the df.with_columns() method. This is still a selection context; the output is just different, comprising of the original data frame with added columns. (In the output of the cell below, you will find the columns added to the far right of the data frame.)\n\ndf.with_columns(\n    f_under_21.alias('female under 21'),\n    conf_ratio.alias('confidence ratio')\n)\n\n\nshape: (102, 17)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\nfemale under 21\nconfidence ratio\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\nf64\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\nfalse\n1.033333\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\nfalse\n1.5\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\nfalse\n1.011364\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\nfalse\n1.1\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\nfalse\n1.326531\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n97\n\"f\"\n23\n70\n85\n77.5\n77.0\n66.5\n77.0\n77.5\n77.0\n74.0\n20\n8\n10\nfalse\n1.040541\n\n\n98\n\"f\"\n70\n90\n85\n87.5\n65.5\n85.5\n87.0\n80.0\n74.0\n80.0\n19\n8\n7\nfalse\n0.925\n\n\n99\n\"f\"\n24\n70\n80\n75.0\n61.5\n81.0\n70.0\n61.0\n65.0\n81.0\n31\n2\n15\nfalse\n0.802469\n\n\n102\n\"f\"\n40\n75\n65\n70.0\n53.0\n37.0\n84.0\n52.0\n81.0\n51.0\n22\n4\n7\nfalse\n1.588235\n\n\n103\n\"f\"\n33\n85\n40\n62.5\n80.0\n27.0\n31.0\n82.5\n81.0\n73.0\n24\n5\n7\nfalse\n1.109589\n\n\n\n\n\n\nFinally, we will do something we’ll want to use going forward. Recall that a subject is said to suffer from insomnia if he or she has an SCI of 16 or below. We might like to add a column to the data frame that specifies whether or not the subject suffers from insomnia, which we do in the code cell below. Note that until now, we have not updated our data frame. To actually update the data frame, we need an assignment operation, df = ....\n\n# Add a column with `True` for insomnia\ndf = df.with_columns((pl.col('sci') &lt;= 16).alias('insomnia'))\n\n# Take a look (.head() gives first five rows)\ndf.head()\n\n\nshape: (5, 16)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\ninsomnia\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\ntrue\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\ntrue\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\ntrue\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\ntrue\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\ntrue",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#polars-selectors",
    "href": "lessons/eda/polars/intro_to_polars.html#polars-selectors",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.8 Polars selectors",
    "text": "2.8 Polars selectors\nWe have seen that we can choose which columns we want to work with in expressions using pl.col(). Thus far, we have used a single string as an argument, but pl.col() is more capable than that. For example, to select three columns of interest, we can do the following.\n\ndf.select(pl.col('age', 'gender', 'percent correct')).head()\n\n\nshape: (5, 3)\n\n\n\nage\ngender\npercent correct\n\n\ni64\nstr\nf64\n\n\n\n\n39\n\"f\"\n72.5\n\n\n42\n\"m\"\n90.0\n\n\n31\n\"f\"\n92.5\n\n\n35\n\"f\"\n87.5\n\n\n74\n\"f\"\n62.5\n\n\n\n\n\n\nWe can also pass regular expressions into pl.col(). If we want all columns, for example, we can use pl.col('*'). To get all columns containing the string 'confidence', we can use pl.col('^.*confidence.*$').\nPersonally, I always struggle with regular expressions. Fortunately, Polars has powerful selectors which help specify which columns are of interest. In addition to facilitating selection based on the names of the columns, selectors allow selection based on the data type of the column as well (actually, so does pl.col(), but it is simplified with selectors).\nWe have to import the selectors separately, which we have done in the top cell of this notebook via import polars.selectors as cs. The cs alias is suggested by the Polars developers.\nAs an example, we can select all columns that have a column heading containing the string 'confidence'.\n\ndf.select(cs.contains('confidence')).head()\n\n\nshape: (5, 6)\n\n\n\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\n\n\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n\n\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n\n\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n\n\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n\n\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n\n\n\n\n\n\nOr, perhaps we want to exclude all columns that are not indicators of performance on a test (participant number, age, and gender). We can specifically exclude columns with cs.exclude().\n\ndf.select(cs.exclude('gender', 'age', 'participant number')).head()\n\n\nshape: (5, 13)\n\n\n\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\ninsomnia\n\n\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\n\n\n\n\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\ntrue\n\n\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\ntrue\n\n\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\ntrue\n\n\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\ntrue\n\n\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\ntrue\n\n\n\n\n\n\nWhat if we want columns with only a float data type?\n\ndf.select(cs.float()).head()\n\n\nshape: (5, 7)\n\n\n\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\n\n\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n\n\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n\n\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n\n\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n\n\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n\n\n\n\n\n\nNote that the sleep measures were omitted because they are integer data types. We could select everything that is numeric if we want to include those.\n\ndf.select(cs.numeric()).head()\n\n\nshape: (5, 14)\n\n\n\nparticipant number\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n8\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\n\n\n16\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\n\n\n18\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\n\n\n22\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\n\n\n27\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\n\n\n\n\n\n\nUnfortunately, this still gave us participant number and age again. We could exclude those explicitly by chaining methods.\n\ndf.select(cs.numeric().exclude('age', 'participant number')).head()\n\n\nshape: (5, 12)\n\n\n\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\n\n\n\n\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\n\n\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\n\n\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\n\n\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\n\n\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\n\n\n\n\n\n\nSelectors also allow for set algebra. As a (contrived) example, let say we want all columns that are not of string data type that have spaces in the column heading and that we also want to exclude the participant number.\n\ndf.select((~cs.string() & cs.contains(' ')).exclude('participant number')).head()\n\n\nshape: (5, 9)\n\n\n\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\n\n\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n\n\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n\n\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n\n\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n\n\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n\n\n\n\n\n\nNotice that we used the complement operator ~ and the intersection operator &. Selectors also support the union operator | and the difference operator -.\nTo close our discussion on selectors, we note that selectors are their own data type:\n\ntype(cs.string())\n\npolars.selectors._selector_proxy_\n\n\nSelectors can be converted to expressions so you can continue computing with the as_expr() method.\n\ntype(cs.string().as_expr())\n\npolars.expr.expr.Expr",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#computing-summary-statistics",
    "href": "lessons/eda/polars/intro_to_polars.html#computing-summary-statistics",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.9 Computing summary statistics",
    "text": "2.9 Computing summary statistics\nTo demonstrate a use of what we have learned so far, we can compute the mean percent correct for insomniacs and normal sleepers. We can filter the data frame according to the insomnia column, select the percent correct column and compute the mean.\nTo put them together in a new data frame, we make a dictionary and convert it to a data frame, which is one of the ways to make a Polars data frame. E.g.,\n\npl.DataFrame(dict(a=[1,2,3], b=[4.5, 5.5, 6.5], c=['one', 'two', 'three']))\n\n\nshape: (3, 3)\n\n\n\na\nb\nc\n\n\ni64\nf64\nstr\n\n\n\n\n1\n4.5\n\"one\"\n\n\n2\n5.5\n\"two\"\n\n\n3\n6.5\n\"three\"\n\n\n\n\n\n\nNow to make our data frame of means for insomniacs and normal sleepers.\n\npl.DataFrame(\n    {\n        'insomniacs': df.filter(pl.col('insomnia')).get_column('percent correct').mean(),\n        'normal sleepers': df.filter(~pl.col('insomnia')).get_column('percent correct').mean()\n    }\n)\n\n\nshape: (1, 2)\n\n\n\ninsomniacs\nnormal sleepers\n\n\nf64\nf64\n\n\n\n\n76.1\n81.461039\n\n\n\n\n\n\nNotice that I used the ~ operator, which is a bit switcher, to get the normal sleepers from the insomnia column. It changes all Trues to Falses and vice versa. In this case, it functions like NOT.\nIt appears as though normal sleepers score better than insomniacs. We will learn techniques to more quantitatively assess that claim when we learn about statistical inference.\nAs we will soon see, what we have done is a split-apply-combine operation, for which there are more elegant and efficient methods using Polars. You will probably never use the approach in the above code cell again.\nWe will do a lot more computing with Polars data frames as the course goes on. For a nifty demonstration demonstration in this lesson, we can quickly compute summary statistics about each column of a data frame using its describe() method.\n\ndf.describe()\n\n\nshape: (9, 17)\n\n\n\nstatistic\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\ninsomnia\n\n\nstr\nf64\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"count\"\n102.0\n\"102\"\n102.0\n102.0\n102.0\n102.0\n102.0\n84.0\n102.0\n93.0\n102.0\n99.0\n102.0\n102.0\n102.0\n102.0\n\n\n\"null_count\"\n0.0\n\"0\"\n0.0\n0.0\n0.0\n0.0\n0.0\n18.0\n0.0\n9.0\n0.0\n3.0\n0.0\n0.0\n0.0\n0.0\n\n\n\"mean\"\n52.04902\nnull\n37.921569\n83.088235\n77.205882\n80.147059\n74.990196\n58.565476\n71.137255\n61.22043\n74.642157\n61.979798\n22.245098\n5.27451\n7.294118\n0.245098\n\n\n\"std\"\n30.020909\nnull\n14.02945\n15.09121\n17.569854\n12.047881\n14.165916\n19.560653\n14.987479\n17.671283\n13.619725\n15.92167\n7.547128\n3.404007\n4.426715\nnull\n\n\n\"min\"\n1.0\n\"f\"\n16.0\n35.0\n20.0\n40.0\n29.5\n7.0\n19.0\n17.0\n24.0\n24.5\n0.0\n0.0\n0.0\n0.0\n\n\n\"25%\"\n26.0\nnull\n26.0\n75.0\n70.0\n72.5\n66.0\n47.0\n64.5\n50.0\n66.0\n51.0\n17.0\n3.0\n4.0\nnull\n\n\n\"50%\"\n53.0\nnull\n37.0\n90.0\n80.0\n85.0\n75.0\n56.5\n71.5\n61.0\n76.0\n61.5\n24.0\n5.0\n7.0\nnull\n\n\n\"75%\"\n78.0\nnull\n45.0\n95.0\n90.0\n87.5\n87.0\n73.0\n80.0\n74.0\n82.5\n73.0\n29.0\n7.0\n10.0\nnull\n\n\n\"max\"\n103.0\n\"m\"\n74.0\n100.0\n100.0\n100.0\n100.0\n92.0\n100.0\n100.0\n100.0\n100.0\n32.0\n15.0\n21.0\n1.0\n\n\n\n\n\n\nThis gives us a data frame with summary statistics.\n\n2.9.1 Outputting a new CSV file\nNow that we added the insomniac column, we might like to save our data frame as a new CSV that we can reload later. We use df.write_csv() for this.\n\ndf.write_csv(\"gfmt_sleep_with_insomnia.csv\")\n\nLet’s take a look at what this file looks like.\n\n!head gfmt_sleep_with_insomnia.csv\n\nparticipant number,gender,age,correct hit percentage,correct reject percentage,percent correct,confidence when correct hit,confidence incorrect hit,confidence correct reject,confidence incorrect reject,confidence when correct,confidence when incorrect,sci,psqi,ess,insomnia\n8,f,39,65,80,72.5,91.0,90.0,93.0,83.5,93.0,90.0,9,13,2,true\n16,m,42,90,90,90.0,75.5,55.5,70.5,50.0,75.0,50.0,4,11,7,true\n18,f,31,90,95,92.5,89.5,90.0,86.0,81.0,89.0,88.0,10,9,3,true\n22,f,35,100,75,87.5,89.5,,71.0,80.0,88.0,80.0,13,8,20,true\n27,f,74,60,65,62.5,68.5,49.0,61.0,49.0,65.0,49.0,13,9,12,true\n28,f,61,80,20,50.0,71.0,63.0,31.0,72.5,64.5,70.5,15,14,2,true\n30,m,32,90,75,82.5,67.0,56.5,66.0,65.0,66.0,64.0,16,9,3,true\n33,m,62,45,90,67.5,54.0,37.0,65.0,81.5,62.0,61.0,14,9,9,true\n34,f,33,80,100,90.0,70.5,76.5,64.5,,68.0,76.5,14,12,10,true\n\n\nVery nice. Notice that by default Polars leaves an empty field for null values, and we do not need the null_values kwarg when we load in this CSV file.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#renaming-columns",
    "href": "lessons/eda/polars/intro_to_polars.html#renaming-columns",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.10 Renaming columns",
    "text": "2.10 Renaming columns\nYou may be annoyed with the rather lengthy syntax of access column names and wish to change them. Actually, you probably do not want to do this. Explicit is better than implicit! And furthermore, high level plotting libraries, as we will soon see, often automatically use column names for axis labels. So, let’s instead lengthen a column name. Say we keep forgetting what ESS stands for an want to rename the ess column to “Epworth Sleepiness Scale.”\nData frames have a nice rename method to do this. To rename the columns, we provide a dictionary where the keys are current column names and the corresponding values are the names we which to check them to. While we are at it, we will choose descriptive names for all three of the sleep quality indices.\n\n# Make a dictionary to rename columns\nrename_dict = {\n    \"ess\": \"Epworth Sleepiness Scale\",\n    \"sci\": \"Sleep Condition Indicator\",\n    \"psqi\": \"Pittsburgh Sleep Quality Index\",\n}\n\n# Rename the columns\ndf = df.rename(rename_dict)\n\ndf.head()\n\n\nshape: (5, 16)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nSleep Condition Indicator\nPittsburgh Sleep Quality Index\nEpworth Sleepiness Scale\ninsomnia\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\ntrue\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\ntrue\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\ntrue\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\ntrue\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\ntrue",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#a-note-on-indexing-and-speed",
    "href": "lessons/eda/polars/intro_to_polars.html#a-note-on-indexing-and-speed",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.11 A note on indexing and speed",
    "text": "2.11 A note on indexing and speed\nAs we have seen Boolean indexing is very convenient and fits nicely into a logical framework which allows us to extract data according to criteria we want. The trade-off is speed. Slicing by Boolean indexing is essentially doing a reverse lookup in a dictionary. We have to loop through all values to find keys that match. This is much slower than directly indexing. Compare the difference in speed for indexing the percent correct by participant number 42 by Boolean indexing versus direct indexing (it’s row 54).\n\nprint(\"Boolean indexing:\")\n%timeit df.filter(pl.col('participant number') == 42)['percent correct'].item()\n\nprint(\"\\nDirect indexing:\")\n%timeit df[54, 'percent correct']\n\nBoolean indexing:\n100 μs ± 471 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\nDirect indexing:\n533 ns ± 5.07 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThe speed difference is stark, differing by two orders of magnitude. For larger data sets, or for analyses that require repeated indexing, this speed consideration may be important. However, Polars does optimization that takes full advantage of parallelization that will accelerate Boolean indexing.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/intro_to_polars.html#computing-environment",
    "href": "lessons/eda/polars/intro_to_polars.html#computing-environment",
    "title": "2  Introduction to data frames and Polars",
    "section": "2.12 Computing environment",
    "text": "2.12 Computing environment\n\n\nCode\n%load_ext watermark\n%watermark -v -p numpy,polars,jupyterlab\n\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\npolars    : 1.27.1\njupyterlab: 4.3.6",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to data frames and Polars</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html",
    "href": "lessons/eda/polars/tidy_data.html",
    "title": "3  Tidy data and split-apply-combine",
    "section": "",
    "text": "3.1 Tidy data\n| Download notebook\nData set download\nWe have dipped our toe into Polars to see its power. In this lesson, we will continue to harness the power of Polars to pull out subsets of data we are interested in, and of vital importance, will introduce the concept of tidy data. I suspect this will be a demarcation in your life. You will have the times in your life before tidy data and after. Welcome to your bright tomorrow.\nHadley Wickham wrote a great article in favor of “tidy data.” Tidy data frames follow the rules:\nThis is less pretty to visualize as a table, but we rarely look at data in tables. Indeed, the representation of data which is convenient for visualization is different from that which is convenient for analysis. A tidy data frame is almost always much easier to work with than non-tidy formats.\nYou may raise some objections about tidy data. Here are a few, and my responses.\nObjection: Looking at a table of tidy data is ugly. It is not intuitively organized. I would almost never display a tidy data table in a publication.\nResponse: Correct! Having tabular data in a format that is easy to read as a human studying a table is a very different thing than having it in a format that is easy to explore and work with using a computer. As Daniel Chen put it, “There are data formats that are better for reporting and data formats that are better for analysis.” We are using the tidy data frames for analysis, not reporting (though we will see in the coming lessons that having the data in a tidy format makes making plots much easier, and plots are a key medium for reporting.)\nObjection: Isn’t it better to sometimes have data arranged in other ways? Say in a matrix?\nResponse: This is certainly true for things like images, or raster-style data in general. It makes more sense to organize an image in a 2D matrix than to have it organized as a data frame with three columns (row in image, column in image, intensity of pixel), where each row corresponds to a single pixel. For an image, indexing it by row and column is always unambiguous, my_image[i, j] means the pixel at row i and column j.\nFor other data, though, the matrix layout suffers from the fact that there may be more than one way to construct a matrix. If you know a data frame is tidy, you already know its structure. You need only to ask what the columns are, and then you immediately know how to access data using Boolean indexing. In other formats, you might have to read and write extensive comments to understand the structure of the data. Of course, you can read and write comments, but it opens the door for the possibility of misinterpretation or mistakes.\nObjection: But what about time series? Clearly, that can be in matrix format. One column is time, and then subsequent columns are observations made at that time.\nResponse: Yes, that is true. But then the matrix-style described could be considered tidy, since each row is a single observation (time point) that has many facets.\nObjection: Isn’t this an inefficient use of memory? There tend to be lots of repeated entries in tidy data frames.\nResponse: Yes, there are more efficient ways of storing and accessing data. But for data sets that are not “big data,” this is seldom a real issue. The extra expense in memory, as well as the extra expense in access, are small prices to pay for the simplicity and speed of the human user in accessing the data.\nObjection: Once it’s tidy, we pretty much have to use Boolean indexing to get what we want, and that can be slower than other methods of accessing data. What about performance?\nResponse: See the previous response. Speed of access really only becomes a problem with big, high-throughput data sets. In those cases, there are often many things you need to be clever about beyond organization of your data.\nConclusion: I really think that tidying a data set allows for fluid exploration. We will focus on tidy data sets going forward. Bringing untidy data into tidy format can be an annoying challenge that often involves lots of file I/O and text parsing operations. As such, it is wise to arrange your data in tidy format starting at acquisition. If your instrumentation prevents you from doing so, you should develop functions and scripts to parse them and convert them into tidy format.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#tidy-data",
    "href": "lessons/eda/polars/tidy_data.html#tidy-data",
    "title": "3  Tidy data and split-apply-combine",
    "section": "",
    "text": "Each variable is a column.\nEach observation is a row.\nEach type of observation has its own separate data frame.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#the-data-set",
    "href": "lessons/eda/polars/tidy_data.html#the-data-set",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.2 The data set",
    "text": "3.2 The data set\nWe will again use the data set from the Beattie, et al. paper on facial matching under sleep deprivation. Let’s load in the original data set and add the column on insomnia as we did in previous part of this lesson.\n\nfname = os.path.join(data_path, \"gfmt_sleep.csv\")\ndf = pl.read_csv(fname, null_values=\"*\")\ndf = df.with_columns((pl.col('sci') &lt;= 16).alias(\"insomnia\"))\n\n# Take a look\ndf.head()\n\n\nshape: (5, 16)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\ninsomnia\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\ntrue\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\ntrue\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\ntrue\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\ntrue\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\ntrue\n\n\n\n\n\n\nThis data set is in tidy format. Each row represents a single test on a single participant. The aspects of that person’s test are given in each column. We already saw the power of having the data in this format when we did Boolean indexing. Now, we will see how this format allows use to easily do an operation we do again and again with data sets, split-apply-combine.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#split-apply-combine",
    "href": "lessons/eda/polars/tidy_data.html#split-apply-combine",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.3 Split-apply-combine",
    "text": "3.3 Split-apply-combine\nLet’s say we want to compute the median percent correct face matchings for subjects with insomnia and the median percent correct face matchings for those without. Ignoring for the second the mechanics of how we would do this with Python, let’s think about it in English. What do we need to do?\n\nSplit the data set up according to the 'insomnia' field, i.e., split it up so we have a separate data set for the two classes of subjects, those with insomnia and those without.\nApply a median function to the activity in these split data sets.\nCombine the results of these averages on the split data set into a new, summary data set that contains the two classes (insomniac and not) and means for each.\n\nWe see that the strategy we want is a split-apply-combine strategy. This idea was put forward by Hadley Wickham in this paper. It turns out that this is a strategy we want to use very often. Split the data in terms of some criterion. Apply some function to the split-up data. Combine the results into a new data frame.\nNote that if the data are tidy, this procedure makes a lot of sense. Choose the column or columns you want to use to split by. All rows with like entries in the splitting column(s) are then grouped into a new data set. You can then apply any function you want into these new data sets. You can then combine the results into a new data frame.\nPolars’s split-apply-combine operations are achieved in a group by/aggregation context, achieved with df.group_by().agg(). You can think of group_by() as the splitting part. The apply-combine part is done by passing expressions into agg(). The result is a data frame with as many rows as there are groups that we split the data frame into.\n\n3.3.1 Median percent correct\nLet’s go ahead and do our first split-apply-combine on this tidy data set. First, we will split the data set up by insomnia condition and then apply a median function.\n\ndf.group_by('insomnia').median()\n\n\nshape: (2, 16)\n\n\n\ninsomnia\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\nbool\nf64\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\ntrue\n46.0\nnull\n39.0\n90.0\n75.0\n75.0\n76.5\n72.0\n71.0\n68.5\n77.0\n65.0\n14.0\n9.0\n7.0\n\n\nfalse\n54.0\nnull\n36.0\n90.0\n80.0\n85.0\n74.5\n55.5\n71.5\n59.0\n75.0\n59.25\n26.0\n4.0\n6.0\n\n\n\n\n\n\nNote that we used the median() method of the GroupBy object. This computes the median of all columns. This is equivalent the following using agg() with the expression pl.col('*').median().\n\ndf.group_by('insomnia').agg(pl.col('*').median())\n\n\nshape: (2, 16)\n\n\n\ninsomnia\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\nbool\nf64\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\nfalse\n54.0\nnull\n36.0\n90.0\n80.0\n85.0\n74.5\n55.5\n71.5\n59.0\n75.0\n59.25\n26.0\n4.0\n6.0\n\n\ntrue\n46.0\nnull\n39.0\n90.0\n75.0\n75.0\n76.5\n72.0\n71.0\n68.5\n77.0\n65.0\n14.0\n9.0\n7.0\n\n\n\n\n\n\nThe median doesn’t make sense for gender. If we only wanted to compute medians of quantities for which it makes sense to do so, we could use selectors.\n\ndf.group_by('insomnia').agg(cs.numeric().exclude('participant number').median())\n\n\nshape: (2, 14)\n\n\n\ninsomnia\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\nbool\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\nfalse\n36.0\n90.0\n80.0\n85.0\n74.5\n55.5\n71.5\n59.0\n75.0\n59.25\n26.0\n4.0\n6.0\n\n\ntrue\n39.0\n90.0\n75.0\n75.0\n76.5\n72.0\n71.0\n68.5\n77.0\n65.0\n14.0\n9.0\n7.0\n\n\n\n\n\n\nIf instead we only wanted the median of the percent correct and confidence when correct, we could do the following.\n\n(\n    df\n    .group_by('insomnia')\n    .agg(\n        pl.col('percent correct', 'confidence when correct').median(), \n    )\n)\n\n\nshape: (2, 3)\n\n\n\ninsomnia\npercent correct\nconfidence when correct\n\n\nbool\nf64\nf64\n\n\n\n\nfalse\n85.0\n75.0\n\n\ntrue\n75.0\n77.0\n\n\n\n\n\n\nWe can also use multiple columns in our group_by() operation. For example, we may wish to look at four groups, male insomniacs, female insomniacs, male non-insomniacs, and female non-insomniacs. To do this, we simply pass in a list of columns into df.group_by().\n\ndf.group_by([\"gender\", \"insomnia\"]).median()\n\n\nshape: (4, 16)\n\n\n\ngender\ninsomnia\nparticipant number\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\n\n\nstr\nbool\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"m\"\ntrue\n55.5\n37.0\n95.0\n82.5\n83.75\n83.75\n55.5\n75.75\n73.25\n81.25\n62.5\n14.0\n9.0\n8.0\n\n\n\"f\"\ntrue\n46.0\n39.0\n80.0\n75.0\n72.5\n76.5\n73.75\n71.0\n68.5\n77.0\n70.5\n14.0\n9.0\n7.0\n\n\n\"f\"\nfalse\n58.0\n36.0\n85.0\n80.0\n85.0\n74.0\n55.0\n70.5\n60.0\n74.0\n58.75\n26.0\n4.0\n7.0\n\n\n\"m\"\nfalse\n41.0\n38.5\n90.0\n80.0\n82.5\n76.0\n57.75\n74.25\n54.75\n76.25\n59.25\n29.0\n3.0\n6.0",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#window-functions",
    "href": "lessons/eda/polars/tidy_data.html#window-functions",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.4 Window functions",
    "text": "3.4 Window functions\nIn the above examples, we split the data frame into groups and applied an aggregating function that gave us back a data frame with as many rows as groups. But what if we do not want to aggregate? For example, say we want to compute the ranking of each participant according to percent correct within each group of insomniacs and normal sleepers. First, let’s see what happens if we use a group by/aggregation context. When we do the grouping, we will retain the order of the entries so that the series of ranks that we acquire match the original order in the data frame. We will also use the 'ordinal' method for ranking, which gives a distinct rank to each entry even in the event of ties.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(\n        pl.col('percent correct')\n        .rank(method='ordinal')\n    )\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\npercent correct\n\n\nbool\nlist[u32]\n\n\n\n\ntrue\n[11, 21, … 10]\n\n\nfalse\n[13, 35, … 7]\n\n\n\n\n\n\nWe see that we indeed get a data frame that has the two rows, one for each group. The 'percent correct' columns has a new data type, a Polars list (not the same as a Python list). If we wanted to convert each entry in the list into a new row of the data frame, we can use the explode() method of a data frame.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(\n        pl.col('percent correct')\n        .rank(method='ordinal')\n    )\n    .explode('percent correct')\n)\n\n\nshape: (102, 2)\n\n\n\ninsomnia\npercent correct\n\n\nbool\nu32\n\n\n\n\ntrue\n11\n\n\ntrue\n21\n\n\ntrue\n23\n\n\ntrue\n19\n\n\ntrue\n3\n\n\n…\n…\n\n\nfalse\n29\n\n\nfalse\n57\n\n\nfalse\n20\n\n\nfalse\n12\n\n\nfalse\n7\n\n\n\n\n\n\nThere are a few annoyances with doing this. First, in the group by/aggregation context, there is no way to maintain the other columns of the data frame as there is in the selection context via the with_columns() method. Second, we have to make a column with a list data type and then explode it.\nHere is where window functions come into play. A window function only operates on a subset of the data, ignoring everything outside of that subset. Since we are applying a function (in our example a rank function) to a subset (group) of the data, we can think of doing a group by followed by a calculation that has the same number of rows in the output as there are rows in the data as a window function.\nWindow functions are implemented in Polars expressions using the over() method. The argument(s) of the over() specify which columns specify groups. The expression is then evaluated individually for each group. To add a column with rankings based on percent correct within each group (insomnia or regular sleeper), we can do the following.\n\n(\n    df\n    .with_columns(\n        pl.col('percent correct')\n        .rank(method='ordinal')\n        .over('insomnia')\n        .alias('percent correct ranked within insomnia groups')\n    )\n)\n\n\nshape: (102, 17)\n\n\n\nparticipant number\ngender\nage\ncorrect hit percentage\ncorrect reject percentage\npercent correct\nconfidence when correct hit\nconfidence incorrect hit\nconfidence correct reject\nconfidence incorrect reject\nconfidence when correct\nconfidence when incorrect\nsci\npsqi\ness\ninsomnia\npercent correct ranked within insomnia groups\n\n\ni64\nstr\ni64\ni64\ni64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\ni64\ni64\ni64\nbool\nu32\n\n\n\n\n8\n\"f\"\n39\n65\n80\n72.5\n91.0\n90.0\n93.0\n83.5\n93.0\n90.0\n9\n13\n2\ntrue\n11\n\n\n16\n\"m\"\n42\n90\n90\n90.0\n75.5\n55.5\n70.5\n50.0\n75.0\n50.0\n4\n11\n7\ntrue\n21\n\n\n18\n\"f\"\n31\n90\n95\n92.5\n89.5\n90.0\n86.0\n81.0\n89.0\n88.0\n10\n9\n3\ntrue\n23\n\n\n22\n\"f\"\n35\n100\n75\n87.5\n89.5\nnull\n71.0\n80.0\n88.0\n80.0\n13\n8\n20\ntrue\n19\n\n\n27\n\"f\"\n74\n60\n65\n62.5\n68.5\n49.0\n61.0\n49.0\n65.0\n49.0\n13\n9\n12\ntrue\n3\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n97\n\"f\"\n23\n70\n85\n77.5\n77.0\n66.5\n77.0\n77.5\n77.0\n74.0\n20\n8\n10\nfalse\n29\n\n\n98\n\"f\"\n70\n90\n85\n87.5\n65.5\n85.5\n87.0\n80.0\n74.0\n80.0\n19\n8\n7\nfalse\n57\n\n\n99\n\"f\"\n24\n70\n80\n75.0\n61.5\n81.0\n70.0\n61.0\n65.0\n81.0\n31\n2\n15\nfalse\n20\n\n\n102\n\"f\"\n40\n75\n65\n70.0\n53.0\n37.0\n84.0\n52.0\n81.0\n51.0\n22\n4\n7\nfalse\n12\n\n\n103\n\"f\"\n33\n85\n40\n62.5\n80.0\n27.0\n31.0\n82.5\n81.0\n73.0\n24\n5\n7\nfalse\n7\n\n\n\n\n\n\nWindow functions and group by operations are similar. Consider computing the median percent correct for each group, insomniacs and normal sleepers. Using a group by/aggregation context, this is accomplished as follows.\n\n(\n    df\n    .group_by('insomnia')\n    .agg(\n        pl.col('percent correct')\n        .median()\n        .alias('median percent correct')\n    )\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\nmedian percent correct\n\n\nbool\nf64\n\n\n\n\nfalse\n85.0\n\n\ntrue\n75.0\n\n\n\n\n\n\nWe can achieve the same result using a window function with in a select context.\n\n(\n    df\n    .select(\n        pl.col('insomnia'), \n        pl.col('percent correct')\n        .median().over('insomnia')\n        .alias('median percent correct')\n    )\n    .unique('median percent correct')\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\nmedian percent correct\n\n\nbool\nf64\n\n\n\n\nfalse\n85.0\n\n\ntrue\n75.0\n\n\n\n\n\n\nA group by/aggregation context is preferred in this case. As a loose rule of thumb, use a group by/aggregation context when you want a resulting data frame with the number of rows being equal to the number of groups. Use a window function in a selection context when you want a resulting data frame with the same number of rows as your input data frame.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#custom-aggregation-and-window-functions",
    "href": "lessons/eda/polars/tidy_data.html#custom-aggregation-and-window-functions",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.5 Custom aggregation and window functions",
    "text": "3.5 Custom aggregation and window functions\nLet’s say we want to compute the coefficient of variation (CoV, the standard deviation divided by the mean) of data in columns of groups in the data frame. There is no built-in function in Polars to do this, but we could construct an expression that does it.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(\n        (pl.col('percent correct').std(ddof=0) / pl.col('percent correct').mean())\n        .alias('coeff of var percent correct')\n    )\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\ncoeff of var percent correct\n\n\nbool\nf64\n\n\n\n\ntrue\n0.171856\n\n\nfalse\n0.138785\n\n\n\n\n\n\nAlternatively, we could write our own Python function to compute the coefficient of variation using Numpy. Say we had such a function lying around that take as input a Numpy array and returns the coefficient of variation as a Numpy array.\n\ndef coeff_of_var(data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute coefficient of variation from an array of data.\"\"\"\n    return np.std(data) / np.mean(data)\n\nWe can use this function as an aggregating function using the map_batches() method of a Polars expression. The argument of map_batches() is a function that take a series as input and returns either a scalar or a series. We therefore need to pass in a function that converts the series to a Numpy array for use in the coeff_of_var() function, which we can accomplish with a lambda function.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(\n        pl.col('percent correct')\n        .map_batches(\n            lambda s: coeff_of_var(s.to_numpy()), \n            return_dtype=float, \n            returns_scalar=True\n        )\n    )\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\npercent correct\n\n\nbool\nf64\n\n\n\n\ntrue\n0.171856\n\n\nfalse\n0.138785\n\n\n\n\n\n\nNote that it is important to supply the data type of the return of the function you are mapping to ensure that Polars correctly assigns data types in the resulting data frame. It is also important to use the return_scalar=True, lest map_batches() gives each entry as a list.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#polars-structs-and-expressions-using-multiple-columns",
    "href": "lessons/eda/polars/tidy_data.html#polars-structs-and-expressions-using-multiple-columns",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.6 Polars Structs and expressions using multiple columns",
    "text": "3.6 Polars Structs and expressions using multiple columns\nAs discussed earlier, Polars expressions strictly take a series as input and return a series as output. What if we want to perform a calculation that requires two columns? This is where the Polars Struct data type comes in. As a simple example, if we cast our whole data frame into a series, we get a series with a struct data type.\n\nstruct_series = pl.Series(df)\n\n# Take a look\nstruct_series\n\n\nshape: (102,)\n\n\n\n\n\n\nstruct[16]\n\n\n\n\n{8,\"f\",39,65,80,72.5,91.0,90.0,93.0,83.5,93.0,90.0,9,13,2,true}\n\n\n{16,\"m\",42,90,90,90.0,75.5,55.5,70.5,50.0,75.0,50.0,4,11,7,true}\n\n\n{18,\"f\",31,90,95,92.5,89.5,90.0,86.0,81.0,89.0,88.0,10,9,3,true}\n\n\n{22,\"f\",35,100,75,87.5,89.5,null,71.0,80.0,88.0,80.0,13,8,20,true}\n\n\n{27,\"f\",74,60,65,62.5,68.5,49.0,61.0,49.0,65.0,49.0,13,9,12,true}\n\n\n…\n\n\n{97,\"f\",23,70,85,77.5,77.0,66.5,77.0,77.5,77.0,74.0,20,8,10,false}\n\n\n{98,\"f\",70,90,85,87.5,65.5,85.5,87.0,80.0,74.0,80.0,19,8,7,false}\n\n\n{99,\"f\",24,70,80,75.0,61.5,81.0,70.0,61.0,65.0,81.0,31,2,15,false}\n\n\n{102,\"f\",40,75,65,70.0,53.0,37.0,84.0,52.0,81.0,51.0,22,4,7,false}\n\n\n{103,\"f\",33,85,40,62.5,80.0,27.0,31.0,82.5,81.0,73.0,24,5,7,false}\n\n\n\n\n\n\nEach entry is an entire row of a data frame. The labels of each column is still present, as can be seen by considering one entry in the series of structs.\n\nstruct_series[0]\n\n{'participant number': 8,\n 'gender': 'f',\n 'age': 39,\n 'correct hit percentage': 65,\n 'correct reject percentage': 80,\n 'percent correct': 72.5,\n 'confidence when correct hit': 91.0,\n 'confidence incorrect hit': 90.0,\n 'confidence correct reject': 93.0,\n 'confidence incorrect reject': 83.5,\n 'confidence when correct': 93.0,\n 'confidence when incorrect': 90.0,\n 'sci': 9,\n 'psqi': 13,\n 'ess': 2,\n 'insomnia': True}\n\n\nThe labels for each entry in a struct (the keys in the dictionary displated above), are called fields, and we can get a list of the fields for the structs in a series.\n\nstruct_series.struct.fields\n\n['participant number',\n 'gender',\n 'age',\n 'correct hit percentage',\n 'correct reject percentage',\n 'percent correct',\n 'confidence when correct hit',\n 'confidence incorrect hit',\n 'confidence correct reject',\n 'confidence incorrect reject',\n 'confidence when correct',\n 'confidence when incorrect',\n 'sci',\n 'psqi',\n 'ess',\n 'insomnia']\n\n\nThe struct_series.struct.field() method allows us to take entries from all entries corresponding to a given field.\n\nstruct_series.struct.field('percent correct')\n\n\nshape: (102,)\n\n\n\npercent correct\n\n\nf64\n\n\n\n\n72.5\n\n\n90.0\n\n\n92.5\n\n\n87.5\n\n\n62.5\n\n\n…\n\n\n77.5\n\n\n87.5\n\n\n75.0\n\n\n70.0\n\n\n62.5\n\n\n\n\n\n\nThe following two operations give the same result.\npl.Series(df).struct.field('percent correct')\ndf.get_column('percent correct')\nSo, if we need to compute with multiple columns with an expression, we can convert whatever columns we need into a series of data type struct. We can then unpack what we need using the struct_series.struct.field() method.\nAs an example, say we have a function that compute the bivariate (a.k.a. Pearson) correlation coeff between two data sets given as Numpy arrays.\n\ndef bivariate_corr(x: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Compute bivariate correlation coefficient for `x` and `y`.\n    Ignores NaNs.\"\"\"\n    # Masked arrays to handle NaNs\n    x = np.ma.masked_invalid(x)\n    y = np.ma.masked_invalid(y)\n    mask = ~x.mask & ~y.mask\n    \n    return np.corrcoef(x[mask], y[mask])[0, 1]\n\nNow we want to compute the bivariate correlation coefficient for confidence when correct and confidence when incorrect for insomniacs and for normal sleepers. In our call to agg(), we use pl.struct to generate a series of data type struct containing the columns we need for the calculation of the correlation coefficient. We then use map_elements() to use the above function to do the calculation.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(\n        pl.struct(['confidence when correct', 'confidence when incorrect'])\n        .map_elements(\n            lambda s:\n            bivariate_corr(\n                s.struct.field('confidence when correct').to_numpy(), \n                s.struct.field('confidence when incorrect').to_numpy()\n            ), \n           return_dtype=float, returns_scalar=True\n        )\n        .alias('bivariate correlation')\n    )\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\nbivariate correlation\n\n\nbool\nf64\n\n\n\n\ntrue\n0.590435\n\n\nfalse\n0.552045\n\n\n\n\n\n\nWhile this example is instructive to demonstrate how to write your own functions to operate on data frames, as is often the case, Polars has a built-in function that computes the bivariate correlation coefficient.\n\n(\n    df\n    .group_by('insomnia', maintain_order=True)\n    .agg(pl.corr('confidence when correct', 'confidence when incorrect'))\n)\n\n\nshape: (2, 2)\n\n\n\ninsomnia\nconfidence when correct\n\n\nbool\nf64\n\n\n\n\ntrue\n0.590435\n\n\nfalse\n0.552045",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#looping-over-a-groupby-object",
    "href": "lessons/eda/polars/tidy_data.html#looping-over-a-groupby-object",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.7 Looping over a GroupBy object",
    "text": "3.7 Looping over a GroupBy object\nWhile the GroupBy methods we have learned so far (like transform() and agg()) are useful and lead to concise code, we sometimes want to loop over the groups of a GroupBy object. This often comes up in plotting applications, as we will see in future lessons. As an example, I will compute the median percent correct for female and males, insomniacs and not (which we already computed using describe()).\n\nfor group_name, sub_df in df.group_by(['gender', 'insomnia']):\n    print(group_name, \": \", sub_df[\"percent correct\"].median())\n\n('m', True) :  83.75\n('f', False) :  85.0\n('m', False) :  82.5\n('f', True) :  72.5\n\n\nBy using the GroupBy object as an iterator, it yields the name of the group (which I assigned as group_name) and the corresponding sub-data frame (which I assigned sub_df). Note that the group name is always given as a tuple, even when only grouping by a single column.\n\nfor (group_name,), sub_df in df.group_by('insomnia'):\n    print(group_name, \": \", sub_df[\"percent correct\"].median())\n\nTrue :  75.0\nFalse :  85.0",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/tidy_data.html#computing-environment",
    "href": "lessons/eda/polars/tidy_data.html#computing-environment",
    "title": "3  Tidy data and split-apply-combine",
    "section": "3.8 Computing environment",
    "text": "3.8 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\npolars    : 1.31.0\njupyterlab: 4.4.5",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tidy data and split-apply-combine</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/styling_data_frames.html",
    "href": "lessons/eda/polars/styling_data_frames.html",
    "title": "4  Styling data frames",
    "section": "",
    "text": "4.1 Computing environment\n| Download notebook\nData set download\nIt is sometimes useful to highlight features in a data frame when viewing them. (Note that this is generally far less useful than making informative plots, which we will come to shortly.) We sometimes also want to make a table for display. For this purpose, Polars works seamlessly with Great Tables package. To convert a Polars data frame to a Great Tables object that can then be stylized, simply use the df.style attribute of a Polars data frame. You can read about the endless possibilities for styling Polars data frames using Great Tables in the documentation.\nHere, we will do a quick demonstration, again using a data set from Beattie, et al. containing results from a study the effects of sleep quality on performance in the Glasgow Facial Matching Test (GMFT). We will make a pretty-looking table also highlighting rows corresponding to women who scored at 90% or above.\nThere is much more you can do, and the documentation of Great Tables has plenty of examples and tips. In my experience, though, it is rare that you will need to style a data frame; results are usually shown graphically.\n%load_ext watermark\n%watermark -v -p polars,great_tables,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\npolars      : 1.33.1\ngreat_tables: 0.18.0\njupyterlab  : 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Styling data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/polars_for_pandas.html",
    "href": "lessons/eda/polars/polars_for_pandas.html",
    "title": "5  Polars for Pandas users",
    "section": "",
    "text": "5.1 A sample data frame\n| Download notebook\nAs of September 2025, Pandas is far and away the most widely used data frame package for Python. We are using Polars primarily because, in my opinion, the API is more intuitive and therefore easier for beginners and experts alike to use. It is also faster, sometimes much faster. It is, however, important to know about Pandas and how to use it because many of your colleagues use it and many packages you may use do, too.\nTherefore, in this part of the lesson, I discuss how to convert a Polars data frame to Pandas, and vice versa. I also provide syntax for doing common tasks in Polars and Pandas. It is also worth reading the section of the Polars user guide comparing Pandas and Polars.\nFor ease of discussion and comparison, we will use a simple data frame that has two categorical columns, 'c1', and 'c2', two quantitative columns as floats, 'q1', and 'q2', and a column, 'i1' of integer values. It also has an identity column, a unique identifier for each row that is useful when converting the data frame to tall format. Note that 'q1' has a null value and 'q2' has a NaN value.\ndata = dict(\n    id=list(range(1, 9)),\n    c1=['a']*4 + ['b']*4,\n    c2=['c', 'd'] * 4,\n    q1=[1.1, 2.2, 3.1, None, 2.9, 1.7, 3.0, 7.3],\n    q2=[4.5, 2.3, np.nan, 1.1, 7.8, 2.3, 1.1, 0.8],\n    i1=[5, 3, 0, 2, 4, 3, 4, 1],\n)\n\ndf = pl.DataFrame(data)\n\n# Take a look\ndf\n\n\nshape: (8, 6)\n\n\n\nid\nc1\nc2\nq1\nq2\ni1\n\n\ni64\nstr\nstr\nf64\nf64\ni64\n\n\n\n\n1\n\"a\"\n\"c\"\n1.1\n4.5\n5\n\n\n2\n\"a\"\n\"d\"\n2.2\n2.3\n3\n\n\n3\n\"a\"\n\"c\"\n3.1\nNaN\n0\n\n\n4\n\"a\"\n\"d\"\nnull\n1.1\n2\n\n\n5\n\"b\"\n\"c\"\n2.9\n7.8\n4\n\n\n6\n\"b\"\n\"d\"\n1.7\n2.3\n3\n\n\n7\n\"b\"\n\"c\"\n3.0\n1.1\n4\n\n\n8\n\"b\"\n\"d\"\n7.3\n0.8\n1",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Polars for Pandas users</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/polars_for_pandas.html#from-polars-to-pandas-and-from-pandas-to-polars",
    "href": "lessons/eda/polars/polars_for_pandas.html#from-polars-to-pandas-and-from-pandas-to-polars",
    "title": "5  Polars for Pandas users",
    "section": "5.2 From Polars to Pandas and from Pandas to Polars",
    "text": "5.2 From Polars to Pandas and from Pandas to Polars\nIf you have a Polars data frame, you can directly convert it to a Pandas data frame using the to_pandas(), method. Let’s do that for our data frame.\n\ndf.to_pandas()\n\n\n\n\n\n\n\n\nid\nc1\nc2\nq1\nq2\ni1\n\n\n\n\n0\n1\na\nc\n1.1\n4.5\n5\n\n\n1\n2\na\nd\n2.2\n2.3\n3\n\n\n2\n3\na\nc\n3.1\nNaN\n0\n\n\n3\n4\na\nd\nNaN\n1.1\n2\n\n\n4\n5\nb\nc\n2.9\n7.8\n4\n\n\n5\n6\nb\nd\n1.7\n2.3\n3\n\n\n6\n7\nb\nc\n3.0\n1.1\n4\n\n\n7\n8\nb\nd\n7.3\n0.8\n1\n\n\n\n\n\n\n\nNote that the null value becomes a NaN. All missing data in Pandas are NaN. (Well, not really. You can have an object data type for a column that permits None values. However, when Pandas reads in data, when there are missing data, it assigns it to be NaN by default.)\nNote also that Pandas has an index displayed on the left side of the data frame. In general, we will not use Pandas indexes.\nSimilarly, if you have a data frame in Pandas, you can convert it to a Polars data frame using the pl.from_pandas() function.\n\npl.from_pandas(pd.DataFrame(data))\n\n\nshape: (8, 6)\n\n\n\nid\nc1\nc2\nq1\nq2\ni1\n\n\ni64\nstr\nstr\nf64\nf64\ni64\n\n\n\n\n1\n\"a\"\n\"c\"\n1.1\n4.5\n5\n\n\n2\n\"a\"\n\"d\"\n2.2\n2.3\n3\n\n\n3\n\"a\"\n\"c\"\n3.1\nnull\n0\n\n\n4\n\"a\"\n\"d\"\nnull\n1.1\n2\n\n\n5\n\"b\"\n\"c\"\n2.9\n7.8\n4\n\n\n6\n\"b\"\n\"d\"\n1.7\n2.3\n3\n\n\n7\n\"b\"\n\"c\"\n3.0\n1.1\n4\n\n\n8\n\"b\"\n\"d\"\n7.3\n0.8\n1",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Polars for Pandas users</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/polars_for_pandas.html#pandas-and-polars-for-common-tasks",
    "href": "lessons/eda/polars/polars_for_pandas.html#pandas-and-polars-for-common-tasks",
    "title": "5  Polars for Pandas users",
    "section": "5.3 Pandas and Polars for common tasks",
    "text": "5.3 Pandas and Polars for common tasks\nBelow is a table listing common tasks with data frame done using Polars and Pandas.\n\n\n\n\n\n\n\n\nDescription\nPandas\nPolars\n\n\n\n\nConvert dictionary to df\npd.DataFrame(data)\npl.DataFrame(data)\n\n\nMake 2D Numpy array into df\npd.DataFrame(my_ar, columns=['col1', 'col2', 'col3'])\npl.DataFrame(my_ar, schema=['col1', 'col2', 'col3'], orient='row')\n\n\nRead from CSV\npd.read_csv(file_name)\npl.read_csv(file_name)\n\n\nLazily read CSV\n—\npl.scan_csv(file_name)\n\n\nRead from Excel\npd.read_excel(file_name)\npl.read_excel(file_name)\n\n\nRead from JSON\npd.read_json(file_name)\npl.read_json(file_name)\n\n\nRead from HDF5\npd.read_hdf(file_name)\n—\n\n\nWrite CSV\ndf.to_csv(fname, index=False)\ndf.write_csv(fname)\n\n\nRename columns\ndf.rename(columns={'c1': 'cat1', 'c2': 'cat2'})\ndf.rename({'c1': 'cat1', 'c2': 'cat2'})\n\n\nGet column 'q1' as series\ndf['q1'] or df.get_column('q1') or df.loc[:,'q1']\ndf['q1'] or df.get_column('q1')\n\n\nGet column 'q1' as data frame\ndf[['q1']] or df.loc[:,['q1']]\ndf.select('q1')\n\n\nGet columns 'c1' and 'q2'\ndf[['c1', 'q2']] or df.loc[:, ['c1', 'q2']]\ndf.select('c1', 'q2')\n\n\nGet columns containing floats\ndf.select_dtypes(float)\ndf.select(cs.float())\n\n\nGet row 4\ndf.loc[4, :]\ndf.row(4) or df.row(4, named=True)\n\n\nGet row 4 as data frame\ndf.loc[[4], :]\ndf[4]\n\n\nGet row where i1 is 2\ndf.loc[df['i1']==2, :]\ndf.row(by_predicate=pl.col('i1')==2) or df.filter(pl.col('i1')==2)\n\n\nSub df with rows where c1 is 'a'\ndf.loc[df['c1']=='a', :]\ndf.filter(pl.col('c1')=='a')\n\n\nSub df where c1 is 'a' and c2 is 'd'\ndf.loc[(df['c1']=='a') & (df['c2']=='d'), :]\ndf.filter((pl.col('c1')=='a') & (pl.col('c2')=='d'))\n\n\nIterate over columns of df\nfor col, s in df.items()\nfor s in df\n\n\nIterate over rows of df\nfor row_ind, row in df.iterrows()\nfor r in df.iter_rows(named=True)\n\n\nGroup by single column\ndf.groupby('c1')\ndf.group_by('c1')\n\n\nGroup by maintaining order\ndf.groupby('c1', sort=False)\ndf.group_by('c1', maintain_order=True)\n\n\nGroup by multiple columns\ndf.groupby(['c1', 'c2'])\ndf.group_by(['c1', 'c2'])\n\n\nIterate over groups\nfor group, subdf in df.groupby('c1')\nfor (group,), subdf in df.group_by('c1')\n\n\nIterate over nested groups\nfor (g1, g2), subdf in df.groupby(['c1', 'c2'])\nfor (g1, g2), subdf in df.group_by(['c1', 'c2'])\n\n\nGroup by and apply mean¹\ndf.groupby('c1').mean(numeric_only=True)\ndf.group_by('c1').mean()\n\n\nGroup by and apply median to one column\ndf.groupby('c1')['q1'].median()\ndf.group_by('c1').agg(pl.col('q1').median())\n\n\nGroup by and apply mean to two columns\ndf.groupby('c1')[['q1', 'q2']].mean()\ndf.group_by('c1').agg(pl.col('q1', 'q2').mean())\n\n\nGroup by and apply custom func to col²\ndf.groupby('c1')['q1'].apply(my_fun)\ndf.group_by('c1').agg(pl.col('q1').map_batches(my_fun, return_dtype=float, returns_scalar=True))\n\n\nGroup by and apply custom func to 2 cols³\ndf.groupby('c1')[['q1', 'q2']].apply(my_fun)\ndf.group_by('c1').agg(pl.struct(['q1', 'q2']).map_batches(my_fun, return_dtype=float, returns_scalar=True))\n\n\nGroup by and rank within each group\ndf.groupby('c1')['q1'].rank()\ndf.select(pl.col('q1').rank().over('c1'))\n\n\nConvert to tall format\ndf.melt(value_name='value', var_name='var', id_vars='id')\ndf.unpivot(value_name='value', variable_name='var', index='id')\n\n\nPivot tall result above\ndf_tall.pivot(columns='var', index='id').reset_index()\ndf_tall.pivot(on='var', index='id')\n\n\nSelect columns with string in name\ndf.filter(regex='q') or df[df.columns[df.columns.str.contains('q')]]\ndf.select(cs.contains('q'))\n\n\nAdd column of zeros to data frame\ndf['new_col'] = 0 or df.assign(new_col=0)\ndf.with_columns(pl.lit(0).alias('new_col'))\n\n\nAdd a Numpy array as column\ndf['new_col'] = my_array or df.assign(new_col=my_array)\ndf.with_columns(pl.Series(my_array).alias('new_col'))\n\n\nMultiply two columns; make new column\ndf['q1q2'] = df['q1'] * df['q2'] or df.assign(q1q2=df['q1'] * df['q2']\ndf.with_columns((pl.col('q1') * pl.col('q2')).alias('q1q2'))\n\n\nApply a function to each row making new col⁴\ndf.assign(new_col=my_fun)\ndf.with_columns(pl.struct(pl.all()).map_elements(my_fun, return_dtype=float).alias('new_col'))\n\n\nDrop rows with missing data\ndf.dropna()\ndf.drop_nulls()\n\n\nSort according to a column\ndf.sort_values(by='i1')\ndf.sort(by='i1')\n\n\nInner join two data frames⁵\npd.merge(df, df2, on=shared_columns)\ndf.join(df2, on=shared_columns)\n\n\nConcatenate data frames vertically\npd.concat((df, df2))\npl.concat((df, df2), how='diagonal')\n\n\nConcatenate data frames horizontally\npd.concat((df, df2), axis=1)\npl.concat((df, df2), how='horizontal')\n\n\n\nFootnotes\n\nNote that in Pandas, NaNs are omitted from calculations like means. In Polars, NaNs are included, and the result will be NaN. However, nulls are not included.\nFor Pandas, the function my_fun must take an array_like data type (list, Numpy array, Pandas Series, etc.) as input. For Polars, the function my_fun must take a Polars Series as input. It is wise to specify the data type of the output of the function (shown as float in the above example, but can be whatever type my_fun returns). A Pandas example: my_fun = lambda x: np.sum(np.sin(x)). A Polars example: my_fun = lambda s: s.exp().sum().\nFor Pandas, the function must take a Pandas DataFrame as an argument. For Polars, it must take a Polars Series with a struct data type. A Pandas example: my_fun = lambda df: (np.sin(df['q1']) * df['q2']).sum(). A Polars example: my_fun = lambda s: (s.struct.field('q1').sin() * s.struct.field('q2')).sum()\nFor Pandas, my_fun must take as its argument a Pandas Series with an index containing the names of the columns of the original data frame. For Polars, my_fun must take as its argument a dictionary with keys given by the names of the columns of the original data frame. The functions may then have the same syntax (though possibly with different type hints). An example: my_fun = lambda r: r['i1'] * np.sin(r['q2']). However, note that in Polars, a null value is treated as None, which means you cannot apply a function to it, multiply by it, etc.\nFor Polars, the on kwarg for df.join() is required. With Pandas, which columns to join on are inferred based on like-names of columns.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Polars for Pandas users</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/polars_for_pandas.html#hierarchical-indexes",
    "href": "lessons/eda/polars/polars_for_pandas.html#hierarchical-indexes",
    "title": "5  Polars for Pandas users",
    "section": "5.4 Hierarchical indexes",
    "text": "5.4 Hierarchical indexes\nPandas supports hierarchical indexes, called MultiIndexes. This is not supported by Polars. Polars will not read a CSV file with hierarchical indexes. If you have a data set in a CSV file with hierarchical indexes, you can convert it to a CSV file in tall format where the MultiIndex has been converted to columns using the bebi103.utils.unpivot_csv() function. This operation is akin to a df.melt() operation on a data frame with a hierarchical index. You can then read the converted CSV file into Polars and begin working with it.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Polars for Pandas users</span>"
    ]
  },
  {
    "objectID": "lessons/eda/polars/polars_for_pandas.html#computing-environment",
    "href": "lessons/eda/polars/polars_for_pandas.html#computing-environment",
    "title": "5  Polars for Pandas users",
    "section": "5.5 Computing environment",
    "text": "5.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,pandas,polars,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\npandas    : 2.3.2\npolars    : 1.33.1\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Polars for Pandas users</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting.html",
    "href": "lessons/eda/plotting/plotting.html",
    "title": "6  Data display",
    "section": "",
    "text": "6.1 The Python visualization landscape\nIn 1977, John Tukey, one of the prominent statisticians and mathematicians in history, published a book entitled Exploratory Data Analysis. In it, he laid out general principles on how researchers should handle their first encounters with their data, before formal statistical inference. Most of us spend a lot of time doing exploratory data analysis, or EDA, without really knowing it. Mostly, EDA involves a graphical exploration of a data set.\nWe start off with a few wise words from John Tukey himself, chosen from that brilliant book.\nClearly data display, or plotting, is central to exploratory data analysis.\nLet us start by looking at some of the many plotting packages available in Python. In a talk at PyCon in 2017, Jake VanderPlas, who is one of the authors of one of them (Altair), gave an overview of the Python visualization landscape. That landscape is depicted below, taken from this visualization of it by Nicolas Rougier. (It is from 2017, so it is dated, and definitely not complete, notably missing Panel and domain-specific plotting like napari and Folium, for example.)\nThe landscape is divided into three main pods based on the low-level renderer of the graphics, JavaScript, Matplotlib, and OpenGL (though Matplotlib is higher-level than JavaScript and OpenGL). We will not discuss packages based on OpenGL. Packages that use JavaScript for rendering are particularly well suited for interactivity in browsers. Interactivity and portability (accomplished by rendering in browsers) are key features of modern plotting libraries, so we will use JavaScript-based plotting in the workshop (as I do in my own work).\nThough we will be using Bokeh (and a little bit of HoloViews/Datashader), for a scientist working in Python, it is important to take note of the following.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data display</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting.html#the-python-visualization-landscape",
    "href": "lessons/eda/plotting/plotting.html#the-python-visualization-landscape",
    "title": "6  Data display",
    "section": "",
    "text": "Figure 6.1: A somewhat dated, but reasonably complete, picture of the landscape of data visualization packages in Python.\n\n\n\n\n\n\nMatplotlib is by far the most widely used plotting package in Python. It was even developed a neuroscientist! Seaborn is also widely used as a higher level statistical plotting package that has Matplotlib as its backend (and also developed by a neuroscientist!). We choose to use Bokeh because it is effective\nThere are many domain-specific packages that have plotting modules. For example, for neuroscientists, MNE, Nilearn, and SpikeInterface are all tools with plotting interfaces. Here, we are focusing on general tools. If you have master of lower-level plotting software, you can get much more out of domain-specific packages. You are also unshackled to do the visualization you want to do, and not just those that are available.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data display</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html",
    "title": "7  Making plots with Bokeh",
    "section": "",
    "text": "7.1 High-level and low-level plotting packages\n| Download notebook\nAs a demonstration of what I mean by high-level and low-level plotting packages, let us first think about one of our tasks we did with Polars with the facial matching data set. We computed the median percent correct for those with and without insomnia. Here’s the code to do it.\nfname = os.path.join(data_path, \"gfmt_sleep.csv\")\ndf = pl.read_csv(fname, null_values=\"*\")\ndf = df.with_columns((pl.col('sci') &lt;= 16).alias(\"insomnia\"))\n\ndf.group_by(\"insomnia\").agg(pl.col('percent correct').median())\n\n\nshape: (2, 2)\n\n\n\ninsomnia\npercent correct\n\n\nbool\nf64\n\n\n\n\nfalse\n85.0\n\n\ntrue\n75.0\nLiterally just a few lines of code. Now what if we tried to do it without Polars? I won’t even go through the many lines of code necessary to read in the data. Consider just this one line.\nThere are elementary tasks that go into it if we were to code it up without using Polars’s delicious functionality. We can loop over the rows in the data frame with a for loop, check to see what the value of the insomnia column is with an if statement, put the value in the percent correct field into an appropriate array based on whether or not the subject suffers from insomnia, and then, given those arrays, sort them and pull out the middle value. Under the hood, all of those steps take place, but because we use Polars’s high-level functionality, those details are invisible to us, and glad we are of that.\nNow, say we want to make a plot of some data. You can imagine that there are many many steps to building that. One way you could build a plot is to hand-generate an SVG file that is a set of specifications for lines and circles and text and whatnot that comprises a plot. (I have actually done this before, writing a C program that hand-generated SVG, and it was paaaaainful.) That would be a very low-level way of generating a plot. Plotting libraries in Python usually take care of the rendering part for you, either rendering the plot as SVG, PDF, PNG, or other formats, including interactive ones that use JavaScript and HTML Canvas that can be viewed in a browser. The plotting libraries then vary in their level of abstraction from the data set.\nLower-level plotting libraries typically are more customizable, but require more boilerplate code to get your data plotted and are therefore more cumbersome. Higher-level plotting libraries aim to make it easier to move directly from a data frame to a plot. Because of this streamlining, though, they are often more difficult to customize.\nThe developers of HoloViz made a nice graphic for this concept (Copyright PyViz authors, downloaded here).\nUsing a low-level plotting library, you can get to any graphic you like, but it takes many steps to do so. Using a high level library, you can rapidly get to many, if not most, of the graphics you like in very few steps. However, you cannot get to all graphics. In a layered approach, in which the higher level libraries give you access to the lower level customizations, you can get to any graphic, and can do so quickly. The layered approach requires proficiency in using the low-level and high-level libraries.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#high-level-and-low-level-plotting-packages",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#high-level-and-low-level-plotting-packages",
    "title": "7  Making plots with Bokeh",
    "section": "",
    "text": "df.group_by(\"insomnia\").agg(pl.col('percent correct').median())\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: Routes to making a plot, copyright PyViz authors.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#bokeh-and-the-summer-school",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#bokeh-and-the-summer-school",
    "title": "7  Making plots with Bokeh",
    "section": "7.2 Bokeh and the summer school",
    "text": "7.2 Bokeh and the summer school\nOne debate I have every time I teach data visualization is what plotting packages, high-level or low-level, to use. I decided to almost exclusively Bokeh, a low-level plotting library first for a few reasons.\n\nThough low-level, generating plot you might like to construct is fairly straightforward. (Read: it’s not that bad for quickly making plots.)\nBy being familiar with a lower-level plotting package, you can then take a layered approach as you learn a higher-level package.\nWe will discuss at least one higher-level package, iqplot, in short order, discussed in the next part of this lesson.\n\nImportantly, note that Bokeh’s submodules often have to be explicitly imported, as we did in the code cell at the top of this notebook. Note also that if you want your plots to be viewable (and interactive) in the notebook, you need to execute\nbokeh.io.output_notebook()\nat the top of the notebook (as we have done). Finally, note that we also have to have installed the Bokeh JupyterLab extension.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#bokehs-grammar-and-our-first-plot-with-bokeh",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#bokehs-grammar-and-our-first-plot-with-bokeh",
    "title": "7  Making plots with Bokeh",
    "section": "7.3 Bokeh’s grammar and our first plot with Bokeh",
    "text": "7.3 Bokeh’s grammar and our first plot with Bokeh\nConstructing a plot with Bokeh consists of four main steps.\n\nCreating a figure on which to populate glyphs (symbols that represent data, e.g., dots for a scatter plot). Think of this figure as a “canvas” which sets the space on which you will “paint” your glyphs.\nDefining a data source that is the reference used to place the glyphs.\nChoose the kind of glyph you would like.\nAnnotate the columns of data source to determine how they are used to place (and possibly color, scale, etc.) the glyph.\n\nAfter completing these steps, you need to render the graphic.\nLet’s go through these steps in generating a scatter plot of confidence when incorrect versus confidence when correct for the face matching under sleep deprivation study. So you have the concrete example in mind, the final graphic will look like this\n\n\n\n  \n\n\n\n\n\n\nOur first step is creating a figure, our “canvas.” In creating the figure, we are implicitly thinking about what kind of representation for our data we want. That is, we have to specify axes and their labels. We might also want to specify the title of the figure, whether or not to have grid lines, and all sorts of other customizations. Naturally, we also want to specify the shape of the figure.\n\n(Almost) all of this is accomplished in Bokeh by making a call to bokeh.plotting.figure() with the appropriate keyword arguments.\n\n# Create the figure, stored in variable `p`\np = bokeh.plotting.figure(\n    frame_width=400,\n    frame_height=300,\n    x_axis_label='confidence when correct',\n    y_axis_label='condifence when incorrect'\n)\n\nThere are many more keyword attributes you can assign, including all of those listed in the Bokeh Plot class and the additional ones listed in the Bokeh Figure class.\n\nNow that we have set up our canvas, we can decide on the data source. We will use the data frame, df as our data source.\nWe will choose dots (or circles) as our glyph. This kind of glyph requires that we specify which column of the data source will serve to place the glyphs along the \\(x\\)-axis and which will serve to place the glyphs along the \\(y\\)-axis.\nWe choose the 'confidence when correct' column to specify the \\(x\\)-coordinate of the glyph and the 'confidence when incorrect' column to specify the \\(y\\)-coordinate. We already made this decision when we set up our axis labels, but we did not necessarily have to make that decision at that point.\n\nSteps 3 and 4 are accomplished by calling one of the glyph methods of the Bokeh Figure instance, p. Since we are choosing dots, the appropriate method is p.scatter(), and we use the source, x, and y kwargs to specify the positions of the glyphs.\n\np.scatter(\n    source=df.to_dict(),\n    x='confidence when correct',\n    y='confidence when incorrect'\n);\n\nNote that the source must be a ColumnDataSource, a special Bokeh class. If we pass in a dictionary (or Polars data frame) for the source keyword argument, Bokeh converts it to a ColumnDataSource. Therefore, we use the to_dict() method to convert the Polars data frame to a dictionary.\nNow that we have built the plot, we can render it in the notebook using bokeh.io.show().\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nIn looking at the plot, notice a toolbar to right of the plot that enables you to zoom and pan within the plot.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#the-importance-of-tidy-data-frames",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#the-importance-of-tidy-data-frames",
    "title": "7  Making plots with Bokeh",
    "section": "7.4 The importance of tidy data frames",
    "text": "7.4 The importance of tidy data frames\nIt might be clear for you now that building a plot in this way requires that the data frame you use be tidy. The organization of tidy data is really what enables this and high level plotting functionality. There is a well-specified organization of the data.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#code-style-in-plot-specifications",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#code-style-in-plot-specifications",
    "title": "7  Making plots with Bokeh",
    "section": "7.5 Code style in plot specifications",
    "text": "7.5 Code style in plot specifications\nSpecifications of plots often involves calls to functions with lots of keyword arguments to specify the plot, and this can get unwieldy without a clear style. You can develop your own style, maybe reading Trey Hunner’s blog post again. I like to do the following.\n\nPut the function call, like p.scatter( or p = bokeh.plotting.figure( on the first line.\nThe closed parenthesis for the function call is on its own line, unindented.\nAny arguments are given as kwargs (even if they can also be specified as positional arguments) at one level of indentation.\n\nNote that you cannot use method chaining when instantiating figures or populating glyphs.\nIf you adhere to a style (which is roughly the style imposed by Black), it makes your code cleaner and easier to read.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#coloring-with-other-dimensions",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#coloring-with-other-dimensions",
    "title": "7  Making plots with Bokeh",
    "section": "7.6 Coloring with other dimensions",
    "text": "7.6 Coloring with other dimensions\nLet’s say we wanted to make the same plot, but with orange circles for insomniacs and blue circles for normal sleepers. To do this, we take advantage of two features of Bokeh.\n\nWe can make multiple calls to p.scatter() to populate more and more glyphs.\np.scatter(), like all of the glyph methods, has many keyword arguments, including color and legend_label, which will enable us to color the glyphs and include a legend.\n\nWe can loop through the data frame grouped by 'insomnia' and populate the glyphs as we go along.\n\n# For convenience\nx = \"confidence when correct\"\ny = \"confidence when incorrect\"\n\n# Make figure\np = bokeh.plotting.figure(\n    width=400,\n    height=300,\n    x_axis_label=x,\n    y_axis_label=y,\n)\n\n# Add glyphs\ncolors = {'normal sleepers': '#1f77b4', 'insomniacs': 'orange'}\nfor (insom,), sub_df in df.group_by('insomnia'):\n    category = \"insomniacs\" if insom else \"normal sleepers\"\n    p.scatter(\n        source=sub_df.to_dict(),\n        x=x,\n        y=y,\n        color=colors[category],\n        legend_label=category,\n    )\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe got the plot we wanted, but the legend is clashing with the data. Fortunately, Bokeh allows us to set attributes of the figure whenever we like. (We will further discuss styling Bokeh plots in a future lesson.) We can therefore set the legend position to be in the upper left corner. We will also set the click_policy for the legend to be 'hide', which will hide glyphs if you click the legend, which can be convenient for viewing cluttered plots (though this one is not cluttered, really).\n\np.legend.location = 'top_left'\np.legend.click_policy = 'hide'\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#adding-tooltips",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#adding-tooltips",
    "title": "7  Making plots with Bokeh",
    "section": "7.7 Adding tooltips",
    "text": "7.7 Adding tooltips\nBokeh’s interactivity is one of its greatest strengths. While we are plotting confidences when correct and incorrect, we have colored with insomniac status. We might also like to have access to other information in our (tidy) data source if we hover over a glyph. Let’s say we want to know the participant number, gender, and age of each participant. We can tell Bokeh to give us this information by adding tooltips when we instantiate the figure.\nThe syntax for a tooltip is a list of 2-tuples, where each tuple represents the tooltip you want. The first entry in the tuple is the label and the second is the column from the data source that has the values. The second entry must be preceded with an @ symbol signifying that it is a field in the data source and not field that is intrinsic to the plot, which is preceded with a $ sign. If there are spaces in the column heading, enclose the column name in braces. (See the documentation for tooltip specification for more information.)\n\n# For convenience\nx = \"confidence when correct\"\ny = \"confidence when incorrect\"\n\n# Make figure\np = bokeh.plotting.figure(\n    width=400,\n    height=300,\n    x_axis_label=x,\n    y_axis_label=y,\n    tooltips=[\n        (\"p-number\", \"@{participant number}\"),\n        (\"gender\", \"@gender\"),\n        (\"age\", \"@age\"),\n    ],\n)\n\n# Add glyphs\ncolors = {'normal sleepers': '#1f77b4', 'insomniacs': 'orange'}\nfor (insom,), sub_df in df.group_by('insomnia'):\n    category = \"insomniacs\" if insom else \"normal sleepers\"\n    p.scatter(\n        source=sub_df.to_dict(),\n        x=x,\n        y=y,\n        color=colors[category],\n        legend_label=category,\n    )\n\np.legend.location = \"top_left\"\np.legend.click_policy = \"hide\"\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#saving-bokeh-plots",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#saving-bokeh-plots",
    "title": "7  Making plots with Bokeh",
    "section": "7.8 Saving Bokeh plots",
    "text": "7.8 Saving Bokeh plots\nAfter you create your plot, you can save it to a variety of formats. Most commonly you would save them as PNG (for presentations), SVG (for publications in the paper of the past), and HTML (for the paper of the future or sharing with colleagues).\nTo save as a PNG for quick use, you can click the disk icon in the tool bar.\nTo save to SVG, you first change the output backend to 'svg' and then you can click the disk icon again, and you will get an SVG rendering of the plot. After saving the SVG, you should change the output backend back to 'canvas' because it has much better in-browser performance.\n\np.output_backend = 'svg'\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nNow, click the disk icon in the plot above to save it.\nAfter saving, we should switch back to canvas.\n\np.output_backend = 'canvas'\n\nYou can also save the figure programmatically using the bokeh.io.export_svgs() function. This requires additional installations, so we will not do it here, but show the code to do it. Again, this will only work if the output backed is 'svg'.\np.output_backend = 'svg'\nbokeh.io.export_svgs(p, filename='insomniac_confidence_correct.svg')\np.output_backend = 'canvas'\nFinally, to save as HTML, you can use the bokeh.io.save() function. This saves your plot as a standalone HTML page. Note that the title kwarg is not the title of the plot, but the title of the web page that will appear on your Browser tab.\n\nbokeh.io.save(\n    p, \n    filename='insomniac_confidence_correct.html', \n    title='Bokeh plot',\n    resources=bokeh.resources.CDN,\n);\n\nThe resulting HTML page has all of the interactivity of the plot and you can, for example, email it to your collaborators for them to explore.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_with_bokeh.html#computing-environment",
    "href": "lessons/eda/plotting/plotting_with_bokeh.html#computing-environment",
    "title": "7  Making plots with Bokeh",
    "section": "7.9 Computing environment",
    "text": "7.9 Computing environment\n\n\nCode\n%load_ext watermark\n%watermark -v -p polars,bokeh,jupyterlab\n\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\npolars    : 1.27.1\nbokeh     : 3.6.2\njupyterlab: 4.3.6",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Making plots with Bokeh</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/plotting_smooth_curves.html",
    "href": "lessons/eda/plotting/plotting_smooth_curves.html",
    "title": "8  Plotting smooth curves",
    "section": "",
    "text": "8.1 Computing environment\n| Download notebook\nSometimes you want to plot smooth functions, as opposed to measured data like we have done so far. To do this, you can use Numpy and/or Scipy to generate arrays of values of smooth functions.\nWe will plot the Airy disk, which we encounter in biology when doing microscopy as the diffraction pattern of light passing through a pinhole. Here is a picture of the diffraction pattern from a laser (with the main peak overexposed).\nThe equation for the radial light intensity of an Airy disk is\n\\[\\begin{align}\n\\frac{I(x)}{I_0} = 4 \\left(\\frac{J_1(x)}{x}\\right)^2,\n\\end{align}\\]\nwhere \\(I_0\\) is the maximum intensity (the intensity at the center of the image) and \\(x\\) is the radial distance from the center. Here, \\(J_1(x)\\) is the first order Bessel function of the first kind. Yeesh. How do we plot that?\nFortunately, SciPy has lots of special functions available. Specifically, scipy.special.j1() computes exactly what we are after! We pass in a NumPy array that has the values of \\(x\\) we want to plot and then compute the \\(y\\)-values using the expression for the normalized intensity.\nTo plot a smooth curve, we use the np.linspace() function with lots of points. We then connect the points with straight lines, which to the eye look like a smooth curve. Let’s try it. We’ll use 400 points, which I find is a good rule of thumb for not-too-quickly-oscillating functions.\nNow that we have the values we want to plot, we could construct a Pandas DataFrame to pass in as the source to p.line(). We do not need to take this extra step, though. If we instead leave source unspecified, and pass in NumPy arrays for x and y, Bokeh will directly use those in constructing the plot.\nWe could also plot dots (which doesn’t make sense here, but we’ll show it just to see who the line joining works to make a plot of a smooth function).\nThere is one detail I swept under the rug here. What happens if we compute the function for \\(x = 0\\)?\nWe get a RuntimeWarning because we divided by zero. We know that\n\\[\\begin{align}\n\\lim_{x\\to 0} \\frac{J_1(x)}{x} = \\frac{1}{2},\n\\end{align}\\]\nso we could write a new function that checks if \\(x = 0\\) and returns the appropriate limit for \\(x = 0\\). In the x array I constructed for the plot, we hopped over zero, so it was never evaluated. If we were being careful, we could write our own Airy function that deals with this.\n%load_ext watermark\n%watermark -v -p numpy,scipy,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\nscipy     : 1.16.1\nbokeh     : 3.7.3\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Plotting smooth curves</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/categorical_axes_bokeh.html",
    "href": "lessons/eda/plotting/categorical_axes_bokeh.html",
    "title": "9  Plots with categorical variables",
    "section": "",
    "text": "9.1 Types of data for plots\n| Download notebook\nData set download\nLet us first consider the different kinds of data we may encounter as we think about constructing a plot.\nIn practice, ordinal data can be cast as quantitative or treated as categorical with an ordering enforced on the categories (e.g., categorical data [1, 2, 3] becomes ['1', '2', '3'].). Temporal data can also be cast as quantitative, (e.g., seconds from the start time). We will therefore focus out attention on quantitative and categorical data.\nWhen we made scatter plots, both types of data were quantitative. We did actually incorporate categorical information in the form of colors of the glyph (insomniacs and normal sleepers being colored differently) and in tooltips.\nBut what if we wanted a single type of measurement, such as percent correct in the facial identification, but were interested in delineating performance of insomniacs and normal sleepers. Here, we have the quantitative percent correct data and the categorical sleeper type. One of our axes is now categorical.\nNote that this kind of plot is commonly encountered in the biological sciences. We repeat a measurement many times for given test conditions and wish to compare the results. The different conditions are the categories, and the axis along which the conditions are represented is called a categorical axis. The quantitative axis contains the result of the measurements from each condition.\nThe rest of this lesson is mostly for reference so you can see how to handle categorical axes with Bokeh. In practice, we will mostly be using iqplot to do this and it is done for your automatically. You may therefore skip the rest of this notebook if you like.\nThat said, for some plotting applications, you may need to adjust details or do things outside of iqplot’s capabilities, so the contents of this lesson can be useful.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Plots with categorical variables</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/categorical_axes_bokeh.html#types-of-data-for-plots",
    "href": "lessons/eda/plotting/categorical_axes_bokeh.html#types-of-data-for-plots",
    "title": "9  Plots with categorical variables",
    "section": "",
    "text": "Quantitative data may have continuously varying (and therefore ordered) values.\nCategorical data has discrete, unordered values that a variable can take.\nOrdinal data has discrete, ordered values. Integers are a classic example.\nTemporal data refers to time, which can be represented as dates.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Plots with categorical variables</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/categorical_axes_bokeh.html#making-a-bar-graph-with-bokeh",
    "href": "lessons/eda/plotting/categorical_axes_bokeh.html#making-a-bar-graph-with-bokeh",
    "title": "9  Plots with categorical variables",
    "section": "9.2 Making a bar graph with Bokeh",
    "text": "9.2 Making a bar graph with Bokeh\nTo demonstrate how to set up a categorical axis with Bokeh, I will make a bar graph of the mean percent correct for insomniacs and normal sleepers. But before I even begin this, I will give you the following piece of advice: Don’t make bar graphs. More on that in a moment.\n\n9.2.1 Setting up a data frame for plotting\nBefore making a plot, we need to set up a data frame amenable for the type of plot we want. We start by reading in the data set and computing the 'insomnia' column, which gives Trues and Falses, as we’ve done in the preceding parts of this lesson.\n\nfname = os.path.join(data_path, \"gfmt_sleep.csv\")\ndf = pl.read_csv(fname, null_values=\"*\")\ndf = df.with_columns((pl.col('sci') &lt;= 16).alias('insomnia'))\n\nFor convenience in plotting the categorical axis, we would rather not have the values on the axis be True or False, but something more descriptive, like insomniac and normal. So, let’s make a column in the data frame, 'sleeper' that has that for us. We can use the replace_strict() method, which take a dictionary as an argument, replacing all instances given by a key in the dictionary with the corresponding value. When using this method, it is good practice to specify the data type of the resulting column (though Polars will attempt to infer it).\n\ndf = df.with_columns(\n    pl.col('insomnia')\n    .replace_strict({True: 'insomniac', False: 'normal'}, return_dtype=pl.String)\n    .alias('sleeper')\n)\n\nNext, we need to make a data frame that has the mean percent correct for each of the two categories of sleeper. We have decided that it is the mean of the respective measurements that will set the height of the bars.\n\ndf_mean = df.group_by(\"sleeper\").agg(pl.col(\"percent correct\").mean())\n\n# Take a look\ndf_mean\n\n\nshape: (2, 2)\n\n\n\nsleeper\npercent correct\n\n\nstr\nf64\n\n\n\n\n\"normal\"\n81.461039\n\n\n\"insomniac\"\n76.1\n\n\n\n\n\n\nNow we’re ready to make the bar graph. Note that we now have only two data points that we are showing on the plot. We have decided to throw out a lot of information from the data we collected to display only two values. Does this strike you as a terrible idea? It should. Don’t do this. We’re just doing it to show how categorical axes are set up using Bokeh.\n\n\n9.2.2 Setting up categorical axes\nTo set up a categorical axis, you need to specify the x_range (or y_range if you want the y-axis to be categorical) as a list with the categories you want on the axis when you instantiate the figure. I will make a horizontal bar graph, so I will specify y_range. I also want my quantitative axis (x in this case) to go from zero to 100, since it signifies a percent. Also, when I instantiate this figure, because it is not very tall and I do not want the reset tool cut off, I will also explicitly set the tools I want in the toolbar.\n\np = bokeh.plotting.figure(\n    height=200,\n    width=400,\n    x_axis_label=\"percent correct\",\n    x_range=[0, 100],\n    y_range=['insomniac', 'normal'],\n    tools=\"save\",\n)\n\nNow that we have the figure, we can put the bars on. The p.hbar() method populates the figure with horizontal bar glyphs. The right kwarg says what column of the data source dictates how far to the right to show the bar, while the height kwarg says how think the bars are.\nI will also turn off the grid lines on the categorical axis, which is commonly done.\n\np.hbar(\n    source=df_mean.to_dict(),\n    y=\"sleeper\",\n    right=\"percent correct\",\n    height=0.6,\n)\n\n# Turn off gridlines on categorical axis\np.ygrid.grid_line_color = None\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe similarly make vertical bar graphs specifying x_range and using p.vbar().\n\np = bokeh.plotting.figure(\n    height=250,\n    width=250,\n    x_range=['normal', 'insomniac'],\n    y_range=[0, 100],\n    y_axis_label=\"average percent correct\",\n)\n\np.vbar(\n    source=df_mean.to_dict(),\n    x=\"sleeper\",\n    top=\"percent correct\",\n    width=0.6,\n)\n\np.xgrid.grid_line_color = None\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Plots with categorical variables</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/categorical_axes_bokeh.html#nested-categorical-axes",
    "href": "lessons/eda/plotting/categorical_axes_bokeh.html#nested-categorical-axes",
    "title": "9  Plots with categorical variables",
    "section": "9.3 Nested categorical axes",
    "text": "9.3 Nested categorical axes\nWe may wish to make a bar graph where we have four bars, normal and insomniac for males and also normal and insomniac for females. To start, we will have to re-make the df_mean data frame, now grouping by gender and sleeper. Furthermore, it will be nicer to label the categories as “female” and “male” instead of “f” and “m”.\n\ndf_mean = (\n    df\n    .group_by([\"gender\", \"sleeper\"])\n    .agg(pl.col(\"percent correct\").mean())\n).with_columns(\n    pl.col('gender')\n    .replace_strict(dict(f='female', m='male'), return_dtype=pl.String)\n    .alias('gender')\n)\n\n# Take a look\ndf_mean\n\n\nshape: (4, 3)\n\n\n\ngender\nsleeper\npercent correct\n\n\nstr\nstr\nf64\n\n\n\n\n\"female\"\n\"normal\"\n82.045455\n\n\n\"female\"\n\"insomniac\"\n73.947368\n\n\n\"male\"\n\"normal\"\n80.0\n\n\n\"male\"\n\"insomniac\"\n82.916667\n\n\n\n\n\n\nBecause of the way Bokeh handles nested categories, we need to create a new column that has a list corresponding to the nested category. To make this, we use the pl.concat_list() function.\n\ndf_mean = df_mean.with_columns(pl.concat_list('gender', 'sleeper').alias('cats'))\n\n# Take a look\ndf_mean\n\n\nshape: (4, 4)\n\n\n\ngender\nsleeper\npercent correct\ncats\n\n\nstr\nstr\nf64\nlist[str]\n\n\n\n\n\"female\"\n\"normal\"\n82.045455\n[\"female\", \"normal\"]\n\n\n\"female\"\n\"insomniac\"\n73.947368\n[\"female\", \"insomniac\"]\n\n\n\"male\"\n\"normal\"\n80.0\n[\"male\", \"normal\"]\n\n\n\"male\"\n\"insomniac\"\n82.916667\n[\"male\", \"insomniac\"]\n\n\n\n\n\n\nNext, we need to set up factors, which give the nested categories. We could extract them from the 'cats' column of the data frame as\nfactors = list(df_mean.get_column('cats'))\nInstead, we will specify them by hand to ensure they are ordered as we would like.\n\nfactors = [\n    (\"female\", \"normal\"),\n    (\"female\", \"insomniac\"),\n    (\"male\", \"normal\"),\n    (\"male\", \"insomniac\"),\n]\n\nFinally, to use these factors in a y_range (or x_range), we need to convert them to a factor range using bokeh.models.FactorRange().\n\np = bokeh.plotting.figure(\n    height=200,\n    width=400,\n    x_axis_label=\"average percent correct\",\n    x_range=[0, 100],\n    y_range=bokeh.models.FactorRange(*factors),\n    tools=\"save\",\n)\n\nNow we are ready to add the bars, taking care to specify the 'cats' column for our y-values.\n\np.hbar(\n    source=df_mean.to_dict(),\n    y=\"cats\",\n    right=\"percent correct\",\n    height=0.6,\n)\n\np.ygrid.grid_line_color = None\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Plots with categorical variables</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/categorical_axes_bokeh.html#computing-environment",
    "href": "lessons/eda/plotting/categorical_axes_bokeh.html#computing-environment",
    "title": "9  Plots with categorical variables",
    "section": "9.4 Computing environment",
    "text": "9.4 Computing environment\n\n%load_ext watermark\n%watermark -v -p polars,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\npolars    : 1.33.1\nbokeh     : 3.7.3\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Plots with categorical variables</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/visualizing_distributions.html",
    "href": "lessons/eda/plotting/visualizing_distributions.html",
    "title": "10  Visualizing distributions",
    "section": "",
    "text": "10.1 Box plots\n| Download notebook\nData set download\nYou can think of an experiment as sampling out of a probability distribution. To make this more concrete, imagine measuring the lengths of eggs laid by a given hen. Each egg will be of a different length than others, but all of the eggs will be about six centimeters long. The probability of getting an egg more than eight centimeters long is very small, whereas the probability of getting an egg between 5.5 and 6.5 centimeters is high. The generative probability distribution, the distribution from which experimental data are sampled, then, has high probability mass around six centimeters and low probability mass away from that. We cannot know the generative distribution. We can approximate it with generative models (which we will do in the latter part of the course).\nHere, we will investigate how to make plots to investigate properties about the (unknown) generative distribution of a set of repeated measurements. We will use the iqplot to make the plots, but will withhold dwelling on its syntax until the next notebook of this lesson. Instead, we will make a given plot and then discuss how it helps us visualize the underlying generative probability distribution of a data set.\nWe will continue using the facial recognition data set. We will make the usual adjustments by adding an 'insomnia' column and also a 'sleeper' column that more meaningfully indicates where the subject is an insomniac or a normal sleeper (with the words “normal” or “insomniac” instead of True and False).\nWe saw in the previous part of the lesson that bar graphs throw out most of the information present in a data set. They only give an approximation of the mean of the generative distribution and nothing else. We can instead report more information.\nA box-and-whisker plot, also just called a box plot is a better option than a bar graph. Indeed, it was invented by John Tukey himself. Instead of condensing your measurements into one value (or two, if you include an error bar) like in a bar graph, you condense them into at least five. It is easier to describe a box plot if you have one to look at.\np = iqplot.box(\n    df,\n    \"percent correct\",\n    cats=[\"gender\", \"sleeper\"],\n    box_kwargs=dict(fill_color=\"#1f77b4\"),\n)\n\nbokeh.io.show(p)\nThe top of a box is the 75th percentile of the measured data. That means that 75 percent of the measurements were less than the top of the box. The bottom of the box is the 25th percentile. The line in the middle of the box is the 50th percentile, also called the median. Half of the measured quantities were less than the median, and half were above. The total height of the box encompasses the measurements between the 25th and 75th percentile, and is called the interquartile region, or IQR. The top whisker extends to the minimum of these two quantities: the largest measured data point and the 75th percentile plus 1.5 times the IQR. Similarly, the bottom whisker extends to the maximum of the smallest measured data point and the 25th percentile minus 1.5 times the IQR. Any data points not falling between the whiskers are then plotted individually, and are typically termed outliers. Note that “outlier” is just a name; it does not imply anything special we should consider in those data points.\nSo, box-and-whisker plots give much more information than a bar plot. They give a reasonable summary of how data are distributed by providing quantiles.\nNot to draw our focus away from visualizing how a data set is distributed, going forward in this notebook, we will not split the data set by gender and sleep preference, but will instead look at the entire data set. Here is a box plot for that.\np = iqplot.box(df, \"percent correct\", frame_height=100)\np.yaxis.visible = False\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing distributions</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/visualizing_distributions.html#plot-all-your-data",
    "href": "lessons/eda/plotting/visualizing_distributions.html#plot-all-your-data",
    "title": "10  Visualizing distributions",
    "section": "10.2 Plot all your data",
    "text": "10.2 Plot all your data\nWhile the box plot is better than a bar graph because it shows quantiles and not just a mean, it is still hiding much of the structure of the data set. Conversely, when plotting x-y data in a scatter plot, you plot all of your data points. Shouldn’t the same be true for plots with categorical variables? You went through all the work to get the data; you should show them all!",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing distributions</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/visualizing_distributions.html#strip-plots",
    "href": "lessons/eda/plotting/visualizing_distributions.html#strip-plots",
    "title": "10  Visualizing distributions",
    "section": "10.3 Strip plots",
    "text": "10.3 Strip plots\nOne convenient way to plot all of your data is a strip plot. In a strip plot, every point is plotted.\n\np = iqplot.strip(df, \"percent correct\", frame_height=100)\np.yaxis.visible = False\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nAn obvious problem with this plot is that the data points overlap. We can get around this issue by adding a jitter to the plot. Instead of lining all of the data points up exactly in line with the category, we randomly “jitter” the points about the centerline, of course while maintaining their position along the quantitative axis.\n\np = iqplot.strip(df, \"percent correct\", frame_height=100, spread=\"jitter\")\np.yaxis.visible = False\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nAlternatively, we can deterministically spread the points in a swarm plot, where the glyphs are positioned so as not to overlap, but retain the same position along the quantitative axis.\n\np = iqplot.strip(df, \"percent correct\", frame_height=100, spread=\"swarm\")\np.yaxis.visible = False\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis plot allows us to make out how the data are distributed. We suspect there is more probability density around 85% or so, with a heavy tail heading toward lower values, ending at 40.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing distributions</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/visualizing_distributions.html#spike-plots",
    "href": "lessons/eda/plotting/visualizing_distributions.html#spike-plots",
    "title": "10  Visualizing distributions",
    "section": "10.4 Spike plots",
    "text": "10.4 Spike plots\nThe swarm plot above essentially provides a count of the number of times a given measurement was observed. Because of the nature of the GFMT, the percent correct values are discrete, coming in 2.5% increments. When we have discrete values like that, a spike plot serves to provide the counts of each measurement value.\n\np = iqplot.spike(df, \"percent correct\")\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nEach spike rises to the number of times a given value was measured.\nSpike plots fail, however, when the measurements do not take on discrete values. To demonstrate, we will add random noise to the percent correct data and try replotting. (We will learn about random number generation with NumPy in a future lesson. For now, this is for purposes of discussing plotting options.)\n\n# Add a bit of noise to the % correct measurements so they are no longer discrete\nrng = np.random.default_rng()\n\ndf = df.with_columns(\n    pl.col('percent correct')\n    .map_elements(\n        lambda s: s + rng.normal(0, 0.5),\n        return_dtype=float\n    )\n    .alias(\"percent correct with noise\")\n)\n\n# Replot swarm plot\np = iqplot.strip(df, \"percent correct with noise\", frame_height=100, spread=\"swarm\")\np.yaxis.visible = False\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe can still roughly make out how the data are distributed in the swarm plot, but the spike plot adds little beyond what we would see with a strip plot sans swarm or jitter.\n\np = iqplot.spike(df, \"percent correct with noise\", frame_height=100)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing distributions</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/visualizing_distributions.html#histograms",
    "href": "lessons/eda/plotting/visualizing_distributions.html#histograms",
    "title": "10  Visualizing distributions",
    "section": "10.5 Histograms",
    "text": "10.5 Histograms\nIf we do not have discrete data, we can instead use a histogram. A histogram is constructed by dividing the measurement values into bins and then counting how many measurements fall into each bin. The bins are then displayed graphically.\nA problem with histograms is that they require a choice of binning. Different choices of bins can lead to qualitatively different appearances of the plot. One choice of number of bins is the Freedman-Diaconis rule, which serves to minimize the integral of the squared difference between the (unknown) underlying probability density function and the histogram. This is the default of iqplot.\n\np = iqplot.histogram(df, \"percent correct with noise\")\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe rug plot at the bottom of the histogram shows all of the measurements (following the “plot all your data” rule).",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing distributions</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/visualizing_distributions.html#ecdfs",
    "href": "lessons/eda/plotting/visualizing_distributions.html#ecdfs",
    "title": "10  Visualizing distributions",
    "section": "10.6 ECDFs",
    "text": "10.6 ECDFs\nHistograms are typically used to display how data are distributed. As an example I will generate Normally distributed data and plot the histogram.\n\nx = rng.normal(size=500)\nbokeh.io.show(iqplot.histogram(x, rug=False, style=\"step_filled\"))\n\n\n  \n\n\n\n\n\nThis looks similar to the standard Normal curve we are used to seeing and is a useful comparison to a probability density function (PDF). However, Histograms suffer from binning bias. By binning the data, you are not plotting all of them. In general, if you can plot all of your data, you should. For that reason, I prefer not to use histograms for studying how data are distributed, but rather prefer to use ECDFs, which enable plotting of all data.\nThe ECDF evaluated at x for a set of measurements is defined as\n\\[\\begin{align}\n\\text{ECDF}(x) = \\text{fraction of measurements } \\le x.\n\\end{align}\\]\nWhile the histogram is an attempt to visualize a probability density function (PDF) of a distribution, the ECDF visualizes the cumulative density function (CDF). The CDF, \\(F(x)\\), and PDF, \\(f(x)\\), both completely define a univariate distribution and are related by\n\\[\\begin{align}\nf(x) = \\frac{\\mathrm{d}F}{\\mathrm{d}x}.\n\\end{align}\\]\nThe definition of the ECDF is all that you need for interpretation. For a given value on the x-axis, the value of the ECDF is the fraction of observations that are less than or equal to that value. Once you get used to looking at CDFs, they will become as familiar as PDFs. A peak in a PDF corresponds to an inflection point in a CDF.\nTo make this more clear, let us look at plot of a PDF and ECDF for familiar distributions, the Gaussian and Binomial.\n\n\n\n\nPDFs and CDFs\n\n\n\nNow that we know what an ECDF is, we can plot it.\n\nbokeh.io.show(iqplot.ecdf(x, style='dots'))\n\n\n  \n\n\n\n\n\nEach dot in the ECDF is a single data point that we measured. Given the above definition of the ECDF, it is defined for all real \\(x\\). So, formally, the ECDF is a continuous function (with discontinuous derivatives at each data point). So, it could be plotted like a staircase according to the formal definition.\n\nbokeh.io.show(iqplot.ecdf(x))\n\n\n  \n\n\n\n\n\nEither method of plotting is fine; there is not any less information in one than the other.\nLet us now plot the percent correct data as an ECDF, both with dots and as a staircase. Visualizing it this way helps highlight the relationship between the dots and the staircase.\n\np = iqplot.ecdf(df, \"percent correct\")\np = iqplot.ecdf(df, \"percent correct\", p=p, style='dots', marker_kwargs=dict(color=\"orange\"))\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe circles are on the concave corners of the staircase.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing distributions</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/visualizing_distributions.html#computing-environment",
    "href": "lessons/eda/plotting/visualizing_distributions.html#computing-environment",
    "title": "10  Visualizing distributions",
    "section": "10.7 Computing environment",
    "text": "10.7 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\npolars    : 1.33.1\nbokeh     : 3.7.3\niqplot    : 0.3.7\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Visualizing distributions</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html",
    "href": "lessons/eda/plotting/iqplot.html",
    "title": "11  High level plotting with iqplot",
    "section": "",
    "text": "11.1 Plots with categorical variables\n| Download notebook\nIn this lesson, we do some plotting with a high-level package iqplot. For fun, we will use a data set from Kleinteich and Gorb that features data about strikes of the tongues of frogs on a target. Let’s get the data frame loaded in so we can be on our way.\nLet us first consider the different kinds of data we may encounter as we think about constructing a plot.\nIn practice, ordinal data can be cast as quantitative or treated as categorical with an ordering enforced on the categories (e.g., categorical data [1, 2, 3] becomes ['1', '2', '3'].). Temporal data can also be cast as quantitative, (e.g., second from the start time). We will therefore focus out attention on quantitative and categorical data.\nWhen we made scatter plots in the previous lesson, both types of data were quantitative. We did actually incorporate categorical information in the form of colors of the glyph (insomniacs and normal sleepers being colored differently) and in tooltips.\nBut what if we wanted a single type of measurement, say impact force, for each frog? Here, we have the quantitative impact force data and the categorical frog ID data. One of our axes is now categorical.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#plots-with-categorical-variables",
    "href": "lessons/eda/plotting/iqplot.html#plots-with-categorical-variables",
    "title": "11  High level plotting with iqplot",
    "section": "",
    "text": "Quantitative data may have continuously varying (and therefore ordered) values.\nCategorical data has discrete, unordered values that a variable can take.\nOrdinal data has discrete, ordered values. Integers are a classic example.\nTemporal data refers to time, which can be represented as dates.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#bar-graph",
    "href": "lessons/eda/plotting/iqplot.html#bar-graph",
    "title": "11  High level plotting with iqplot",
    "section": "11.2 Bar graph",
    "text": "11.2 Bar graph\nTo demonstrate how to set up a categorical axis with Bokeh, I will make a bar graph of the mean impact force for each of the four frogs. But before I even begin this, I will give you the following piece of advice: Don’t make bar graphs. More on that in a moment.\nBefore we do that, we need to compute the means from the inputted data frame.\n\ndf_mean = df[['ID', 'impact force (mN)']].group_by('ID').mean()\n\n# Take a look\ndf_mean\n\n\nshape: (4, 2)\n\n\n\nID\nimpact force (mN)\n\n\nstr\nf64\n\n\n\n\n\"III\"\n550.1\n\n\n\"I\"\n1530.2\n\n\n\"II\"\n707.35\n\n\n\"IV\"\n419.1\n\n\n\n\n\n\nTo set up a categorical axis, you need to specify the x_range (or y_range if you want the y-axis to be categorical) as a list with the categories you want on the axis when you instantiate the figure. I will make a horizontal bar graph, so I will specify y_range. Also, when I instantiate this figure, because it is not very tall and I do not want the reset tool cut off, I will also explicitly set the tools I want in the toolbar.\n\np = bokeh.plotting.figure(\n    frame_height=200,\n    frame_width=400,\n    x_axis_label='impact force (mN)',\n    y_range=list(df_mean['ID'].sort(descending=True)),\n    tools='pan,wheel_zoom,save,reset'\n)\n\nNow that we have the figure, we can put the bars on. The p.hbar() method populates the figure with horizontal bar glyphs. The right kwarg says what column of the data source dictates how far to the right to show the bar, while the height kwarg says how think the bars are.\nI will also ensure the quantitative axis starts at zero and turn off the grid lines on the categorical axis, which is commonly done.\n\np.hbar(\n    source=df_mean.to_dict(),\n    y='ID',\n    right='impact force (mN)',\n    height=0.6\n)\n\n# Turn off gridlines on categorical axis\np.ygrid.grid_line_color = None\n\n# Start axes at origin on quantitative axis\np.x_range.start = 0\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nWe similarly make vertical bar graphs specifying x_range and using p.vbar().\n\np = bokeh.plotting.figure(\n    frame_height=250,\n    frame_width=250,\n    y_axis_label='impact force (mN)',\n    x_range=list(df_mean['ID'].sort()),\n)\n\np.vbar(\n    source=df_mean.to_dict(),\n    x='ID',\n    top='impact force (mN)',\n    width=0.6\n)\n\np.xgrid.grid_line_color = None\np.y_range.start = 0\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#iqplot",
    "href": "lessons/eda/plotting/iqplot.html#iqplot",
    "title": "11  High level plotting with iqplot",
    "section": "11.3 iqplot",
    "text": "11.3 iqplot\nGenerating the bar graphs was not too painful, even tough we used Bokeh, a low-level plotting library. Nonetheless, we would like to make plots more declaratively. We do not want to have to explicitly pre-process the data, set up the categorical axis, etc. We would like to just provide a data set, say which column(s) is/are categorical and which is quantitative, and then just get our plot.\niqplot generates plots from tidy data frames where one or more columns contain categorical data and the column of interest in the plot is quantitative.\nThere are seven types of plots that iqplot can generate. As you will see, all four of these modes of plotting are meant to give a picture about how the quantitative measurements are distributed for each category.\n\nBox plots: iqplot.box()\nStrip plots: iqplot.strip()\nSpike plots: iqplot.spike()\nStrip-box plots (strip and box plots overlaid): iqplot.stripbox()\nHistograms: iqplot.histogram()\nStrip-histogram plots (strip and histograms overlaid): iqplot.striphistogram()\nECDFs: iqplot.ecdf()\n\nThis first seven arguments are the same for all plots. They are:\n\ndata: A tidy data frame or Numpy array.\nq: The column of the data frame to be treated as the quantitative variable.\ncats: A list of columns in the data frame that are to be considered as categorical variables in the plot. If None, a single box, strip, histogram, or ECDF is plotted.\nq_axis: Along which axis, x or y that the quantitative variable varies. The default is 'x'.\npalette: A list of hex colors to use for coloring the markers for each category. By default, it uses the Glasbey Category 10 color palette from colorcet.\norder: If specified, the ordering of the categories to use on the categorical axis and legend (if applicable). Otherwise, the order of the inputted data frame is used.\np: If specified, the bokeh.plotting.Figure object to use for the plot. If not specified, a new figure is created.\n\nIf data is given as a Numpy array, it is the only required argument. If data is given as a Pandas DataFrame, q must also be supplied. All other arguments are optional and have reasonably set defaults. Any extra kwargs not in the function call signature are passed to bokeh.plotting.figure() when the figure is instantiated.\nWith this in mind, we will put iqplot to use on facial identification data set to demonstrate how we can make each of the seven kinds of plots using the frog data set.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#box-plots-with-iqplot",
    "href": "lessons/eda/plotting/iqplot.html#box-plots-with-iqplot",
    "title": "11  High level plotting with iqplot",
    "section": "11.4 Box plots with iqplot",
    "text": "11.4 Box plots with iqplot\nAs I discuss below, bar graphs are almost never a good choice for visualization. You distill all of the information in the data set down to one or two summary statistics, and then use giant glyphs to show them. As a start for improvement, you could distill the data set down to five or so summary statistics and show those graphically, as opposed to just one or two.\nBox plots provide such a summary. I will first make one using iqplot.box() and then describe how a box plot is interpreted.\n\np = iqplot.box(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThe line in the middle of each box is the median and the top and bottom of the box at the 75th and 25th percentiles, respectively. The distance between the 25th and 75th percentiles is called the interquartile range, or IQR. The whiskers of the box plot extend to the most extreme data point within 1.5 times the interquartile range. If any data points are more extreme than the end of the whisker, they are shown individually, and are often referred to as outliers.\nA box plot can use a useful visualization if you have many data points and it is difficult to plot them all. I rarely find that there are situations where all data cannot be plotted, either with strip plots of ECDFs, which we will cover in a moment, so I generally do not use box plots. Nonetheless, I do not find them too objectionable, as they effectively display important nonparametric summary statistics of your data set.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#plot-all-your-data",
    "href": "lessons/eda/plotting/iqplot.html#plot-all-your-data",
    "title": "11  High level plotting with iqplot",
    "section": "11.5 Plot all your data",
    "text": "11.5 Plot all your data\nBox plots summarize a data set with summary statistics, but what not plot all your data? You work hard to acquire them. You should show them all. This is a mantra to live by.\n\nPlot all of your data.\n\nLet’s do that now.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#strip-plots",
    "href": "lessons/eda/plotting/iqplot.html#strip-plots",
    "title": "11  High level plotting with iqplot",
    "section": "11.6 Strip plots",
    "text": "11.6 Strip plots\nA strip plot is like a scatter plot; it puts a glyph for every measured data point. The only difference is that one of the axes is categorical. In this case, you are plotting all of your data.\n\np = iqplot.strip(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis is a good plot to make since you are plotting all of your data, but it does have the problem that you cannot tell if multiple data points overlap. We will deal with this in the next lesson when we discuss styling plots.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#spike-plots",
    "href": "lessons/eda/plotting/iqplot.html#spike-plots",
    "title": "11  High level plotting with iqplot",
    "section": "11.7 Spike plots",
    "text": "11.7 Spike plots\nA spike plot is useful when you want to see how many times a specific value was encountered in your data set. Let’s make a spike plot for the number of times specific impact times were observed.\n\np = iqplot.spike(\n    df,\n    q='impact time (ms)'\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nNotice that an impact time of 31 ms was observed eight times, though most impact times were observed once or twice. In the above plot, we have not had a categorical variable to demonstrate how a spike plot looks. If we do have a categorical variable, it is more difficult to display counts on the y-axis, so the proportion of the measurements with a given value are displayed.\n\np = iqplot.spike(\n    df,\n    q='impact time (ms)',\n    cats='ID',\n)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#strip-box-plots",
    "href": "lessons/eda/plotting/iqplot.html#strip-box-plots",
    "title": "11  High level plotting with iqplot",
    "section": "11.8 Strip-box plots",
    "text": "11.8 Strip-box plots\nIt is sometimes useful to overlay strip plots with box plots, as the box plots show useful quantile information. This is accomplished using the iqplot.stripbox() function.\n\np = iqplot.stripbox(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#histograms",
    "href": "lessons/eda/plotting/iqplot.html#histograms",
    "title": "11  High level plotting with iqplot",
    "section": "11.9 Histograms",
    "text": "11.9 Histograms\nIn plotting all of our data in a strip plot, we can roughly see how the data are distributed. There are more measurements where there are more glyphs. We ofter seek to visualize the distribution of the data. Histograms are commonly used for this. They are typically interpreted to as an empirical representation of the probability density function.\n\np = iqplot.histogram(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nNote that by default, iqplot includes a rug plot at the bottom of the histogram, showing each measurement. The number of bins are automatically chosen using the Freedman-Diaconis rule.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#strip-histogram",
    "href": "lessons/eda/plotting/iqplot.html#strip-histogram",
    "title": "11  High level plotting with iqplot",
    "section": "11.10 Strip-histogram",
    "text": "11.10 Strip-histogram\nStrip plots may also be combined with histograms. By default, the histograms are normalized and mirrored, similar to a violin plot.\n\np = iqplot.striphistogram(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#ecdfs",
    "href": "lessons/eda/plotting/iqplot.html#ecdfs",
    "title": "11  High level plotting with iqplot",
    "section": "11.11 ECDFs",
    "text": "11.11 ECDFs\nI just mentioned that histograms are typically used to display how data are distributed, but it was hard to make out the distributions in the above plot, partly because we do not have very many measurements. As another example I will generate Normally distributed data and plot the histogram. (We will learn how to generate data like this when we study random number generation with NumPy in a future lesson. For now, this is for purposes of discussing plotting options.)\nNote that iqplot can take a Numpy array as the data argument, and makes a plot assuming it contains a single data set.\n\n# Generate normally distributed data\nrng = np.random.default_rng(3252)\nx = rng.normal(size=500)\n\n# Plot the histogram\np = iqplot.histogram(x, rug=False)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis looks similar to the standard Normal curve we are used to seeing and is a useful comparison to a probability density function (PDF). However, Histograms suffer from binning bias. By binning the data, you are not plotting all of them. In general, if you can plot all of your data, you should. For that reason, I prefer not to use histograms for studying how data are distributed, but rather prefer to use ECDFs, which enable plotting of all data.\nThe ECDF evaluated at x for a set of measurements is defined as\n\\[\\begin{align}\n\\text{ECDF}(x) = \\text{fraction of measurements } \\le x.\n\\end{align}\\]\nWhile the histogram is an attempt to visualize a probability density function (PDF) of a distribution, the ECDF visualizes the cumulative density function (CDF). The CDF, \\(F(x)\\), and PDF, \\(f(x)\\), both completely define a univariate distribution and are related by\n\\[\\begin{align}\nf(x) = \\frac{\\mathrm{d}F}{\\mathrm{d}x}.\n\\end{align}\\]\nThe definition of the ECDF is all that you need for interpretation. Once you get used to looking at CDFs, they will become as familiar to you as PDFs. A peak in a PDF corresponds to an inflection point in a CDF.\nTo make this more clear, let us look at plot of a PDF and ECDF for familiar distributions, the Gaussian and Binomial.\n\n\n\n\n\n\nFigure 11.1: Top: The PDF (left) and CDF (right) of the Normal (a.k.a. Gaussian) distribution. Bottom: The PMF (left) and CDF (right) of the Binomial distribution.\n\n\n\nNow that we know how to interpret ECDFs, lets plot the ECDF for our dummy Normally-distributed data.\n\np = iqplot.ecdf(x)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nLet’s make a set of ECDFs for our frog data.\n\np = iqplot.ecdf(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThough we do not see points, this is still plotting all of your data. The concave corners of the staircase correspond to the measured data. This can be seen by overlaying the “dot” version of the ECDFs.\n\np = iqplot.ecdf(\n    data=df,\n    q=\"impact force (mN)\",\n    cats=\"ID\",\n    p=p,\n    style='dots',\n    show_legend=False,\n)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#sec-no-bar-graphs",
    "href": "lessons/eda/plotting/iqplot.html#sec-no-bar-graphs",
    "title": "11  High level plotting with iqplot",
    "section": "11.12 Don’t make bar graphs",
    "text": "11.12 Don’t make bar graphs\nBar graphs, especially with error bars (in which case they are called dynamite plots), are typically awful. They are pervasive in biology papers. I have yet to find a single example where a bar graph is the best choice. Strip plots (with jitter) or even box plots, are more informative and almost always preferred. In fact, ECDFs are often better even than these. Here is a simple message:\n\nDon’t make bar graphs.\n\nWhat should I do instead you ask? The answer is simple: plot all of your data when you can. If you can’t, box plots are always better than bar graphs.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/iqplot.html#computing-environment",
    "href": "lessons/eda/plotting/iqplot.html#computing-environment",
    "title": "11  High level plotting with iqplot",
    "section": "11.13 Computing environment",
    "text": "11.13 Computing environment\n\n\nCode\n%load_ext watermark\n%watermark -v -p numpy,polars,bokeh,iqplot,jupyterlab\n\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\npolars    : 1.27.1\nbokeh     : 3.6.2\niqplot    : 0.3.7\njupyterlab: 4.3.6",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>High level plotting with iqplot</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/styling_bokeh.html",
    "href": "lessons/eda/plotting/styling_bokeh.html",
    "title": "12  Styling Bokeh plots",
    "section": "",
    "text": "12.1 Styling Bokeh plots as they are built\n| Download notebook\nWe have seen how to use Bokeh (and the higher-level plotting package iqplot) to make interactive plots. We have seen how to adjust plot size, axis labels, glyph color, etc. We have also seen how to style plots generated with iqplot. But we have just started to touch the surface of how we might customize plots. In this lesson, we investigate ways to stylize Bokeh plots to our visual preferences.\nWe will again make use of the face-matching data set. We’ll naturally start by loading the data set.\nBokeh figures and renderers (which are essentially the glyphs) have a plethora of attributes pertaining to visual appearance that may be adjusted at instantiation and after making a plot.\nA color palette an ordering of colors that are used for glyphs, usually corresponding to categorical data. Colorcet’s Glasbey Category 10 provides a good palette for categorical data, and we store this as our categorical colors for plotting.\ncat_colors = colorcet.b_glasbey_category10\nNow we can build the plot. Since the data are percentages, we will set the axes to go from zero to 100 and enforce that the figure is square. We will also include a title as well so we can style that.\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=300,\n    x_axis_label=\"confidence when correct\",\n    y_axis_label=\"confidence when incorrect\",\n    title=\"GMFT with sleep conditions\",\n    x_range=[0, 100],\n    y_range=[0, 100],\n)\nIn styling this plot, we will also put the legend outside of the plot area. This is a bit trickier than what we have been doing using the legend_label kwarg in p.scatter(). To get a legend outside of the plot area, we need to:\nNow, we add the glyphs, storing them as variables normal_glyph and insom_glyph.\nnormal_glyph = p.scatter(\n    source=df.filter(~pl.col('insomnia')).to_dict(),\n    x=\"confidence when correct\",\n    y=\"confidence when incorrect\",\n    color=cat_colors[0],\n)\n\ninsom_glyph = p.scatter(\n    source=df.filter(pl.col('insomnia')).to_dict(),\n    x=\"confidence when correct\",\n    y=\"confidence when incorrect\",\n    color=cat_colors[1],\n)\nNow we can construct and add the legend.\n# Construct legend items\nlegend_items = [('normal', [normal_glyph]), ('insomnia', [insom_glyph])]\n\n# Instantiate legend\nlegend = bokeh.models.Legend(items=legend_items, click_policy='hide')\n\n# Add the legend to the right of the plot\np.add_layout(legend, 'right')\nNow, let’s take a look at this beauty!\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Styling Bokeh plots</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/styling_bokeh.html#styling-bokeh-plots-as-they-are-built",
    "href": "lessons/eda/plotting/styling_bokeh.html#styling-bokeh-plots-as-they-are-built",
    "title": "12  Styling Bokeh plots",
    "section": "",
    "text": "Assign each glyph to a variable.\nInstantiate a bokeh.models.Legend object using the stored variables containing the glyphs. This is instantiated as bokeh.models.Legend(items=legend_items), where legend_items is a list of 2-tuples. In each 2-tuple, the first entry is a string with the text used to label the glyph. The second entry is a list of glyphs that have the label.\nAdd the legend to the figure using the add_layout() method.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Styling Bokeh plots</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/styling_bokeh.html#styling-bokeh-plots-after-they-are-built",
    "href": "lessons/eda/plotting/styling_bokeh.html#styling-bokeh-plots-after-they-are-built",
    "title": "12  Styling Bokeh plots",
    "section": "12.2 Styling Bokeh plots after they are built",
    "text": "12.2 Styling Bokeh plots after they are built\nAfter building a plot, we sometimes want to adjust styling. To do so, we need to change attributes of the object p. For example, let’s look at the font of the x-axis label.\n\np.xaxis.axis_label_text_font\n\n'helvetica'\n\n\nWe can also look at the style and size of the font.\n\np.xaxis.axis_label_text_font_style, p.xaxis.axis_label_text_font_size\n\n('italic', '13px')\n\n\nSo, the default axis labels for Bokeh are italicized 13 pt Helvetica. I personally think this choice is fine, but we may have other preferences.\nTo find out all of the available options to tweak, I usually type something like p. and hit tab to see what the options are. Finding p.xaxis is an option, then type p.xaxis. and hit tab again to see the styling option there.\nUsing this technique, we can set some obnoxious styling for this plot. I will make all of the fonts non-italicized, large papyrus. I can also set the background and grid colors. Note that in Bokeh, any named CSS color or any valid HEX code, entered as a string, is a valid color.\nBefore we do the obnoxious styling, we will do one adjustment that is useful. Note in the above plot that the glyphs at the end of the plot are cropped. We would like the whole glyph to show. To do that, we set the level of the glyphs to be 'overlay'. To do that, we extract the first two elements of the list of renderers, which contains the glyphs, and set the level attribute.\n\np.renderers[0].level = 'overlay'\np.renderers[1].level = 'overlay'\n\nNow we can proceed to make our obnoxious styling.\n\n# Obnoxious fonts\np.xaxis.major_label_text_font = 'papyrus'\np.xaxis.major_label_text_font_size = '14pt'\np.xaxis.axis_label_text_font = 'papyrus'\np.xaxis.axis_label_text_font_style = 'normal'\np.xaxis.axis_label_text_font_size = '20pt'\np.yaxis.major_label_text_font = 'papyrus'\np.yaxis.major_label_text_font_size = '14pt'\np.yaxis.axis_label_text_font = 'papyrus'\np.yaxis.axis_label_text_font_style = 'normal'\np.yaxis.axis_label_text_font_size = '20pt'\np.title.text_font = 'papyrus'\np.title.text_font_size = '18pt'\np.legend.label_text_font = 'papyrus'\n\n# Align the title center\np.title.align = 'center'\n\n# Set background and grid color\np.background_fill_color = 'blanchedalmond'\np.legend.background_fill_color = 'chartreuse'\np.xgrid.grid_line_color = 'azure'\np.ygrid.grid_line_color = 'azure'\n\n# Make the ticks point inward (I *hate* this!)\n# Units are pixels that the ticks extend in and out of plot\np.xaxis.major_tick_out = 0\np.xaxis.major_tick_in = 10\np.xaxis.minor_tick_out = 0\np.xaxis.minor_tick_in = 5\np.yaxis.major_tick_out = 0\np.yaxis.major_tick_in = 10\np.yaxis.minor_tick_out = 0\np.yaxis.minor_tick_in = 5\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis is truly hideous, but it demonstrates how we can go about styling plots after they are made.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Styling Bokeh plots</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/styling_bokeh.html#bokeh-themes",
    "href": "lessons/eda/plotting/styling_bokeh.html#bokeh-themes",
    "title": "12  Styling Bokeh plots",
    "section": "12.3 Bokeh themes",
    "text": "12.3 Bokeh themes\nBokeh has several built-in themes which you can apply to all plots in a given document (e.g., in a notebook). Please see the documentation for details about the built-in themes. I personally prefer the default styling to all of their themes, but your opinion may differ.\nYou may also specify custom themes using JSON or YAML. As an example, we can specify a theme such that plots are styled like the default style of the excellent plotting packages Vega-Altair/Vega-Lite/Vega. If we use JSON formatting, we can specify a theme as a dictionary of dictionaries, as below.\n\naltair_theme_dict = {\n    \"attrs\": {\n        \"Axis\": {\n            \"axis_line_color\": \"dimgray\",\n            \"minor_tick_out\": 0,\n            \"major_tick_in\": 0,\n            \"major_tick_line_color\": \"dimgray\",\n            \"major_label_text_font_size\": \"7.5pt\",\n            \"axis_label_text_font_size\": \"8pt\",\n            \"axis_label_text_font_style\": \"bold\",\n        },\n        \"Scatter\": {\n            \"fill_alpha\": 0, \n            \"line_width\": 2, \n            \"size\": 5, \n            \"line_alpha\": 0.7,\n        },\n        \"ContinuousTicker\": {\n            \"desired_num_ticks\": 10\n        },\n        \"figure\": {\n            \"frame_width\": 350, \n            \"frame_height\": 300,\n        },\n        \"Grid\": {\n            \"grid_line_color\": \"lightgray\",\n            \"level\": \"underlay\",\n        },\n        \"Legend\": {\n            \"border_line_color\": None,\n            \"background_fill_color\": None,\n            \"label_text_font_size\": \"7.5pt\",\n            \"title_text_font_size\": \"8pt\",\n            \"title_text_font_style\": \"bold\",\n        },\n        \"Renderer\": {\n            \"level\": \"overlay\"\n        },\n        \"Title\": {\n            \"align\": \"center\",\n        },\n    }\n}\n\nTo activate the theme, we convert it to a Bokeh theme and then add it to the curdoc(), or the current document.\n\naltair_theme = bokeh.themes.Theme(json=altair_theme_dict)\n\nbokeh.io.curdoc().theme = altair_theme\n\nNow the theme is activated, and future plots will have this theme by default. Let’s remake our plot using this theme. For convenience later on, I will write a function to generate this scatter plot that we will use to test various styles.\n\ndef gfmt_plot():\n    \"\"\"Make a plot for testing out styles in this notebook.\"\"\"\n    p = bokeh.plotting.figure(\n        frame_width=300,\n        frame_height=300,\n        x_axis_label=\"confidence when correct\",\n        y_axis_label=\"confidence when incorrect\",\n        title=\"GMFT with sleep conditions\",\n        x_range=[0, 100],\n        y_range=[0, 100],\n    )\n\n    normal_glyph = p.scatter(\n        source=df.filter(~pl.col('insomnia')).to_dict(),\n        x=\"confidence when correct\",\n        y=\"confidence when incorrect\",\n        color=cat_colors[0],\n    )\n\n    insom_glyph = p.scatter(\n        source=df.filter(pl.col('insomnia')).to_dict(),\n        x=\"confidence when correct\",\n        y=\"confidence when incorrect\",\n        color=cat_colors[1],\n    )\n\n    # Construct legend items\n    legend_items = [('normal', [normal_glyph]), ('insomnia', [insom_glyph])]\n\n    # Instantiate legend\n    legend = bokeh.models.Legend(items=legend_items, click_policy='hide')\n\n    # Add the legend to the right of the plot\n    p.add_layout(legend, 'right')\n    \n    return p\n\nbokeh.io.show(gfmt_plot())\n\n\n  \n\n\n\n\n\nWe could also style our plots to resemble the default “dark” styling of Seaborn.\n\nseaborn_theme_dict = {\n    \"attrs\": {\n        \"figure\": {\n            \"background_fill_color\": \"#eaeaf2\",\n            \"frame_height\": 300,\n            \"frame_width\": 350,\n        },\n        \"Axis\": {\n            \"axis_line_color\": None,\n            \"minor_tick_out\": 0,\n            \"major_tick_in\": 0,\n            \"major_tick_out\": 0,\n            \"major_label_text_font_size\": \"7.5pt\",\n            \"axis_label_text_font_size\": \"7.5pt\",\n            \"axis_label_text_font_style\": \"normal\",\n        },\n        \"Legend\": {\n            \"border_line_color\": \"darkgray\",\n            \"background_fill_color\": \"#eaeaf2\",\n            \"border_line_width\": 0.75,\n            \"label_text_font_size\": \"7.5pt\",\n        },\n        \"Grid\": {\n            \"grid_line_color\": \"#FFFFFF\", \n            \"grid_line_width\": 0.75,\n        },\n        \"Title\": {\n            \"align\": \"center\",\n            'text_font_style': 'normal',\n            'text_font_size': \"8pt\",\n        },\n    }\n}\n\nseaborn_theme = bokeh.themes.Theme(json=seaborn_theme_dict)\nbokeh.io.curdoc().theme = seaborn_theme\n\nLet’s make the plot, yet again, with this new styling.\n\nbokeh.io.show(gfmt_plot())\n\n\n  \n\n\n\n\n\nFinally, we can specify a style I like. Note that I do not specify that the glyphs are at an overlay level, since by default Bokeh will scale the axes such that the glyphs are fully contained in the plot area. I also put the toolbar above the plot, which is usually not a problem because I generally prefer not to title my plots, opting instead for good textual description in captions or in surrounding text.\n\njb_theme_dict = {\n    \"attrs\": {\n        \"Axis\": {\n            \"axis_line_color\": \"dimgray\",\n            \"major_tick_line_color\": \"dimgray\",\n            \"major_label_text_font_size\": \"7.5pt\",\n            \"axis_label_text_font_size\": \"9pt\",\n            \"axis_label_text_font_style\": \"bold\",\n        },\n        \"Scatter\": {\n            \"size\": 5, \n            \"fill_alpha\": 0.8,\n            \"line_width\": 0,\n        },\n        \"figure\": {\n            \"frame_height\": 300,\n            \"frame_width\": 350,\n            \"toolbar_location\": \"above\",\n        },\n        \"Grid\": {\n            \"grid_line_color\": \"lightgray\",\n            \"level\": \"underlay\",\n        },\n        \"Legend\": {\n            \"border_line_color\": \"darkgray\",\n            \"border_line_width\": 0.75,\n            \"background_fill_color\": \"#ffffff\",\n            \"background_fill_alpha\": 0.7,\n            \"label_text_font\": \"helvetica\", \n            \"label_text_font_size\": \"7.5pt\",\n            \"title_text_font\": \"helvetica\", \n            \"title_text_font_size\": \"8pt\", \n            \"title_text_font_style\": \"bold\", \n        },\n        \"Renderer\": {\n            \"level\": \"overlay\"\n        },\n        \"Title\": {\n            \"text_font\": \"helvetica\",\n            \"text_font_size\": \"10pt\",\n            'text_font_style': 'bold',\n        },\n    }\n}\n\njb_theme = bokeh.themes.Theme(json=jb_theme_dict)\nbokeh.io.curdoc().theme = jb_theme\n\nbokeh.io.show(gfmt_plot())\n\n\n  \n\n\n\n\n\nFinally, if I were to make this particular plot, I would do it without a title and with axes leaving a little buffer.\n\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=300,\n    x_axis_label=\"confidence when correct\",\n    y_axis_label=\"confidence when incorrect\",\n    x_range=[-2.5, 102.5],\n    y_range=[-2.5, 102.5],\n)\n\nnormal_glyph = p.scatter(\n    source=df.filter(~pl.col('insomnia')).to_dict(),\n    x=\"confidence when correct\",\n    y=\"confidence when incorrect\",\n    color=cat_colors[0],\n)\n\ninsom_glyph = p.scatter(\n    source=df.filter(pl.col('insomnia')).to_dict(),\n    x=\"confidence when correct\",\n    y=\"confidence when incorrect\",\n    color=cat_colors[1],\n)\n\n# Construct legend items\nlegend_items = [('normal', [normal_glyph]), ('insomnia', [insom_glyph])]\n\n# Instantiate legend\nlegend = bokeh.models.Legend(items=legend_items, click_policy='hide')\n\n# Add the legend to the right of the plot\np.add_layout(legend, 'right')\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nYou can play with these themes and develop your own style as you see fit. As you can see, Bokeh is highly configurable, and you can really make the plots your own!",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Styling Bokeh plots</span>"
    ]
  },
  {
    "objectID": "lessons/eda/plotting/styling_bokeh.html#computing-environment",
    "href": "lessons/eda/plotting/styling_bokeh.html#computing-environment",
    "title": "12  Styling Bokeh plots",
    "section": "12.4 Computing environment",
    "text": "12.4 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\npolars    : 1.33.1\nbokeh     : 3.7.3\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Styling Bokeh plots</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/manipulating_data_frames_intro.html",
    "href": "lessons/eda/wrangling/manipulating_data_frames_intro.html",
    "title": "13  Manipulating data frames and wrangling data",
    "section": "",
    "text": "13.1 What’s to come\n| Download notebook\nWe have already seen how to perform calculations with data frames by:\nThese methods are useful for wrangling data frames, doing things like unit conversions or computing statistical point estimates. There are plenty of other methods you can find in the Polars documentation. The Polars docs, and questions on Stack Overflow are among your best references for usage.\nIn the next sections of this lesson, we discuss how to reshape and combine data frames. In these operations, we are not really doing calculations with data, but rather are working to organize our data set into an easily usable, tidy form. We will cover\nAll of these methods are useful to bring inputted data into formats that are convenient to work with.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Manipulating data frames and wrangling data</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/manipulating_data_frames_intro.html#whats-to-come",
    "href": "lessons/eda/wrangling/manipulating_data_frames_intro.html#whats-to-come",
    "title": "13  Manipulating data frames and wrangling data",
    "section": "",
    "text": "Creating a data frame from scratch (as opposed to reading in data from a file)\nJoining and concatenating data frames (putting two or more data frames together into one)\nReshaping data frames by unpivoting and pivoting.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Manipulating data frames and wrangling data</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/manipulating_data_frames_intro.html#polars-documentation",
    "href": "lessons/eda/wrangling/manipulating_data_frames_intro.html#polars-documentation",
    "title": "13  Manipulating data frames and wrangling data",
    "section": "13.2 Polars documentation",
    "text": "13.2 Polars documentation\nThe Pandas documentation is a great resource for learning about these methods. In particular, the section of the user guide on joining concatenating, unpivoting, and pivoting is very useful. They use very small, contrived data frames to demonstrate the concepts. There are benefits to this approach, certainly, but I find it is often easier to grasp the concepts with real examples. In the following sections of this lesson, we will approach these topics with real examples. You can think of the these lessons as valuable introductions and supplements to the official Polars documentation.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Manipulating data frames and wrangling data</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/merging_dataframes.html",
    "href": "lessons/eda/wrangling/merging_dataframes.html",
    "title": "14  Merging and concatenating data frames",
    "section": "",
    "text": "14.1 The frog tongue strike data set\n| Download notebook\nData set download\nIt often happens that experiments consist of multiple data files that need to be brought together into a single data frame to work with in exploratory data analysis and subsequent analyses. Through its concatenation and merging capabilities, Polars provides powerful tools for handling this sort of data.\nAs usual, we will work with a real data set to learn about concatenation and merging of data frames. The data set we will use comes from a fun paper about the adhesive properties of frog tongues. The reference is Kleinteich and Gorb, Tongue adhesion in the horned frog Ceratophrys sp., Sci. Rep., 4, 5225, 2014. You might also want to check out a New York Times feature on the paper here.\nIn this paper, the authors investigated various properties of the adhesive characteristics of the tongues of horned frogs when they strike prey. The authors had a striking pad connected to a cantilever to measure forces. They also used high speed cameras to capture the strike and record relevant data.\nTo get an idea of the experimental set up, you can check out this movie, kindly sent to me by Thomas Kleinteich. If video does not play in your browser, you may download it here.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Merging and concatenating data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/merging_dataframes.html#the-frog-tongue-strike-data-set",
    "href": "lessons/eda/wrangling/merging_dataframes.html#the-frog-tongue-strike-data-set",
    "title": "14  Merging and concatenating data frames",
    "section": "",
    "text": "Your browser does not support display of this video.\n\n\n\n14.1.1 The data files\nI pulled data files from the Kleinteich and Gorb paper. You can download the data files here: https://s3.amazonaws.com/bebi103.caltech.edu/data/frog_strikes.zip.\nThere are four files, one for each of the four frogs, labeled with IDs I, II, III, and IV, that were studied. To see the format of the files, we can look at the content of the file for frog I. You can use\nhead -n 20 ../data/frog_strikes_I.csv\nfrom the command line. Here is the content of the first data file.\n# These data are from Kleinteich and Gorb, Sci. Rep., 4, 5225, 2014.\n# Frog ID: I\n# Age: adult\n# Snout-vent-length (SVL): 63 mm\n# Body weight: 63.1 g\n# Species: Ceratophrys cranwelli crossed with Ceratophrys cornuta\ndate,trial number,impact force (mN),impact time (ms),impact force / body weight,adhesive force (mN),time frog pulls on target (ms),adhesive force / body weight,adhesive impulse (N-s),total contact area (mm2),contact area without mucus (mm2),contact area with mucus / contact area without mucus,contact pressure (Pa),adhesive strength (Pa)\n2013_02_26,3,1205,46,1.95,-785,884,1.27,-0.290,387,70,0.82,3117,-2030\n2013_02_26,4,2527,44,4.08,-983,248,1.59,-0.181,101,94,0.07,24923,-9695\n2013_03_01,1,1745,34,2.82,-850,211,1.37,-0.157,83,79,0.05,21020,-10239\n2013_03_01,2,1556,41,2.51,-455,1025,0.74,-0.170,330,158,0.52,4718,-1381\n2013_03_01,3,493,36,0.80,-974,499,1.57,-0.423,245,216,0.12,2012,-3975\n2013_03_01,4,2276,31,3.68,-592,969,0.96,-0.176,341,106,0.69,6676,-1737\n2013_03_05,1,556,43,0.90,-512,835,0.83,-0.285,359,110,0.69,1550,-1427\n2013_03_05,2,1928,46,3.11,-804,508,1.30,-0.285,246,178,0.28,7832,-3266\n2013_03_05,3,2641,50,4.27,-690,491,1.12,-0.239,269,224,0.17,9824,-2568\n2013_03_05,4,1897,41,3.06,-462,839,0.75,-0.328,266,176,0.34,7122,-1733\n2013_03_12,1,1891,40,3.06,-766,1069,1.24,-0.380,408,33,0.92,4638,-1879\n2013_03_12,2,1545,48,2.50,-715,649,1.15,-0.298,141,112,0.21,10947,-5064\n2013_03_12,3,1307,29,2.11,-613,1845,0.99,-0.768,455,92,0.80,2874,-1348\n2013_03_12,4,1692,31,2.73,-677,917,1.09,-0.457,186,129,0.31,9089,-3636\n2013_03_12,5,1543,38,2.49,-528,750,0.85,-0.353,153,148,0.03,10095,-3453\n2013_03_15,1,1282,31,2.07,-452,785,0.73,-0.253,290,105,0.64,4419,-1557\n2013_03_15,2,775,34,1.25,-430,837,0.70,-0.276,257,124,0.52,3019,-1677\n2013_03_15,3,2032,60,3.28,-652,486,1.05,-0.257,147,134,0.09,13784,-4425\n2013_03_15,4,1240,34,2.00,-692,906,1.12,-0.317,364,260,0.28,3406,-1901\n2013_03_15,5,473,40,0.76,-536,1218,0.87,-0.382,259,168,0.35,1830,-2073\nThe first lines all begin with # signs, signifying that they are comments. They do give important information about the frog, though.\nThe first line after the comments are the headers, giving the column names for the data frame we will load.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Merging and concatenating data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/merging_dataframes.html#concatenating-data-frames",
    "href": "lessons/eda/wrangling/merging_dataframes.html#concatenating-data-frames",
    "title": "14  Merging and concatenating data frames",
    "section": "14.2 Concatenating data frames",
    "text": "14.2 Concatenating data frames\nWe would like to have all of the data frames be together in one data frame so we can conveniently do things like make plots comparing the four frogs. Let’s read in the data sets and make a list of data frames.\n\n# On a local machine, we would do this: fnames = glob.glob('../data/frog_strikes_*.csv')\n# But for Colab compatibility, we will do it by hand\nfnames = [\n    os.path.join(data_path, f\"frog_strikes_{frog_id}.csv\")\n    for frog_id in [\"I\", \"II\", \"III\", \"IV\"]\n]\n\ndfs = [pl.read_csv(f, comment_prefix=\"#\") for f in fnames]\n\n# Take a look at first data frame\ndfs[0].head()\n\n\nshape: (5, 14)\n\n\n\ndate\ntrial number\nimpact force (mN)\nimpact time (ms)\nimpact force / body weight\nadhesive force (mN)\ntime frog pulls on target (ms)\nadhesive force / body weight\nadhesive impulse (N-s)\ntotal contact area (mm2)\ncontact area without mucus (mm2)\ncontact area with mucus / contact area without mucus\ncontact pressure (Pa)\nadhesive strength (Pa)\n\n\nstr\ni64\ni64\ni64\nf64\ni64\ni64\nf64\nf64\ni64\ni64\nf64\ni64\ni64\n\n\n\n\n\"2013_02_26\"\n3\n1205\n46\n1.95\n-785\n884\n1.27\n-0.29\n387\n70\n0.82\n3117\n-2030\n\n\n\"2013_02_26\"\n4\n2527\n44\n4.08\n-983\n248\n1.59\n-0.181\n101\n94\n0.07\n24923\n-9695\n\n\n\"2013_03_01\"\n1\n1745\n34\n2.82\n-850\n211\n1.37\n-0.157\n83\n79\n0.05\n21020\n-10239\n\n\n\"2013_03_01\"\n2\n1556\n41\n2.51\n-455\n1025\n0.74\n-0.17\n330\n158\n0.52\n4718\n-1381\n\n\n\"2013_03_01\"\n3\n493\n36\n0.8\n-974\n499\n1.57\n-0.423\n245\n216\n0.12\n2012\n-3975\n\n\n\n\n\n\nWe have successfully loaded in all of the data frames. They all have the same columns (as given by the CSV files). So, we wish to tape the data frames together vertically. We can use the pl.concat() function to do this.\nBefore we do that, though, we might notice a problem. We will not have information to tell us which frog is which. We might therefore like to add a column to each data frame that has the frog ID, and then concatenate them. We can parse the ID of the frog from the file name, as we can see by looking at the file names.\n\nfnames\n\n['../data/frog_strikes_I.csv',\n '../data/frog_strikes_II.csv',\n '../data/frog_strikes_III.csv',\n '../data/frog_strikes_IV.csv']\n\n\nSo, for each data frame/file name pair, we extract the Roman numeral and add a column to the data frame containing the frog ID. To do this, we use a Polars literal, accessible with pl.lit(), which means that we want to insert a specific value (in this case, \"I\", \"II\", \"III\", or \"IV\") into a data frame as a column.\n\nfor i, f in enumerate(fnames):\n    frog_id = f[f.rfind('_')+1:f.rfind('.')]\n    dfs[i] = dfs[i].with_columns(pl.lit(frog_id).alias('ID'))\n    \n# Take a look\ndfs[0].head()\n\n\nshape: (5, 15)\n\n\n\ndate\ntrial number\nimpact force (mN)\nimpact time (ms)\nimpact force / body weight\nadhesive force (mN)\ntime frog pulls on target (ms)\nadhesive force / body weight\nadhesive impulse (N-s)\ntotal contact area (mm2)\ncontact area without mucus (mm2)\ncontact area with mucus / contact area without mucus\ncontact pressure (Pa)\nadhesive strength (Pa)\nID\n\n\nstr\ni64\ni64\ni64\nf64\ni64\ni64\nf64\nf64\ni64\ni64\nf64\ni64\ni64\nstr\n\n\n\n\n\"2013_02_26\"\n3\n1205\n46\n1.95\n-785\n884\n1.27\n-0.29\n387\n70\n0.82\n3117\n-2030\n\"I\"\n\n\n\"2013_02_26\"\n4\n2527\n44\n4.08\n-983\n248\n1.59\n-0.181\n101\n94\n0.07\n24923\n-9695\n\"I\"\n\n\n\"2013_03_01\"\n1\n1745\n34\n2.82\n-850\n211\n1.37\n-0.157\n83\n79\n0.05\n21020\n-10239\n\"I\"\n\n\n\"2013_03_01\"\n2\n1556\n41\n2.51\n-455\n1025\n0.74\n-0.17\n330\n158\n0.52\n4718\n-1381\n\"I\"\n\n\n\"2013_03_01\"\n3\n493\n36\n0.8\n-974\n499\n1.57\n-0.423\n245\n216\n0.12\n2012\n-3975\n\"I\"\n\n\n\n\n\n\nGood! Now all data frames have an 'ID' column, and we can concatenate. The pl.concat() function takes as input a list of data frames to be concatenated and stacks them on top of each other.\n\n# Concatenate data frames\ndf = pl.concat(dfs)\n\n# Make sure we got them all\nprint(\n    \"Number of rows:\", len(df), \"\\nUnique IDs:\", df.get_column(\"ID\").unique().to_list()\n)\n\nNumber of rows: 80 \nUnique IDs: ['III', 'I', 'II', 'IV']\n\n\nCheck!",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Merging and concatenating data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/merging_dataframes.html#creating-a-dataframe-from-scratch",
    "href": "lessons/eda/wrangling/merging_dataframes.html#creating-a-dataframe-from-scratch",
    "title": "14  Merging and concatenating data frames",
    "section": "14.3 Creating a DataFrame from scratch",
    "text": "14.3 Creating a DataFrame from scratch\nLooking back at the headers of the original data files, we see that there is information present in the header that we would like to have in our data frame. For example, it would be nice to know if each strike came from an adult or juvenile. Or what the snout-vent length was. Working toward the goal of including this in our data frame, we will first construct a new data frame containing information about each frog.\n\n14.3.1 Data frames from dictionaries\nOne way do create this new data frame is to first construct a dictionary with the respective fields. Since these data sets are small, we can look at the files and make the dictionary by hand.\n\ndata_dict = {\n    \"ID\": [\"I\", \"II\", \"III\", \"IV\"],\n    \"age\": [\"adult\", \"adult\", \"juvenile\", \"juvenile\"],\n    \"SVL (mm)\": [63, 70, 28, 31],\n    \"body weight (g)\": [63.1, 72.7, 12.7, 12.7],\n    \"species\": [\"cross\", \"cross\", \"cranwelli\", \"cranwelli\"],\n}\n\nNow that we have this dictionary, we can convert it into a DataFrame by instantiating a pl.DataFrame class with it, using the data kwarg.\n\n# Make it into a DataFrame\ndf_frog_info = pl.DataFrame(data=data_dict)\n\n# Take a look\ndf_frog_info\n\n\nshape: (4, 5)\n\n\n\nID\nage\nSVL (mm)\nbody weight (g)\nspecies\n\n\nstr\nstr\ni64\nf64\nstr\n\n\n\n\n\"I\"\n\"adult\"\n63\n63.1\n\"cross\"\n\n\n\"II\"\n\"adult\"\n70\n72.7\n\"cross\"\n\n\n\"III\"\n\"juvenile\"\n28\n12.7\n\"cranwelli\"\n\n\n\"IV\"\n\"juvenile\"\n31\n12.7\n\"cranwelli\"\n\n\n\n\n\n\nNice!\n\n\n14.3.2 Data frames from numpy arrays\nSometimes the data sets are not small enough to construct a dictionary by hand. Oftentimes, we have a two-dimensional array of data that we want to make into a DataFrame. As an example, let’s say we have a Numpy array where the first column is snout vent length and the second is weight.\n\ndata = np.array([[63, 70, 28, 31], [63.1, 72.7, 12.7, 12.7]]).transpose()\n\n# Verify that it's what we think it is\ndata\n\narray([[63. , 63.1],\n       [70. , 72.7],\n       [28. , 12.7],\n       [31. , 12.7]])\n\n\nTo make this into a DataFrame, we again create pl.DataFrame instance, but this time we also specify the schema keyword argument to label the columns.\n\ndf_demo = pl.DataFrame(data=data, schema=[\"SVL (mm)\", \"weight (g)\"])\n\n# Take a look\ndf_demo\n\n\nshape: (4, 2)\n\n\n\nSVL (mm)\nweight (g)\n\n\nf64\nf64\n\n\n\n\n63.0\n63.1\n\n\n70.0\n72.7\n\n\n28.0\n12.7\n\n\n31.0\n12.7\n\n\n\n\n\n\nThat also works. Generally, any two-dimensional Numpy array can be converted into a DataFrame in this way. You just need to supply column names.\n\n\n14.3.3 Programmatically creating a data frame\nHand-entering data should be minimized. The information about each frog was hand-entered once by the experimenter. We should not hand enter them again. We therefore should parse the comment lines of input files to get the pertinent information.\nNote, though, that in the case of a single experiment with only four data sets, hand entering might be faster and indeed less error prone than doing it programmatically. We should definitely do it programmatically if we have a large number of data files or will ever do an experiment with the same file format again.\nSo, let’s programmatically parse the files. We start by writing a function to parse the metadata from a single file. Recall that the comment lines look like this:\n# These data are from Kleinteich and Gorb, Sci. Rep., 4, 5225, 2014.\n# Frog ID: I\n# Age: adult\n# Snout-vent-length (SVL): 63 mm\n# Body weight: 63.1 g\n# Species: Ceratophrys cranwelli crossed with Ceratophrys cornuta\n(The function below will not work with Colab because open() does not work for files specified by a URL.)\n\ndef parse_frog_metadata(fname):\n    with open(fname, 'r') as f:\n        # Citation line, ignore.\n        f.readline()\n        \n        # Frog ID\n        line = f.readline()\n        frog_id = line[line.find(':')+1:].strip()\n        \n        # Age\n        line = f.readline()\n        age = line[line.find(':')+1:].strip()\n        \n        # SVL, assume units given as mm\n        line = f.readline()\n        svl = line[line.find(':')+1:line.rfind(' ')].strip()\n        \n        # Body weight, assume units given as g\n        line = f.readline()\n        body_weight = line[line.find(':')+1:line.rfind(' ')].strip()\n\n        # Species (either cranwelli or cross)\n        line = f.readline()\n        species = line[line.find(':')+1:].strip()\n        if 'cross' in species:\n            species = 'cross'\n        else:\n            species = 'cranwelli'\n\n    return frog_id, age, svl, body_weight, species\n\nLet’s take it for a spin.\n\nparse_frog_metadata(os.path.join(data_path, 'frog_strikes_I.csv'))\n\n('I', 'adult', '63', '63.1', 'cross')\n\n\nLooks good! Now we can create a list of tuples to use as data for making a data frame.\n\ndata = [parse_frog_metadata(f) for f in fnames]\n    \n# Take a look\ndata\n\n[('I', 'adult', '63', '63.1', 'cross'),\n ('II', 'adult', '70', '72.7', 'cross'),\n ('III', 'juvenile', '28', '12.7', 'cranwelli'),\n ('IV', 'juvenile', '31', '12.7', 'cranwelli')]\n\n\nWe now input this list of tuples, plus the column names, into pl.DataFrame(), and we’ve got our data frame. We do have to specify that this list of tuples is row-oriented, so Polars knows that each tuple is a row and not a column.\n\ndf_frog_info = pl.DataFrame(\n    data=data, \n    schema=[\"ID\", \"age\", \"SVL (mm)\", \"body weight (g)\", \"species\"],\n    orient='row',\n)\n\n# Take a look\ndf_frog_info\n\n\nshape: (4, 5)\n\n\n\nID\nage\nSVL (mm)\nbody weight (g)\nspecies\n\n\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n\"I\"\n\"adult\"\n\"63\"\n\"63.1\"\n\"cross\"\n\n\n\"II\"\n\"adult\"\n\"70\"\n\"72.7\"\n\"cross\"\n\n\n\"III\"\n\"juvenile\"\n\"28\"\n\"12.7\"\n\"cranwelli\"\n\n\n\"IV\"\n\"juvenile\"\n\"31\"\n\"12.7\"\n\"cranwelli\"",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Merging and concatenating data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/merging_dataframes.html#joining-dataframes",
    "href": "lessons/eda/wrangling/merging_dataframes.html#joining-dataframes",
    "title": "14  Merging and concatenating data frames",
    "section": "14.4 Joining DataFrames",
    "text": "14.4 Joining DataFrames\nWe want to add the information about the frogs into our main data frame, df, that we have been working with. Specifically, for each row of the data frame, we also want to include the frog’s age, snout-vent length, body weight, and species. So, we want to take the data frame with all of the information about the tongue strikes and combine it with the data frame containing information about each frog. This combining of data frames is a join operation. In this case, we join on the 'ID' column, since the value of the that column in each data frame indicates the frog we are talking about.\nTo perform a join operation we use the df.join() method (or df.join_asof() method for approximate matches). Its default join strategy is an inner join, in which the entries of a given row are included in the joined data frame if and only if the entry 'ID' column of the respective frames match. You can read more about available join strategies in the documentation.\n\ndf = df.join(df_frog_info, on='ID')\n\n# Take a look\ndf.head()\n\n\nshape: (5, 19)\n\n\n\ndate\ntrial number\nimpact force (mN)\nimpact time (ms)\nimpact force / body weight\nadhesive force (mN)\ntime frog pulls on target (ms)\nadhesive force / body weight\nadhesive impulse (N-s)\ntotal contact area (mm2)\ncontact area without mucus (mm2)\ncontact area with mucus / contact area without mucus\ncontact pressure (Pa)\nadhesive strength (Pa)\nID\nage\nSVL (mm)\nbody weight (g)\nspecies\n\n\nstr\ni64\ni64\ni64\nf64\ni64\ni64\nf64\nf64\ni64\ni64\nf64\ni64\ni64\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n\"2013_02_26\"\n3\n1205\n46\n1.95\n-785\n884\n1.27\n-0.29\n387\n70\n0.82\n3117\n-2030\n\"I\"\n\"adult\"\n\"63\"\n\"63.1\"\n\"cross\"\n\n\n\"2013_02_26\"\n4\n2527\n44\n4.08\n-983\n248\n1.59\n-0.181\n101\n94\n0.07\n24923\n-9695\n\"I\"\n\"adult\"\n\"63\"\n\"63.1\"\n\"cross\"\n\n\n\"2013_03_01\"\n1\n1745\n34\n2.82\n-850\n211\n1.37\n-0.157\n83\n79\n0.05\n21020\n-10239\n\"I\"\n\"adult\"\n\"63\"\n\"63.1\"\n\"cross\"\n\n\n\"2013_03_01\"\n2\n1556\n41\n2.51\n-455\n1025\n0.74\n-0.17\n330\n158\n0.52\n4718\n-1381\n\"I\"\n\"adult\"\n\"63\"\n\"63.1\"\n\"cross\"\n\n\n\"2013_03_01\"\n3\n493\n36\n0.8\n-974\n499\n1.57\n-0.423\n245\n216\n0.12\n2012\n-3975\n\"I\"\n\"adult\"\n\"63\"\n\"63.1\"\n\"cross\"\n\n\n\n\n\n\nNote that the entries for the added columns were repeated appropriately, e.g., the body weight column had 63 for every row corresponding to frog I.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Merging and concatenating data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/merging_dataframes.html#at-long-last-a-plot",
    "href": "lessons/eda/wrangling/merging_dataframes.html#at-long-last-a-plot",
    "title": "14  Merging and concatenating data frames",
    "section": "14.5 At long last, a plot!",
    "text": "14.5 At long last, a plot!\nWhile the purpose of this part of the lesson was to learn how to concatenate and merge data frames, going through all of that wrangling effort would somehow be unsatisfying if we we didn’t generate a plot. Let’s compare the impact force on a per-mass basis for each frog.\n\np = iqplot.strip(\n    df,\n    q=\"impact force / body weight\",\n    cats=\"ID\",\n    color_column=\"age\",\n    spread=\"jitter\",\n    x_axis_label=\"impact force / body weight (mN/g)\",\n    y_axis_label=\"frog ID\"\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nApparently Frog III consistently packs a powerful punch, er…. tongue, for its body weight.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Merging and concatenating data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/merging_dataframes.html#computing-environment",
    "href": "lessons/eda/wrangling/merging_dataframes.html#computing-environment",
    "title": "14  Merging and concatenating data frames",
    "section": "14.6 Computing environment",
    "text": "14.6 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\npolars    : 1.33.1\nbokeh     : 3.7.3\niqplot    : 0.3.7\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Merging and concatenating data frames</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/unpivoting.html",
    "href": "lessons/eda/wrangling/unpivoting.html",
    "title": "15  Making a data frame tall",
    "section": "",
    "text": "15.1 The data set\n| Download notebook\nData set download\nIn this part of the lesson, we will perform some wrangling on a data set that involves:\nWe will use a data set from Angela Stathopoulos’s lab, acquired to study morphogen profiles in developing fruit fly embryos. The original paper is Reeves, Trisnadi, et al., Dorsal-ventral gene expression in the Drosophila embryo reflects the dynamics and precision of the Dorsal nuclear gradient, Dev. Cell., 22, 544-557, 2012, and the data set may be downloaded here: https://s3.amazonaws.com/bebi103.caltech.edu/data/Reeves2012_data.xlsx.\nIn this experiment, Reeves, Trisnadi, and coworkers measured expression levels of a fusion of Dorsal, a morphogen transcription factor important in determining the dorsal-ventral axis of the developing organism, and Venus, a yellow fluorescent protein along the dorsal/ventral– (DV) coordinate. They put this construct on the third chromosome, while wild type dorsal is on the second. Instead of the wild type, they had a homozygous dorsal-null mutant on the second chromosome. The Dorsal-Venus construct rescues wild type behavior, so they could use this construct to study Dorsal gradients.\nDorsal shows higher expression on the ventral side of the organism, thus giving a gradient in expression from dorsal to ventral which can be ascertained by the spatial distribution of Venus fluorescence intensity.\nThis can be seen in the image below, which is a cross-section of a fixed embryo with anti-Dorsal staining. The bottom of the image is the ventral side and the top is the dorsal side of the embryo. The DV coordinate system is defined by the yellow line. The image is adapted from the Reeves, Trisnadi, et al. paper.\nA quick note on nomenclature: Dorsal (capital D) is the name of the protein product of the gene dorsal (italicized). The dorsal (adjective) side of the embryo is its back. The ventral side is its belly. Dorsal is expressed more strongly on the ventral side of the developing embryo. This can be confusing.\nTo quantify the gradient, Reeves, Trisnadi, and coworkers had to first choose a metric for describing it. They chose to fit the measured profile of fluorescence intensity with a Gaussian peak (plus background) and use the standard deviation of that Gaussian as a metric for the width of the Dorsal gradient.\nIn this lesson, we will use the gradient widths as outputted from this procedure. The units of the widths are dimensionless, consistent with the coordinate system shown in the image above. I asked one of the authors for the data sets used in making the figures. She sent me a MS Excel file that had a separate sheet for each of several figures in the paper that I asked about. We will focus on the data used for Fig. 1F of the paper. In this figure, the authors seek to demonstrate that live imaging with their Venus-Dorsal construct gives a Dorsal gradient of similar width as would be obtained by fixing wild type cells and doing Dorsal antibody staining (the gold standard). These wild type embryos were analyzed as whole mounts and also as cross-sections. They also tried anti-Dorsal staining and anti-Venus staining in the Venus-Dorsal construct. Finally, they also measured gradient widths of a GFP-Dorsal construct that fails to complete development.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making a data frame tall</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/unpivoting.html#the-data-set",
    "href": "lessons/eda/wrangling/unpivoting.html#the-data-set",
    "title": "15  Making a data frame tall",
    "section": "",
    "text": "Fluorescently labeled Dorsal in a cross-section of a Drosophila embryo. Adapted from Reeves, Trisnadi, et al., Dev. Cell., 2012.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making a data frame tall</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/unpivoting.html#loading-in-an-excel-sheet",
    "href": "lessons/eda/wrangling/unpivoting.html#loading-in-an-excel-sheet",
    "title": "15  Making a data frame tall",
    "section": "15.2 Loading in an Excel sheet",
    "text": "15.2 Loading in an Excel sheet\nGenerally, you should store your data sets in portable formats, like CSV, JSON, XML, HDF5, OME-TIFF, etc., and not proprietary formats. Nonetheless, software like Microsoft Excel is widely used, and you will often receive data sets in this format. Fortunately, Polars can read Excel files, provided they are from fairly recent versions of Excel.\nTo read in this data set, we will use pl.read_excel(). Importantly, because an Excel document may have many sheets, we need to specify the sheet name we want, in this case 'Fig 1F'.\n\ndf = pl.read_excel(os.path.join(data_path, \"Reeves2012_data.xlsx\"), sheet_name=\"Fig 1F\")\n\ndf.head()\n\n\nshape: (5, 8)\n\n\n\nwt wholemounts\nwt cross-sections\nanti-Dorsal dl1/+dl-venus/+\nanti-gfp dl1/+dl-venus/+\nVenus (live) dl1/+dl-venus/+\nanti-Dorsal dl1/+dl-GFP/+\nanti-gfp dl1/+dl-GFP/+\nGFP (live) dl1/+dl-GFP/+\n\n\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n0.1288\n0.1327\n0.1482\n0.1632\n0.1666\n0.2248\n0.2389\n0.2412\n\n\n0.1554\n0.1457\n0.1503\n0.1671\n0.1753\n0.1891\n0.2035\n0.1942\n\n\n0.1306\n0.1447\n0.1577\n0.1704\n0.1705\n0.1705\n0.1943\n0.2186\n\n\n0.1413\n0.1282\n0.1711\n0.1779\nnull\n0.1735\n0.2\n0.2104\n\n\n0.1557\n0.1487\n0.1342\n0.1483\nnull\n0.2135\n0.256\n0.2463\n\n\n\n\n\n\nThe data frame is not tidy. Each entry corresponds to one observation, not each row. The column headings contain important metadata, the genotype (wt, dl1/+dl-venus/+, or dl1/+dl-GFP/+) and the method (wholemounts, cross-sections, anti-Dorsal, anti-gfp, Venus (live), and GFP (live)).\nThe data set has other issues we need to clean up. The column 'anti-gfp dl1/+dl-venus/+' is mislabeled; it should be 'anti-Venus dl1/+dl-venus/+'. We would also like to clean up the genotypes, putting in a semicolon to separate the chromosomes. The wild type columns have the genotype first ('wt') followed by the method, whereas the other columns have the method first, followed by genotype.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making a data frame tall</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/unpivoting.html#parsing-the-column-names",
    "href": "lessons/eda/wrangling/unpivoting.html#parsing-the-column-names",
    "title": "15  Making a data frame tall",
    "section": "15.3 Parsing the column names",
    "text": "15.3 Parsing the column names\nWe will start our process of tidying this data set by changing the column names. They are pretty messy, so this is best done by hand in this case. We will rename the columns with strings where the genotype comes first, followed by the method for measuring the gradient width, separated by an underscore.\n\ncol_names = {\n    'wt wholemounts': 'WT_whole mount',\n    'wt cross-sections': 'WT_cross-section',\n    'anti-Dorsal dl1/+dl-venus/+': 'dl1/+ ; dl-venus/+_anti-Dorsal',\n    'anti-gfp dl1/+dl-venus/+': 'dl1/+ ; dl-venus/+_anti-Venus',\n    'Venus (live)  dl1/+dl-venus/+': 'dl1/+ ; dl-venus/+_Venus (live)',\n    'anti-Dorsal  dl1/+dl-GFP/+': 'dl1/+ ; dl-gfp/+_anti-Dorsal',\n    'anti-gfp  dl1/+dl-GFP/+ ': 'dl1/+ ; dl-gfp/+_anti-GFP',\n    'GFP (live)  dl1/+dl-GFP/+': 'dl1/+ ; dl-gfp/+_GFP (live)'\n}\n\ndf = df.rename(col_names)\n\ndf.head()\n\n\nshape: (5, 8)\n\n\n\nWT_whole mount\nWT_cross-section\ndl1/+ ; dl-venus/+_anti-Dorsal\ndl1/+ ; dl-venus/+_anti-Venus\ndl1/+ ; dl-venus/+_Venus (live)\ndl1/+ ; dl-gfp/+_anti-Dorsal\ndl1/+ ; dl-gfp/+_anti-GFP\ndl1/+ ; dl-gfp/+_GFP (live)\n\n\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n0.1288\n0.1327\n0.1482\n0.1632\n0.1666\n0.2248\n0.2389\n0.2412\n\n\n0.1554\n0.1457\n0.1503\n0.1671\n0.1753\n0.1891\n0.2035\n0.1942\n\n\n0.1306\n0.1447\n0.1577\n0.1704\n0.1705\n0.1705\n0.1943\n0.2186\n\n\n0.1413\n0.1282\n0.1711\n0.1779\nnull\n0.1735\n0.2\n0.2104\n\n\n0.1557\n0.1487\n0.1342\n0.1483\nnull\n0.2135\n0.256\n0.2463",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making a data frame tall</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/unpivoting.html#unpivot-the-data-frame",
    "href": "lessons/eda/wrangling/unpivoting.html#unpivot-the-data-frame",
    "title": "15  Making a data frame tall",
    "section": "15.4 Unpivot the data frame",
    "text": "15.4 Unpivot the data frame\nWhen we unpivot the data frame, the data within it, called values, become a single column. The column names, called variables also populate a new column. So, to unpivot it, we need to specify what we want to call the values and what we want to call the variable. The unpivot() method does the rest!\n\ndf = df.unpivot(\n    variable_name='genotype_method', \n    value_name='gradient width'\n).drop_nulls()\n\n# Take a look\ndf.head()\n\n\nshape: (5, 2)\n\n\n\ngenotype_method\ngradient width\n\n\nstr\nf64\n\n\n\n\n\"WT_whole mount\"\n0.1288\n\n\n\"WT_whole mount\"\n0.1554\n\n\n\"WT_whole mount\"\n0.1306\n\n\n\"WT_whole mount\"\n0.1413\n\n\n\"WT_whole mount\"\n0.1557\n\n\n\n\n\n\nNice! We now have a tidy data frame. Note that we also dropped the null values, since the nulls from the original columns come along for the ride when unpivoting.\nNote that df.unpivot() has other options. For example, you can specify columns that do not comprise data, but should still be included in the unpivoted data frame using the id_vars keyword argument. That does not apply to this data frame, but comes up often. As a final comment, note that unpivoting is sometimes called melting, as it is when using Pandas.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making a data frame tall</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/unpivoting.html#splitting-the-genotype_method-column",
    "href": "lessons/eda/wrangling/unpivoting.html#splitting-the-genotype_method-column",
    "title": "15  Making a data frame tall",
    "section": "15.5 Splitting the genotype_method column",
    "text": "15.5 Splitting the genotype_method column\nNow our goal is to convert the 'genotype_method' column into two columns, one encoding genotype and the other the method. To do this, we first use Polars’s string methods to split the entries in the column at the underscore. This gives a series of list data types. We then convert the list to a struct, where the fields are the column labels we want when we split the column into two, in this case 'genotype' and 'method'. Finally, we can unnest the column with our structs.\n\ndf = df.with_columns(\n    pl.col('genotype_method')\n    .str.split('_')\n    .list.to_struct(fields=['genotype', 'method'])\n).unnest('genotype_method')\n\n# Take a look\ndf.head()\n\n\nshape: (5, 3)\n\n\n\ngenotype\nmethod\ngradient width\n\n\nstr\nstr\nf64\n\n\n\n\n\"WT\"\n\"whole mount\"\n0.1288\n\n\n\"WT\"\n\"whole mount\"\n0.1554\n\n\n\"WT\"\n\"whole mount\"\n0.1306\n\n\n\"WT\"\n\"whole mount\"\n0.1413\n\n\n\"WT\"\n\"whole mount\"\n0.1557\n\n\n\n\n\n\nLooking at the data frame above, it is very tall. Each row only has a single value, that is, a single measurement, and every other column in the row is metadata associated with that measurement, specifically the genotype and method. Unpivoting operations generally make data frames taller, with more rows than before unpivoting.\n\n15.5.1 Using the data frame\nLet’s make a plot like Fig. 1F of the Reeves, Trisnadi, et al. paper, but not with boxes, rather as a strip plot.\n\np = iqplot.strip(\n    data=df,\n    q='gradient width',\n    cats=['genotype', 'method'],\n    color_column='genotype',\n    spread=\"jitter\",\n    frame_height=350,\n    frame_width=450,\n)\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making a data frame tall</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/unpivoting.html#computing-environment",
    "href": "lessons/eda/wrangling/unpivoting.html#computing-environment",
    "title": "15  Making a data frame tall",
    "section": "15.6 Computing environment",
    "text": "15.6 Computing environment\n\n%load_ext watermark\n%watermark -v -p polars,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\npolars    : 1.33.1\nbokeh     : 3.8.0\niqplot    : 0.3.7\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Making a data frame tall</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/pivoting.html",
    "href": "lessons/eda/wrangling/pivoting.html",
    "title": "16  Making a data frame wide",
    "section": "",
    "text": "16.1 Exploring the data set\n| Download notebook\nData set download\nWe have seen how unpivoting a data frame can bring it to tidy format, but a tall format is often not the only tidy option nor the easiest to work with. As usual, this is best seen by example, and we will use a subset of the Palmer penguins data set, which you can download here: https://s3.amazonaws.com/bebi103.caltech.edu/data/penguins_subset.csv, to demonstrate. The data set consists of measurements of three different species of penguins acquired at the Palmer Station in Antarctica. The measurements were made between 2007 and 2009 by Kristen Gorman.\nAs we work toward getting the data in a useful tidy format, we will learn some additional wrangling techniques.\nFirst, let’s take a look at the data set stored in the CSV file.\n!head ../data/penguins_subset.csv\n\nGentoo,Gentoo,Gentoo,Gentoo,Adelie,Adelie,Adelie,Adelie,Chinstrap,Chinstrap,Chinstrap,Chinstrap\nbill_depth_mm,bill_length_mm,flipper_length_mm,body_mass_g,bill_depth_mm,bill_length_mm,flipper_length_mm,body_mass_g,bill_depth_mm,bill_length_mm,flipper_length_mm,body_mass_g\n16.3,48.4,220.0,5400.0,18.5,36.8,193.0,3500.0,18.3,47.6,195.0,3850.0\n15.8,46.3,215.0,5050.0,16.9,37.0,185.0,3000.0,16.7,42.5,187.0,3350.0\n14.2,47.5,209.0,4600.0,19.5,42.0,200.0,4050.0,16.6,40.9,187.0,3200.0\n15.7,48.7,208.0,5350.0,18.3,42.7,196.0,4075.0,20.0,52.8,205.0,4550.0\n14.1,48.7,210.0,4450.0,18.0,35.7,202.0,3550.0,18.7,45.4,188.0,3525.0\n15.0,49.6,216.0,4750.0,19.1,39.8,184.0,4650.0,18.2,49.6,193.0,3775.0\n15.7,49.3,217.0,5850.0,18.4,40.8,195.0,3900.0,17.5,48.5,191.0,3400.0\n15.2,49.2,221.0,6300.0,18.4,36.6,184.0,3475.0,18.2,49.2,195.0,4400.0\nWe see that we have two header rows. The first gives the species and the second the quantity that is being measured. Apparently, then, each row of data has information for three different penguins, one from each species. This is not a tidy data set!",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Making a data frame wide</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/pivoting.html#reading-the-data-set-into-a-polars-data-frame",
    "href": "lessons/eda/wrangling/pivoting.html#reading-the-data-set-into-a-polars-data-frame",
    "title": "16  Making a data frame wide",
    "section": "16.2 Reading the data set into a Polars data frame",
    "text": "16.2 Reading the data set into a Polars data frame\nWe start by naively reading this data set using pl.read_csv().\n\ndf = pl.read_csv(os.path.join(data_path, \"penguins_subset.csv\"))\ndf.head()\n\n\nshape: (5, 12)\n\n\n\nGentoo\nGentoo_duplicated_0\nGentoo_duplicated_1\nGentoo_duplicated_2\nAdelie\nAdelie_duplicated_0\nAdelie_duplicated_1\nAdelie_duplicated_2\nChinstrap\nChinstrap_duplicated_0\nChinstrap_duplicated_1\nChinstrap_duplicated_2\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\nstr\n\n\n\n\n\"bill_depth_mm\"\n\"bill_length_mm\"\n\"flipper_length_mm\"\n\"body_mass_g\"\n\"bill_depth_mm\"\n\"bill_length_mm\"\n\"flipper_length_mm\"\n\"body_mass_g\"\n\"bill_depth_mm\"\n\"bill_length_mm\"\n\"flipper_length_mm\"\n\"body_mass_g\"\n\n\n\"16.3\"\n\"48.4\"\n\"220.0\"\n\"5400.0\"\n\"18.5\"\n\"36.8\"\n\"193.0\"\n\"3500.0\"\n\"18.3\"\n\"47.6\"\n\"195.0\"\n\"3850.0\"\n\n\n\"15.8\"\n\"46.3\"\n\"215.0\"\n\"5050.0\"\n\"16.9\"\n\"37.0\"\n\"185.0\"\n\"3000.0\"\n\"16.7\"\n\"42.5\"\n\"187.0\"\n\"3350.0\"\n\n\n\"14.2\"\n\"47.5\"\n\"209.0\"\n\"4600.0\"\n\"19.5\"\n\"42.0\"\n\"200.0\"\n\"4050.0\"\n\"16.6\"\n\"40.9\"\n\"187.0\"\n\"3200.0\"\n\n\n\"15.7\"\n\"48.7\"\n\"208.0\"\n\"5350.0\"\n\"18.3\"\n\"42.7\"\n\"196.0\"\n\"4075.0\"\n\"20.0\"\n\"52.8\"\n\"205.0\"\n\"4550.0\"\n\n\n\n\n\n\nOoof! This is nasty. The second header row is included with the data, which results in inferring every column data type to be a string. Polars only allows for a single header row, so we cannot load this data set.\nThis is a fairly common occurrence with human-made tabular data. Researchers will have hierarchical column headings. In this case, the first header row is species and the second is the quantity that is being measured for the respective species. (Note that I did this intentionally for instructional purposes; this is not what the original penguins data set had.)\nTo convert this type of structure into a tidy format, we can perform an unpivoting operation where each of the levels of the hierarchical column headings become rows. Polars will not do this for you, since it forbids hierarchical column headings (hierarchical indexing is not a good idea, in my opinion, so I see why Polars forbids it). I therefore wrote a function for the bebi103 package that takes a CSV file, possibly with a hierarchical index, and unpivots it to give a new CSV file. We can use this on the penguins data set and then load in the result.\n\nbebi103.utils.unpivot_csv(\n    os.path.join(data_path, \"penguins_subset.csv\"), \n    os.path.join(data_path, \"penguins_tall.csv\"),\n    header_names=['species', 'quantity'],\n    retain_row_index=True,\n    row_index_name='penguin_id',\n    force_overwrite=True\n)\n\ndf = pl.read_csv(os.path.join(data_path, \"penguins_tall.csv\"))\n\ndf.head(10)\n\n\nshape: (10, 4)\n\n\n\npenguin_id\nspecies\nquantity\nvalue\n\n\ni64\nstr\nstr\nf64\n\n\n\n\n0\n\"Gentoo\"\n\"bill_depth_mm\"\n16.3\n\n\n0\n\"Gentoo\"\n\"bill_length_mm\"\n48.4\n\n\n0\n\"Gentoo\"\n\"flipper_length_mm\"\n220.0\n\n\n0\n\"Gentoo\"\n\"body_mass_g\"\n5400.0\n\n\n0\n\"Adelie\"\n\"bill_depth_mm\"\n18.5\n\n\n0\n\"Adelie\"\n\"bill_length_mm\"\n36.8\n\n\n0\n\"Adelie\"\n\"flipper_length_mm\"\n193.0\n\n\n0\n\"Adelie\"\n\"body_mass_g\"\n3500.0\n\n\n0\n\"Chinstrap\"\n\"bill_depth_mm\"\n18.3\n\n\n0\n\"Chinstrap\"\n\"bill_length_mm\"\n47.6\n\n\n\n\n\n\nNotice that we kept the row index in the column 'penguin_id' to make sure that we didn’t lose track of what penguin each measurement was associated with. Note also that each penguin has a unique identifier when combined with the species. E.g., Adelie penguin with penguin ID 0 has four measurements associated with it.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Making a data frame wide</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/pivoting.html#pivoting-from-tall-to-wide-format",
    "href": "lessons/eda/wrangling/pivoting.html#pivoting-from-tall-to-wide-format",
    "title": "16  Making a data frame wide",
    "section": "16.3 Pivoting from tall to wide format",
    "text": "16.3 Pivoting from tall to wide format\nLooking at the melded data frame above, it is very tall. Each row only has a single value, that is, a single measurement, and every other column in the row is metadata associated with that measurement, specifically which penguin/species and which quantity was being measured.\nWhile this tall format is tidy, we can imagine using a wide format, in which each row is a specific penguin and each column is a quantity measured on that penguin. This is also a tidy format. In some cases, a tall format is more convenient, and in others a wide format is more convenient.\nTo convert from a tall to a wide format, we perform a pivot. A pivot operation classifies columns of a tall data frame in three ways.\n\nThe column(s) we pivot on gets converted into column headings of the pivoted data frame.\nThe value column’s entries get populated under the new column headings determined by the on column.\nThe index column(s) are essentially along for the ride. They are retained as columns. If the index columns are not unique, then you need to specify an aggregating operation that will be applied to the values associated with rows that have like entries in the index column(s).\n\nLet us now perform the pivot. We want 'quantity' as the “on” column, since it takes a column or columns of a data frame and converts them to column headings. In this case, the value column is 'value', and 'penguin_id' and 'species' columns are the index columns. They uniquely define a penguin, so we do not need to provide any aggregating function.\n\ndf = df.pivot(\n    on='quantity', \n    index=['penguin_id', 'species'], \n    values='value'\n)\n\n# Take a look\ndf.head()\n\n\nshape: (5, 6)\n\n\n\npenguin_id\nspecies\nbill_depth_mm\nbill_length_mm\nflipper_length_mm\nbody_mass_g\n\n\ni64\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n0\n\"Gentoo\"\n16.3\n48.4\n220.0\n5400.0\n\n\n0\n\"Adelie\"\n18.5\n36.8\n193.0\n3500.0\n\n\n0\n\"Chinstrap\"\n18.3\n47.6\n195.0\n3850.0\n\n\n1\n\"Gentoo\"\n15.8\n46.3\n215.0\n5050.0\n\n\n1\n\"Adelie\"\n16.9\n37.0\n185.0\n3000.0\n\n\n\n\n\n\nExcellent! We now have a wide, but still tidy, data frame.\nThe 'penguin_id' column is dispensable, but we will keep it for now, since we will demonstrate a pivot operation from this wide data frame in a moment.\n\n16.3.1 A couple of plots for fun\nNow that we’ve done all this work and our data set is tidy, let’s make a plot for fun. First, we’ll plot the ECDFs of the bill lengths.\n\nbokeh.io.show(\n    iqplot.ecdf(\n        data=df,\n        cats='species',\n        q='bill_length_mm',\n        x_axis_label='bill length (mm)',\n        frame_width=400,\n    )\n)\n\n\n  \n\n\n\n\n\nWe can also plot bill length versus flipper length to see if we can see a difference among the species. It is also useful to have a hover tool that shows bill depth and body mass.\n\n# Create figure\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=300,\n    x_axis_label=\"bill length (mm)\",\n    y_axis_label=\"flipper length (mm)\",\n    toolbar_location=\"above\",\n    tooltips=[('bill depth', '@bill_depth_mm'), ('body mass', '@body_mass_g')]\n)\n\n# Build legend as we populate glyphs\nlegend_items = []\nfor color, ((species,), sub_df) in zip(bokeh.palettes.Category10_3, df.group_by(\"species\")):\n    glyph = p.scatter(\n        source=sub_df.to_dict(), x=\"bill_length_mm\", y=\"flipper_length_mm\", color=color\n    )\n    legend_items.append((species, [glyph]))\n\n# Place legend\nlegend = bokeh.models.Legend(items=legend_items, location=\"center\")\np.add_layout(legend, \"right\")\np.legend.click_policy = \"hide\"\n\nbokeh.io.show(p)",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Making a data frame wide</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/pivoting.html#an-important-note-about-tidiness",
    "href": "lessons/eda/wrangling/pivoting.html#an-important-note-about-tidiness",
    "title": "16  Making a data frame wide",
    "section": "16.4 An important note about tidiness",
    "text": "16.4 An important note about tidiness\nIt is important to note that there is more than one way to make a data set tidy. In the example of the Palmer penguin data set, we saw two legitimate ways of making the data frame tidy. In our preferred wide version, each row corresponded to a measurement of a single penguin, which had several variables associated with it. In the tall version, each row corresponded to a single feature of a penguin.\nTo demonstrate that the tall version is workable, but more cumbersome, we can make the same plots as above. First, we’ll unpivot the data frame to make it tall. We need to specify which columns as index columns for the unpivot because we want the 'penguin_id' and 'species' columns to remain in the tall data frame (which we did not have to do in the previous lesson).\n\ndf = df.unpivot(index=[\"penguin_id\", \"species\"])\n\ndf.head()\n\n\nshape: (5, 4)\n\n\n\npenguin_id\nspecies\nvariable\nvalue\n\n\ni64\nstr\nstr\nf64\n\n\n\n\n0\n\"Gentoo\"\n\"bill_depth_mm\"\n16.3\n\n\n0\n\"Adelie\"\n\"bill_depth_mm\"\n18.5\n\n\n0\n\"Chinstrap\"\n\"bill_depth_mm\"\n18.3\n\n\n1\n\"Gentoo\"\n\"bill_depth_mm\"\n15.8\n\n\n1\n\"Adelie\"\n\"bill_depth_mm\"\n16.9\n\n\n\n\n\n\nPlotting the ECDFs is not really a problem with this form of the data frame. We just need to use a filter context to pull out the bill length rows.\n\nbokeh.io.show(\n    iqplot.ecdf(\n        data=df.filter(pl.col('variable') == 'bill_length_mm'),\n        q=\"value\",\n        cats=\"species\",\n        frame_width=400,\n        x_axis_label=\"bill length (mm)\",\n    )\n)\n\n\n  \n\n\n\n\n\nMaking the scatter plot, however, is much more difficult and involves a lot of filtering by hand.\n\n# Set up figure\np = bokeh.plotting.figure(\n    frame_width=300,\n    frame_height=300,\n    x_axis_label=\"bill length (mm)\",\n    y_axis_label=\"flipper length (mm)\",\n    toolbar_location=\"above\",\n)\n\n# Expressions for filtering\nbill_length = pl.col('variable') == \"bill_length_mm\"\nflipper_length = pl.col('variable') == \"flipper_length_mm\"\n\n# Build legend as we populate glyphs\nlegend_items = []\nfor color, ((species,), sub_df) in zip(bokeh.palettes.Category10_3, df.group_by('species')):\n    # Slice out bill and flipper lengths for species\n    bill = sub_df.filter(bill_length).get_column('value').to_numpy()\n    flipper = sub_df.filter(flipper_length).get_column('value').to_numpy()\n    \n    # Populate glyph\n    glyph = p.scatter(bill, flipper, color=color)\n    legend_items.append((species, [glyph]))\n\n# Build and place legend\nlegend = bokeh.models.Legend(items=legend_items, location=\"center\")\np.add_layout(legend, \"right\")\np.legend.click_policy = \"hide\"\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThis works fine, but is more cumbersome. Importantly, we could not use a column data source in the plot to enable display of more data upon hover. The moral of the story is that you should tidy your data, but you should think carefully about in what way you want your data to be tidy.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Making a data frame wide</span>"
    ]
  },
  {
    "objectID": "lessons/eda/wrangling/pivoting.html#computing-environment",
    "href": "lessons/eda/wrangling/pivoting.html#computing-environment",
    "title": "16  Making a data frame wide",
    "section": "16.5 Computing environment",
    "text": "16.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,pandas,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\npandas    : 2.3.2\nbokeh     : 3.8.0\niqplot    : 0.3.7\njupyterlab: 4.4.7",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Making a data frame wide</span>"
    ]
  },
  {
    "objectID": "lessons/eda/eda_lesson_exercise.html",
    "href": "lessons/eda/eda_lesson_exercise.html",
    "title": "17  EDA lesson exercises",
    "section": "",
    "text": "Exercise 1\n| Download notebook\nWhat is split-apply-combine and why is it important that a data set is tidy when doing split-apply-combine operations?",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>EDA lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/eda/eda_lesson_exercise.html#exercise-2",
    "href": "lessons/eda/eda_lesson_exercise.html#exercise-2",
    "title": "17  EDA lesson exercises",
    "section": "Exercise 2",
    "text": "Exercise 2\nWhat is the difference between joining and concatenating data frames?",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>EDA lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/eda/eda_lesson_exercise.html#exercise-3",
    "href": "lessons/eda/eda_lesson_exercise.html#exercise-3",
    "title": "17  EDA lesson exercises",
    "section": "Exercise 3",
    "text": "Exercise 3\nDescribe the difference between categorical and quantitative variables. How are they fundamentally different in the way we plot them?",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>EDA lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/eda/eda_lesson_exercise.html#exercise-4",
    "href": "lessons/eda/eda_lesson_exercise.html#exercise-4",
    "title": "17  EDA lesson exercises",
    "section": "Exercise 4",
    "text": "Exercise 4\nGive pros and cons for using a histogram for display of repeated measurements. Then give pros and cons for using an ECDF.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>EDA lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/eda/eda_lesson_exercise.html#exercise-5",
    "href": "lessons/eda/eda_lesson_exercise.html#exercise-5",
    "title": "17  EDA lesson exercises",
    "section": "Exercise 5",
    "text": "Exercise 5\nWrite down any questions or points of confusion that you have.",
    "crumbs": [
      "Exploratory data analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>EDA lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_and_sampling.html",
    "href": "lessons/probability_and_sampling/probability_and_sampling.html",
    "title": "Probability: The foundation for generative modeling",
    "section": "",
    "text": "Any data set we encounter was generated by some process, usually a process that involved the ingenuity, blood, sweat, and tears of an experimenter. It we want to learn something more general about nature from acquired data, we need to have a model for the data generation process. Rob Phillips said it beautifully in his book Physical Biology of the Cell, “Quantitative data demand quantitative models.” We call models that describe the process of generating data generative models.\nWe will see in the following lessons that building generative models requires the mathematical machinery of probability and we model data generation with generative probability distributions.\nWhen we perform an experiment and obtain data, we are sampling out of the generative distribution. The true generative distribution is unknown, but by sampling out of it, we gain insights about the generative process. For example, if I measure the heights of a collection of humans, I learn something about the generative distribution just by investigating the samples out of it (the measured data).\nSimilarly, we can learn a lot about probability distributions, including model generative distributions, by sampling out of them directly using random number generation. In this section, we will also learn about the techniques for doing so.\nWe will proceed with a lack of formality, but will nonetheless give useful working definitions of probability and aspects thereof with an eye for putting them to use for modeling and interpreting data.",
    "crumbs": [
      "Probability: The foundation for generative modeling"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html",
    "href": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html",
    "title": "18  Probability: definitions and interpretations",
    "section": "",
    "text": "18.1 Interpretations of probability\nI will be a little formal1 for a moment here as we construct this mathematical notion of probability. First, we need to define the world of possibilities. We denote by \\(\\Omega\\) a sample space, which is the set of all outcomes we could observe in a given experiment. We define an event \\(A\\) to be a subset of \\(\\Omega\\) (\\(A\\subseteq\\Omega\\)). Two events, \\(A_i\\) and \\(A_j\\) are disjoint, also called mutually exclusive, if \\(A_i \\cap A_j = \\emptyset\\). That is to say that two events are disjoint if they do not overlap at all in the sample space; they do not share any outcomes. So, in common terms, the sample space \\(\\Omega\\) contains all possible outcomes of an experiment. An event \\(A\\) is a given outcome or set of outcomes. Two events are disjoint if they are totally different from each other.\nWe define the probability of event \\(A\\) to be \\(P(A)\\), where \\(P\\) is a probability function. It maps the event \\(A\\) to a real number between zero and one. In order to be a probability, the function \\(P\\) must satisfy the following axioms.\nPutting together these axioms, we see that probability consists of positive real numbers that are distributed among the events of a sample space. The sum total of these real numbers over all of the sample space is one. So, a probability function and a sample space go hand-in-hand. For many of our applications, the sample space consists of set of numbers like the real numbers, integers, and subsets of real numbers and integers.\nBefore we go on to talk more about probability, it will help to be thinking about how we can apply it to understand measured data. To do that, we need to think about how probability is interpreted. Note that these are interpretations of probability, not definitions. We have already defined probability, and both of the two dominant interpretations below are valid.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability: definitions and interpretations</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#interpretations-of-probability",
    "href": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#interpretations-of-probability",
    "title": "18  Probability: definitions and interpretations",
    "section": "",
    "text": "18.1.1 Frequentist probability.\nIn the frequentist interpretation of probability, the probability \\(P(A)\\) represents a long-run frequency over a large number of identical repetitions of an experiment. These repetitions can be, and often are, hypothetical. The event \\(A\\) is restricted to propositions about random variables, quantities that can very meaningfully from experiment to experiment.2 So in the frequentist view, we are using probability to understand how the results of an experiment might vary from repetition to repetition.\n\n\n18.1.2 Bayesian probability.\nHere, \\(P(A)\\) is interpreted to directly represent the degree of belief, or plausibility, about \\(A\\). So, \\(A\\) can be any logical proposition, not just a random variable.\nYou may have heard about a split, or even a fight, between people who use Bayesian and frequentist interpretations of probability applied to statistical inference. There is no need for a fight. The two ways of approaching statistical inference differ in their interpretation of probability, the tool we use to quantify uncertainty. Both are valid.\nIn my opinion, the Bayesian interpretation of probability is more intuitive to apply to scientific inference. It always starts with a simple probabilistic expression and proceeds to quantify plausibility. It is conceptually cleaner to me, since we can talk about plausibility of anything, including parameter values. In other words, Bayesian probability serves to quantify our own knowledge, or degree of certainty, about a hypothesis or parameter value. Conversely, in frequentist statistical inference, the parameter values are fixed (they are not random variables; they cannot vary meaningfully from experiment to experiment), and we can only study how repeated experiments will convert the real parameter value to an observation.\nThat is my opinion, and I view fights over such things counterproductive. Frequentist methods are also very useful and powerful, and in this class, we will almost exclusively use them. Next term, we will use almost exclusively Bayesian methods.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability: definitions and interpretations</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#the-sum-rule-the-product-rule-and-conditional-probability",
    "href": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#the-sum-rule-the-product-rule-and-conditional-probability",
    "title": "18  Probability: definitions and interpretations",
    "section": "18.2 The sum rule, the product rule, and conditional probability",
    "text": "18.2 The sum rule, the product rule, and conditional probability\nThe sum rule, which may be derived from the axioms defining probability, says that the probability of all events must add to unity. Let \\(A^c\\) be all events except \\(A\\), called the complement of \\(A\\). Then, the sum rule states that\n\\[\\begin{aligned}\n  P(A) + P(A^c) = 1.\\end{aligned}\\]\nNow, let’s say that we are interested in event \\(A\\) happening given that event \\(B\\) happened. So, \\(A\\) is conditional on \\(B\\). We denote this conditional probability as \\({P(A\\mid B)}\\). Given this notion of conditional probability, we can write the sum rule as\n\\[\\begin{aligned}\n\\text{(sum rule)} \\qquad P(A\\mid B) + P(A^c \\mid B) = 1,\n\\end{aligned}\\]\nfor any \\(B\\).\nThe product rule states that\n\\[\\begin{aligned}\n  P(A, B) = P(A\\mid B)\\, P(B),\\end{aligned}\\]\nwhere \\(P(A,B)\\) is the probability of both \\(A\\) and \\(B\\) happening. (It could be written as \\(P(A\\cap B)\\).) The product rule is also referred to as the definition of conditional probability. It can similarly be expanded as we did with the sum rule.\n\\[\\begin{aligned}\n\\text{(product rule)} \\qquad P(A, B\\mid C) = P(A\\mid B, C)\\, P(B \\mid C),\n\\end{aligned}\\]\nfor any \\(C\\).",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability: definitions and interpretations</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#bayess-theorem",
    "href": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#bayess-theorem",
    "title": "18  Probability: definitions and interpretations",
    "section": "18.3 Bayes’s Theorem",
    "text": "18.3 Bayes’s Theorem\nNote that because “and” is commutative, \\(P(A, B) = P(B, A)\\). We apply the product rule to both sides of this seemingly trivial equality.\n\\[\\begin{aligned}\nP(A \\mid B)\\, P(B) =  P(A, B)\n= P(B,A) = P(B \\mid A)\\, P(A).\n\\end{aligned}\\]\nIf we take the terms at the beginning and end of this equality and rearrange, we get\n\\[\\begin{aligned}\n\\text{(Bayes's theorem)} \\qquad  P(A \\mid B) = \\frac{P(B \\mid A)\\, P(A)}{P(B)}.\n\\end{aligned}\\]\nThis result is called Bayes’s theorem. This result holds for probability, regardless of how it is interpreted, frequentist, Bayesian, or otherwise.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability: definitions and interpretations</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#marginalization",
    "href": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#marginalization",
    "title": "18  Probability: definitions and interpretations",
    "section": "18.4 Marginalization",
    "text": "18.4 Marginalization\nLet \\(\\{A_i\\}\\) be a set of outcomes indexed by \\(i\\). Then,\n\\[\\begin{aligned}\n\\begin{aligned}\n1 &= P(A_j\\mid B) + P(A_j^c \\mid B) \\nonumber \\\\\n&= P(A_j\\mid B) + \\sum_{i\\ne j}P(A_i\\mid B) \\nonumber \\\\\n&= \\sum_iP(A_i\\mid B).\\end{aligned}\n\\end{aligned}\\]\nNow, Bayes’s theorem gives us an expression for \\(P(A_i\\mid B)\\), so we can compute the sum.\n\\[\\begin{aligned}\n\\begin{aligned}\n\\sum_iP(A_i\\mid B) &= \\sum_i\\frac{P(B \\mid A_i)\\, P(A_i)}{P(B)} \\nonumber \\\\\n&= \\frac{1}{P(B)}\\sum_i P(B \\mid A_i)\\, P(A_i) \\nonumber \\\\\n&= 1.\n\\end{aligned}\n\\end{aligned}\\]\nTherefore, we have\n\\[\\begin{aligned}\nP(B) = \\sum_i P(B \\mid A_i)\\, P(A_i).\\end{aligned}\\]\nUsing the definition of conditional probability, we also have\n\\[\\begin{aligned}\nP(B) = \\sum_i P(B,A_i)\n\\end{aligned}\\]\nThis process of eliminating a variable (in this case the \\(A_i\\)'s) in the joint distribution by summing is called marginalization.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability: definitions and interpretations</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#footnotes",
    "href": "lessons/probability_and_sampling/definition_and_interpretation_of_probability.html#footnotes",
    "title": "18  Probability: definitions and interpretations",
    "section": "",
    "text": "But not too formal. For example, we are not discussing \\(\\sigma\\) algebras, measurability, etc.↩︎\nMore formally, a random variable transforms the possible outcomes of an experiment to real numbers.↩︎",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Probability: definitions and interpretations</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_distributions.html",
    "href": "lessons/probability_and_sampling/probability_distributions.html",
    "title": "19  Probability distributions",
    "section": "",
    "text": "19.1 Joint and conditional distributions and Bayes’s theorem for PDFs\nSo far we have talked about probability of events, and we have in mind measurements, parameter values and hypotheses as the events. We have a bit of a problem, though, if the sample space consists of real numbers, which we often encounter in our experiments and modeling. The probability of getting a single real value is identically zero. This is my motivation for introducing probability distributions, but the concept is more general and has much more utility than just dealing with sample spaces containing real numbers. Importantly, probability distributions provide the link between outcomes in the sample space to probability. Probability distributions describe both discrete quantities (like integers) and continuous quantities (like real numbers).\nThough we cannot assign a nonzero the probability for an outcome from a sample space containing all of the real numbers, we can assign a probability that the outcome is less than some real number. Notationally, we write this as\n\\[\\begin{aligned}\nP(\\text{having outcome that is}\\le y) = F(y).\n\\end{aligned} \\tag{19.1}\\]\nThe function \\(F(y)\\), which returns a probability, is called a cumulative distribution function (CDF), or just distribution function. It contains all of the information we need to know about how probability is assigned to \\(y\\). A CDF for a Normal distribution is shown in the left panel of the figure below.\nRelated to the CDF for a continuous quantity is the probability density function, or PDF. The PDF is given by the derivative of the CDF,\n\\[\\begin{aligned}\nf(y) = \\frac{\\mathrm{d}F(y)}{\\mathrm{d}y}.\n\\end{aligned} \\tag{19.2}\\]\nNote that \\(f(y)\\) is not the probability of outcome \\(y\\). Rather, the probability that of outcome \\(y\\) lying between \\(y_0\\) and \\(y_1\\) is\n\\[\\begin{aligned}\nP(y_0\\le y \\le y_1) = F(y_1) - F(y_0) = \\int_{y_0}^{y_1}\\mathrm{d}y\\,f(y).\n\\end{aligned} \\tag{19.3}\\]\nNote that with this definition of the probability density function, satisfaction of the axiom that all probabilities sum to zero (equivalently stated as \\(F(y\\to\\infty) = 1\\)) necessitates that the probability density function is normalized. That is,\n\\[\\begin{aligned}\n\\int_{-\\infty}^\\infty \\mathrm{d}t\\, f(y) = 1.\n\\end{aligned} \\tag{19.4}\\]\nConversely, for a discrete quantity, we have a probability mass function, or PMF,\n\\[\\begin{aligned}\nf(y) = P(y).\n\\end{aligned} \\tag{19.5}\\]\nThe PMF is a probability, unlike the PDF. An example of a CDF and a PMF for a discrete distribution are shown in the figure below. In this example, \\(n\\) is the outcome of the roll of a fair die (\\(n\\in\\{1,2,3,4,5,6\\}\\)).\nWe have defined a PDF as \\(f(x)\\), that is, describing a single variable \\(x\\). We can have joint distributions with a PDF \\(f(x, y)\\).\nWe may also have conditional distributions that have PDF \\(f(x\\mid y)\\). This is interpreted similarly to conditional probabilities we have already seen. \\(f(x\\mid y)\\) is the probability density function for \\(x\\), given \\(y\\). As similar relation between joint and conditional PDFs holds as in the case of joint and conditional probabilities.\n\\[\\begin{aligned}\nf(x\\mid y) = \\frac{f(x,y)}{f(y)}.\n\\end{aligned} \\tag{19.6}\\]\nThat this holds is not at all obvious. One immediate issue is that we are conditioning on an event \\(y\\) that has zero probability. We will not carefully derive why this holds, but state it without proof.\nAs a consequence, Bayes's theorem also holds for PDFs, as it does for probabilities.1\n\\[\\begin{aligned}\nf(\\theta\\mid y) = \\frac{f(y\\mid \\theta)\\,f(\\theta)}{f(y)}.\n\\end{aligned} \\tag{19.7}\\]\nNotationally in this course, we will use \\(f\\) to describe a PDF or PMF of a random variable and \\(g\\) to describe the PMF or PDF of a parameter or other logical conjecture that is not measured data or a random variable. For example, \\(f(y \\mid \\theta)\\) is the PDF for a continuous measured quantity and \\(g(\\theta)\\) is the PDF for a parameter value. In this notation, Bayes’s theorem is\n\\[\\begin{aligned}\ng(\\theta\\mid y) = \\frac{f(y\\mid \\theta)\\,g(\\theta)}{f(y)}.\n\\end{aligned} \\tag{19.8}\\]\nFinally, we can marginalize probability distribution functions to get marginalized PDFs.\n\\[\\begin{aligned}\nf(x) = \\int \\mathrm{d}y\\,f(x,y) = \\int\\mathrm{d}y\\,f(x\\mid y)\\,f(y).\n\\end{aligned} \\tag{19.9}\\]\nIn the case of a discrete distribution, we can compute marginal a marginal PMF.\n\\[\\begin{aligned}\nf(x) = \\sum_i\\,f(x,y_i) = \\sum_i f(x\\mid y_i)\\,f(y_i).\n\\end{aligned} \\tag{19.10}\\]",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_distributions.html#change-of-variables-formula-for-continuous-distributions",
    "href": "lessons/probability_and_sampling/probability_distributions.html#change-of-variables-formula-for-continuous-distributions",
    "title": "19  Probability distributions",
    "section": "19.2 Change of variables formula for continuous distributions",
    "text": "19.2 Change of variables formula for continuous distributions\nAs a last note about probability distributions, I discuss the change of variables formula. Say I have a continuous probability distribution with PDF \\(f_X(x)\\). I have included the subscript \\(X\\) to denote that this is a PDF describing the variable \\(X\\). If I wish to change variables to instead get a continuous distribution in \\(y=y(x)\\), or \\(f_Y(y) = f_Y(y(x))\\), how do I get \\(f_Y\\)? We must enforce that the distributions be normalized;\n\\[\\begin{align}\n\\int \\mathrm{d}x\\, f_X(x) =  \\int \\mathrm{d}y\\, f_Y(y) = 1.\n\\end{align} \\tag{19.11}\\]\nThus, we must have \\(\\left|\\mathrm{d}y\\,f_Y(y)\\right| = \\left|\\mathrm{d}x\\,f_x(x)\\right|\\). Equivalently, we have\n\\[\\begin{align}\nf_Y(y) = \\left|\\frac{\\mathrm{d} x}{\\mathrm{d}y}\\right|\\,f_X(x).\n\\end{align} \\tag{19.12}\\]\nThis is the change of variables formula.\n\n19.2.1 Generalization to multiple dimensions\nGenerically, if we have a set of variables \\(\\mathbf{x}\\) that are transformed into a new set of parameters \\(\\mathbf{y} = \\mathbf{y}(\\mathbf{x})\\), then\n\\[\\begin{align}\nf_Y(y) = \\left|\\frac{\\partial(x_1, x_2, \\ldots)}{\\partial(y_1, y_2, \\ldots)}\\right|f_X(x),\n\\end{align} \\tag{19.13}\\]\nwhere the first factor on the right hand side is the Jacobian, which is the absolute value of the determinant of the Jacobi matrix,\n\\[\\begin{aligned}\n\\begin{align}\n\\frac{\\partial(x_1, x_2, \\ldots)}{\\partial(y_1, y_2, \\ldots)} = \\begin{pmatrix}\n\\frac{\\partial x_1}{\\partial y_1} & \\frac{\\partial x_1}{\\partial y_2} & \\cdots \\\\\n\\frac{\\partial x_2}{\\partial y_1} & \\frac{\\partial x_2}{\\partial y_2} & \\cdots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{pmatrix}   .\n\\end{align}\n\\end{aligned} \\tag{19.14}\\]\n\n\n19.2.2 An example of change of variables\nImagine I have a random variable that is Exponentially distributed, such that\n\\[\\begin{align}\nf_X(x) = \\beta \\, \\mathrm{e}^{-\\beta x}.\n\\end{align} \\tag{19.15}\\]\nNow saw that I want to rescale \\(x\\) so that I instead get a distribution in \\(y = a x\\). Here, \\(g(x) = a x\\) and \\(g^{-1}(y) = y/a\\). So, we have\n\\[\\begin{align}\nf_Y(y) = \\left|\\frac{\\mathrm{d}}{\\mathrm{d}y}\\,\\frac{y}{a}\\right|\\,f_X(y/a)\n= \\frac{1}{a}\\,\\beta\\,\\mathrm{e}^{-\\beta y / a}.\n\\end{align} \\tag{19.16}\\]\nThe distribution is again Exponential, but the rate has been rescaled, \\(\\beta \\to \\beta/a\\). This makes sense; we have rescaled \\(x\\) by our change of variables, so the rate should be rescaled accordingly.\n\n\n19.2.3 Another example of change of variables: the Log-Normal distribution\nNow imagine I have a random variable that is Normally distributed and I wish to determine how \\(y = \\mathrm{e}^{x}\\) is distributed.\n\\[\\begin{align}\nf_X(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\,\\mathrm{e}^{-(x-\\mu)^2/2\\sigma^2}.\n\\end{align} \\tag{19.17}\\]\nHere, \\(g(x) = \\mathrm{e}^x\\) and \\(g^{-1}(y) = \\ln y\\). Again applying the change of variables formula,\n\\[\\begin{align}\nf_Y(y) = \\left|\\frac{\\mathrm{d}\\,\\ln y}{\\mathrm{d}y}\\right|\\,f_X(\\ln y)\n= \\frac{1}{y\\sqrt{2\\pi \\sigma^2}}\\,\\mathrm{e}^{-(\\ln y-\\mu)^2/2\\sigma^2},\n\\end{align} \\tag{19.18}\\]\nwhich is indeed the PDF of the Log-Normal distribution.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_distributions.html#sec-probability-distribution-stories",
    "href": "lessons/probability_and_sampling/probability_distributions.html#sec-probability-distribution-stories",
    "title": "19  Probability distributions",
    "section": "19.3 Probability distributions as stories",
    "text": "19.3 Probability distributions as stories\nThe building blocks of statistical models are probability distributions. Specifying a model amounts to choosing probability distributions that describe the process of data generation. In some cases, you need to derive the distribution based on specific considerations or your experiment or model (or even numerically compute it when it cannot be written in closed form). In many practical cases, though, your model is composed of standard probability distributions. These distributions have stories associated with them. That is, the mathematical particulars of the distribution follow from a description of a data generation process. For example, the story behind the Bernoulli distribution is as follows. The outcome of a coin flip is Bernoulli distributed. So in building models, if your data generation process matches the story of a distribution, you know that this is the distribution to choose for your model.\nThe Distribution Explorer is a useful tool to connect distributions to stories and obtain their PDFs/PMFs and CDFs, as well as syntax for usage in popular software packages. I encourage you to explore the Explorer!",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_distributions.html#footnotes",
    "href": "lessons/probability_and_sampling/probability_distributions.html#footnotes",
    "title": "19  Probability distributions",
    "section": "",
    "text": "This is very subtle. Jaynes’s book, Probability: The Logic of Science, Cambridge University Press, 2003, for more one these subtleties.↩︎",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/random_number_generation.html",
    "href": "lessons/probability_and_sampling/random_number_generation.html",
    "title": "20  Random number generation",
    "section": "",
    "text": "| Download notebook\n\nRandom number generation (RNG) is the process by which a sequence of random numbers may be drawn. When using a computer to draw the random numbers, the numbers are not completely random. The notion of “completely random” is nonsensical because of the infinitude of numbers. Random numbers must be drawn from some probability distribution. Furthermore, in most computer applications, the random numbers are actually pseudorandom. They depend entirely on an input seed and are then generated by a deterministic algorithm from that seed.\nSo what do (pseudo)random number generators do? RNGs are capable of (approximately) drawing integers from a Discrete Uniform distribution. For example, NumPy’s default built-in generator, the PCG64 generator, generates 128 bit numbers, allowing for \\(2^{128}\\), or about \\(10^{38}\\), possible integers. Importantly, each draw of of a random integer is (approximately) independent of all others.\nIn practice, the drawn integers are converted to floating-point numbers (since a double floating-point number has far less than 128 bits) on the interval [0, 1) by dividing a generated random integer by \\(2^{138}\\). Effectively, then, the random number generators provide draws out of a Uniform distribution on the interval [0, 1).\nTo convert from random numbers on a Uniform distribution to random numbers from a nonuniform distribution, we need a transform. For many named distributions convenient transforms exist. For example, the Box-Muller transform is often used to get random draws from a Normal distribution. In the absence of a clever transform, we can use a distribution’s quantile function, also known as a percent-point function, which is the inverse CDF, \\(F^{-1}(y)\\). For example, the quantile function for the Exponential distribution is\n\\[\n\\begin{aligned}\nF^{-1}(p) = -\\beta^{-1}\\,\\ln(1-p),\n\\end{aligned} \\tag{20.1}\\]\nwhere \\(p\\) is the value of the CDF, ranging from zero to one. We first draw \\(p\\) out of a Uniform distribution on [0, 1), and then compute \\(F^{-1}(p)\\) to get a draw from an Exponential distribution. A graphical illustration of using a quantile function to draw 50 random numbers out of Gamma(5, 2) is shown below.\n\n\nCode\nimport numpy as np\nimport scipy.stats as st\n\nimport bokeh.io\nimport bokeh.plotting\nbokeh.io.output_notebook(hide_banner=True)\n\nrng = np.random.default_rng(seed=12341234)\nalpha = 5\nbeta = 2\ny = np.linspace(0, 8, 400)\ncdf = st.gamma.cdf(y, alpha, loc=0, scale=1 / beta)\n\nudraws = rng.uniform(size=50)\n\np = bokeh.plotting.figure(\n    width=300,\n    height=200,\n    x_axis_label=\"y\",\n    y_axis_label=\"F(y; 2, 5)\",\n    x_range=[0, 8],\n    y_range=[0, 1],\n)\np.xgrid.grid_line_color = None\np.ygrid.grid_line_color = None\n\np.line(y, cdf, line_width=2)\n\nfor u in udraws:\n    x_vals = [0] + [st.gamma.ppf(u, alpha, loc=0, scale=1 / beta)] * 2\n    y_vals = [u, u, 0]\n    p.line(x_vals, y_vals, color=\"gray\", line_width=0.5)\n\np.scatter(np.zeros_like(udraws), udraws, marker=\"x\", color=\"black\", line_width=0.5, size=7)\np.scatter(\n    st.gamma.ppf(udraws, alpha, loc=0, scale=1 / beta),\n    np.zeros_like(udraws),\n    line_width=0.5,\n    fill_color=None,\n    line_color=\"black\",\n)\n\np.renderers[0].level = \"overlay\"\np.renderers[-1].level = \"overlay\"\np.renderers[-2].level = \"overlay\"\n\nbokeh.io.show(p)\n\n\n\n  \n\n\n\nEach draw from a Uniform distribution, marked by an × on the vertical axis, is converted to a draw from a Gamma distribution, marked by ○ on the horizontal axis, by computing the inverse CDF. Because the draws of the random integers from which the draws from a Uniform distribution on \\([0, 1)\\) are independent, we get independent draws from the Gamma distribution.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Random number generation</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html",
    "title": "21  Random number generation using Numpy",
    "section": "",
    "text": "21.1 Uniform random numbers\n| Download notebook\nA good portion of the random number generation functionality you will need is in the np.random module. It allows for draws of independent random numbers for many convenient named distributions. The scipy.stats module offers even more distributions, but for most applications, Numpy’s generators suffice and are typically faster than using Scipy, which has more overhead.\nLet’s start by generating random numbers from a Uniform distribution.\nnp.random.uniform(low=0, high=1, size=10)\n\narray([0.40312242, 0.04051189, 0.03851483, 0.41999136, 0.99686689,\n       0.86482983, 0.57146661, 0.71342451, 0.20395884, 0.32728968])\nThe function uniform() in the np.random module generates random numbers on the interval [low, high) from a Uniform distribution. The size keyword argument is how many random numbers you wish to generate, and is a keyword argument in all Numpy’s functions to draw from specific distributions. The random numbers are returned as a Numpy array.\nWe can check to make sure it is appropriately drawing random numbers out of the Uniform distribution by plotting the cumulative distribution function. We’ll generate 1,000 random numbers and plot them along with the CDF of a Uniform distribution.\n# Generate random numbers\nx = np.random.uniform(low=0, high=1, size=1000)\n\n# Plot the ECDF of randomly generated numbers\np = iqplot.ecdf(x, marker_kwargs={\"fill_color\": None},)\n\np.line(\n    x=[0, 1], y=[0, 1], line_width=2, line_color=\"orange\",\n)\n\nbokeh.io.show(p)\nSo, it looks like our random number generator is doing a good job.\nGenerating random numbers on the uniform interval is one of the most commonly used RNG applications. For example, you can simulate flipping a biased (unfair) coin by drawing from a Uniform distribution and then asking if the random number if less than the bias.\n# Generate 20 random numbers on uniform interval\nx = np.random.uniform(low=0, high=1, size=20)\n\n# Make the coin flips (&lt; 0.7 means we have a 70% chance of heads)\nheads = x &lt; 0.7\n\n# Show which were heads, and count the number of heads\nprint(heads)\nprint(\"\\nThere were\", np.sum(heads), \"heads.\")\n\n[ True  True False  True  True  True  True False  True  True  True  True\n  True  True  True False False False  True  True]\n\nThere were 15 heads.\nOf course, you could also do this by drawing out of a Binomial distribution.\nprint(f\"There were {np.random.binomial(20, 0.7)} heads.\")\n\nThere were 12 heads.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html#sec-choice-of-generator",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html#sec-choice-of-generator",
    "title": "21  Random number generation using Numpy",
    "section": "21.2 Choice of generator",
    "text": "21.2 Choice of generator\nAs of version 1.23 of Numpy, the algorithm under the hood of calls to functions like np.random.uniform() is the Mersenne Twister Algorithm for generating random numbers. It is a very widely used and reliable method for generating random numbers. However, starting with version 1.17, the numpy.random module offers random number generators with better speed and statistical performance, including a 64-bit permuted congruential generator (PCG64). Going forward, the preferred approach to doing random number generation is to first instantiate a generator of your choice, and then use its methods to generate numbers out of probability distributions.\nLet’s set up a PCG64 generator, which is Numpy’s default (though this will soon be updated to the PCG64 DXSM, which works better for massively parallel generation, per Numpy’s documentation).\n\nrng = np.random.default_rng()\n\nNow that we have the generator, we can use it to draw numbers out of distributions. The syntax is the same as before, except rng replaces np.random.\n\nrng.uniform(low=0, high=1, size=20)\n\narray([0.83950893, 0.18995913, 0.7455994 , 0.65977687, 0.50251107,\n       0.3920528 , 0.98069072, 0.43620495, 0.42139065, 0.67577344,\n       0.44188521, 0.42143143, 0.64836288, 0.29192271, 0.17708979,\n       0.92594815, 0.04850722, 0.06206417, 0.09710099, 0.75790207])\n\n\nOr, for the Binomial,\n\nrng.binomial(20, 0.7)\n\n18",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html#sec-seeding-rngs",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html#sec-seeding-rngs",
    "title": "21  Random number generation using Numpy",
    "section": "21.3 Seeding random number generators",
    "text": "21.3 Seeding random number generators\nNow, just to demonstrate that random number generation is deterministic, we will explicitly seed the random number generator (which is usually seeded with a number representing the date/time to avoid repeats) to show that we get the same random numbers.\n\n# Instantiate generator with a seed\nrng = np.random.default_rng(seed=3252)\n\n# Draw random numbers\nrng.uniform(size=10)\n\narray([0.18866535, 0.04418857, 0.02961285, 0.22083971, 0.43341773,\n       0.13166813, 0.42112164, 0.43507845, 0.61380912, 0.30627603])\n\n\nIf we reinstantiate with the same seed, we get the same sequence of random numbers.\n\n# Re-seed the RNG\nrng = np.random.default_rng(seed=3252)\n\n# Draw random numbers\nrng.uniform(size=10)\n\narray([0.18866535, 0.04418857, 0.02961285, 0.22083971, 0.43341773,\n       0.13166813, 0.42112164, 0.43507845, 0.61380912, 0.30627603])\n\n\nThe random number sequence is exactly the same. If we choose a different seed, we get totally different random numbers.\n\nrng = np.random.default_rng(seed=3253)\nrng.uniform(size=10)\n\narray([0.31390226, 0.73012457, 0.05800998, 0.01557021, 0.29825701,\n       0.10106784, 0.06329107, 0.58614237, 0.52023168, 0.52779988])\n\n\nIf you are writing tests, it is often useful to seed the random number generator to get reproducible results. Otherwise, it is best to use the default seed, based on the date and time, so that you get a new set of random numbers in your applications each time do computations.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-dists",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-dists",
    "title": "21  Random number generation using Numpy",
    "section": "21.4 Drawing random numbers out of other distributions",
    "text": "21.4 Drawing random numbers out of other distributions\nSay we wanted to draw random samples from a Normal distribution with mean μ and standard deviation σ.\n\n# Set parameters\nmu = 10\nsigma = 1\n\n# Draw 100000 random samples\nx = rng.normal(mu, sigma, size=100000)\n\n# Plot the histogram\np = iqplot.histogram(x, rug=False, density=True, y_axis_label=\"approximate PDF\",)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nIt looks Normal, but, again, comparing the resulting ECDF is a better way to look at this. We’ll check out the ECDF with 1000 samples so as not to choke the browser with the display. I will also make use of the theoretical CDF for the Normal distribution available from the scipy.stats module.\n\n# Compute theoretical CDF\nx_theor = np.linspace(6, 14, 400)\ny_theor = st.norm.cdf(x_theor, mu, sigma)\n\n# Plot the ECDF of randomly generated numbers\np = iqplot.ecdf(x, marker_kwargs={\"fill_color\": None},)\n\np.line(\n    x=x_theor, y=y_theor, line_width=2, line_color=\"orange\",\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nYup, right on!",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-choice",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-choice",
    "title": "21  Random number generation using Numpy",
    "section": "21.5 Choosing elements from an array",
    "text": "21.5 Choosing elements from an array\nIt is often useful to randomly choose elements from an existing array. (Actually, this is probably the functionality we will use the most, since it is used in bootstrapping.) The rng.choice() function does this. You equivalently could do this using rng.integers(), where the integers represent indices in the array, except rng.choice() has a great keyword argument, replace, which allows random draws with or without replacement. For example, say you had 50 samples that you wanted to send to a facility for analysis, but you can only afford to send 20. If we used rng.integers(), we might have a problem.\n\nrng = np.random.default_rng(seed=126969234)\nrng.integers(0, 51, size=20)\n\narray([12, 31, 47, 26,  3,  5, 46, 49, 26, 38, 24, 17, 46, 26,  6, 17, 35,\n        4, 13, 29])\n\n\nSample 17 was selected twice and sample 26 was selected thrice. This is not unexpected. We can use rng.choice() instead.\n\nrng.choice(np.arange(51), size=20, replace=False)\n\narray([27, 34,  0, 46,  2, 48, 35, 50, 40, 12, 28, 19, 37, 38, 11, 23, 45,\n       15, 29, 32])\n\n\nNow, because we chose replace=False, we do not get any repeats.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-shuffle",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-shuffle",
    "title": "21  Random number generation using Numpy",
    "section": "21.6 Shuffling an array",
    "text": "21.6 Shuffling an array\nSimilarly, the rng.permutation() function is useful. It takes the entries in an array and shuffles them! Let’s shuffle a deck of cards.\n\nrng.permutation(np.arange(53))\n\narray([12, 18,  2, 34, 27, 10,  0, 30, 49,  7,  5, 35, 11, 23, 37, 17,  4,\n       44, 15, 28, 14,  8, 40, 21, 39, 36, 46, 24, 33, 20, 22,  1, 41, 45,\n       50, 26, 16, 42, 52,  3,  9, 48, 38, 25, 43, 51, 19, 47, 32,  6, 13,\n       29, 31])",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-when-do-we-need",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html#sec-rng-when-do-we-need",
    "title": "21  Random number generation using Numpy",
    "section": "21.7 When do we need RNG?",
    "text": "21.7 When do we need RNG?\nAnswer: VERY OFTEN! We will use random number generator extensively as we explore probability distributions.\nIn many ways, probability is the language of biology. Molecular processes have energetics that are comparable to the thermal energy, which means they are always influenced by random thermal forces. The processes of the central dogma, including DNA replication, are no exceptions. This gives rise to random mutations, which are central to understanding how evolution works. And of course neuronal firing is also probabilistic. If we want to understand how biology works, it is often useful to use random number generators to model the processes.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/rng_with_numpy.html#computing-environment",
    "href": "lessons/probability_and_sampling/rng_with_numpy.html#computing-environment",
    "title": "21  Random number generation using Numpy",
    "section": "21.8 Computing environment",
    "text": "21.8 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 9.1.0\n\nnumpy     : 2.1.3\nscipy     : 1.15.2\nbokeh     : 3.6.2\niqplot    : 0.3.7\njupyterlab: 4.3.6",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Random number generation using Numpy</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/luria_delbruck.html",
    "href": "lessons/probability_and_sampling/luria_delbruck.html",
    "title": "22  Simulating the Luria-Delbrück distribution",
    "section": "",
    "text": "22.1 Simulating data generation\n| Download notebook\nWe have seen that we can draw random numbers out of distributions for which we have a convenient transform or access to a quantile function. We have also seen that we can derive a probability mass function or probability density function from the story of a distribution, and those PMFs/PDFs give us complete information about the distribution. Sometimes, though, it is difficult or impossible to derive a PMF or PDF from a story. In other situations, we may know the PMF or PDF buy cannot derive a transform nor an easily evaluated quantile function. In these cases, we can simulate the story of the distribution using random number generation.\nIn this section, as an example, we will learn how to simulate the Luria-Delbrück distribution.\nThe Luria-Delbrück distribution (also known as a jackpot distribution) is a classic example from the biological sciences of a distribution whose story is easy to state, but whose PMF is very difficult to write down. (It was written down by Lea and Colson, but the expression fills a couple pages and is not terribly usable.)\nWhen mutations occur in nature, they are often deleterious to the organism. However, mutations are a critical part of the genetic heritage of living organisms, arising in every type of organism and allowing life to evolve and adapt to new environments. In 1943, the question of how microorganisms acquire mutations was described in a famous paper by Salvador Luria and Max Delbrück (S. E. Luria and M. Delbrück, Genetics, 28, 491–511, 1943). At the time, there were two prominent theories of genetic inheritance, the “random mutation hypothesis,” in which mutations arose randomly in the absence of an environmental cue, and the “adaptive immunity hypothesis, in which mutations occur as an adaptive response to an environmental stimulus. See the figure below.\nTo test these two hypotheses, Luria and Delbrück grew many parallel cultures of bacteria and then plated each culture on agar containing phages (which infect and kill nearly all of the bacteria). Although most bacteria are unable to survive in the presence of phages, often mutations could enable a few survivors to give rise to resistant mutant colonies. If the adaptive immunity hypothesis is correct, mutations occur only after bacteria come in contact with phages, thus only after plating the bacteria on phage-agar plates. Under the random mutation hypothesis, some bacteria already have the immunity before being exposed.\nThe story of the Luria-Delbrück distribution arises from the random mutation hypothesis. Start with a single cell that cannot survive being exposed to phage. When it divides, there is a probability \\(\\theta\\) one of the daughter cells gets a mutation that will impart survivability. This is true for each division. Once a mutation that imparts survival is obtained, it is passed to all subsequent generations. The number \\(n\\) of survivors in \\(N\\) individual cells exposed to the stress is distributed according to the Luria-Delbrück distribution.\nWe will not attempt to write down the PMF, but will instead sample out of the Luria-Delbrück distribution by directly simulating its story. To do the simulation, we note that if we have a population of \\(M\\) unmutated cells, we will have \\(M\\) cell divisions. The number of cell divisions that result in a mutation is Binomially distributed, Binom(M, θ). So, to simulate the a Luria-Delbrück experiment, at each round of cell division, we draw a random number of a Binomial distribution parametrized by the number of cells that have not yet had mutations and \\(\\theta\\). For after the \\(g\\)th set of cell divisions, we have \\(2^g\\) total cells. If we have \\(n_\\mathrm{mut}\\) cells with favorable mutations when we had \\(2^{g-1}\\) cells, then we have \\(2 n_\\mathrm{mut} + n_\\mathrm{mut}^\\mathrm{new}\\), where \\(n_\\mathrm{mut}^\\mathrm{new}\\) is drawn from \\(\\text{Binom}(2^{g-1}-n_\\mathrm{mut}, \\theta)\\), after the \\(g\\)th set of cell divisions. Let’s code it up!\n@numba.njit\ndef draw_random_mutation(n_gen, theta):\n    \"\"\"Draw a sample out of the Luria-Delbruck distribution\n\n    Parameters\n    ----------\n    n_gen : int\n        Number of generations. At the end of the experiment, there are\n        `2**n_gen` cells.\n    theta : float\n        Probability of obtaining a mutation in a single cell division.\n\n    Returns\n    -------\n    output : int\n        Number of cells that will survive stress.\n    \"\"\"\n    # Initialize number of mutants\n    n_mut = 0\n\n    for g in range(n_gen + 1):\n        n_mut = 2 * n_mut + np.random.binomial(2**(g-1) - 2 * n_mut, theta)\n        \n    return n_mut\nThis function draws a single number of survivors. To get a picture of the distribution, we need to make many, many draws, so we write a function to call this function repeatedly to get the samples.\n@numba.njit\ndef sample_random_mutation(n_gen, theta, n_samples=1):\n    \"\"\"Sample out of the Luria-Delbruck distribution\n\n    Parameters\n    ----------\n    n_gen : int\n        Number of generations. At the end of the experiment, there are\n        `2**n_gen` cells.\n    theta : float\n        Probability of obtaining a mutation in a single cell division.\n    n_samples : int\n        Number of samples to draw\n\n    Returns\n    -------\n    output : Numpy array of ints\n        Draws of number of cells that will survive stress.\n    \"\"\"    \n    # Initialize samples\n    samples = np.empty(n_samples)\n    \n    # Draw the samples\n    for i in range(n_samples):\n        samples[i] = draw_random_mutation(n_gen, theta)\n        \n    return samples\nLet’s put it to use! We will draw a million samples for 16 generations with a mutation rate of \\(10^{-5}\\).\nsamples = sample_random_mutation(16, 1e-5, 1_000_000).astype(int)\nLuria and Delbrück knew that if the Fano factor, the ratio of the variance to the mean, was much bigger than one, the adaptive immunity hypothesis (which has a predicted Fano factor of 1—can you explain why?). We can compute the Fano factor of the Luria-Delbrück distribution from the samples.\nprint(\n    \"\"\"\nRandom mutation hypothesis\n--------------------------\nmean:        {mean:.4f}\nvariance:    {var:.4f}\nFano factor: {fano:.4f}\n\"\"\".format(\n        mean=np.mean(samples),\n        var=np.var(samples),\n        fano=np.var(samples) / np.mean(samples),\n    )\n)\n\n\nRandom mutation hypothesis\n--------------------------\nmean:        5.2874\nvariance:    21690.4510\nFano factor: 4102.2952\nWow! Huge variance and bit Fano factor. We can get a feeling for the PMF by making a spike plot (We will avoid an ECDF in this case because we have LOTS of samples and we want to take it easy on the browser.)\n# Print probability of getting zero\nprint(f\"Fraction with zero survivors: {np.sum(samples==0) / len(samples)}\")\n\nbokeh.io.show(\n    iqplot.spike(\n        samples, \n        fraction=True,\n        x_range=[0.5, 2e5],\n        y_range=[1/len(samples), 1],\n        x_axis_type='log', \n        y_axis_type='log', \n        x_axis_label='number of survivors',\n    )\n)\n\nFraction with zero survivors: 0.519538\nWe have a very heavy tail, which decays according to a power law. We also see artifacts due to the discrete nature of the cell divisions, where powers of two are more likely. Using just the powers of two, the apparent power law is approximately \\(P(n) \\sim n^{-1}\\), which is not even normalizable in the limit of an infinite number of plated cells. The fact that we have a finite number of cells keeps the PMF normalizable. A very heavy tail, indeed!",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simulating the Luria-Delbrück distribution</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/luria_delbruck.html#simulating-data-generation",
    "href": "lessons/probability_and_sampling/luria_delbruck.html#simulating-data-generation",
    "title": "22  Simulating the Luria-Delbrück distribution",
    "section": "",
    "text": "Figure 22.1: Luria and Delbrück assessed two competing hypotheses for the role genetic mutations play in evolution by natural selection.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simulating the Luria-Delbrück distribution</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/luria_delbruck.html#computing-environment",
    "href": "lessons/probability_and_sampling/luria_delbruck.html#computing-environment",
    "title": "22  Simulating the Luria-Delbrück distribution",
    "section": "22.2 Computing environment",
    "text": "22.2 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\nbokeh     : 3.7.3\niqplot    : 0.3.7\njupyterlab: 4.4.5",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simulating the Luria-Delbrück distribution</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_and_sampling_lesson_exercise.html",
    "href": "lessons/probability_and_sampling/probability_and_sampling_lesson_exercise.html",
    "title": "23  Probability and sampling lesson exercises",
    "section": "",
    "text": "Exercise 1\n| Download notebook\nWhat is the relationship between a cumulative distribution function (CDF) and a probability density function (PDF)? What is the relationship between as CDF and a probability mass function (PMF)?",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Probability and sampling lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_and_sampling_lesson_exercise.html#exercise-2",
    "href": "lessons/probability_and_sampling/probability_and_sampling_lesson_exercise.html#exercise-2",
    "title": "23  Probability and sampling lesson exercises",
    "section": "Exercise 2",
    "text": "Exercise 2\nHow would you expect each of the following to be distributed?\na) The amount of time between repressor-operator binding events.\nb) The number of times a repressor binds its operator in a given hour.\nc) The amount of time (in total minutes of baseball played) between no-hitters in Major League Baseball.\nd) The number of no-hitters in a Major League Baseball season.\ne) The winning times of the Belmont Stakes.\nTo answer this question, try to match these stories to the stories of named distributions. For those of you not familiar with baseball, a no-hitter is a game in which a team concedes no hits to the opposing team. There have only been a few hundred no-hitters in over 200,000 MLB games. The Belmont Stakes is a major horse race that has been run each year for over 150 years.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Probability and sampling lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/probability_and_sampling/probability_and_sampling_lesson_exercise.html#exercise-3",
    "href": "lessons/probability_and_sampling/probability_and_sampling_lesson_exercise.html#exercise-3",
    "title": "23  Probability and sampling lesson exercises",
    "section": "Exercise 3",
    "text": "Exercise 3\nWrite down any questions or points of confusion that you have.",
    "crumbs": [
      "Probability: The foundation for generative modeling",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Probability and sampling lesson exercises</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/nonparametric_inference.html",
    "href": "lessons/nonparametric/nonparametric_inference.html",
    "title": "Nonparametric inference and confidence intervals",
    "section": "",
    "text": "Now that we have a basis in probability theory, we proceed to statistical inference. One of the most widely used concepts in statistical inference is the plug-in principle. In fact, you have probably employed it countless times but didn’t even realize it! In what follows, we will formally define the plug-in principle and the important concept of confidence intervals for our first foray into inference.\nSpecifically, we will learn about how to perform statistical inference when we do not have a generative model in mind. Because there is no generative model, there are no parameters, and, as such, this class of inference problems are referred to as nonparametric. As you may have guessed, we will employ the plug-in principle to use the empirical distribution as an approximation for the unknown true generative distribution.",
    "crumbs": [
      "Nonparametric inference and confidence intervals"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/point_estimates.html",
    "href": "lessons/nonparametric/plugin/point_estimates.html",
    "title": "24  Plug-in estimates",
    "section": "",
    "text": "24.1 The plug-in principle\nWhen we make measurements, we are observing samples out of an underlying, typically unknown, generative distribution. Take as an example measurements of the count of RNA transcripts of a given gene in individual cells. We are likely to observe counts where the probability mass function of the generative distribution are is high and unlikely to observe counts where the PMF of the generative distribution is low.\nIf we have a full understanding of the generative distribution, we have learned how the data were generated, and thereby have an understanding of the physical, chemical, or biological phenomena we are studying. Statistical inference involves deducing properties of these (unknown) generative distributions.\nIn this lecture, we will start with nonparametric inference, which is statistical inference where no model is assumed; conclusions are drawn from the data alone. The approach we will take is heavily inspired by Allen Downey’s wonderful book, Think Stats and from Larry Wasserman’s book All of Statistics.\nLet’s first think about how to get an estimate for a parameter value, given the data. While what we are about to do is general, for now it is useful to have in your mind a concrete example. Imagine we have a data set that is a set of repeated measurements, such as the repeated measurements of lengths of eggs laid by C. elegans worms of a given genotype.\nWe could have a generative model in mind, and we will do this in coming lessons. Instead, we will assume we only know that there is a generative distribution, but nothing about what it may be. Let \\(F(x)\\) be the cumulative distribution function (CDF) for the generative distribution.\nA statistical functional is a functional of the CDF, \\(T(F)\\). A parameter \\(\\theta\\) of a probability distribution can be defined from a functional, \\(\\theta = T(F)\\). For example, the mean, variance, and median are all statistical functionals.\n\\[\\begin{aligned}\n\\begin{aligned}\n    &\\text{mean} \\equiv \\mu = \\int_{-\\infty}^\\infty \\mathrm{d}x\\,x\\,f(x) = \\int_{-\\infty}^\\infty \\mathrm{d}F(x)\\,x, \\\\[1em]\n    &\\text{variance} \\equiv \\sigma^2 = \\int_{-\\infty}^\\infty \\mathrm{d}x\\,(x-\\mu)^2\\,f(x) = \\int_{-\\infty}^\\infty \\mathrm{d}F(x)\\,(x-\\mu)^2, \\\\[1em]\n    &\\text{median} \\equiv m = F^{-1}(1/2).\\end{aligned}\n\\end{aligned}\\]\nNow, say we made a set of \\(n\\) measurements, \\(\\{x_1, x_2, \\ldots x_n\\}\\). You can think of this as a set of C. elegans egg lengths if you want to have an example in your mind. We define the empirical cumulative distribution function, \\(\\hat{F}(x)\\) from our data as\n\\[\\begin{aligned}\n   \\hat{F}(x) = \\frac{1}{n}\\sum_{i=1}^n I(x_i \\le x),\n\\end{aligned}\\]\nwith\n\\[\\begin{aligned}\n\\begin{aligned}\n   I(x_i \\le x) = \\left\\{\n   \\begin{array}{ccl}\n       1 && x_i \\le x \\\\[0.5em]\n       0 && x_i &gt; x.\n   \\end{array}\n   \\right.\n\\end{aligned}\n\\end{aligned}\\]\nWe have already seen this form of the ECDF when we were studying exploratory data analysis. Remember that the probability density function (PDF), \\(f(x)\\), is related to the CDF by\n\\[\\begin{aligned}\n   f(x) = \\frac{\\mathrm{d}F}{\\mathrm{d}x}.\n\\end{aligned}\\]\nWe can then differentiate the ECDF to get the empirical density function, \\(\\hat{f}(x)\\) as\n\\[\\begin{aligned}\n   \\hat{f}(x) = \\frac{1}{n}\\sum_{i=1}^n \\delta(x - x_i),\n\\end{aligned}\\]\nwhere \\(\\delta(x)\\) is the Dirac delta function.\nWith the ECDF (and empirical density function), we have now defined an empirical distribution that is dependent only on the data. We now define a plug-in estimate of a parameter \\(\\theta\\) as\n\\[\\begin{aligned}\n   \\hat{\\theta} = T(\\hat{F}).\n\\end{aligned}\\]\nIn other words, to get a plug-in estimate a parameter \\(\\theta\\), we need only to compute the functional using the empirical distribution. That is, we simply “plug in” the empirical CDF for the actual CDF.\nThe plug-in estimate for the median is easy to calculate.\n\\[\\begin{aligned}\n   \\hat{m} = \\hat{F}^{-1}(1/2),\n\\end{aligned}\\]\nor the middle-ranked data point. The plug-in estimate for the mean or variance seem at face to be a bit more difficult to calculate, but the following general theorem will help. Consider a functional of the form of an expectation value, \\(r(x)\\).\n\\[\\begin{aligned}\n\\begin{aligned}\n   \\int\\mathrm{d}\\hat{F}(x)\\,r(x) &= \\int \\mathrm{d}x \\,r(x)\\, \\hat{f}(x)\n   = \\int \\mathrm{d}x\\, r(x) \\left[\\frac{1}{n}\\sum_{i=1}^n\\delta(x - x_i)\\right] \\nonumber \\\\[1em]\n   &= \\frac{1}{n}\\sum_{i=1}^n\\int \\mathrm{d}x \\,r(x) \\delta(x-x_i)\n   = \\frac{1}{n}\\sum_{i=1}^n r(x_i).\n\\end{aligned}\n\\end{aligned}\\]\nA functional of this form is called a linear statistical functional. The result above means that the plug-in estimate for a linear functional of a distribution is the arithmetic mean of the observed \\(r(x)\\) themselves. The plug-in estimate of the mean, which has \\(r(x) = x\\), is\n\\[\\begin{aligned}\n   \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n x_i \\equiv \\bar{x},\n\\end{aligned}\\]\nwhere we have defined \\(\\bar{x}\\) as the traditional sample mean (the arithmetic mean of the measured data), which we have just shown is the plug-in estimate. This plug-in estimate is implemented in the np.mean() function. The plug-in estimate for the variance is\n\\[\\begin{aligned}\n   \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2\n   = \\frac{1}{n}\\sum_{i=1}^n x_i^2 - \\bar{x}^2.\n\\end{aligned}\\]\nThis plug-in estimate is implemented in the np.var() function.\nNote that we are denoting the mean and variance as \\(\\mu\\) and \\(\\sigma^2\\), but these are not in general the parameters with the same common name and symbols from a Normal distribution. Any distribution has a first moment (called a mean) and a second central moment (called a variance), unless they do not exist, as is the case, e.g., with a Cauchy distribution. In this context, we denote by \\(\\mu\\) and \\(\\sigma^2\\) the mean and variance of the unknown underlying univariate generative distribution.\nWe can compute plug-in estimates for more complicated parameters as well. For example, for a bivariate distribution, the correlation between the two variables \\(x\\) and \\(y\\), is defined with\n\\[\\begin{aligned}\n   \\rho = \\frac{\\left\\langle (x-\\mu_x)(y-\\mu_y)\\right\\rangle}{\\sigma_x \\sigma_y},\n\\end{aligned}\\]\nwhere the expectation in the numerator is called the covariance between \\(x\\) and \\(y\\). It is of large magnitude of \\(x\\) and \\(y\\) vary together and close to zero if they are nearly independent of each other. The plug-in estimate for the correlation is\n\\[\\begin{aligned}\n   \\hat{\\rho} = \\frac{\\sum_i(x_i - \\bar{x})(y_i-\\bar{y})}{\\sqrt{\\left(\\sum_i(x_i-\\bar{x})^2\\right)\\left(\\sum_i(y_i-\\bar{y})^2\\right)}}.\n\\end{aligned}\\]",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Plug-in estimates</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/bias.html",
    "href": "lessons/nonparametric/plugin/bias.html",
    "title": "25  Bias",
    "section": "",
    "text": "25.1 Bias of the plug-in estimate for the mean\nThe bias of an estimate is the difference between the expectation value of the point estimate and value of the parameter.\n\\[\\begin{aligned}\n   \\text{bias}_F(\\hat{\\theta}, \\theta) = \\langle \\hat{\\theta} \\rangle - \\theta\n   = \\int\\mathrm{d}x\\, \\hat{\\theta}f(x) - T(F).\n\\end{aligned}\\]\nNote that the expectation value of \\(\\hat{\\theta}\\) is computed over the (unknown) generative distribution whose PDF is \\(f(x)\\).\nWe often want a small bias because we want to choose estimates that give us back the parameters we expect. Let's first investigate the bias of the plug-in estimate of the mean. As a reminder, the plug-in estimate is\n\\[\\begin{aligned}\n   \\hat{\\mu} = \\bar{x},\n\\end{aligned}\\]\nwhere \\(\\bar{x}\\) is the arithmetic mean of the observed data. To compute the bias of the plug-in estimate, we need to compute \\(\\langle \\hat{\\mu}\\rangle\\) and compare it to \\(\\mu\\).\n\\[\\begin{aligned}\n   \\langle \\hat{\\mu}\\rangle = \\langle \\bar{x}\\rangle = \\frac{1}{n}\\left\\langle\\sum_i x_i\\right\\rangle\n   = \\frac{1}{n}\\sum_i \\left\\langle x_i\\right\\rangle\n   = \\langle x\\rangle\n   = \\mu.\n\\end{aligned}\\]\nBecause \\(\\langle \\hat{\\mu}\\rangle = \\mu\\), the bias in the plug-in estimate for the mean is zero. It is said to be unbiased.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Bias</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/bias.html#bias-of-the-plug-in-estimate-for-the-variance",
    "href": "lessons/nonparametric/plugin/bias.html#bias-of-the-plug-in-estimate-for-the-variance",
    "title": "25  Bias",
    "section": "25.2 Bias of the plug-in estimate for the variance",
    "text": "25.2 Bias of the plug-in estimate for the variance\nTo compute the bias of the plug-in estimate for the variance, first recall that the variance, as the second central moment, is computed as\n\\[\\begin{aligned}\n   \\sigma^2 = \\langle x^2 \\rangle - \\langle x\\rangle^2.\n\\end{aligned}\\]\nSo, the expectation value of the plug-in estimate is\n\\[\\begin{aligned}\n\\begin{aligned}\n\\left\\langle \\hat{\\sigma}^2 \\right\\rangle &= \\left\\langle\\frac{1}{n}\\sum_i x_i^2 - \\bar{x}^2\\right\\rangle \\\\[1em]\n&= \\left\\langle\\frac{1}{n}\\sum_i x_i^2\\right\\rangle - \\left\\langle\\bar{x}^2\\right\\rangle\\\\[1em]\n&= \\frac{1}{n}\\sum_i \\left\\langle x_i^2\\right\\rangle  - \\left\\langle\\bar{x}^2\\right\\rangle \\\\[1em]\n&= \\langle x^2 \\rangle - \\left\\langle\\bar{x}^2\\right\\rangle\\\\[1em]\n&= \\mu^2 + \\sigma^2 - \\left\\langle\\bar{x}^2\\right\\rangle.\n\\end{aligned}\n\\end{aligned}\\]\nWe now need to compute \\(\\left\\langle\\bar{x}^2\\right\\rangle\\), which is a little trickier. We will use the fact that the measurements are independent, so \\(\\left\\langle x_i x_j\\right\\rangle = \\langle x_i \\rangle \\langle x_j\\rangle\\) for \\(i\\ne j\\).\n\\[\\begin{aligned}\n\\begin{aligned}\n\\left\\langle\\bar{x}^2\\right\\rangle\n&= \\left\\langle\\left(\\frac{1}{n}\\sum_ix_i\\right)^2\\right\\rangle \\\\[1em]\n&= \\frac{1}{n^2}\\left\\langle\\left(\\sum_ix_i\\right)^2 \\right\\rangle \\\\[1em]\n&= \\frac{1}{n^2}\\left\\langle\\sum_i x_i^2 + 2\\sum_i\\sum_{j&gt;i}x_i x_j\\right\\rangle \\nonumber \\\\[1em]\n&= \\frac{1}{n^2}\\left(\\sum_i \\left\\langle x_i^2\\right\\rangle\n+ 2\\sum_i\\sum_{j&gt;i}\\left\\langle x_i x_j\\right\\rangle \\right) \\\\[1em]\n&= \\frac{1}{n^2}\\left(n(\\sigma^2 + \\mu^2)\n+ 2\\sum_i\\sum_{j&gt;i}\\langle x_i\\rangle \\langle x_j\\rangle\\right) \\nonumber \\\\[1em]\n&=\\frac{1}{n^2}\\left(n(\\sigma^2 + \\mu^2) + n(n-1)\\langle x\\rangle^2\\right)\\\\[1em]\n&= \\frac{1}{n^2}\\left(n\\sigma^2 + n^2\\mu^2\\right) \\\\[1em]\n&= \\frac{\\sigma^2}{n} + \\mu^2.\n\\end{aligned}\n\\end{aligned}\\]\nThus, we have\n\\[\\begin{aligned}\n\\left\\langle \\hat{\\sigma}^2 \\right\\rangle = \\left(1-\\frac{1}{n}\\right)\\sigma^2.\n\\end{aligned}\\]\nTherefore, the bias is\n\\[\\begin{aligned}\n   \\text{bias} = -\\frac{\\sigma^2}{n}.\n\\end{aligned}\\]\nIf \\(\\hat{\\sigma}^2\\) is the plug-in estimate for the variance, an unbiased estimator would instead be\n\\[\\begin{aligned}\n  \\frac{n}{n-1}\\,\\hat{\\sigma}^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2.\n\\end{aligned}\\]",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Bias</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/bias.html#justification-of-using-plug-in-estimates.",
    "href": "lessons/nonparametric/plugin/bias.html#justification-of-using-plug-in-estimates.",
    "title": "25  Bias",
    "section": "25.3 Justification of using plug-in estimates.",
    "text": "25.3 Justification of using plug-in estimates.\nDespite the apparent bias in the plug-in estimate for the variance, we will normally just use plug-in estimates going forward. (We will use the hat, e.g. \\(\\hat{\\theta}\\), to denote an estimate, which can be either a plug-in estimate or not.) Note that the bootstrap procedures we lay out in what follows do not need to use plug-in estimates, but we will use them for convenience. Why do this? The bias is typically small. We just saw that the biased and unbiased estimators of the variance differ by a factor of \\(n/(n-1)\\), which is negligible for large \\(n\\). In fact, plug-in estimates tend to have much smaller error than the confidence intervals for the parameter estimate, which we will discuss next.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Bias</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/confidence_intervals.html",
    "href": "lessons/nonparametric/plugin/confidence_intervals.html",
    "title": "26  Confidence intervals",
    "section": "",
    "text": "26.1 The frequentist interpretation of probability\nBefore we start taking about confidence intervals, it is important to recall the frequentist interpretation of probability. Under this interpretation, the probability \\(P(A)\\) represents a long-run frequency of event \\(A\\) over a large number of identical repetitions of an experiment. In our calculation of confidence intervals and in performance of null hypothesis significance tests, we will directly apply this interpretation of probability again and again, using our computers to “repeat” experiments many times and tally the frequencies of what we see.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/confidence_intervals.html#confidence-intervals",
    "href": "lessons/nonparametric/plugin/confidence_intervals.html#confidence-intervals",
    "title": "26  Confidence intervals",
    "section": "26.2 Confidence intervals",
    "text": "26.2 Confidence intervals\nConsider the following question. If I were to do the experiment again, what value might I expect for my plug-in estimate for my functional? What if I did it again and again and again? These are reasonable questions because the plug-in estimate is something that can vary meaningfully from experiment to experiment. Remember, with the frequentist interpretation of probability, we cannot assign a probability to a parameter value. A parameter has one value, and that’s that. We can describe the long-term frequency of observing results about random variables. Because a plug-in estimate for a statistical functional does vary from experiment to experiment, it is a random variable. So, we can define a 95% confidence interval as follows.\n\nIf an experiment is repeated over and over again, the estimate I compute for a parameter, \\(\\hat{\\theta}\\), will lie between the bounds of the 95% confidence interval for 95% of the experiments.\n\nWhile this is a correct definition of a confidence interval, some statisticians prefer another. To quote Larry Wasserman from his book, All of Statistics,\n\n[The above definition] is correct but useless since we rarely repeat the same experiment over and over. A better interpretation is this: On day 1, you collect data and construct a 95 percent confidence interval for a parameter \\(\\theta_1\\). On day 2, you collect new data and construct a 95 percent confidence interval for an unrelated parameter \\(\\theta_2\\). On day 3, you collect new data and construct a 95 percent confidence interval for an unrelated parameter \\(\\theta_3\\). You continue this way constructing confidence intervals for a sequence of unrelated parameters \\(\\theta_1, \\theta_2, \\ldots\\). Then 95 percent of your intervals will trap the true parameter value. There us no need to introduce the idea of repeating the same experiment over and over.\n\nIn other words, the confidence interval describes the construction of the confidence interval itself. 95% of the time, it will contain the true (unknown) parameter value. Wasserman’s description contains a reference to the true parameter value, so if you are going to talk about the true parameter value, this description is useful. However, the first definition of the confidence interval is quite useful if you want to think about how repeated experiments will end up.\nWe will use the first definition in thinking about how to construct a confidence interval. To construct the confidence interval, then, we will repeat the experiment over and over again, each time computing \\(\\hat{\\theta}\\). We will then generate an ECDF of our \\(\\hat{\\theta}\\) values, and report the 2.5th and 97.5th percentile to get our 95% confidence interval. But wait, how will we repeat the experiment so many times?",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/confidence_intervals.html#bootstrap-confidence-intervals",
    "href": "lessons/nonparametric/plugin/confidence_intervals.html#bootstrap-confidence-intervals",
    "title": "26  Confidence intervals",
    "section": "26.3 Bootstrap confidence intervals",
    "text": "26.3 Bootstrap confidence intervals\nRemember that the data come from a generative distribution with CDF \\(F(x)\\). Doing an experiment where we make \\(n\\) measurements amounts to drawing \\(n\\) numbers out of \\(F(x)\\)1. So, we could draw out of \\(F(x)\\) over and over again. The problem is, we do not know what \\(F(x)\\) is. However, we do have an empirical approximation for \\(F(x)\\), namely \\(\\hat{F}(x)\\). So, we could draw \\(n\\) samples out of \\(\\hat{F}(x)\\), compute \\(\\hat{\\theta}\\) from these samples, and repeat. This procedure is called bootstrapping.\nTo get the terminology down, a bootstrap sample, \\(\\mathbf{x}^*\\), is a set of \\(n\\) \\(x\\) values drawn from \\(\\hat{F}(x)\\). A bootstrap replicate is the estimate \\(\\hat{\\theta}^*\\) obtained from the bootstrap sample \\(\\mathbf{x}^*\\). To generate a bootstrap sample, consider an array of measured values \\(\\mathbf{x}\\). We draw \\(n\\) values out of this array with replacement to give us \\(\\mathbf{x}^*\\). This is equivalent to sampling out of \\(\\hat{F}(x)\\).\nSo, the recipe for generating a bootstrap confidence interval is as follows.\n\nGenerate \\(B\\) independent bootstrap samples. Each one is generated by drawing \\(n\\) values out of the data array with replacement.\nCompute \\(\\hat{\\theta}^*\\) for each bootstrap sample to get the bootstrap replicates.\nThe central \\(100 (1-\\alpha)\\) percent confidence interval consists of the percentiles \\(100\\alpha/2\\) and \\(100(1-\\alpha/2)\\) of the bootstrap replicates.\n\nThis procedure works for any estimate \\(\\hat{\\theta}\\), be it the mean, median, variance, skewness, kurtosis, or any other thing you can think of. Note that we use the empirical distribution, so there is never any assumption of an underlying \"true\" distribution. We are employing the plug-in principle for repeating experiments. Instead of sampling out of the generative distribution (which is what performing an experiment is), we plug-in the empirical distribution and sample out of it, instead. Thus, we are doing nonparametric inference on what we would expect for parameters coming out of unknown distributions; we only know the data.\nThere are plenty of subtleties and improvements to this procedure, but this is most of the story. We will discuss the mechanics of how to programmatically generate bootstrap replicates in forthcoming lessons, but we have already covered the main idea.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/plugin/confidence_intervals.html#footnotes",
    "href": "lessons/nonparametric/plugin/confidence_intervals.html#footnotes",
    "title": "26  Confidence intervals",
    "section": "",
    "text": "We’re being loose with language here. We’re drawing out of the distribution that has CDF \\(F(x)\\), but we’re saying “draw out of F” for short.↩︎",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/intro_bootstrap.html",
    "href": "lessons/nonparametric/bootstrap/intro_bootstrap.html",
    "title": "27  Performing bootstrap calculations",
    "section": "",
    "text": "27.1 The data set\n| Download notebook\nDataset download\nWe have discussed the plug-in principle, in which we approximate the distribution function of the generative model, \\(F\\), with the empirical distribution function, \\(\\hat{F}\\). We then approximate all statistical functionals of \\(F\\) using \\(\\hat{F}\\); \\(T(F)\\approx T(\\hat{F})\\). Using the frequentist perspective, we talked about using the plug-in principle and bootstrapping to compute confidence intervals and do hypothesis testing. In this lesson, we will go over how we implement this in practice. We will work with some real data on the reproductive health of male bees that have been treated with pesticide.\nIt is important as you go through this lesson to remember that you do not know what the generative distribution is. We are only approximating it using the plug-in principle. Since we do not know what \\(F\\) is, we do not know what its parameters are, so we therefore cannot estimate them. Rather, we can study summary statistics, which are plug-in estimates for statistical functionals of the generative distribution, such as means, variances, or even the ECDF itself, and how they may vary from experiment to experiment. In this sense, we are doing nonparametric inference.\nNeonicotinoid pesticides are thought to have inadvertent effects on service-providing insects such as bees. A study of this was featured in the New York Times. The original paper by Straub and coworkers may be found here. The authors put their data in the Dryad repository, which means that it is free to all to work with!\n(Do you see a trend here? If you want people to think deeply about your results, explore them, learn from them, in general further science with them, make your data publicly available. Strongly encourage the members of your lab to do the same.)\nWe will look at the sperm quality of drone bees using this data set. First, let’s load in the data set and check it out.\ndf = pl.read_csv(\n    os.path.join(data_path, \"bee_sperm.csv\"),\n    null_values=[\"NO SPERM\", \"No Sperm\"],\n    comment_prefix=\"#\",\n)\ndf.head()\n\n\nshape: (5, 18)\n\n\n\nSpecimen\nTreatment\nEnvironment\nTreatmentNCSS\nSample ID\nColony\nCage\nSample\nSperm Volume per 500 ul\nQuantity\nViabilityRaw (%)\nQuality\nAge (d)\nInfertil\nAliveSperm\nQuantity Millions\nAlive Sperm Millions\nDead Sperm Millions\n\n\ni64\nstr\nstr\ni64\nstr\ni64\ni64\ni64\ni64\ni64\nf64\nf64\ni64\ni64\ni64\nf64\nf64\nf64\n\n\n\n\n227\n\"Control\"\n\"Cage\"\n1\n\"C2-1-1\"\n2\n1\n1\n2150000\n2150000\n96.726381\n96.726381\n14\n0\n2079617\n2.15\n2.079617\n0.070383\n\n\n228\n\"Control\"\n\"Cage\"\n1\n\"C2-1-2\"\n2\n1\n2\n2287500\n2287500\n96.349808\n96.349808\n14\n0\n2204001\n2.2875\n2.204001\n0.083499\n\n\n229\n\"Control\"\n\"Cage\"\n1\n\"C2-1-3\"\n2\n1\n3\n87500\n87500\n98.75\n98.75\n14\n0\n86406\n0.0875\n0.086406\n0.001094\n\n\n230\n\"Control\"\n\"Cage\"\n1\n\"C2-1-4\"\n2\n1\n4\n1875000\n1875000\n93.287421\n93.287421\n14\n0\n1749139\n1.875\n1.749139\n0.125861\n\n\n231\n\"Control\"\n\"Cage\"\n1\n\"C2-1-5\"\n2\n1\n5\n1587500\n1587500\n97.792506\n97.792506\n14\n0\n1552456\n1.5875\n1.552456\n0.035044\nWe are interested in the number of alive sperm in the samples. Let’s first explore the data by making ECDFs for the two groups we will compare, those treated with pesticide (“Pesticide”) and those that are not (“Control”).\np = iqplot.ecdf(df, q=\"Alive Sperm Millions\", cats=\"Treatment\")\n\nbokeh.io.show(p)\nThe visual inspection of the ECDFs suggests that indeed the control drones have more alive sperm than those treated with pesticide. But how variable would these ECDFs be if we repeated the experiment?",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Performing bootstrap calculations</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/intro_bootstrap.html#bootstrap-samples-and-ecdfs",
    "href": "lessons/nonparametric/bootstrap/intro_bootstrap.html#bootstrap-samples-and-ecdfs",
    "title": "27  Performing bootstrap calculations",
    "section": "27.2 Bootstrap samples and ECDFs",
    "text": "27.2 Bootstrap samples and ECDFs\nTo address this question, we can generate bootstrap samples from the experimental data and make lots of ECDFs. We can then plot them all to see how the ECDF might vary. Recall that a bootstrap sample from a data set of \\(n\\) repeated measurements is generated by drawing \\(n\\) data points out of the original data set with replacement. The rng.choice() function enables random draws of elements out of an array. Let’s generate 100 bootstrap samples and plot their ECDFs to visualize how our data set might change as we repeat the experiment.\n\nrng = np.random.default_rng()\n\n# Set up Numpy arrays for convenience (also much better performance)\ndfs = df.partition_by('Treatment', as_dict=True)\n\nalive_ctrl = dfs[('Control',)].get_column(\"Alive Sperm Millions\").to_numpy()\nalive_pest = dfs[('Pesticide',)].get_column(\"Alive Sperm Millions\").to_numpy()\n\n\n# ECDF values for plotting\nctrl_ecdf = np.arange(1, len(alive_ctrl)+1) / len(alive_ctrl)\npest_ecdf = np.arange(1, len(alive_pest)+1) / len(alive_pest)\n\n# Make 100 bootstrap samples and plot them\nfor _ in range(100):\n    bs_ctrl = rng.choice(alive_ctrl, size=len(alive_ctrl))\n    bs_pest = rng.choice(alive_pest, size=len(alive_pest))\n\n    # Add semitransparent ECDFs to the plot\n    p.scatter(np.sort(bs_ctrl), ctrl_ecdf, color=bokeh.palettes.Category10_3[0], alpha=0.02)\n    p.scatter(np.sort(bs_pest), pest_ecdf, color=bokeh.palettes.Category10_3[1], alpha=0.02)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nFrom this graphical display, we can already see that the ECDFs do not strongly overlap in 100 bootstrap samples, so there is likely a real difference between the two treatments.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Performing bootstrap calculations</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/intro_bootstrap.html#speeding-up-sampling-with-numba",
    "href": "lessons/nonparametric/bootstrap/intro_bootstrap.html#speeding-up-sampling-with-numba",
    "title": "27  Performing bootstrap calculations",
    "section": "27.3 Speeding up sampling with Numba",
    "text": "27.3 Speeding up sampling with Numba\nWe can make sampling faster (though note that the slowness in generating the above plot is not in resampling, but in adding glyphs to the plot) by employing just-in-time compilation, wherein the Python code is compiled at runtime into machine code. This results in a substantial speed boost. Numba is a powerful tool for this purpose, and we will use it. Bear in mind, though, that not all Python code is Numba-able. In order to just-in-time compile a function, we need to decorate its definition with the @numba.njit decorator, which tells the Python interpreter to use Numba to just-in-time compile the function. Note that Numba works on basic Python objects like tuples and Numpy arrays, so be sure to pass Numpy arrays into the respective functions.\nRandom number generation is supported, and instances of Numpy’s generators can be passed into Numba’d functions. However, to avoid always having to pass in an instance of a Numpy RNG into Numba functions, we will use Numpy’s old interface to the the Mersenne Twister bit generator, which uses a syntax like np.random.normal().\n\n@numba.njit\ndef draw_bs_sample(data):\n    \"\"\"Draw a bootstrap sample from a 1D data set.\"\"\"\n    return np.random.choice(data, size=len(data))\n\nLet’s quickly quantify the speed boost. Before we do the timing, we will run the Numba’d version once to give it a chance to do the JIT compilation.\n\n# Run once to JIT\ndraw_bs_sample(alive_ctrl)\n\nprint('Using the Numpy default RNG (PCG64):')\n%timeit rng.choice(alive_ctrl, size=len(alive_ctrl))\n\nprint('\\nUsing the Numpy Mersenne Twister:')\n%timeit np.random.choice(alive_ctrl, size=len(alive_ctrl))\n\nprint(\"\\nUsing a Numba'd Mersenne Twister:\")\n%timeit draw_bs_sample(alive_ctrl)\n\nUsing the Numpy default RNG (PCG64):\n4.33 μs ± 10.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\nUsing the Numpy Mersenne Twister:\n4.87 μs ± 16.9 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\nUsing a Numba'd Mersenne Twister:\n1.73 μs ± 1.81 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThat’s a significant increase and can make a big difference when generating lots of bootstrap samples.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Performing bootstrap calculations</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/intro_bootstrap.html#bootstrap-replicates-and-confidence-intervals",
    "href": "lessons/nonparametric/bootstrap/intro_bootstrap.html#bootstrap-replicates-and-confidence-intervals",
    "title": "27  Performing bootstrap calculations",
    "section": "27.4 Bootstrap replicates and confidence intervals",
    "text": "27.4 Bootstrap replicates and confidence intervals\nWe have plotted the ECDF of the data, which is instructive, but we would also like to get plug-in estimates for the generative distribution. Remember, when doing nonparametric plug-in estimates, we plug in the ECDF for the CDF. We do not need to specify what the distribution (described mathematically by the CDF, or equivalently by the PDF) is, just that we approximate it by the empirical distribution.\nWe have laid out the procedure to compute a confidence interval.\n\nGenerate B independent bootstrap samples.\nCompute the plug-in estimate of the statistical functional of interest for each bootstrap sample to get the bootstrap replicates.\nThe 100(1 – α) percent confidence interval consists of the percentiles 100 α/2 and 100(1 – α/2) of the bootstrap replicates.\n\nA key step here is computing the bootstrap replicate. We will write a function to draw bootstrap replicates of a statistic of interest. To do so, we pass in a function, which we will call stat_fun. Because we do not know what function we will pass in, we cannot just-in-time compile this functions.\n\ndef draw_bs_reps(data, stat_fun, size=1):\n    \"\"\"Draw boostrap replicates computed with stat_fun from 1D data set.\"\"\"\n    return np.array([stat_fun(draw_bs_sample(data)) for _ in range(size)])\n\nWe will also write a few functions for commonly computed statistics, which enables us to use Numba to greatly speed up the process of generating bootstrap replicates. Note that in our decorator, we use the parallel=True keyword argument. Within the for loop to compute the bootstrap replicates, as use numba.prange() as a drop-in replacement for range(). When we do it this way, Numba automatically parallelizes the calculation across available CPUs.\n\n@numba.njit(parallel=True)\ndef draw_bs_reps_mean(data, size=1):\n    \"\"\"Draw boostrap replicates of the mean from 1D data set.\"\"\"\n    out = np.empty(size)\n    for i in numba.prange(size):\n        out[i] = np.mean(draw_bs_sample(data))\n    return out\n\n\n@numba.njit(parallel=True)\ndef draw_bs_reps_median(data, size=1):\n    \"\"\"Draw boostrap replicates of the median from 1D data set.\"\"\"\n    out = np.empty(size)\n    for i in numba.prange(size):\n        out[i] = np.median(draw_bs_sample(data))\n    return out\n\n\n@numba.njit(parallel=True)\ndef draw_bs_reps_std(data, size=1):\n    \"\"\"Draw boostrap replicates of the standard deviation from 1D data set.\"\"\"\n    out = np.empty(size)\n    for i in numba.prange(size):\n        out[i] = np.std(draw_bs_sample(data))\n    return out\n\nNow, let’s get bootstrap replicates for the mean of each of the two treatments.\n\nbs_reps_mean_ctrl = draw_bs_reps_mean(alive_ctrl, size=10000)\nbs_reps_mean_pest = draw_bs_reps_mean(alive_pest, size=10000)\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\nWe can now compute the confidence intervals by computing the percentiles using the np.percentile() function.\n\n# 95% confidence intervals\nmean_ctrl_conf_int = np.percentile(bs_reps_mean_ctrl, [2.5, 97.5])\nmean_pest_conf_int = np.percentile(bs_reps_mean_pest, [2.5, 97.5])\n\nprint(\"\"\"\nMean alive sperm count 95% conf int control (millions):   [{0:.2f}, {1:.2f}]\nMean alive sperm count 95% conf int treatment (millions): [{2:.2f}, {3:.2f}]\n\"\"\".format(*(tuple(mean_ctrl_conf_int) + tuple(mean_pest_conf_int))))\n\n\nMean alive sperm count 95% conf int control (millions):   [1.67, 2.07]\nMean alive sperm count 95% conf int treatment (millions): [1.11, 1.50]\n\n\n\n\n27.4.1 Display of confidence intervals\nWhile the textual confidence intervals shown above are useful, it is often desired to display confidence intervals graphically. The bebi103 package has a utility to do this. It requires as input a list of dictionaries where each dictionary contains an estimate, a confidence interval, and a label.\n\nsummaries = [\n    dict(label=\"control\", estimate=np.mean(alive_ctrl), conf_int=mean_ctrl_conf_int),\n    dict(label=\"treated\", estimate=np.mean(alive_pest), conf_int=mean_pest_conf_int),\n]\n\nbokeh.io.show(bebi103.viz.confints(summaries, x_axis_label=\"alive sperm (millions)\"))\n\n\n  \n\n\n\n\n\n\n\n27.4.2 ECDF of bootstrap replicates\nWe can also use the bootstrap replicates to plot the probability distribution of mean alive sperm count. Remember: This is not a confidence interval on a parameter value. That does not make sense in frequentist statistics, and furthermore we have not assumed any parametric model. It is the confidence interval describing what we would get as a plug-in estimate for the mean if we did the experiment over and over again.\nWhen we plot the ECDF of the bootstrap replicates, we will thin them so as not to choke the browser with too many points. Since we will do this again, we write a quick function to do it.\n\ndef plot_bs_reps_ecdf(ctrl, pest, q, thin=1, **kwargs):\n    \"\"\"Make plot of bootstrap ECDFs for control and pesticide treatment.\"\"\"\n    x_ctrl = np.sort(ctrl)[::thin]\n    x_pest = np.sort(pest)[::thin]\n\n    df = pl.DataFrame(\n        data={\n            \"treatment\": [\"control\"] * len(x_ctrl) + [\"pesticide\"] * len(x_pest),\n            q: np.concatenate((x_ctrl, x_pest)),\n        }\n    )\n\n    p = iqplot.ecdf(\n        df, q=q, cats=\"treatment\", frame_height=200, frame_width=350, **kwargs\n    )\n\n    return p\n\n\np = plot_bs_reps_ecdf(\n    bs_reps_mean_ctrl, bs_reps_mean_pest, \"mean alive sperm (millions)\", thin=50\n)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\nThese are both nice, Normal distributions with almost no overlap in the tails. We also saw in the computation of the 95% confidence intervals above that there is no overlap. This is expected, the mean alive sperm count should be Normally distributed as per the central limit theorem.\nWe can do the same procedure for other statistical quantities that do not follow the central limit theorem. The procedure is exactly the same. We will do it for the median.\n\n# Get the bootstrap replicates\nbs_reps_median_ctrl = draw_bs_reps_median(alive_ctrl, size=10000)\nbs_reps_median_pest = draw_bs_reps_median(alive_pest, size=10000)\n\n# 95% confidence intervals\nmedian_ctrl_conf_int = np.percentile(bs_reps_median_ctrl, [2.5, 97.5])\nmedian_pest_conf_int = np.percentile(bs_reps_median_pest, [2.5, 97.5])\n\n# Plot of confidence interval\nsummaries = [\n    dict(label=\"control\", estimate=np.median(alive_ctrl), conf_int=median_ctrl_conf_int),\n    dict(label=\"treated\", estimate=np.median(alive_pest), conf_int=median_pest_conf_int),\n]\n\np_conf_int = bebi103.viz.confints(summaries, x_axis_label=\"alive sperm (millions)\")\n\n# Plot ECDFs of bootstrap replicates\np_ecdf = plot_bs_reps_ecdf(\n    bs_reps_median_ctrl, bs_reps_median_pest, \"median alive sperm (millions)\", thin=50\n)\n\n# Show both plots\nbokeh.io.show(bokeh.layouts.column([p_conf_int, bokeh.models.Spacer(height=50), p]))\n\n\n  \n\n\n\n\n\nThe results are similar, and we clearly see clear non-normality in the ECDFs.\nNote that plotting ECDFs of bootstrap replicates is not a normal reporting mechanism for bootstrap confidence intervals. We show them here to illustrate how bootstrapping works.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Performing bootstrap calculations</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/intro_bootstrap.html#computing-environment",
    "href": "lessons/nonparametric/bootstrap/intro_bootstrap.html#computing-environment",
    "title": "27  Performing bootstrap calculations",
    "section": "27.5 Computing environment",
    "text": "27.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,numba,bokeh,iqplot,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\npolars    : 1.31.0\nnumba     : 0.61.2\nbokeh     : 3.7.3\niqplot    : 0.3.7\njupyterlab: 4.4.5",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Performing bootstrap calculations</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/pairs_bootstrap_correlation.html",
    "href": "lessons/nonparametric/bootstrap/pairs_bootstrap_correlation.html",
    "title": "28  Pairs bootstrap and correlation",
    "section": "",
    "text": "28.1 Correlation\n| Download notebook\nDataset download\nWe continue our analysis of the drone sperm quality data set. Let’s load it and remind ourselves of the content.\nWe might wish to investigate how two measured quantities are correlated. For example, if the number of dead sperm and the number of alive sperm are closely correlated, this would mean that a given drone produces some quantity of sperm and some fraction tend to be dead. Let’s take a look at this.\n# Set up plot on log scale\np = bokeh.plotting.figure(\n    frame_height=300,\n    frame_width=300,\n    x_axis_label=\"alive sperm (millions)\",\n    y_axis_label=\"dead sperm (millions)\",\n    x_axis_type=\"log\",\n    y_axis_type=\"log\",\n)\n\n# Only use values greater than zero for log scale\ninds = (pl.col(\"Alive Sperm Millions\") &gt; 0) & (pl.col(\"Dead Sperm Millions\") &gt; 0)\n\n# Populate glyphs\nfor color, ((treatment,), sub_df) in zip(\n    bokeh.palettes.Category10_3, df.filter(inds).group_by(\"Treatment\")\n):\n    p.scatter(\n        source=sub_df.to_dict(),\n        x=\"Alive Sperm Millions\",\n        y=\"Dead Sperm Millions\",\n        color=color,\n        legend_label=treatment,\n    )\n\np.legend.location = \"bottom_right\"\n\nbokeh.io.show(p)\nThere seems to be some correlation (on a log scale), but it is difficult to tell. We can compute the correlation with the bivariate correlation coefficient, also known as the Pearson correlation. It is the plug-in estimate of the correlation between variables (in this case alive and dead sperm). The correlation is the covariance divided by the geometric mean of the individual variances\nThe bivariate correlation coefficient is implemented with np.corrcoef(), but we will code our own and JIT it for speed.\n@numba.njit\ndef bivariate_r(x, y):\n    \"\"\"\n    Compute plug-in estimate for the bivariate correlation coefficient.\n    \"\"\"\n    return (\n        np.sum((x - np.mean(x)) * (y - np.mean(y)))\n        / np.std(x)\n        / np.std(y)\n        / np.sqrt(len(x))\n        / np.sqrt(len(y))\n    )\nWe can use it to compute the bivariate correlation coefficient for the logarithm of alive and dead sperm.\nbivariate_r(\n    np.log(df.filter(inds).get_column(\"Alive Sperm Millions\").to_numpy()),\n    np.log(df.filter(inds).get_column(\"Dead Sperm Millions\").to_numpy()),\n)\n\n0.5219944217488051",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Pairs bootstrap and correlation</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/pairs_bootstrap_correlation.html#pairs-bootstrap-confidence-intervals",
    "href": "lessons/nonparametric/bootstrap/pairs_bootstrap_correlation.html#pairs-bootstrap-confidence-intervals",
    "title": "28  Pairs bootstrap and correlation",
    "section": "28.2 Pairs bootstrap confidence intervals",
    "text": "28.2 Pairs bootstrap confidence intervals\nHow can we get a confidence interval on a correlation coefficient? We can again apply the bootstrap, but this time, the replicate is a pair of data, in this case a dead sperm count/alive sperm count pair. The process of drawing pairs of data points from an experiment and then computing bootstrap replicates from them is called pairs bootstrap. Let’s code it up for this example with the bivariate correlation.\nOur strategy in coding up the pairs bootstrap is to draw bootstrap samples of the indices of measurement and use those indices to select the pairs.\n\n@numba.njit\ndef draw_bs_sample(data):\n    \"\"\"Draw a bootstrap sample from a 1D data set.\"\"\"\n    return np.random.choice(data, size=len(data))\n\n\n@numba.njit\ndef draw_bs_pairs(x, y):\n    \"\"\"Draw a pairs bootstrap sample.\"\"\"\n    inds = np.arange(len(x))\n    bs_inds = draw_bs_sample(inds)\n    \n    return x[bs_inds], y[bs_inds]\n\nWith our pairs sampling function in place, we can write a function to compute replicates.\n\n@numba.njit(parallel=True)\ndef draw_bs_pairs_reps_bivariate(x, y, size=1):\n    \"\"\"\n    Draw bootstrap pairs replicates.\n    \"\"\"\n    out = np.empty(size)\n\n    for i in numba.prange(size):\n        out[i] = bivariate_r(*draw_bs_pairs(x, y))\n\n    return out\n\nFinally, we can put it all together to compute confidence intervals on the correlation. To start, we extract all of the relevant measurements as Numpy arrays to allow for faster resampling (and that’s what our Numba’d functions require).\n\n# Extract NumPy arrays (only use values greater than zero for logs)\ninds = (pl.col(\"Alive Sperm Millions\") &gt; 0) & (pl.col(\"Dead Sperm Millions\") &gt; 0)\ndfs = df.filter(inds).partition_by('Treatment', as_dict=True)\n\nalive_ctrl = dfs[('Control',)].get_column(\"Alive Sperm Millions\").to_numpy()\nalive_pest = dfs[('Pesticide',)].get_column(\"Alive Sperm Millions\").to_numpy()\ndead_ctrl = dfs[('Control',)].get_column(\"Dead Sperm Millions\").to_numpy()\ndead_pest = dfs[('Pesticide',)].get_column(\"Dead Sperm Millions\").to_numpy()\n\nNow we can compute the bootstrap replicates using our draw_bs_pairs_reps_bivariate() function.\n\n# Get reps\nbs_reps_ctrl = draw_bs_pairs_reps_bivariate(\n    np.log(alive_ctrl), np.log(dead_ctrl), size=10_000\n)\n\nbs_reps_pest = draw_bs_pairs_reps_bivariate(\n    np.log(alive_pest), np.log(dead_pest), size=10_000\n)\n\nOMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n\n\nAnd from the replicates, we can compute and print the 95% confidence interval.\n\n# Get the confidence intervals\nconf_int_ctrl = np.percentile(bs_reps_ctrl, [2.5, 97.5])\nconf_int_pest = np.percentile(bs_reps_pest, [2.5, 97.5])\n\n# Plot confidence intervals\nsummaries = [\n    dict(\n        label=\"control\",\n        estimate=bivariate_r(np.log(alive_ctrl), np.log(dead_ctrl)),\n        conf_int=conf_int_ctrl,\n    ),\n    dict(\n        label=\"treated\",\n        estimate=bivariate_r(np.log(alive_pest), np.log(dead_pest)),\n        conf_int=conf_int_pest,\n    ),\n]\n\nbokeh.io.show(\n    bebi103.viz.confints(summaries, x_axis_label=\"bivariate correlation of logs\")\n)\n\n\n  \n\n\n\n\n\nWe see a clear correlation in both samples, with a wide, but positive, confidence interval. Note that we did this analysis on a log scale, since the data span several orders of magnitude.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Pairs bootstrap and correlation</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/bootstrap/pairs_bootstrap_correlation.html#computing-environment",
    "href": "lessons/nonparametric/bootstrap/pairs_bootstrap_correlation.html#computing-environment",
    "title": "28  Pairs bootstrap and correlation",
    "section": "28.3 Computing environment",
    "text": "28.3 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,polars,numba,bokeh,bebi103,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\npolars    : 1.31.0\nnumba     : 0.61.2\nbokeh     : 3.7.3\nbebi103   : 0.1.28\njupyterlab: 4.4.5",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Pairs bootstrap and correlation</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/nonparametric_lesson_exercise.html",
    "href": "lessons/nonparametric/nonparametric_lesson_exercise.html",
    "title": "29  Nonparametric inference lesson exercise",
    "section": "",
    "text": "Exercise 1\nWhat is the plug-in principle and how is it used in non-parametric statistics?",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Nonparametric inference lesson exercise</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/nonparametric_lesson_exercise.html#exercise-2",
    "href": "lessons/nonparametric/nonparametric_lesson_exercise.html#exercise-2",
    "title": "29  Nonparametric inference lesson exercise",
    "section": "Exercise 2",
    "text": "Exercise 2\nWhat is a bootstrap sample and what is a bootstrap replicate?",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Nonparametric inference lesson exercise</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/nonparametric_lesson_exercise.html#exercise-3",
    "href": "lessons/nonparametric/nonparametric_lesson_exercise.html#exercise-3",
    "title": "29  Nonparametric inference lesson exercise",
    "section": "Exercise 3",
    "text": "Exercise 3\nWhat is a confidence interval for an estimate? How do the plug-in principle and bootstrapping combine to compute a confidence interval?",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Nonparametric inference lesson exercise</span>"
    ]
  },
  {
    "objectID": "lessons/nonparametric/nonparametric_lesson_exercise.html#exercise-4",
    "href": "lessons/nonparametric/nonparametric_lesson_exercise.html#exercise-4",
    "title": "29  Nonparametric inference lesson exercise",
    "section": "Exercise 4",
    "text": "Exercise 4\nWrite down any questions or points of confusion that you have.",
    "crumbs": [
      "Nonparametric inference and confidence intervals",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Nonparametric inference lesson exercise</span>"
    ]
  },
  {
    "objectID": "homework/plotting_an_elephant.html",
    "href": "homework/plotting_an_elephant.html",
    "title": "HW 1.1: Plotting an elephant",
    "section": "",
    "text": "This problem is worth 30 points.\nIn this problem, we will practice skills with manipulating Numpy arrays and making plots with a fun example.\na) Read this little gem of an article. It has a good lesson, but it is important to note, as you will learn if you take part (b) of this course, a model is not automatically invalid because it has a lot of parameters. More important are what the parameters are and to what physical quantities they relate. Nonetheless, it is often desirable to have simpler models for many reasons, including interpretability.\nb) Based on the anecdote about John von Neumann, Mayer, Khairy, and Howard worked out a scheme to draw an elephant with four complex parameters. The complex parameters are\n\\[\\begin{align}\n&p_1 = -60 - 12 i,\\\\[1em]\n&p_2 = -30 + 14 i,\\\\[1em]\n&p_3 = 8 - 50 i,\\\\[1em]\n&p_4 = -10 - 18 i.\n\\end{align}\\]\nFor notational ease, let \\(r_j\\) be the real part of parameter \\(j\\) and \\(i_j\\) by the imaginary part of parameter \\(j\\). For example, \\(r_1 = -60\\) and \\(i_3 = -50\\).\nUsing these parameters, Mayer, Khairy, and Howard worked out a parametric curve for the shape of an elephant based on a truncated Fourier series. You can write the \\(x\\) and \\(y\\) values of a smooth parametric curve as a Fourier series as\n\\[\\begin{align}\nx(t) = A_{0,x} + \\sum_{k = 1}^\\infty A_{k,x} \\cos kt + \\sum_{k = 1}^\\infty B_{k,x} \\sin kt, \\\\[1em]\ny(t) = A_{0,y} + \\sum_{k = 1}^\\infty A_{k,y} \\cos kt + \\sum_{k = 1}^\\infty B_{k,y} \\sin kt,\n\\end{align}\\]\nwhere \\(t\\) ranges from zero to \\(2\\pi\\). Mayer, Khairy, and Howard worked out that you can get an elephant using\n\\[\\begin{align}\n&A_{1,x} = r_1,\\;B_{1,x} = r_2, \\; B_{2,x} = r_3, \\; B_{3,x} = r_4,\\\\[1em]\n&A_{3,y} = i_1,\\;A_{5,y} = i_2, \\; B_{1,y} = i_3, \\; B_{2,y} = i_4,\n\\end{align}\\]\nwith all other Fourier coefficients being zero.\nCompute a smooth curve for an elephant using this formula and plot it. (This problem is not meant to teach you about Fourier series, but rather to practice creating and manipulating Numpy arrays and making plots.)\nc) For fun, you can make a little scene for your elephant by adding other glyphs to the plot. You can read Bokeh’s documentation to learn about what you can do. You may wish to investigate patches and box annotations, among others. (This part of the problem is not graded.)",
    "crumbs": [
      "Homework",
      "HW 1.1: Plotting an elephant"
    ]
  },
  {
    "objectID": "homework/penguins_split_apply_combine.html",
    "href": "homework/penguins_split_apply_combine.html",
    "title": "HW 1.2: Palmer penguins and split-apply-combine",
    "section": "",
    "text": "This problem is worth 30 points.\nData set download\n\nThe Palmer penguins data set is a nice data set with which to practice various data science skills. For this exercise, we will use as subset of it, which you can download here: https://s3.amazonaws.com/bebi103.caltech.edu/data/penguins_subset.csv. The data set consists of measurements of three different species of penguins acquired at the Palmer Station in Antarctica. The measurements were made between 2007 and 2009 by Kristen Gorman.\na) Take a look at the CSV file containing the data set. Is it in tidy format? Why or why not?\nb) You can convert the CSV file to a “tall” format using the bebi103.utils.unpivot_csv() function. You can do that with the following function call, where path_to_penguins is a string containing the path to the penguin_subset.csv file.\nbebi103.utils.unpivot_csv(\n    path_to_penguins,\n    \"penguins_tall.csv\",\n    n_header_rows=2,\n    header_names=[\"species\", \"quantity\"],\n    comment_prefix=\"#\",\n    retain_row_index=True,\n    row_index_name='penguin_id',\n)    \nAfter running that function, load in the data set stored in the penguins_tall.csv file and store it in a variable named df_tall. Is this a tidy data set?\nc) Perform the following operations to make a new DataFrame from the one you loaded in to generate a new DataFrame. You do not need to worry about what these operations do (that is the topic of next week, just do them to answer this question):\ndf = (\n    df_tall\n    .pivot(\n        index=['penguin_id', 'species'], on='quantity', values='value'\n    )\n    .select(pl.exclude('penguin_id'))\n)\nIs the resulting data frame df tidy? Why or why not?\nd) Using the data frame you created in part (c), slice out all of the bill lengths for Gentoo penguins.\ne) Make a new data frame containing the mean measured bill depth, bill length, body mass in kg, and flipper length for each species. You can use millimeters for all length measurements.\nf) Make a scatter plot of bill length versus flipper length with the glyphs colored by species.",
    "crumbs": [
      "Homework",
      "HW 1.2: Palmer penguins and split-apply-combine"
    ]
  },
  {
    "objectID": "homework/mt_ecdf.html",
    "href": "homework/mt_ecdf.html",
    "title": "HW 2.1: Microtubule catastrophe and ECDFs",
    "section": "",
    "text": "This problem is a team problem.\nThis problem is worth 35 points.\nData set download\n\nAn ECDF evaluated at point x is defined as\nECDF(x) = fraction of data points ≤ x.\nThe ECDF is defined on the entire real number line, with \\(\\mathrm{ECDF}(x\\to-\\infty) = 0\\) and \\(\\mathrm{ECDF}(x\\to\\infty) = 1\\). However, the ECDF is often plotted as discrete points, \\(\\{(x_i, y_i)\\}\\), where for point \\(i\\), \\(x_i\\) is the value of the measured quantity and \\(y_i\\) is \\(\\mathrm{ECDF}(x_i)\\). For example, if I have a set of measured data with values (1.1, –6.7, 2.3, 9.8, 2.3), the points on the ECDF plot are\n\n\n\nx\ny\n\n\n\n\n–6.7\n0.2\n\n\n1.1\n0.4\n\n\n2.3\n0.6\n\n\n2.3\n0.8\n\n\n9.8\n1.0\n\n\n\nIn this problem, you will use you newly acquired skills using Numpy and Bokeh to compute ECDFs from a real data set and plot them.\nGardner, Zanic, and coworkers investigated the dynamics of microtubule catastrophe, the switching of a microtubule from a growing to a shrinking state. In particular, they were interested in the time between the start of growth of a microtubule and the catastrophe event. They monitored microtubules by using tubulin (the monomer that comprises a microtubule) that was labeled with a fluorescent marker. As a control to make sure that fluorescent labels and exposure to laser light did not affect the microtubule dynamics, they performed a similar experiment using differential interference contrast (DIC) microscopy. They measured the time until catastrophe with labeled and unlabeled tubulin.\nWe will look at the data used to generate Fig. 2a of their paper. In the end, you will generate a plot similar to that figure.\na) Write a function with the call signature ecdfvals(data), which takes a one-dimensional Numpy array (or Polars Series; the same construction of your function should work for both) of data and returns the x and y values for plotting the ECDF in the “dots” style, as in Fig. 2a of the Gardner, Zanic, et al. paper. As a reminder,\n\nECDF(x) = fraction of data points ≤ x.\n\nWhen you write this function, you may only use base Python and the standard library, in addition to Numpy and Polars.\nb) Use the ecdfvals() function that you wrote to plot the ECDFs shown in Fig. 2a of the Gardner, Zanic, et al. paper. By looking this plot, do you think that the fluorescent labeling makes a difference in the onset of catastrophe? (We will do a more careful statistical inference later in the course, but for now, does it pass the eye test? Eye tests are an important part of EDA, but certainly not an end point.) You can access the data set here: https://s3.amazonaws.com/bebi103.caltech.edu/data/gardner_time_to_catastrophe_dic_tidy.csv",
    "crumbs": [
      "Homework",
      "HW 2.1: Microtubule catastrophe and ECDFs"
    ]
  },
  {
    "objectID": "homework/cool_gal4.html",
    "href": "homework/cool_gal4.html",
    "title": "HW 2.2: EDA for a temperature controlled Gal4-UAS system",
    "section": "",
    "text": "This problem is worth 65 points.\nDataset download\n\nOne of my former students, Han Wang, published (Wang, H., …, Sternberg, P.W. (2017). cGAL, a temperature-robust GAL4-UAS system for Caenorhabditis elegans, Nat. Methods, 14(2), 145-148) an improved Gal4/UAS system in C. elegans. Briefly, the Gal4-UAS system was hijacked from budding yeast and incorporated into the genomes of other organisms, Drosophila being the first. The idea is to insert the Gal4 gene into the genome such that it is under control of a driver that is native to the organism. When Gal4 is expressed, it binds to UAS (upstream activation sequence) and it is activating, leading to expression of the UAS target gene.\nIn the paper, Han used the system with UAS activating production of green fluorescent protein (GFP). The Gal4 production is driven by Pmyo-2, which is only expressed in the pharynx of the worm.\nThe Gal4/UAS system typically works only at high temperatures. This does not work as well in worms that are stored at lower temperatures. Han therefore engineered a “cool” Gal4, which works at lower temperatures. To test how the new system worked, he measured the GFP fluorescence signal in the pharynx of worms.\nHe generously donated his data set for us to work with. He sent me a MS Excel file with the data, along with some comments via email. Here is what he said in that email about the data set (verbatim):\n\nSC (orignal Gal4)\nSK (cool Gal4)\nm3 Pmyo-2::GFP fusion (control; measure of driver expression)\n15 20 and 25 at the end for the name of each column shows the experimental temperature.\n\nYou can download the MS Excel file here: https://s3.amazonaws.com/bebi103.caltech.edu/data/wang_cool_gal4.xlsx.\na) Load and tidy the DataFrame. Be sure to remove any NaNs.\nb) Do some exploratory data analysis of the data set. That is, make some instructive plots. Discuss why you chose to visualize the data set the way(s) you did. What can you say about Han’s cool Gal4 just by looking at the plots?",
    "crumbs": [
      "Homework",
      "HW 2.2: EDA for a temperature controlled Gal4-UAS system"
    ]
  },
  {
    "objectID": "homework/beetle_hypnotists.html",
    "href": "homework/beetle_hypnotists.html",
    "title": "HW 3.1: Beetle hypnotists",
    "section": "",
    "text": "This problem is worth 70 points.\nData set download\n\nThe Parker lab at Caltech studies rove beetles that can infiltrate ant colonies. In one of their experiments, they place a rove beetle and an ant in a circular area and track the movements of the ants. They do this by using a deep learning algorithm to identify the head, thorax, abdomen, and right and left antennae. While deep learning applied to biological images is a beautiful and useful topic, we will not cover it in this course (be on the lookout for future courses that do!). We will instead work with a data set that is the output of the deep learning algorithm.\nFor the experiment you are considering in this problem, an ant and a beetle were placed in a circular arena and recorded with video at a frame rate of 28 frames per second. The positions of the body parts of the ant were tracked throughout the video recording. You can download the data set here. Hint: As of October 2025, Polars does not natively open ZIP files. In order to open them, you can use built-in utilities in the Python standard library and send bytes directly to pl.read_csv(). For this file, it would look something like this:\nimport io, os, zipfile\nwith zipfile.ZipFile(os.path.join(data_path, 'ant_joint_locations.zip')) as fz:\n    with fz.open('ant_joint_locations.csv') as f:\n        df = pl.read_csv(io.BytesIO(f.read()), comment_prefix='#')\nTo save you from having to unzip and read the comments for the data file, here they are:\n# This data set was kindly donated by Julian Wagner from Joe Parker's lab at \n# Caltech. In the experiment, an ant and a beetle were placed in a circular\n# arena and recorded with video at a frame rate of 28 frames per second. \n# The positions of the body parts the ant are tracked throughout the video\n# recording.\n#\n# The experiment aims to distinguish the ant behavior in the presence of\n# a beetle from the genus Sceptobius, which secretes a chemical that modifies\n# the behavior of the ant, versus in the presence of a beetle from the species\n# Dalotia, which does not.\n#\n# The data set has the following columns.\n#  frame : frame number from the video acquisition\n#  beetle_treatment : Either dalotia or sceptobius\n#  ID : The unique integer identifier of the ant in the experiment\n#  bodypart : The body part being tracked in the experiment. Possible values\n#             are head, thorax, abdomen, antenna_left, antenna_right.\n#  x_coord : x-coordinate of the body part in units of pixels\n#  y_coord : y-coordinate of the body part in units of pixels\n#  likelihood : A rating, ranging from zero to one, given by the deep learning\n#               algorithm that approximately quantifies confidence that the\n#               body part was correctly identified.\n#\n# The interpixel distance for this experiment was 0.8 millimeters.\nYour task in this problem is to extract records of interest out of the tidy data frame containing the data from the experiment, perform calculations on the data, and make informative plots.\na) The columns x_coord and y_coord give the coordinates of the ant’s body parts in units of pixels. Create a column 'x (mm)' and a column 'y (mm)' in the data frame that has the coordinates in units of millimeters. Also create a column 'time (sec)' that gives the time since recording started in seconds.\nb) Make a plot displaying the position over time of the thorax of an ant or ants placed in an arena with a Dalotia beetle and position over time of an ant or ants with a Sceptobius beetle. I am intentionally not giving more specification for your plot. You need to make decisions about how to effectively extract and display the data. Think carefully about your visualizations. This is in many ways how you let your data speak.\nc) From this quick graphical exploratory analysis, what would you say about the relative activities of ants with Dalotia versus Sceptobius rove beetles?",
    "crumbs": [
      "Homework",
      "HW 3.1: Beetle hypnotists"
    ]
  },
  {
    "objectID": "homework/marginal_binomial.html",
    "href": "homework/marginal_binomial.html",
    "title": "HW 3.2: A marginal distribution",
    "section": "",
    "text": "This problem is worth 30 points.\n\nYou may submit this problem as a PDF, either with clearly typed mathematics or with neat handwriting, if you like.\nSay we are doing an experiment measuring the sex of the progeny of fruit flies. We randomly select one male fruit fly and one female fruit fly. We then mate them, select \\(N\\) of their progeny, and count the number \\(n\\) of males. We do this experiment many times to get an idea about the distribution describing \\(n\\).\na) We initially choose a Binomial distribution for \\(n\\). That is,\n\\[\\begin{align}\nn \\sim \\text{Binom}(N, \\theta).\n\\end{align}\\]\nHere, \\(\\theta\\) is the probability that offspring from flies is male. Explain why this is a reasonable model.\nb) Upon analyzing the data using some of the techniques we will learn later in the class, we decide we need to update the model. We suspect that each male-female pair of flies may not have the same propensity for male offspring. Therefore, we decide not to take \\(\\theta\\) as a fixed parameter for all pairs of flies, but rather to model \\(\\theta\\) as coming from its own Beta distribution. Our new model is\n\\[\\begin{align}\n&\\theta \\sim \\text{Beta}(\\alpha, \\beta),\\\\[1em]\n&n \\sim \\text{Binom}(N, \\theta).\n\\end{align}\\]\nWrite down the joint probability density/mass function, \\(f(n, \\theta ; N, \\alpha, \\beta)\\).\nc) Both \\(n\\) and \\(\\theta\\) are random variables (they vary meaningfully from experiment to experiment), but we can only observe \\(n\\). We therefore we want to know the probability mass function for \\(n\\). Show that\n\\[\\begin{align}\nf(n ; N,\\alpha, \\beta) = \\binom{N}{n}\\,\\frac{B(n+\\alpha, N-n+\\beta)}{B(\\alpha, \\beta)}.\n\\end{align}\\]\nwhere \\(B(x, y)\\) denotes a Beta function.\nd) As you may note in the Distribution Explorer, the Beta distribution may also be parametrized with \\(\\phi = \\alpha / (\\alpha + \\beta)\\) and \\(\\kappa = \\alpha + \\beta\\). Show that in the limit of \\(\\kappa \\to \\infty\\),\n\\[\\begin{align}\nf(n ; N, \\phi) = \\binom{N}{n}\\,\\phi^n\\,(1-\\phi)^{N-n},\n\\end{align}\\]\nwhich is again a Binomial PMF. You do not need to explictly take any limits or perform any integrals. You can use stories.",
    "crumbs": [
      "Homework",
      "HW 3.2: A marginal distribution"
    ]
  },
  {
    "objectID": "appendices/notation.html",
    "href": "appendices/notation.html",
    "title": "Appendix A — Notation",
    "section": "",
    "text": "Below are mathematical notational rules used throughout the course.\n\nScalar quantities as denoted as italicized symbols, such as \\(x\\), \\(y\\), \\(\\mu\\), and \\(\\sigma\\).\nVector quantities (first-rank tensors) are denoted in bold, such as \\(\\mathbf{x}\\), \\(\\mathbf{y}\\), \\(\\boldsymbol{\\mu}\\), and \\(\\boldsymbol{\\sigma}\\).\nMatrix quantities (second-rank tensors) are denoted with sans serif capital letters, such as \\(\\mathsf{A}\\), \\(\\mathsf{W}\\), and \\(\\mathsf{\\sigma}\\).\nThe one exception to the boldface and sans serif convention is when we denote a generic set of data or parameters. In that case, we use standard italicized symbols like \\(\\theta\\) (typically for a set of parameters) or \\(z\\) (typically for a set of latent variables).\nSubscripts typically denote an element of a vector, such as \\(x_i\\), or an element of a matrix, such as \\(A_{ij}\\). They can also denote an entry in a non-ordered collection, such as \\(M_i\\).\nTransposes are denoted with a superscript \\(\\mathsf{T}\\).\nVector dot products result in a scalar and are denoted with a dot, such as \\(\\mathbf{x}\\cdot\\mathbf{y}\\). Note that this is denoted as \\(\\mathbf{x}^\\mathsf{T}\\mathbf{x}\\) in some texts, but we will not use that notation. Writing out the sum, this is\n\n\\[\\begin{aligned}\n\\mathbf{x}\\cdot\\mathbf{y} = \\sum_{i}x_i\\, y_i.\n\\end{aligned}\n\\tag{A.1}\\]\n\nMatrix-vector products result in a vector are also denoted with a dot, such as \\(\\mathsf{A}\\cdot\\mathbf{x}\\). Writing out the sum, this is\n\n\\[\\begin{aligned}\n\\mathsf{A}\\cdot\\mathbf{x} = \\begin{pmatrix}\\sum_{i}A_{i1} x_i \\\\ \\sum_{i}A_{i2} x_i \\\\ \\vdots \\end{pmatrix}\n\\end{aligned}\n\\tag{A.2}\\]\n\nMatrix-matrix multiplication results in a matrix and is also denoted with a dot, such as \\(\\mathsf{A}\\cdot\\mathsf{B}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/setting_up_python_computing_environment.html",
    "href": "appendices/python_basics/setting_up_python_computing_environment.html",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "",
    "text": "B.1 Why Python?\n| Download notebook\nThere are plenty of programming languages that are widely used in data science and in scientific computing more generally. Some of these, in addition to Python, are Matlab/Octave, Mathematica, R, Julia, Java, JavaScript, Rust, and C++.\nI have chosen to use Python. I believe language wars are counterproductive and welcome anyone to port the code we use to any language of their choice, I nonetheless feel we should explain this choice.\nPython is a flexible programming language that is widely used in many applications. This is in contrast to more domain-specific languages like R and Julia. It is easily extendable, which is in many ways responsible for its breadth of use. We find that there is a decent Python-based tool for many applications we can dream up, certainly in data science. However, the Python-based tool is often not the very best for the particular task at hand, but it is almost always pretty good. Thus, knowing Python is like having a Swiss Army knife; you can wield it to effectively accomplish myriad tasks. Finally, we also find that it has a shallow learning curve with most students.\nFurthermore, Python is widely used in machine learning and AI. The development of packages like TensorFlow, PyTorch, JAX, Keras, and scikit-learn have led to very widespread adoption of Python.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/setting_up_python_computing_environment.html#jupyter-notebooks",
    "href": "appendices/python_basics/setting_up_python_computing_environment.html#jupyter-notebooks",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.2 Jupyter notebooks",
    "text": "B.2 Jupyter notebooks\nThe materials of this course are constructed from Jupyter notebooks. To quote Jupyter’s documentation,\n\nJupyter Notebook and its flexible interface extends the notebook beyond code to visualization, multimedia, collaboration, and more. In addition to running your code, it stores code and output, together with markdown notes, in an editable document called a notebook.\n\nThis allows for executable documents that have code, but also richly formatted text and graphics, enabling the reader to interact with the material as they read it.\nSpecifically, notebooks are comprised of cells, where each cell contains either executable Python code or text.\nWhile you read the materials, you can read the HTML-rendered versions of the notebooks. To execute (and even edit!) code in the notebooks, you will need to run them. There are many options available to run Jupyter notebooks. Here are a few we have found useful.\n\nJupyterLab: This is a browser-based interface to Jupyter notebooks and more (including a terminal application, text editor, file manager, etc.). As of March 2025, Chrome, Firefox, Safari, and Edge are supported.\nVSCode: This is an excellent source code editor that supports Jupyter notebooks. Be sure to read the documentation on how to use Jupyter notebooks in VSCode. This may be an especially good option for Windows users.\nGoogle Colab: Google offers this service to run notebooks in the cloud on their machines. There are a few caveats, though. First, not all packages and updates are available in Colab. Furthermore, not all interactivity that will work natively in Jupyter notebooks works with Colab. If a notebook sits idle for too long, you will be disconnected from Colab. Finally, there is a limit to resources that are available for free, and as of March 2025, that limit is unpublished and can vary. All of the notebooks in the HTML rendering of this book have an “Open in Colab” button at the upper right that allows you to launch the notebook in Colab. This is a quick-and-easy way to execute the book’s contents.\n\nFor our work in this programming bootcamp, I encourage you to use either JupyterLab in the browser or VSCode, with Colab as a backup if you’re having trouble.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/setting_up_python_computing_environment.html#marimo",
    "href": "appendices/python_basics/setting_up_python_computing_environment.html#marimo",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.3 Marimo",
    "text": "B.3 Marimo\nMarimo offers a very nice notebook interface that is a departure from Jupyter notebooks in its structure. The biggest departure is that Marimo notebooks are specifically for Python, as opposed to being language agnostic like Jupyter. As a result, Marimo notebooks can offer many features not seen in Jupyter notebooks (without add-ons). The two most compelling, at least to me, are\n\nMarimo notebooks are simple .py files which allow for easier version control and simple execution as scripts.\nMarimo notebooks are reactive, meaning that the ordering of the cells is irrelevant and the notebook runs all cells that need to be rerun as a result of a change of value of a variable in any given cell.\n\nIn the course, we will use Jupyter notebooks, but you are welcome to play with Marimo notebooks. Upon completing the installation instructions in this notebook, Marimo will be installed.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/setting_up_python_computing_environment.html#installing-python-tools",
    "href": "appendices/python_basics/setting_up_python_computing_environment.html#installing-python-tools",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.4 Installing Python tools",
    "text": "B.4 Installing Python tools\nPrior to embarking on your journey into data analysis, you need to have a functioning Python distribution installed on your computer. We will use pixi, a relatively new package manager that I have found very effective.\nImportantly, it does so in a project-based way. That is, for each project, you use Pixi to create and manage the packages needed for that project. Our “project” here is our course!\nPixi is a package management tool that allows installation of packages. Importantly, it does so in a project-based way. That is, for each project, you use Pixi to create and manage the packages needed for that project. Our “project” here is our data analysis/statistical inference course.\nStep 1: Install Pixi. To install Pixi, you need access to the command line. For macOS users, hit Command-space, type in “terminal” and open the Terminal app. In Windows, open PowerShell by opening the Start Menu, typing “PowerShell” in the search bar, and selecting “Windows PowerShell.” I assume you know how to get access to the command line if you are using Linux.\nOn the command line, do the following.\nmacOS or Linux\ncurl -fsSL https://pixi.sh/install.sh | sh\nWindows\npowershell -ExecutionPolicy ByPass -c \"irm -useb https://pixi.sh/install.ps1 | iex\"\nStep 2: Create a directory for your work in the course. You might want to name the directory bebi103a/, which is what I have named it. You can do this either with the command line of your graphical file management program (e.g., Finder for macOS).\nStep 3 Navigate to the directory you created on the command line. For example, if the directory is bebi103/ in your home directory and you are in your home directory, you can do\ncd bebi103\non the command line.\nStep 4 Download the requisite Pixi files: pixi.toml, pixi.lock. These files need to be stored in the directory you created in step 3. You may download them by right-clicking those links, or by doing the following on the command line.\nmacOS or Linux\ncurl -fsSL https://raw.githubusercontent.com/bebi103a/bebi103a.github.io/refs/heads/main/pixi.toml -o ./pixi.toml\ncurl -fsSL https://raw.githubusercontent.com/bebi103a/bebi103a.github.io/refs/heads/main/pixi.lock -o ./pixi.lock\nWindows\nirm -useb https://raw.githubusercontent.com/bebi103a/bebi103a.github.io/refs/heads/main/pixi.toml -OutFile pixi.toml\n\nirm -useb https://raw.githubusercontent.com/bebi103a/bebi103a.github.io/refs/heads/main/pixi.lock -OutFile pixi.lock\nStep 5 Install the environment! Do the following on the command line.\npixi install\nStep 6 To be able to use all of the packages, you need to invoke a Pixi shell. To do so, execute the following on the command line.\npixi shell\nYou are now good to go! After you are done working, to exit the Pixi shell, hit Control-D.\nFor doing work for this class, you will need to cd into the directory you created in step 2 and execute pixi shell every time you open a new terminal (or PowerShell) window.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/setting_up_python_computing_environment.html#launching-jupyterlab",
    "href": "appendices/python_basics/setting_up_python_computing_environment.html#launching-jupyterlab",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.5 Launching JupyterLab",
    "text": "B.5 Launching JupyterLab\nOnce you have invoked a Pixi shell, you can launch JupyterLab via your operating system’s terminal program (Terminal on macOS and PowerShell on Windows). To do so, enter the following on the command line (after having run pixi shell).\njupyter lab\nYou will have an instance of JupyterLab running in your default browser. If you want to specify the browser, you can, for example, type\njupyter lab --browser=firefox\non the command line.\nAlternatively, if you are using VSCode, you can use its menu system to open .ipynb files. Make sure you select the Python kernel corresponding to your environment. You can read the documentation here. Hint: You may need to restart VSCode after doing the above installations so it is aware of your pixi environment.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/setting_up_python_computing_environment.html#checking-your-distribution",
    "href": "appendices/python_basics/setting_up_python_computing_environment.html#checking-your-distribution",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "B.6 Checking your distribution",
    "text": "B.6 Checking your distribution\nLet’s now run a quick test to make sure things are working properly. We will make a quick plot that requires some of the scientific libraries we will use.\nLaunch a Jupyter notebook in JupyterLab. In the first cell (the box next to the [ ]: prompt), paste the code below. To run the code, press Shift+Enter while the cursor is active inside the cell. You should see a plot that looks like the one below. If you do, you have a functioning Python environment for scientific computing!\n\nimport numpy as np\nimport bokeh.plotting\nimport bokeh.io\n\nbokeh.io.output_notebook()\n\n# Generate plotting values\nt = np.linspace(0, 2*np.pi, 200)\nx = 16 * np.sin(t)**3\ny = 13 * np.cos(t) - 5 * np.cos(2*t) - 2 * np.cos(3*t) - np.cos(4*t)\n\np = bokeh.plotting.figure(height=250, width=275)\np.line(x, y, color='red', line_width=3)\ntext = bokeh.models.Label(x=0, y=0, text='BE/Bi 103 a', text_align='center')\np.add_layout(text)\n\nbokeh.io.show(p)\n\n    \n    \n        \n        Loading BokehJS ...",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/setting_up_python_computing_environment.html#computing-environment",
    "href": "appendices/python_basics/setting_up_python_computing_environment.html#computing-environment",
    "title": "Appendix B — Configuring your computer to use Python for scientific computing",
    "section": "Computing environment",
    "text": "Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\nbokeh     : 3.7.3\njupyterlab: 4.4.5",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Configuring your computer to use Python for scientific computing</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_jupyterlab.html",
    "href": "appendices/notebooks_and_latex/intro_to_jupyterlab.html",
    "title": "Appendix C — Introduction to JupyterLab",
    "section": "",
    "text": "C.1 The Python interpreter\n| Download notebook\nIn this lesson, you will learn about different ways of interacting with the Python interpreter and importantly the basics on how to use JupyterLab. All of your homework will be submitted as Jupyter notebooks, so this is something you will need to master. It will be useful for you to go over the intro to LaTeX to learn how to use \\(\\LaTeX\\) in your Jupyter notebooks.\nYou should, of course, read the official JupyterLab documentation as well.\nWe will start by introducing the Python interpreter.\nBefore diving into the Python interpreter, I pause here to remind you that this course is not meant to teach Python syntax (though you will learn that). The things you learn here are meant to help you understand how to use your computer for data analysis more generally. Think of it this way: part of the mission of this course is to help you unleash the power of your computer on your biological problems. Python is just the language of instruction. That said, let’s start talking about how Python works.\nPython is an interpreted language, which means that each line of code you write is translated, or interpreted, into a set of instructions that your machine can understand by the Python interpreter. This stands in contrast to compiled languages. For these languages (the dominant ones being Fortran, C, and C++), your entire code is translated into machine language before you ever run it. When you execute your program, it is already in machine language.\nSo, whenever you want your Python code to run, you give it to the Python interpreter.\nThere are many ways to launch the Python interpreter. One way is to type\non the command line. This launches the vanilla Python interpreter. We will never really use this in the class. Rather, we will have a greatly enhanced Python experience, either using IPython, a feature-rich, enhanced interactive Python available through JupyterLab’s console, or using a notebook, also launchable in JupyterLab.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to JupyterLab</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#the-python-interpreter",
    "href": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#the-python-interpreter",
    "title": "Appendix C — Introduction to JupyterLab",
    "section": "",
    "text": "python",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to JupyterLab</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#hello-world.-and-the-print-function",
    "href": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#hello-world.-and-the-print-function",
    "title": "Appendix C — Introduction to JupyterLab",
    "section": "C.2 “Hello, world.” and the print() function",
    "text": "C.2 “Hello, world.” and the print() function\nTraditionally, the first program anyone writes when learning a new language is called “Hello, world.” In this program, the words “Hello, world.” are printed on the screen. The original Hello, world. was likely written by Brian Kernighan, one of the inventors of Unix, and the author of the classic and authoritative book on the C programming language. In his original, the printed text was “hello, world” (no period nor capital H), but people use lots of variants.\nWe will first write and run this little program using a JupyterLab console. After launching JupyterLab, you probably already have the Launcher in your JupyterLab window. If you do not, you can expand the Files tab at the left of your JupyterLab window (if it is not already expanded) by clicking on that tab, or alternatively hit ctrl+b (or cmd+b on macOS). At the top of the Files tab is a + sign, which gives you a Jupyter Launcher.\nIn the Jupyter Launcher, click the Python 3 icon under Console. This will launch a console, which has a large white space above a prompt that says In []:. You can enter Python code in this prompt, and it will be executed.\nTo print Hello, world., enter the code below. To execute the code, hit shift+enter.\n\nprint('Hello, world.')\n\nHello, world.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to JupyterLab</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#py-files",
    "href": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#py-files",
    "title": "Appendix C — Introduction to JupyterLab",
    "section": "C.3 .py files",
    "text": "C.3 .py files\nNow let’s use our new knowledge of the print() function to have our computer say a bit more than just Hello, world. Type these lines in at the prompt, hitting enter each time you need a new line. After you’ve typed them all in, hit shift+enter to run them.\n\n# The first few lines from The Zen of Python by Tim Peters\nprint('Beautiful is better than ugly.')\nprint('Explicit is better than implicit.')\nprint('Simple is better than complex.')\nprint('Complex is better than complicated.')\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\n\n\nNote that the first line is preceded with a # sign, and the Python interpreter ignored it. The # sign denotes a comment, which is ignored by the interpreter, but very important for the human!\nWhile the console prompt was nice entering all of this, a better option is to store them in a file, and then have the Python interpreter run the lines in the file. This is how you typically store Python code, and the suffix of such files is .py.\nSo, let’s create a .py file. To do this, use the JupyterLab Launcher to launch a text editor. Once it is launched, you can right click on the tab of the text editor window to change the name. We will call this file zen.py. Within this file, enter the four lines of code you previously entered in the console prompt. Be sure to save it.\nTo run the code in this file, you can invoke the Python interpreter at the command line, followed by the file name. I.e., enter\npython zen.py\nat the command line. Note that when you run code this way, the interpreter exits after completion of running the code, and you do not get a prompt.\nTo run the code in this file using the Jupyter console, you can use the %run magic function.\n%run zen.py\nTo shut down the console, you can click on the Running tab at the left of the JupyterLab window and click on SHUTDOWN next to the console.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to JupyterLab</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#jupyter",
    "href": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#jupyter",
    "title": "Appendix C — Introduction to JupyterLab",
    "section": "C.4 Jupyter",
    "text": "C.4 Jupyter\nAt this point, we have introduced JupyterLab, its text editor, and the console, as well as the Python interpreter itself. You might be asking….\n\nC.4.1 What is Jupyter?\nFrom the Project Jupyter website: &gt;Project Jupyter is an open source project was born out of the IPython Project in 2014 as it evolved to support interactive data science and scientific computing across all programming languages.\nSo, Jupyter is an extension of IPython the pushes interactive computing further. It is language agnostic as its name suggests. The name “Jupyter” is a combination of Julia (a newer excellent language for scientific computing), Python (which you know and love), and R (the dominant tool for statistical computation). However, you can run over 40 different languages in a JupyterLab, not just Julia, Python, and R.\nCentral to Jupyter/JupyterLab are Jupyter notebooks. In fact, the document you are reading right now was generated from a Jupyter notebook. We will use Jupyter notebooks extensively in the course, along with .py files.\n\n\nC.4.2 Why Jupyter notebooks?\nWhen writing code you will reuse, you should develop fully tested modules using .py files. You can always import those modules when you are using a Jupyter notebook. So, a Jupyter notebook is not good for an application where you are building reusable code or scripts. However, Jupyter notebooks are very useful in the following applications.\n\nExploring data/analysis. Jupyter notebooks are great for trying things out with code, or exploring a data set. This is an important part of the research process. The layout of Jupyter notebooks is great for organizing thoughts as you synthesize them.\nDeveloping image processing pipelines. This is really just a special case of (1), but it worth mentioning separately because Jupyter notebooks are especially useful when figuring out what steps are best for extracting useful data from images, which happens all-too-often in biology. Using the Jupyter notebook, you can write down what you hope to accomplish in each step of processing and then graphically show the results as images as you go through the analysis.\nSharing your thinking in your analysis. Because you can combine nicely formatted text and executable code, Jupyter notebooks are great for sharing how you go about doing your calculations with collaborators and with readers of your publications. Famously, LIGO used a Jupyter notebook to explain the signal processing involved in their first discovery of a gravitational wave.\nPedagogy. All of the content in this class, including this lesson, was developed using Jupyter notebooks!\n\nNow that we know what Jupyter notebooks are and what the motivation is for using them, let’s start!\n\n\nC.4.3 Launching a Jupyter notebook\nTo launch a Jupyter notebook, click on the Notebook icon of the JupyterLab launcher. If you want to open an existing notebook, click on it in the Files tab of the JupyterLab window and open it.\n\n\nC.4.4 Cells\nA Jupyter notebook consists of cells. The two main types of cells you will use are code cells and markdown cells, and we will go into their properties in depth momentarily. First, an overview.\nA code cell contains actual code that you want to run. You can specify a cell as a code cell using the pulldown menu in the toolbar of your Jupyter notebook. Otherwise, you can can hit esc and then y (denoted “esc, y”) while a cell is selected to specify that it is a code cell. Note that you will have to hit enter after doing this to start editing it.\nIf you want to execute the code in a code cell, hit “shift + enter.” Note that code cells are executed in the order you shift-enter them. That is to say, the ordering of the cells for which you hit “Shift + Enter” is the order in which the code is executed. If you did not explicitly execute a cell early in the document, its results are not known to the Python interpreter. This is a very important point and is often a source of confusion and frustration for students.\nMarkdown cells contain text. The text is written in markdown, a lightweight markup language. You can read about its syntax here. Note that you can also insert HTML into markdown cells, and this will be rendered properly. As you are typing the contents of these cells, the results appear as text. Hitting “Shift + Enter” renders the text in the formatting you specify.\nYou can specify a cell as being a markdown cell in the Jupyter toolbar, or by hitting “esc, m” in the cell. Again, you have to hit enter after using the quick keys to bring the cell into edit mode.\nIn general, when you want to add a new cell, you can click the + icon on the notebook toolbar. The shortcut to insert a cell below is “esc, b” and to insert a cell above is “esc, a.” Alternatively, you can execute a cell and automatically add a new one below it by hitting “alt + enter.”\n\n\nC.4.5 Code cells\nBelow is an example of a code cell printing hello, world. Notice that the output of the print statement appears in the same cell, though separate from the code block.\n\n# Say hello to the world.\nprint('Hello, world.')\n\nHello, world.\n\n\nIf you evaluate a Python expression that returns a value, that value is displayed as output of the code cell. This only happens, however, for the last line of the code cell.\n\n# Would show 9 if this were the last line, but it is not, so shows nothing\n4 + 5\n\n# I hope we see 11.\n5 + 6\n\n11\n\n\nNote that if the last line does not return a value, such as if we assigned a variable, there is no visible output from the code cell.\n\n# Variable assignment, so no visible output.\na = 5 + 6\n\n\n# However, now if we ask for a, its value will be displayed\na\n\n11\n\n\n\n\nC.4.6 Display of graphics\nWe will be using Bokeh almost exclusively during the course. To make sure the Bokeh plots get shown in the notebook, you should execute\nbokeh.io.output_notebook()\nin your notebook. It is good practice to execute this in the first cell of the notebook. Let us now make a plot using Bokeh.\n\n# Generate data to plot\nx = np.linspace(0, 2 * np.pi, 200)\ny = np.exp(np.sin(np.sin(x)))\n\n# Set up plot\np = bokeh.plotting.figure(\n    frame_height=200,\n    frame_width=250,\n    x_axis_label='x',\n    y_axis_label='y',\n    x_range=[0, 2 * np.pi],\n)\n\n# Populate glyph\np.line(x, y, line_width=2)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\n\n\nC.4.7 Proper formatting of cells\nGenerally, it is a good idea to keep cells simple. You can define one function, or maybe two or three closely related functions, in a single cell, and that’s about it. When you define a function, you should make sure it is properly commented with a descriptive doc string. Below is an example of how I might generate a plot of the Lorenz attractor (which I choose just because it is fun) with code cells and markdown cells with discussion of what I am doing. (The doc string in this function is nice, but longer than that is necessary for submitted homework in class. At least something akin to the first line of the doc string must appear in function definitions in your submitted notebooks.)\nBetween cells, you should explain with text what you are doing. Let’s look at a fun example.\nWe will use scipy.integrate.odeint() to numerically integrate the Lorenz attractor. We therefore first define a function that returns the right hand side of the system of ODEs that define the Lorentz attractor.\n\ndef lorenz_attractor(r, t, p):\n    \"\"\"\n    Compute the right hand side of system of ODEs for Lorenz attractor.\n    \n    Parameters\n    ----------\n    r : array_like, shape (3,)\n        (x, y, z) position of trajectory.\n    t : dummy_argument\n        Dummy argument, necessary to pass function into \n        scipy.integrate.odeint\n    p : array_like, shape (3,)\n        Parameters (s, k, b) for the attractor.\n        \n    Returns\n    -------\n    output : ndarray, shape (3,)\n        Time derivatives of Lorenz attractor.\n        \n    Notes\n    -----\n    .. Returns the right hand side of the system of ODEs describing\n       the Lorenz attractor.\n        x' = s * (y - x)\n        y' = x * (k - z) - y\n        z' = x * y - b * z\n    \"\"\"\n    # Unpack variables and parameters\n    x, y, z = r\n    s, p, b = p\n    \n    return np.array([s * (y - x), \n                     x * (p - z) - y, \n                     x * y - b * z])\n\nWith this function in hand, we just have to pick our initial conditions and time points and run the numerical integration.\n\n# Parameters to use\np = np.array([10.0, 28.0, 8.0 / 3.0])\n\n# Initial condition\nr0 = np.array([0.1, 0.0, 0.0])\n\n# Time points to sample\nt = np.linspace(0.0, 30.0, 4000)\n\n# Use scipy.integrate.odeint to integrate Lorentz attractor\nr = scipy.integrate.odeint(lorenz_attractor, r0, t, args=(p,))\n\n# Unpack results into x, y, z.\nx, y, z = r.transpose()\n\nNow, we’ll construct a plot of the trajectory using Bokeh.\n\n# Set up plot\np = bokeh.plotting.figure(\n    frame_height=200,\n    frame_width=200,\n    x_axis_label='x',\n    y_axis_label='z',\n)\n\n# Populate glyph\np.line(x, z)\n\nbokeh.io.show(p)\n\n\n  \n\n\n\n\n\n\n\nC.4.8 Best practices for code cells\nHere is a summary of some general rules for composing and formatting your code cells.\n\nKeep the width of code in cells below 80 characters. This is not a hard limit, but you should strive for it and consider 88 characters a hard limit.\nKeep your code cells short. If you find yourself having one massive code cell, break it up.\nProvide complete doc strings for any functions you define. You can and should have comments in your code, but you really should not need much because your markdown cells around the code cells should clearly describe what you are trying to do.\nDo all of your imports in the first code cell at the top of the notebook. With the exception of “from ... import ...” imports, import one module per line. You should also include bokeh.io.output_notebook() in the top cell as well when using Bokeh.\nFor submitting assignments, always display your graphics in the notebook.\n\n\n\nC.4.9 Markdown cells\nMarkdown cells contain text. The text is written in markdown, a lightweight markup language. The list of syntactical constructions at this link are pretty much all you need to know for standard markdown. Note that you can also insert HTML into markdown cells, and this will be rendered properly. As you are typing the contents of these cells, the results appear as text. Hitting “shift + enter” renders the text in the formatting you specify.\nYou can specify a cell as being a markdown cell in the Jupyter tool bar, or by hitting “esc, m” in the cell. Again, you have to hit enter after using the quick keys to bring the cell into edit mode.\nIn addition to HTML, some \\(\\LaTeX\\) expressions may be inserted into markdown cells. \\(\\LaTeX\\) (pronounced “lay-tech”) is a document markup language that uses the \\(\\TeX\\) typesetting software. It is particularly well-suited for beautiful typesetting of mathematical expressions. In Jupyter notebooks, the \\(\\LaTeX\\) mathematical input is rendered using software called MathJax. This is usually run off of a remote server, so if you are not connected to the internet, your equations may not be rendered. You will use \\(\\LaTeX\\) extensively in preparation of your assignments. There are plenty of resources on the internet for getting started with \\(\\LaTeX\\), but you will only need a tiny subset of its functionality in your assignments, and the next part of this lesson, plus cheat sheets you may find by Google (such as this one) are useful.\n\n\nC.4.10 Quick keys\nThere are some keyboard shortcuts that are convenient to use in JupyterLab. (They do not all work in Colab.) We already encountered Shift + Enter to run a code cell. Importantly, pressing Esc brings you into command mode in which you are not editing the contents of a single cell, but are doing things like adding cells. Below are some useful quick keys. If two keys are separated by a + sign, they are pressed simultaneously, and if they are separated by a - sign, they are pressed in succession.\n\n\n\nQuick keys\nmode\naction\n\n\n\n\nEsc - m\ncommand\nswitch cell to Markdown cell\n\n\nEsc - y\ncommand\nswitch cell to code cell\n\n\nEsc - a\ncommand\ninsert cell above\n\n\nEsc - b\ncommand\ninsert cell below\n\n\nEsc - d - d\ncommand\ndelete cell\n\n\nAlt + Enter\nedit\nexecute cell and insert a cell below\n\n\n\nThere are many others (and they are shown in the pulldown menus within JupyterLab), but these are the ones I seem to encounter most often.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to JupyterLab</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#rendering-of-notebooks-as-html",
    "href": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#rendering-of-notebooks-as-html",
    "title": "Appendix C — Introduction to JupyterLab",
    "section": "C.5 Rendering of notebooks as HTML",
    "text": "C.5 Rendering of notebooks as HTML\nWhen you submit homework, you will also submit an HTML rendering of your notebooks. To save a notebook as HTML, you can click File → Export Notebook As... → Export Notebook to HTML.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to JupyterLab</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#computing-environment",
    "href": "appendices/notebooks_and_latex/intro_to_jupyterlab.html#computing-environment",
    "title": "Appendix C — Introduction to JupyterLab",
    "section": "C.6 Computing environment",
    "text": "C.6 Computing environment\nAt the end of every lesson, and indeed at the end (or beginning) of any notebook you make, you should include information about the computing environment including the version numbers of all packages you use. This helps reproducibility. The watermark package is quite useful for this. The watermark package is an IPython magic extension. These extensions allow convenient functionality within IPython or Jupyter notebooks. In general, to use magic functions, you precede them with a % sign (or a double %%) in a cell. We use the built-in %load_ext magic function to load watermark, and then we use %watermark to invoke it.\nWe use the -v flag to ask watermark to give us the Python and IPython verison numbers and the -p flag to give us version numbers on specified packages we’ve used. We can also use a -m flag to give information about the machine running the notebook, and you should do that, but I will not do that for this course to avoid clutter.\nYour versions might not always match (especially if you are using Colab), but doing this is good practice and can help with debugging.\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,bokeh,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.5\nIPython version      : 9.4.0\n\nnumpy     : 2.2.6\nscipy     : 1.16.0\nbokeh     : 3.7.3\njupyterlab: 4.4.5",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Introduction to JupyterLab</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_latex.html",
    "href": "appendices/notebooks_and_latex/intro_to_latex.html",
    "title": "Appendix D — Introduction to LaTeX",
    "section": "",
    "text": "D.1 Basic inline LaTeX\n| Download notebook\nIn this tutorial, you will learn some of the basics on how to use \\(\\LaTeX\\) to display equations in Jupyter notebooks. For looking up symbols you may need, you can use any of the many cheat sheets you can find by asking Google. I have provided a few that will come up often in this course at the end of this lesson.\nIn this lesson, whenever a LaTeX expression is shown, the raw Markdown/LaTeX is shown beneath it. (The word LaTeX is generally stylized as \\(\\LaTeX\\), but I get tired of reading that, so going forward, I will just write “LaTeX.”)\nTo embed LaTeX within text, simply encapsulate the LaTeX portions in dollar signs ($). MathJax takes care of the rest. As an example, consider the sentence below and the markdown/LaTeX code to render it.\nEinstein told us that \\(E = mc^2\\).\nNotice how the equation is properly rendered, with mathematical variables in italics. Note also how ^2 was used to raise to a power. If the power has more than one character in it, it should be enclosed in braces ({}). In fact, braces are used to generally group symbols in LaTeX.\nEuler told us that \\(\\mathrm{e}^{i \\pi} - 1 = 0\\).\nAside from the grouping braces, there are several other syntactical items of note. First, notice that I made the special character \\(\\pi\\) with \\pi. In general, a backward slash precedes special symbols or commands in LaTeX. If we want another Greek letter, like \\(\\theta\\), we use \\theta. Now, also note that I used “\\mathrm{e}” for the base of the natural logarithm. I was signaling to LaTeX that I wanted the character written in Roman font, and not italics, so I used \\mathrm. Anything in the braces following the function \\mathrm is rendered in Roman font. Note the difference.\nThis is \\(e\\). This is \\(\\mathrm{e}\\)\nNow, back to grouping things in braces. We can do similar groupings using braces with with subscripts.\nThe dot product of two \\(n\\)-vectors is \\(\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i\\).\nHere, I have used $\\mathbf{a}$ to make the character a boldface, denoting a vector. Note that we denote subscripts with an underscore. Notice also that the bounds of the sum use the same underscore and caret notation as for subscripts and superscripts.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Introduction to LaTeX</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_latex.html#basic-inline-latex",
    "href": "appendices/notebooks_and_latex/intro_to_latex.html#basic-inline-latex",
    "title": "Appendix D — Introduction to LaTeX",
    "section": "",
    "text": "Einstein told us that $E = mc^2$.\n\n\nEuler told us that $\\mathrm{e}^{i \\pi} - 1 = 0$.\n\n\nThis is $e$. This is $\\mathrm{e}$.\n\n\nThe dot product of two $n$-vectors is $\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i$.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Introduction to LaTeX</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_latex.html#displaying-equations-on-separate-lines",
    "href": "appendices/notebooks_and_latex/intro_to_latex.html#displaying-equations-on-separate-lines",
    "title": "Appendix D — Introduction to LaTeX",
    "section": "D.2 Displaying equations on separate lines",
    "text": "D.2 Displaying equations on separate lines\nThe bounds on the summation in the above example may look a little funny to you because they are not above and below the summation symbol. This is because this particular equation is written inline. If we had separated it from the text, it renders differently.\nWe can make an equation appear centered on a new line, like\n\\[\\begin{align}\n\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i.\n\\end{align}\\]\nWe can make an equation appear centered on a new line, like\n\n\\begin{align}\n    \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i.\n\\end{align}\nThe align environment in LaTeX specifies that you want centered equations, separated from the text. It is called align because it allows you to align the equations. You separate lines in the equations with a double backslash (//). Insert an ampersand (&) in each line at the alignment point. All equations will be aligned at the location of the ampersand symbols (and, of course, the ampersands will not appear in the rendered equations).\nFor a three-vector consisting of \\(x\\), \\(y\\), and \\(z\\) components,\n\\[\\begin{align}\n\\mathbf{a} \\cdot \\mathbf{b} &= \\sum_{i=1}^n a_i b_i \\\\\n&= a_x b_x + a_y b_y + a_z b_z.\n\\end{align}\\]\nFor a three-vector consisting of $x$, $y$, and $z$ components,\n\n\\begin{align}\n    \\mathbf{a} \\cdot \\mathbf{b} &= \\sum_{i=1}^n a_i b_i \\\\\n    &= a_x b_x + a_y b_y + a_z b_z.\n\\end{align}\nNote that I always put an extra blank line before the \\begin{align} statement. This is not necessary, but I think things look better with the extra space.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Introduction to LaTeX</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_latex.html#fractions-and-an-example-of-fine-tuning",
    "href": "appendices/notebooks_and_latex/intro_to_latex.html#fractions-and-an-example-of-fine-tuning",
    "title": "Appendix D — Introduction to LaTeX",
    "section": "D.3 Fractions (and an example of fine-tuning)",
    "text": "D.3 Fractions (and an example of fine-tuning)\nTo display fractional quantities, we use the \\frac{}{} command. \\frac is always followed by two sets of braces; the numerator is contained in the first, and the denominator is contained in the second. As an example, we can write an equation you will become intimately familiar with if you take the second term of this course,\n\\[\\begin{align}\nP(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)}\n\\end{align}\\]\n\\begin{align}\n    P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)}\n\\end{align}\nThe right hand side has a nicely-formatted fraction. I did a little extra fine-tuning in this equation. I’ll show the equation again without the fine-tuning, which used the \\mid and \\, commands.\n\\[\\begin{align}\nP(A | B) = \\frac{P(B | A) P(A)}{P(B)}.\n\\end{align}\\]\n\\begin{align}\n    P(A | B) = \\frac{P(B | A) P(A)}{P(B)}.\n\\end{align}\nFirst, the \\mid command should be used in conditional probabilities. Just using a vertical bar (|) results in crowding. Similarly, I used the \\, command to insert a little extra space between the two probabilities in the numerator. This makes the equation a bit easier to read. This \\, operator is especially important when defining integrals. We can put a little space between the \\(\\mathrm{d}x\\) and the integrand.\n\\[\\begin{align}\n\\text{good: } &\\int_0^{2\\pi} \\mathrm{d}x \\, \\sin x. \\\\[1em]\n\\text{bad: } &\\int_0^{2\\pi} \\mathrm{d}x \\sin x.\n\\end{align}\\]\n\\begin{align}\n    \\text{good: } &\\int_0^{2\\pi} \\mathrm{d}x \\, \\sin x. \\\\[1em]\n    \\text{bad: } &\\int_0^{2\\pi} \\mathrm{d}x \\sin x.\n\\end{align}\nNote that I inserted extra space after the new line. Specifically, \\\\[1em] instructs LaTeX to insert a space equation to the width of an M character between the equations. I often do this to keep things clear.\nIt is also very important to note that I used \\(\\sin\\) and not \\(sin\\). Mathematical functions should be in Roman font and are invoked with a backslash. Otherwise, the characters are interpreted as separate variables. To be clear:\n\\[\\begin{align}\n\\text{good: } &\\sin x. \\\\[1em]\n\\text{bad: } & sin x.\n\\end{align}\\]\n\\begin{align}\n    \\text{good: } &\\sin x. \\\\[1em]\n    \\text{bad: } & sin x.\n\\end{align}\nFinally, notice that I was able to put text in the equation like this: \\text{good: }.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Introduction to LaTeX</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_latex.html#grouping-operators-and-more-fine-tuning",
    "href": "appendices/notebooks_and_latex/intro_to_latex.html#grouping-operators-and-more-fine-tuning",
    "title": "Appendix D — Introduction to LaTeX",
    "section": "D.4 Grouping operators (and more fine-tuning)",
    "text": "D.4 Grouping operators (and more fine-tuning)\nCompare the following equations.\n\\[\\begin{align}\n\\text{good: } &\\sum_{i=1}^n i^3 = \\left(\\sum_{i=1}^n i\\right)^2. \\\\[1em]\n\\text{bad: }  &\\sum_{i=1}^n i^3 = (\\sum_{i=1}^n i)^2.\n\\end{align}\\]\n\\begin{align}\n    \\text{good: } &\\sum_{i=1}^n i^3 = \\left(\\sum_{i=1}^n i\\right)^2. \\\\[1em]\n    \\text{bad: }  &\\sum_{i=1}^n i^3 = (\\sum_{i=1}^n i)^2.\n\\end{align}\nIn the second equation, I did not use the \\left( and \\right) construction for parentheses and the result looks pretty awful. In LaTeX, the height of anything that is encapsulated by \\left( and \\right) scales the parentheses appropriately. You can use \\left and \\right with many symbols. An important example is \\left\\{. Note that to display braces in an equation, you have to use \\{ because just a plain brace ({) has a different meaning.\n(By the way, that equation is true, and pretty amazing. It says that the sum of the first \\(n\\) cubes of integers is equal to the sum of the first \\(n\\) integers squared!)\nFinally, if you use \\left. or \\right., LaTeX will simply scale the opposite symbol to match the height of the text, but will suppress printing the other. For example,\n\\[\\begin{align}\n\\left. \\frac{1}{x + 2} \\right|_0^2 = -\\frac{1}{4}.\n\\end{align}\\]\n\\begin{align}\n    \\left. \\frac{1}{x + 2} \\right|_0^2 = -\\frac{1}{4}.\n\\end{align}\nThis is also useful if you are going to use / for a division operation. Compare the following.\n\\[\\begin{align}\n\\text{good: } & \\left. x^2 \\middle/ y^2 \\right. \\\\[1em]\n\\text{bad: } & x^2 / y^2\n\\end{align}\\]\n\\begin{align}\n    \\text{good: } & \\left. x^2 \\middle/ y^2 \\right. \\\\[1em]\n    \\text{bad: } & x^2 / y^2\n\\end{align}\nHere, we used the \\middle operator to scale the length of the division sign.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Introduction to LaTeX</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_latex.html#matrices-and-arrays",
    "href": "appendices/notebooks_and_latex/intro_to_latex.html#matrices-and-arrays",
    "title": "Appendix D — Introduction to LaTeX",
    "section": "D.5 Matrices and arrays",
    "text": "D.5 Matrices and arrays\nOn occasion, you’ll need to express matrices. This is most easily done using the pmatrix environment. For example, a covariance matrix for two variables might be written as\n\\[\\begin{align}\n\\sigma^2 = \\begin{pmatrix}\n\\sigma_1^2 & \\sigma_{12} \\\\\n\\sigma_{12} & \\sigma_2^2\n\\end{pmatrix}.\n\\end{align}\\]\n\\begin{align}\n    \\sigma^2 = \\begin{pmatrix}\n    \\sigma_1^2 & \\sigma_{12} \\\\\n    \\sigma_{12} & \\sigma_2^2 \n    \\end{pmatrix}.\n\\end{align}\nOnce in the pmatrix environment, each row has entries separated by an ampersand. The row ends with a \\\\. Each row must have the same number of entries.\nYou may also need to represent an values stacked on top of each other. For example, we might specify a piecewise linear function like this.\n\\[\\begin{align}\n\\text{rectifier}(x) = \\left\\{\n\\begin{array}{cl}\n0 & x \\le 0 \\\\\nx & x &gt; 0.\n\\end{array}\n\\right.\n\\end{align}\\]\n\\begin{align}\n    \\text{rectifier}(x) = \\left\\{\n    \\begin{array}{cl}\n    0 & x \\le 0 \\\\\n    x & x &gt; 0.\n    \\end{array}\n    \\right.\n\\end{align}\nThe array environment allows arrays of text. The {cl} after \\begin{array} indicates that two columns are wanted, with the first column being centered and the second being left-aligned. If we chose instead {lr}, the first column is left-aligned and the second column is right-aligned.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Introduction to LaTeX</span>"
    ]
  },
  {
    "objectID": "appendices/notebooks_and_latex/intro_to_latex.html#useful-latex-symbols-for-bebi-103",
    "href": "appendices/notebooks_and_latex/intro_to_latex.html#useful-latex-symbols-for-bebi-103",
    "title": "Appendix D — Introduction to LaTeX",
    "section": "D.6 Useful LaTeX symbols for BE/Bi 103",
    "text": "D.6 Useful LaTeX symbols for BE/Bi 103\nFollowing is a list of some symbols you may find useful in this class.\n\n\n\n\n\n\n\nLaTeX\nsymbol\n\n\n\n\n\\approx\n\\(\\approx\\)\n\n\n\\sim\n\\(\\sim\\)\n\n\n\\propto\n\\(\\propto\\)\n\n\n\\le\n\\(\\le\\)\n\n\nge\n\\(\\ge\\)\n\n\n\\pm\n\\(\\pm\\)\n\n\n\\in\n\\(\\in\\)\n\n\n\\ln\n\\(\\ln\\)\n\n\n\\exp\n\\(\\exp\\)\n\n\n\\prod_{i\\in D}\n\\({\\displaystyle \\prod_{i\\in D}}\\)\n\n\n\\sum_{i\\in D}\n\\({\\displaystyle \\sum_{i\\in D}}\\)\n\n\n\\frac{\\partial f}{\\partial x}\n\\({\\displaystyle \\frac{\\partial f}{\\partial x}}\\)\n\n\n\\sqrt{x}\n\\(\\sqrt{x}\\)\n\n\n\\bar{x}\n\\(\\bar{x}\\)\n\n\n\\langle x \\rangle\n\\(\\langle x \\rangle\\)\n\n\n\\left\\langle \\frac{x}{y} \\right\\rangle\n\\(\\left\\langle \\frac{x}{y} \\right\\rangle\\)",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Introduction to LaTeX</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html",
    "href": "appendices/python_basics/hello_world.html",
    "title": "Appendix E — Hello, world.",
    "section": "",
    "text": "E.1 The Python interpreter\n| Download notebook\nIn this appendix, we introduce some of the basic syntax and ideas behind the Python programming language and the Jupyter interface.\nPython is an interpreted language, which means that each line of code you write is translated, or interpreted, into a set of instructions that your machine can understand by the Python interpreter. This stands in contrast to compiled languages. For these languages (the dominant ones being Fortran, C, and C++), your entire code is translated into machine language before you ever run it. When you execute your program, it is already in machine language.\nSo, whenever you want your Python code to run, you give it to the Python interpreter.\nThere are many ways to launch the Python interpreter. One way is to type\non the command line of a terminal. This launches the vanilla Python interpreter. Because we are using Python code to explore biological circuit design, we will never really use this. Rather, we will have a greatly enhanced Python experience using Jupyter notebooks. Nevertheless, as you go beyond notebooks and do more sophisticated computing in your adventures with biological circuits, it is good to know about running Python outside of notebooks, so we will do that now.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html#the-python-interpreter",
    "href": "appendices/python_basics/hello_world.html#the-python-interpreter",
    "title": "Appendix E — Hello, world.",
    "section": "",
    "text": "python",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html#hello-world.-and-the-print-function",
    "href": "appendices/python_basics/hello_world.html#hello-world.-and-the-print-function",
    "title": "Appendix E — Hello, world.",
    "section": "E.2 Hello, world. and the print() function",
    "text": "E.2 Hello, world. and the print() function\nTraditionally, the first program anyone writes when learning a new language is called “Hello, world.” In this program, the words “Hello, world.” are printed on the screen. The original Hello, world. was likely written by Brian Kernighan, one of the inventors of Unix, and the author of the classic and authoritative book on the C programming language. In his original, the printed text was “hello, world” (no period or capital H), but people use lots of variants.\nWe will first write and run this little program using a JupyterLab console. After launching JupyterLab, you probably already have the Launcher in your JupyterLab window. If you do not, you can expand the Files tab at the left of your JupyterLab window (if it is not already expanded) by clicking on that tab, or alternatively hit ctrl+b (or cmd+b on macOS). At the top of the Files tab is a + sign, which gives you a Jupyter Launcher.\nIn the Jupyter Launcher, click the Python 3 icon under Console. This will launch a console, which has a large white space above a prompt that says In []:. You can enter Python code in this prompt, and it will be executed.\nTo print Hello, world., enter the code below. To execute the code, hit shift+enter.\n\nprint('Hello, world.')\n\nHello, world.\n\n\nHooray! We just printed Hello, world. to the screen. To do this, we used Python’s built-in print() function. The print() function takes a string as an argument. It then prints that string to the screen. We will learn more about function syntax later, but we can already see the rough syntax with the print() function.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html#py-files",
    "href": "appendices/python_basics/hello_world.html#py-files",
    "title": "Appendix E — Hello, world.",
    "section": "E.3 .py files",
    "text": "E.3 .py files\nNow let’s use our new knowledge of the print() function to have our computer say a bit more than just Hello, world. Type these lines in at the prompt, hitting enter each time you need a new line. After you’ve typed them all in, hit shift+enter to run them.\n\n# The first few lines from The Zen of Python by Tim Peters\nprint('Beautiful is better than ugly.')\nprint('Explicit is better than implicit.')\nprint('Simple is better than complex.')\nprint('Complex is better than complicated.')\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\n\n\nNote that the first line is preceded with a # sign, and the Python interpreter ignored it. The # sign denotes a comment, which is ignored by the interpreter, but very very important for the human!\nWhile the console prompt was nice for entering all of this, a better option is to store them in a file, and then have the Python interpreter run the lines in the file. This is how you typically store Python code, and the suffix of such files is .py.\nSo, let’s create a .py file. To do this, use the JupyterLab Launcher to launch a text editor. Once it is launched, you can right click on the tab of the text editor window to change the name. We will call this file zen.py. Within this file, enter the four lines of code you previously entered in the console prompt. Be sure to save it.\nTo run the code in this file, you can invoke the Python interpreter at the command line, followed by the file name. I.e., enter\npython zen.py\nat the command line. Note that when you run code this way, the interpreter exits after completion of running the code, and you do not get a prompt.\nTo run the code in this file using the Jupyter console, you can use the %run magic function.\n\n%run zen.py\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\n\n\nTo shut down the console, you can click on the Running tab at the left of the JupyterLab window and click on SHUTDOWN next to the console.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html#jupyter",
    "href": "appendices/python_basics/hello_world.html#jupyter",
    "title": "Appendix E — Hello, world.",
    "section": "E.4 Jupyter",
    "text": "E.4 Jupyter\nAt this point, we have introduced JupyterLab, its text editor, and the console, as well as the Python interpreter itself. You might be asking….\n\nE.4.1 What is Jupyter?\nFrom the Project Jupyter website: &gt;Project Jupyter is an open source project was born out of the IPython Project in 2014 as it evolved to support interactive data science and scientific computing across all programming languages.\nSo, Jupyter is an extension of IPython that pushes interactive computing further. It is language agnostic as its name suggests. The name “Jupyter” is a combination of Julia (a newer language for scientific computing), Python (which you know and love), and R (the dominant tool for statistical computation). However, you can run over 40 different languages in a JupyterLab, not just Julia, Python, and R.\nCentral to Jupyter/JupyterLab are Jupyter notebooks. In fact, the document you are reading right now was generated from a Jupyter notebook.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html#why-jupyter-notebooks",
    "href": "appendices/python_basics/hello_world.html#why-jupyter-notebooks",
    "title": "Appendix E — Hello, world.",
    "section": "E.5 Why Jupyter notebooks?",
    "text": "E.5 Why Jupyter notebooks?\nWhen writing code you will reuse, you should develop fully tested modules using .py files. You can always import those modules when you are using a Jupyter notebook (more on modules and importing them later in this appendix). So, a Jupyter notebook is not good for an application where you are building reusable code or scripts. However, Jupyter notebooks are very useful in the following applications.\n\nExploring data/analysis. Jupyter notebooks are great for trying things out with code, or exploring a data set. This is an important part of the research process. The layout of Jupyter notebooks is great for organizing thoughts as you synthesize them.\nSharing your thinking in your analysis. Because you can combine nicely formatted text and executable code, Jupyter notebooks are great for sharing how you go about doing your calculations with collaborators and with readers of your publications. Famously, LIGO used a Jupyter notebook to explain the signal processing involved in their first discovery of a gravitational wave.\nPedagogy. All of the content in this book, including this appendix, was developed using Jupyter notebooks!\n\nNow that we know what Jupyter notebooks are and what the motivation is for using them, let’s start!\n\nE.5.1 Launching a Jupyter notebook\nYou can launch JupyterLab via your operating system’s terminal program (Terminal on macOS and PowerShell on Windows). If you are on a Mac, open the Terminal program. You can do this hitting Command + space bar and searching for “terminal.” Using Windows, you should launch PowerShell. You can do this by hitting Windows + R and typing “powershell” in the text box.\nYou need to make sure you are using the caltech_datasai environment whenever you launch JupyterLab, so you should do conda activate caltech_datasai each time you open a terminal.\nNow that you have activated the caltech_datasai environment, you can launch JupyterLab by typing\njupyter lab\non the command line. You will have an instance of JupyterLab running in your default browser. If you want to specify the browser, you can, for example, type\njupyter lab --browser=firefox\non the command line.\nAlternatively, if you are using VSCode, you can use its menu system to open .ipynb files. Within the JupyterLab window, you will have the option to launch a notebook, a console, a terminal, or a text editor.\n\n\nE.5.2 Cells\nA Jupyter notebook consists of cells. The two main types of cells you will use are code cells and markdown cells, and we will go into their properties in depth momentarily. First, an overview.\nA code cell contains actual code that you want to run. You can specify a cell as a code cell using the pulldown menu in the toolbar of your Jupyter notebook. Otherwise, you can can press Esc and then y (denoted Esc - y“) while a cell is selected to specify that it is a code cell. Note that you will have to hit enter after doing this to start editing it.\nIf you want to execute the code in a code cell, hit Enter while holding down the Shift key (denoted Shift + Enter). Note that code cells are executed in the order you shift-enter them. That is to say, the ordering of the cells for which you hit Shift + Enter is the order in which the code is executed. If you did not explicitly execute a cell early in the document, its results are not known to the Python interpreter. This is a very important point and is often a source of confusion and frustration for students.\nMarkdown cells contain text. The text is written in markdown, a lightweight markup language. You can read about its syntax here. Note that you can also insert HTML into markdown cells, and this will be rendered properly. As you are typing the contents of these cells, the results appear as text. Hitting Shift + Enter renders the text in the formatting you specify.\nMarkdown cells can also render LaTeX for mathematics. For example,\nThe FitzHugh-Nagumo model is a simplification of the Hodgkin-Huxley model with two ordinaray differential equations,\n\n\\begin{align}\n\\frac{\\mathrm{d}v}{\\mathrm{d}t} &= v - \\frac{v^3}{3} - w + RI,\\\\[1em]\n\\tau\\,\\frac{\\mathrm{d}w}{\\mathrm{d}t} &= v + a + b w.\n\\end{align}\nrenders as:\nThe FitzHugh-Nagumo model is a simplification of the Hodgkin-Huxley model with two ordinaray differential equations,\n\\[\\begin{align}\n\\frac{\\mathrm{d}v}{\\mathrm{d}t} &= v - \\frac{v^3}{3} - w + RI,\\\\[1em]\n\\tau\\,\\frac{\\mathrm{d}w}{\\mathrm{d}t} &= v + a + b w.\n\\end{align}\\]\nYou can specify a cell as being a markdown cell in the Jupyter toolbar, or by hitting Esc - m in the cell. Again, you have to hit enter after using the quick keys to bring the cell into edit mode.\nIn general, when you want to add a new cell, you can click the + icon on the notebook toolbar. The shortcut to insert a cell below is Esc - b and to insert a cell above is Esc - a. Alternatively, you can execute a cell and automatically add a new one below it by hitting Alt + Enter.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html#code-cells",
    "href": "appendices/python_basics/hello_world.html#code-cells",
    "title": "Appendix E — Hello, world.",
    "section": "E.6 Code cells",
    "text": "E.6 Code cells\nBelow is an example of a code cell printing hello, world. Notice that the output of the print statement appears in the same cell, though separate from the code block.\n\n# Say hello to the world.\nprint('hello, world.')\n\nhello, world.\n\n\nIf you evaluate a Python expression that returns a value, that value is displayed as output of the code cell. This only happens for the last line of the code cell.\n\n# Would show 9 if this were the last line, but it is not, so shows nothing\n4 + 5\n\n# I hope we see 11.\n5 + 6\n\n11\n\n\nNote, however, if the last line does not return a value, such as if we assigned value to a variable, there is no visible output from the code cell.\n\n# Variable assignment, so no visible output.\na = 5 + 6\n\n\n# However, now if we ask for a, its value will be displayed\na\n\n11\n\n\n\nE.6.1 Display of graphics\nWhen we discuss plotting with Bokeh, you will learn about displaying graphics in Jupyter notebooks.\n\n\nE.6.2 Quick keys\nThere are some keyboard shortcuts that are convenient to use in JupyterLab. We already encountered many of them. Importantly, pressing Esc brings you into command mode in which you are not editing the contents of a single cell, but are doing things like adding cells. Below are some useful quick keys. If two keys are separated by a + sign, they are pressed simultaneously, and if they are separated by a - sign, they are pressed in succession.\n\n\n\nQuick keys\nmode\naction\n\n\n\n\nEsc - m\ncommand\nswitch cell to Markdown cell\n\n\nEsc - y\ncommand\nswitch cell to code cell\n\n\nEsc - a\ncommand\ninsert cell above\n\n\nEsc - b\ncommand\ninsert cell below\n\n\nEsc - d - d\ncommand\ndelete cell\n\n\nAlt + Enter\nedit\nexecute cell and insert a cell below\n\n\n\nThere are many others (and they are shown in the pulldown menus within JupyterLab), but these are the ones I seem to encounter most often.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/hello_world.html#computing-environment",
    "href": "appendices/python_basics/hello_world.html#computing-environment",
    "title": "Appendix E — Hello, world.",
    "section": "Computing environment",
    "text": "Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.12.9\nIPython version      : 8.30.0\n\njupyterlab: 4.3.6",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Hello, world.</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html",
    "href": "appendices/python_basics/variables_operators_types.html",
    "title": "Appendix F — Variables, operators, and types",
    "section": "",
    "text": "F.1 Determining the type\n| Download notebook\nWhether you are programming in Python or pretty much any other language, you will be working with variables. While the precise definition of a variable will vary from language to language, we’ll focus on Python variables here.\nWe will talk more about objects later, but a variable, like everything in Python, is an object. For now, you can think of it in the following way. The following can be properties of a variable: 1. The type of variable. E.g., is it an integer, like 2, or a string, like 'Hello, world.'? 2. The value of the variable.\nDepending on the type of the variable, you can do different things to it and other variables of similar type. This, as with most things, is best explored by example. We will go through some of the properties of variables and things you can do to them.\nFirst, we will use Python’s built-in type() function to determine the type of some variables.\ntype(2)\n\nint\ntype(2.3)\n\nfloat\ntype('Hello, world.')\n\nstr\nThe type function told us that 2 is an int (short for integer), 2.3 is a float (short for floating point number, basically a real number that is not an integer), and 'Hello, world.' is a str (short for string). Note that the single quotes around the characters indicate that it is a string. So, '1' is a string, but 1 is an integer.\nNote that we can also express floats using scientific notation; \\(4.5\\times 10^{-7}\\) is expressed as 4.5e-7.\ntype(4.5e-7)\n\nfloat",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#determining-the-type",
    "href": "appendices/python_basics/variables_operators_types.html#determining-the-type",
    "title": "Appendix F — Variables, operators, and types",
    "section": "",
    "text": "F.1.1 A note on strings\nWe just saw that strings can be enclosed in single quotes. In Python, we can equivalently enclose them in double quotes. E.g.,\n'my string'\nand\n\"my string\"\nare the same thing. We can also denote a string with triple quotes. So,\n\"\"\"my string\"\"\"\n'''my string'''\n\"my string\"\n'my string'\nare all the same thing. The difference with triple quotes is that it allows a string to extend over multiple lines.\n\n# A multi-line string\nmy_str = \"\"\"It was the best of times,\nit was the worst of times...\"\"\"\n\nprint(my_str)\n\nIt was the best of times,\nit was the worst of times...\n\n\nNote, though, we cannot do this with single quotes.\n\n# This is a SyntaxError\nmy_str = 'It was the best of times,\nit was the worst of times...'\n\n\n  Cell In[6], line 2\n    my_str = 'It was the best of times,\n             ^\nSyntaxError: unterminated string literal (detected at line 2)",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#operators",
    "href": "appendices/python_basics/variables_operators_types.html#operators",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.2 Operators",
    "text": "F.2 Operators\nOperators allow you to do things with variables, like add them. They are represented by special symbols, like + and *. For now, we will focus on arithmetic operators. Python’s arithmetic operators are\n\n\n\naction\noperator\n\n\n\n\naddition\n+\n\n\nsubtraction\n-\n\n\nmultiplication\n*\n\n\ndivision\n/\n\n\nraise to power\n**\n\n\nmodulo\n%\n\n\nfloor division\n//\n\n\n\nWarning: Do not use the ^ operator to raise to a power. That is actually the operator for bitwise XOR, which we will not really use. Observe firey death if you use these inappropriately:\n\n10^200\n\n194\n\n\nInstead of raising 10 to the 200th power, Python performed a bitwise XOR as illustrated below:\n\n\n\na\nBinary\nDecimal\n\n\n\n\nInput 1\n00001010\n10\n\n\nInput 2\n11001000\n200\n\n\nOutput\n11000010\n194\n\n\n\nNote: if you want to see how a decimal number is represented in binary, you can use the following:\n\n'{0:b}'.format(194)\n\n'11000010'\n\n\n\nF.2.1 Operations on integers\nLet’s see how these operators work on integers.\n\n2 + 3\n\n5\n\n\n\n2 - 3\n\n-1\n\n\n\n2 * 3\n\n6\n\n\n\n2 / 3\n\n0.6666666666666666\n\n\n\n2 ** 3\n\n8\n\n\n\n2 % 3\n\n2\n\n\n\n2 // 3\n\n0\n\n\nThis is what we would expect. An import note, though. If you are using Python 2, division of integers defaults to floor division. Some legacy code is written in Python 2, though it officially sunset on New Years Day 2020.\n\n\nF.2.2 Operations on floats\nLet’s try floats.\n\n2.1 + 3.2\n\n5.300000000000001\n\n\nWait a minute! We know 2.1 + 3.2 = 5.3, but Python gives 5.300000000000001. This is due to the fact that floating point numbers are stored with a finite number of binary bits. There will always be some rounding errors. This means that as far as the computer is concerned, it cannot tell you that 2.1 + 3.2 and 5.3 are equal. This is important to remember when dealing with floats, as we will see in the next lesson.\n\n2.1 - 3.2\n\n-1.1\n\n\n\n# Very very close to zero because of finite precision\n5.3 - (2.1 + 3.2)\n\n-8.881784197001252e-16\n\n\n\n2.1 * 3.2\n\n6.720000000000001\n\n\n\n2.1 / 3.2\n\n0.65625\n\n\n\n2.1 ** 3.2\n\n10.74241047739471\n\n\n\n2.1 % 3.2\n\n2.1\n\n\n\n2.1 // 3.2\n\n0.0\n\n\nAside from the floating point precision issue I already pointed out, everything is like we would expect. Note, though, that we cannot divide by zero.\n\n2.1 / 0.0\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[24], line 1\n----&gt; 1 2.1 / 0.0\n\nZeroDivisionError: float division by zero\n\n\n\nWe cannot do it with ints, either.\n\n2 / 0\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 2 / 0\n\nZeroDivisionError: division by zero\n\n\n\n\n\nF.2.3 Operations on integers and floats\nThis proceeds as we think it should.\n\n2.1 + 3\n\n5.1\n\n\n\n2.1 - 3\n\n-0.8999999999999999\n\n\n\n2.1 * 3\n\n6.300000000000001\n\n\n\n2.1 / 3\n\n0.7000000000000001\n\n\n\n2.1 ** 3\n\n9.261000000000001\n\n\n\n2.1 % 3\n\n2.1\n\n\n\n2.1 ** 3\n\n9.261000000000001\n\n\nAnd again we have the rounding errors, but everything is otherwise intuitive.\n\n\nF.2.4 Operations on strings\nNow let’s try some of these operations on strings. This idea of applying mathematical operations to strings seems strange, but let’s just mess around and see what we get.\n\n'Hello, ' + 'world.'\n\n'Hello, world.'\n\n\nAdding strings together concatenates them! This is also intuitive. How about subtracting strings?\n\n'Hello, ' - 'world'\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[34], line 1\n----&gt; 1 'Hello, ' - 'world'\n\nTypeError: unsupported operand type(s) for -: 'str' and 'str'\n\n\n\nThat stands to reason. Subtracting strings does not make sense. The Python interpreter was kind enough to give us a nice error message saying that we cannot have a str and a str operand type for the subtraction operation. It also makes sense that we can’t do multiplication, raising of power, etc., with two strings. How about multiplying a string by an integer?\n\n'Hello, world.' * 3\n\n'Hello, world.Hello, world.Hello, world.'\n\n\nYes, this makes sense! Multiplication by an integer is the same thing as just adding multiple times, so the Python interpreter concatenates the string several times.\nAs a final note on operators with strings, watch out for this:\n\n'4' + '2'\n\n'42'\n\n\nThe result is not 6, but it is a string containing the characters '4' and '2'.\n\n\nF.2.5 Order of operations\nThe order of operations is also as we would expect. Exponentiation comes first, followed by multiplication and division, floor division, and modulo. Next comes addition and subtraction. In order of precedence, our arithmetic operator table is\n\n\n\nprecedence\noperators\n\n\n\n\n1\n**\n\n\n2\n*, /, //, %\n\n\n3\n+, -\n\n\n\nYou can also group operations with parentheses. Operations within parentheses is are always evaluated first. As a watchout, do not use excessive parentheses. So often, we see students not trusting the order of operations and polluting their code with lots of parentheses, making it unreadable. This has been the source of countless bugs we have encountered in student code through the years.\nLet’s practice order-of-operations.\n\n1 + 4**2\n\n17\n\n\n\n1 + 4/2\n\n3.0\n\n\n\n1**3 + 2**3 + 3**3 + 4**3\n\n100\n\n\n\n(1 + 2 + 3 + 4)**2\n\n100\n\n\nInterestingly, we also demonstrated that the sum of the first \\(n\\) cubes is equal to the sum of the first \\(n\\) integers squared. Fun!",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#variables-and-assignment-operators",
    "href": "appendices/python_basics/variables_operators_types.html#variables-and-assignment-operators",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.3 Variables and assignment operators",
    "text": "F.3 Variables and assignment operators\nSo far, we have essentially just used Python as an oversized desktop calculator. We would really like to be able to think about our computational problems symbolically. We mentioned variables at the beginning of the tutorial, but in practice we were just using numbers and strings directly. We would like to say that a variable, a, represents an integer and another variable b represents another integer. Then, we could do things like add a and b. So, we see immediately that the variables have to have a type associated with them so the Python interpreter knows what to do when we use operators with them. A variable should also have a value associated with it, so the interpreter knows, e.g., what to add.\nIn order to create, or instantiate, a variable, we can use an assignment operator. This operator is the equals sign. So, let’s make variables a and b and add them.\n\na = 2\nb = 3\na + b\n\n5\n\n\nGreat! We get what we expect! And we still have a and b.\n\na, b\n\n(2, 3)\n\n\nNow, we might be tempted to say, “a is two.” No. a is not two. a is a variable that has a value of 2. A variable in Python is not just its value. A variable also carries with it a type. It also has more associated with it under the hood of the interpreter that we will not get into. So, you can think about a variable as a map to an address in RAM (called a pointer in computer-speak) that stores information, including a type and a value.\n\nF.3.1 Assignment/increment operators\nNow, let’s say we wanted to update the value of a by adding 4.1 to it. Python will do some magic for us.\n\nprint(type(a), a)\n\na = a + 4.1\n\nprint(type(a), a)\n\n&lt;class 'int'&gt; 2\n&lt;class 'float'&gt; 6.1\n\n\nWe see that a was initially an integer with a value of 2. But we added 4.1 to it, so the Python interpreter knew to change its type to a float and update its value.\nThis operation of updating a value can also be accomplished with an increment operator.\n\na = 2\na += 4.1\na\n\n6.1\n\n\nThe += operator told the interpreter to take the value of a and add 4.1 to it, changing the type of a in the intuitive way if need be. The other six arithmetic operators have similar constructions, -=, *=, /=, //=, %=, and **=.\n\na = 2\na **= 3\na\n\n8",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#type-conversion",
    "href": "appendices/python_basics/variables_operators_types.html#type-conversion",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.4 Type conversion",
    "text": "F.4 Type conversion\nSuppose you have a variable of one type, and you want to convert it to another. For example, say you have a string, '42', and you want to convert it to an integer. This would happen if you were reading information from a text file, which by definition is full of strings, and you wanted to convert some string to a number. This is done as follows.\n\nmy_str = '42'\nmy_int = int(my_str)\nprint(my_int, type(my_int))\n\n42 &lt;class 'int'&gt;\n\n\nConversely, we can convert an int back to a str.\n\nstr(my_int)\n\n'42'\n\n\nWhen converting a float to an int, the interpreter does not round the result, but gives the floor.\n\nint(2.9)\n\n2\n\n\nAlso consider our string concatenation warning/example from above:\n\nprint('4' + '2')\nprint(int('4') + int('2'))\n\n42\n6",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#relational-operators",
    "href": "appendices/python_basics/variables_operators_types.html#relational-operators",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.5 Relational operators",
    "text": "F.5 Relational operators\nSuppose we want to compare the values of two numbers. We may want to know if they are equal for example. The operator used to test for equality is ==, an example of a relational operator (also called a comparison operator).\n\nF.5.1 The equality relational operator\nLet’s test out the == to see how it works.\n\n5 == 5\n\nTrue\n\n\n\n5 == 4\n\nFalse\n\n\nNotice that using the operator gives either True or False. These are important keywords in Python that indicate truth. True and False have a special type, called bool, short for Boolean.\n\ntype(True)\n\nbool\n\n\n\ntype(False)\n\nbool\n\n\nThe equality operator, like all relational operators in Python, also works with variables, testing for equality of their values. Equality of the variables themselves uses identity operators, described below.\n\na = 4\nb = 5\nc = 4\n\na == b\n\nFalse\n\n\n\na == c\n\nTrue\n\n\nNow, let’s try it out with some floats.\n\n5.3 == 5.3\n\nTrue\n\n\n\n2.1 + 3.2 == 5.3\n\nFalse\n\n\nYikes! Python is telling us that 2.1 + 3.2 is not 5.3. This is floating point arithmetic haunting us. Note that floating point numbers that can be exactly represented with binary numbers do not have this problem.\n\n2.2 + 3.2 == 5.4\n\nTrue\n\n\nThis behavior is unpredictable, so here is a rule.\n\nNever use the == operator with floats.\n\n\n\nF.5.2 Other relational operators\nAs you might expect, there are other relational operators. The relational operators are\n\n\n\nEnglish\nPython\n\n\n\n\nis equal to\n==\n\n\nis not equal to\n!=\n\n\nis greater than\n&gt;\n\n\nis less than\n&lt;\n\n\nis greater than or equal to\n&gt;=\n\n\nis less than or equal to\n&lt;=\n\n\n\nWe can try some of them out!\n\n4 &lt; 5\n\nTrue\n\n\n\n5.7 &lt;= 3\n\nFalse\n\n\n\n'michael jordan' &gt; 'lebron james'\n\nTrue\n\n\nWhoa. What happened on that last one? The Python interpreter has weighed in on the debate about the greatest basketball player of all time. It clearly thinks Michael Jordan is better than LeBron James, but that seems kind of subjective. To understand what the interpreter is doing, we need to understand how it compares strings.\n\n\nF.5.3 A brief aside on Unicode\nIn Python, characters are encoded with Unicode. This is a standardized library of characters from many languages around the world that contains over 100,000 characters. Each character has a unique number associated with it. We can access what number is assigned to a character using Python’s built-in ord() function.\n\nord('a')\n\n97\n\n\n\nord('λ')\n\n955\n\n\nThe relational operators on characters compare the values that the ord function returns. So, using a relational operator on 'a' and 'b' means you are comparing ord('a') and ord('b'). When comparing strings, the interpreter first compares the first character of each string. If they are equal, it compares the second character, and so on. So, the reason that 'michael jordan' &gt; 'lebron james' gives a value of True is because ord('m') &gt; ord('l').\nNote that a result of this scheme is that testing for equality of strings means that all characters must be equal. This is the most common use case for relational operators with strings.\n\n'lebron' == 'lebron james'\n\nFalse\n\n\n\n'lebron' == 'LeBron'\n\nFalse\n\n\n\n'LeBron James' == 'LeBron James'\n\nTrue\n\n\n\n'AGTCACAGTA' == 'AGTCACAGCA'\n\nFalse\n\n\n\n\nF.5.4 Chaining relational operators\nPython allow chaining of relational operators.\n\n4 &lt; 6 &lt; 6.1 &lt; 9.3\n\nTrue\n\n\n\n4 &lt; 6.1 &lt; 6 &lt; 9.3\n\nFalse\n\n\nThis is convenient do to. However, it is important not to do the following, even though it is legal.\n\n4 &lt; 6.1 &gt; 5\n\nTrue\n\n\nIn other words, do not mix the direction of the relational operators. You could run into trouble because, in this case, 5 and 4 are never compared. An expression with different relations among all three numbers also returns True.\n\n4 &lt; 6.1 &gt; 3\n\nTrue\n\n\nSo, I issue a warning.\n\nDo not mix the directions of chained relational operators.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#identity-operators",
    "href": "appendices/python_basics/variables_operators_types.html#identity-operators",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.6 Identity operators",
    "text": "F.6 Identity operators\nIdentity operators check to see if two variables occupy the same space in memory; i.e., they are the same object (we’ll learn more about objects as we go along). This is different that the equality relational operator, ==, which checks to see if two variables have the same value. The two identity operators are in the table below.\n\n\n\nEnglish\nPython\n\n\n\n\nis the same object\nis\n\n\nis not the same object\nis not\n\n\n\nThat’s right. The operators are pretty much the same as English! Let’s see these operators in action and get at the difference between == and is. Let’s use the is operator to investigate how Python stored variables in memory, starting with floats.\n\na = 5.6\nb = 5.6\n\na == b, a is b\n\n(True, False)\n\n\nEven though a and b have the same value, they are stored in different places in memory. They can occupy the same place in memory if we do a b = a assignment.\n\na = 5.6\nb = a\n\na == b, a is b\n\n(True, True)\n\n\nBecause we assigned b = a, they necessarily have the same (immutable) value. So, the two variables occupy the same place in memory for efficiency.\n\na = 5.6\nb = a\na = 6.1\n\na == b, a is b\n\n(False, False)\n\n\nIn the last two examples, we see that assigning b = a, where a is a float in this case, means that a and b occupy the same memory. However, reassigning the value of a resulted in the interpreter placing a in a new space in memory. We can double check the values.\nIntegers sometimes do not behave the same way, however.\n\na = 5\nb = 5\n\na == b, a is b\n\n(True, True)\n\n\nEven though we assigned a and b separately, they occupy the same place in memory. This is because Python employs integer caching for all integers between -5 and 256. This caching does not happen for more negative or larger integers.\n\na = 350\nb = 350\n\na is b\n\nFalse\n\n\nNow, let’s look at strings.\n\na = 'Hello, world.'\nb = 'Hello, world.'\n\na == b, a is b\n\n(True, False)\n\n\nSo, even though a and b have the same value, they do not occupy the same place in memory. If we do a b = a assignment, we get similar results as with floats.\n\na = 'Hello, world.'\nb = a\n\na == b, a is b\n\n(True, True)\n\n\nLet’s try string assignment again with a different string.\n\na = 'python'\nb = 'python'\n\na == b, a is b\n\n(True, True)\n\n\nWait a minute! If we choose a string 'python', it occupies the same place in memory as another variable with the same value, but that was not the case for 'Hello, world.'. This is a result of Python also doing string interning which allows for (sometimes very) efficient string processing. Whether two strings occupy the same place in memory depends on what the strings are.\nThe caching and interning might be a problem, but you generally do not need to worry about it for immutable variables. Being immutable means that once the variables are created, their values cannot be changed. If we do change the value the variable gets a new place in memory. All variables we’ve encountered so far, ints, floats, and strs, are immutable. We will see encounter mutable data types in future lesson, in which case it really does matter practically to you as a programmer whether or not two variables are in the same location in memory.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#logical-operators",
    "href": "appendices/python_basics/variables_operators_types.html#logical-operators",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.7 Logical operators",
    "text": "F.7 Logical operators\nLogical operators can be used to connect relational and identity operators. Python has three logical operators.\n\n\n\nLogic\nPython\n\n\n\n\nAND\nand\n\n\nOR\nor\n\n\nNOT\nnot\n\n\n\nThe and operator means that if both operands are True, return True. The or operator gives True if either of the operands are True. Finally, the not operator negates the logical result.\nThat might be as clear as mud to you. It is easier to learn this, as usual, by example.\n\nTrue and True\n\nTrue\n\n\n\nTrue and False\n\nFalse\n\n\n\nTrue or False\n\nTrue\n\n\n\nTrue or True\n\nTrue\n\n\n\nnot False and True\n\nTrue\n\n\n\nnot(False and True)\n\nTrue\n\n\n\nnot False or True\n\nTrue\n\n\n\nnot (False or True)\n\nFalse\n\n\n\n7 == 7 or 7.6 == 9.1\n\nTrue\n\n\n\n7 == 7 and 7.6 == 9.1\n\nFalse\n\n\nI think these examples will help you get the hang of it. Note that it is important to specify the ordering of your operations, particularly when using the not operator.\nNote also that\na &lt; b &lt; c\nis equivalent to\n(a &lt; b) and (b &lt; c)\nWith these new types of operators in hand, we can construct a more complete table of operator precedence.\n\n\n\nprecedence\noperators\n\n\n\n\n1\n**\n\n\n2\n*, /, //, %\n\n\n3\n+, -\n\n\n4\n&lt;, &gt;, &lt;=, &gt;=\n\n\n5\n==, !=\n\n\n6\n=, +=, -=, *=, /=, **=, %=, //=\n\n\n7\nis, is not\n\n\n8\nand, or, not",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#operators-we-left-out",
    "href": "appendices/python_basics/variables_operators_types.html#operators-we-left-out",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.8 Operators we left out",
    "text": "F.8 Operators we left out\nWe have left out a few operators in Python. Two that we left out are the membership operators, in and not in, which we will visit in forthcoming sections of this appendix. The others we left out are bitwise operators and operators on sets, which we will not be covering.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#the-numerical-values-of-true-and-false",
    "href": "appendices/python_basics/variables_operators_types.html#the-numerical-values-of-true-and-false",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.9 The numerical values of True and False",
    "text": "F.9 The numerical values of True and False\nAs we move to conditionals, it is important to take a moment to evaluate the numerical values of the keywords True and False. They have numerical values of 1 and 0, respectively.\n\nTrue == 1\n\nTrue\n\n\n\nFalse == 0\n\nTrue\n\n\nYou can do arithmetic on True and False, but you will get implicit type conversion.\n\nTrue + False\n\n1\n\n\n\ntype(True + False)\n\nint",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#conditionals",
    "href": "appendices/python_basics/variables_operators_types.html#conditionals",
    "title": "Appendix F — Variables, operators, and types",
    "section": "F.10 Conditionals",
    "text": "F.10 Conditionals\nConditionals are used to tell your computer to do a set of instructions depending on whether or not a Boolean is True. In other words, we are telling the computer:\nif something is true:\n    do task a\notherwise:\n    do task b\nIn fact, the syntax in Python is almost exactly the same. As an example, let’s ask whether or not a codon is the canonical start codon (AUG).\n\ncodon = 'AUG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\n\nThis codon is the start codon.\n\n\nThe syntax of the if statement is apparent in the above example. The Boolean expression, codon == 'AUG', is called the condition. If it is True, the indented statement below it is executed. This brings up a very important aspect of Python syntax.\n\nIndentation matters.\n\nAny lines with the same level of indentation will be evaluated together.\n\ncodon = 'AUG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\n    print('Same level of intentation, so still printed!')\n\nThis codon is the start codon.\nSame level of intentation, so still printed!\n\n\nWhat happens if our codon is not the start codon?\n\ncodon = 'AGG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\n\nNothing is printed. This is because we did not tell Python what to do if the Boolean expression codon == 'AUG' evaluated False. We can add that with an else clause in the conditional.\n\ncodon = 'AGG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\nelse:\n    print('This codon is not the start codon.')\n\nThis codon is not the start codon.\n\n\nGreat! Now, we have a construction that can choose which action to take depending on a value. So, if we’re zooming along an RNA sequence, we could pick out the start codon and infer where translation would start. Now, what if we want to know if we hit a canonical stop codon (UAA, UAG, or UGA)? We can nest the conditionals!\n\ncodon = 'UAG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\nelse:\n    if codon == 'UAA' or codon == 'UAG' or codon == 'UGA':\n        print('This codon is a stop codon.')\n    else:\n        print('This codon is neither a start nor stop codon.')\n\nThis codon is a stop codon.\n\n\nNotice that the indentation defines which clause the statement belongs to. E.g., the second if statement is executed as part of the first else clause.\nWhile this nesting is very nice, we can be more concise by using an elif clause.\n\ncodon = 'UGG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\nelif codon == 'UAA' or codon == 'UAG' or codon == 'UGA':\n    print('This codon is a stop codon.')\nelse:\n    print('This codon is neither a start nor stop codon.')\n\nThis codon is neither a start nor stop codon.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/variables_operators_types.html#computing-environment",
    "href": "appendices/python_basics/variables_operators_types.html#computing-environment",
    "title": "Appendix F — Variables, operators, and types",
    "section": "Computing environment",
    "text": "Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.10.9\nIPython version      : 8.10.0\n\njupyterlab: 3.5.3",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Variables, operators, and types</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/lists_and_tuples.html",
    "href": "appendices/python_basics/lists_and_tuples.html",
    "title": "Appendix G — Lists and tuples",
    "section": "",
    "text": "G.1 Lists\n| Download notebook\nIn this tutorial, we will explore two important data types in Python, lists and tuples. They are both sequences of objects. Just like a string is a sequence (that is, an ordered collection) of characters, lists and tuples are sequences of arbitrary objects, called items or elements. They are a way to make a single object that contains many other objects. We will start our discussion with lists.\nAs usual, it is easiest to explore new topics by example. We’ll start by creating a list.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/lists_and_tuples.html#lists",
    "href": "appendices/python_basics/lists_and_tuples.html#lists",
    "title": "Appendix G — Lists and tuples",
    "section": "",
    "text": "G.1.1 List creation\nWe create lists by putting Python values or expressions inside square brackets, separated by commas. For example:\n\nmy_list_1 = [1, 2, 3, 4]\ntype(my_list_1)\n\nlist\n\n\nWe observe here that although the elements of the list are ints, the type of the list is list. Actually, any Python expression can be inside a list (including another list!):\n\nmy_list_2 = [1, 2.4, 'a string', ['a string in another list', 5]]\nmy_list_2\n\n[1, 2.4, 'a string', ['a string in another list', 5]]\n\n\n\nmy_list_3 = [2+3, 5*3, 4**2]\nmy_list_3\n\n[5, 15, 16]\n\n\nmy_list_2 contains ints, a float, a string and another list. And our third list contains expressions that get evaluated when the list as a whole gets created.\nWe can also create a list by type conversion. For example, we can convert a string into a list of characters.\n\nmy_str = 'A string.'\nlist(my_str)\n\n['A', ' ', 's', 't', 'r', 'i', 'n', 'g', '.']\n\n\n\n\nG.1.2 List operators\nOperators on lists behave much like operators on strings. The + operator on lists means list concatenation.\n\n[1, 2, 3] + [4, 5, 6]\n\n[1, 2, 3, 4, 5, 6]\n\n\nThe * operator on lists means list replication and concatenation.\n\n[1, 2, 3] * 3\n\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n\n\n\nG.1.2.1 Membership operators\nMembership operators are used to determine if an item is in a list. The two membership operators are:\n\n\n\nEnglish\noperator\n\n\n\n\nis a member of\nin\n\n\nis not a member of\nnot in\n\n\n\n\nThe result of the operator is True or False. Let’s look at my_list_2 again:\n\nmy_list_2 = [1, 2.4, 'a string', ['a string in another list', 5]]\n1 in my_list_2\n\nTrue\n\n\n\n['a string in another list', 5] in my_list_2\n\nTrue\n\n\n\n'a string in another list' in my_list_2\n\nFalse\n\n\n\n7 not in my_list_2\n\nTrue\n\n\nImportantly, we see that the string 'a string in another list' is not in my_list_2. This is because that string itself is not one of the four items of my_list_2. The string 'a string in another list' is in a list that is an item in my_list_2.\nNow, these membership operators offer a great convenience for conditionals. Remember our example about stop codons?\n\ncodon = 'UGG'\n\nif codon == 'AUG':\n    print('This codon is the start codon.')\nelif codon == 'UAA' or codon == 'UAG' or codon == 'UGA':\n    print('This codon is a stop codon.')\nelse:\n    print('This codon is neither a start nor stop codon.')\n\nThis codon is neither a start nor stop codon.\n\n\nWe can rewrite this much more cleanly, and with a lower chance of bugs, using a list and the in operator.\n\n# Make a list of stop codons\nstop_codons = ['UAA', 'UAG', 'UGA']\n\n# Specify codon\ncodon = 'UGG'\n\n# Check to see if it is a start or stop codon\nif codon == 'AUG':\n    print('This codon is the start codon.')\nelif codon in stop_codons:\n    print('This codon is a stop codon.')\nelse:\n    print('This codon is neither a start nor stop codon.')\n\nThis codon is neither a start nor stop codon.\n\n\nThe simple expression\ncodon in stop_codons\nreplaced the more verbose\ncodon == 'UAA' or codon == 'UAG' or codon == 'UGA'\nMuch nicer!\n\n\n\nG.1.3 List indexing\nImagine that we would like to access an item in a list. Because a list is ordered, we can ask for the first item, the second item, the nth item, the last item, etc. This is done using a bracket notation. We first write the name of our list and then enclosed in square brackets we write the location (index) of the desired element:\n\nmy_list = [1, 2.4, 'a string', ['a string in another list', 5]]\n\nmy_list[1]\n\n2.4\n\n\nWait a minute! Shouldn’t my_list[1] give the first item in the list? It seems to give the second. This is because indexing in Python starts at zero. This is very important. (Historical note: Why Python uses 0-based indexing It is also worth reading Edsgar Dijkstra’s thoughts on the matter.)\n\nIndexing in Python starts at zero.\n\nNow that we know that, let’s look at the items in the list.\n\nprint(my_list[0])\nprint(my_list[1])\nprint(my_list[2])\nprint(my_list[3])\n\n1\n2.4\na string\n['a string in another list', 5]\n\n\nWe can also index the list that is within my_list by adding another set of brackets.\n\nmy_list[3][0]\n\n'a string in another list'\n\n\nSo, now we have the basics of list indexing. There are more ways to specify items in a list. We’ll look at some of these now, but in order to do it, it helps to have a simpler list. We’ll therefore create a list that goes from zero to ten.\n\nmy_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nmy_list[4]\n\n4\n\n\nWe already knew that would be the result. We can use negative indexing as well! This just means we start indexing from the last entry, starting at -1.\n\nmy_list[-1]\n\n10\n\n\n\nmy_list[-3]\n\n8\n\n\nThis is very convenient for indexing in reverse. Now make it more clear, here are the forward and backward indices for the list:\n\n\n\nValues\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nForward indices\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nReverse indices\n-11\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n\n\n\n\n\nG.1.4 List slicing\nNow, what if we want to pull out multiple items in a list, called slicing? We can use colons (:) for that.\n\nmy_list[0:5]\n\n[0, 1, 2, 3, 4]\n\n\nWe got elements 0 through 4. When using the colon indexing, my_list[i:j], we get items i through j-1. I.e., the range is inclusive of the first index and exclusive of the last. If the slice’s final index is larger than the length of the sequence, the slice ends at the last element.\n\nmy_list[3:1000]\n\n[3, 4, 5, 6, 7, 8, 9, 10]\n\n\nNow, we can also use negative indices with colons.\n\nmy_list[0:-3]\n\n[0, 1, 2, 3, 4, 5, 6, 7]\n\n\nAgain, note that we only went to index -4.\nWe can also specify a stride. The stride comes after a second colon. For example, if we only wanted the even numbers, we could do the following.\n\nmy_list[0::2]\n\n[0, 2, 4, 6, 8, 10]\n\n\nNotice that we did not enter anything for the end value of the slice. If the end is left blank, the default is to include the entire string. Similarly, we can leave out the start index, as its default is zero.\n\nmy_list[::2]\n\n[0, 2, 4, 6, 8, 10]\n\n\nSo, in general, the indexing scheme is:\n    my_list[start:end:stride]\n\nIf there are no colons, a single element is returned.\nIf there are any colons, we are slicing the list, and a list is returned.\nIf there is one colon, stride is assumed to be 1.\nIf start is not specified, it is assumed to be zero.\nIf end is not specified, the interpreted assumed you want the entire list.\nIf stride is not specified, it is assumed to be 1.\n\nWith this in hand, we do lots of crazy slicing. We can even use a negative stride, which results in reversing the list.\n\nmy_list[::-1]\n\n[10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n\n\nNote that the meaning of the “start” and “end” index is a bit ambiguous when you have a negative stride. When the stride is negative, we still slice from start to end, but then reverse the order.\nNow, let’s look at a few examples (inspired by Brett Slatkin).\n\nprint(my_list[2::2])\nprint(my_list[2:-1:2])\nprint(my_list[-2::-2])\nprint(my_list[-2:2:-2])\nprint(my_list[2:2:-2])\n\n[2, 4, 6, 8, 10]\n[2, 4, 6, 8]\n[9, 7, 5, 3, 1]\n[9, 7, 5, 3]\n[]\n\n\nYou can see that it takes a lot of thought to understand what the slices actually are. So, here is some good advice: Do not use start, end, and slice all at the same time (even though you can). Do the stride first and then the slice, on separate lines. For example, if we wanted just the even numbers, but not the first and last (this was the my_list[2:-1:2] example we just did), we would do\n\n# Extract evens\nevens = my_list[::2]\n\n# Cut off end values\nevens_without_end_values = evens[1:-1]\n\nevens_without_end_values\n\n[2, 4, 6, 8]\n\n\nThis is more verbose, but much easier to read and understand.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/lists_and_tuples.html#mutability",
    "href": "appendices/python_basics/lists_and_tuples.html#mutability",
    "title": "Appendix G — Lists and tuples",
    "section": "G.2 Mutability",
    "text": "G.2 Mutability\nLists are mutable. That means that you can change their values without creating a new list. (You cannot change the data type or identity.) Let’s see this by example.\n\nmy_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nmy_list[3] = 'four'\n\nmy_list\n\n[1, 2, 3, 'four', 5, 6, 7, 8, 9, 10]\n\n\nThe other data types we have encountered so far, ints, floats, and strs, are immutable. You cannot change their values without reassigning them. To see this, we’ll use the id() function, which tells us where in memory that the variable is stored. (Note: this identity is unique to the Python interpreter, and should not be considered an actual physical address in memory.)\n\na = 689\nprint(id(a))\n\na = 690\nprint(id(a))\n\n4400362064\n4400362832\n\n\nSo, we see that the identity of a, an integer, changed when we tried to change its value. So, we didn’t actually change its value; we made a new variable. With lists, though, this is not the case.\n\nprint(id(my_list))\n\nmy_list[0] = 'zero'\nprint(id(my_list))\n\n4402226560\n4402226560\n\n\nIt is still the same list! This is very important to consider when we do assignments.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/lists_and_tuples.html#pitfall-aliasing",
    "href": "appendices/python_basics/lists_and_tuples.html#pitfall-aliasing",
    "title": "Appendix G — Lists and tuples",
    "section": "G.3 Pitfall: Aliasing",
    "text": "G.3 Pitfall: Aliasing\nAliasing is a subtle issue which can come up when assigning lists to variables. Let’s look at an example. We will make a list, then assign a new variable to the list (which we will momentarily erroneously think of as making a copy of the list) and then change a value of an entry in the “copied” list.\n\nmy_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nmy_list_2 = my_list     # copy of my_list?\nmy_list_2[0] = 'a'\n\nmy_list_2\n\n['a', 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nNow, let’s look at our original list to see what it looks like.\n\nmy_list\n\n['a', 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nSo we see that assigning a list to a variable does not copy the list! Instead, you just get a new reference to the same value. This has the real potential to introduce a nasty bug that will bite you!\nThere is a way we can avoid this problem by using list slices. If both the slice’s starting index and the slice’s ending index of a list are left out, the slice is a copy of the entire list in a new hunk of memory.\n\nmy_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nmy_list_2 = my_list[:]\nmy_list_2[0] = 'a'\n\nmy_list_2\n\n['a', 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\nmy_list\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nAnother option is to use a data type that is very much like a list, except it is immutable.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/lists_and_tuples.html#tuples",
    "href": "appendices/python_basics/lists_and_tuples.html#tuples",
    "title": "Appendix G — Lists and tuples",
    "section": "G.4 Tuples",
    "text": "G.4 Tuples\nA tuple is just like a list, except it is immutable (basically a read-only list). (What I just said there is explosive, as described in this blog post. Tuples do have many other capabilities beyond what you would expect from just bring “a read-only list,” but for us just beginning now, we can think of it that way.) A tuple is created just like a list, except we use parentheses instead of brackets. The only watch-out is that a tuple with a single item needs to include a comma after the item.\n\nmy_tuple = (0,)\n\nnot_a_tuple = (0) # this is just the number 0 (normal use of parantheses)\n\ntype(my_tuple), type(not_a_tuple)\n\n(tuple, int)\n\n\nWe can also create a tuple by doing a type conversion. We can convert our list to a tuple.\n\nmy_list = [1, 2.4, 'a string', ['a sting in another list', 5]]\n\nmy_tuple = tuple(my_list)\n\nmy_tuple\n\n(1, 2.4, 'a string', ['a sting in another list', 5])\n\n\nNote that the list within my_list did not get converted to a tuple. It is still a list, and it is mutable.\n\nmy_tuple[3][0] = 'a string in a list in a tuple'\n\nmy_tuple\n\n(1, 2.4, 'a string', ['a string in a list in a tuple', 5])\n\n\nHowever, if we try to change an item in a tuple, we get an error.\n\nmy_tuple[1] = 7\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[37], line 1\n----&gt; 1 my_tuple[1] = 7\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\nEven though the list within the tuple is mutable, we still cannot change the identity of that list.\n\nmy_tuple[3] = ['a', 'new', 'list']\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[38], line 1\n----&gt; 1 my_tuple[3] = ['a', 'new', 'list']\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\nG.4.1 Slicing of tuples\nSlicing of tuples is the same as lists, except a tuple is returned from the slicing operation, not a list.\n\nmy_tuple = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n# Reverse\nmy_tuple[::-1]\n\n(10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)\n\n\n\n# Odd numbers\nmy_tuple[1::2]\n\n(1, 3, 5, 7, 9)\n\n\n\n\nG.4.2 The + operator with tuples\nAs with lists we can concatenate tuples with the + operator.\n\nmy_tuple + (11, 12, 13, 14, 15)\n\n(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)\n\n\n\n\nG.4.3 Membership operators with tuples\nMembership operators work the same as with lists.\n\n5 in my_tuple\n\nTrue\n\n\n\n'LeBron James' not in my_tuple\n\nTrue\n\n\n\n\nG.4.4 Tuple unpacking\nIt is like a multiple assignment statement that is best seen through example.\n\nmy_tuple = (1, 2, 3)\n(a, b, c) = my_tuple\n\na\n\n1\n\n\n\nb\n\n2\n\n\n\nc\n\n3\n\n\nThis is useful when we want to return more than one value from a function and further using the values as stored in different variables. We will make use of this as the bootcamp goes on; we’ll be writing functions in just a couple lessons!\nNote that the parentheses are dispensable.\n\na, b, c = my_tuple\n\nprint(a, b, c)\n\n1 2 3",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/lists_and_tuples.html#wisdom-on-tuples-and-lists",
    "href": "appendices/python_basics/lists_and_tuples.html#wisdom-on-tuples-and-lists",
    "title": "Appendix G — Lists and tuples",
    "section": "G.5 Wisdom on tuples and lists",
    "text": "G.5 Wisdom on tuples and lists\nAt face, tuples and lists are very similar, differing essentially only in mutability. The differences are actually more profound, as described in the aforementioned blog post. We will make extensive use of them in our programs.\n“When should I use a tuple and when should I use a list?” you ask. Here is my advice.\n\nAlways use tuples instead of lists unless you need mutability.\n\nThis keeps you out of trouble. It is very easy to inadvertently change one list, and then another list (that is actually the same, but with a different variable name) gets mangled. That said, mutability is often very useful, so you can use it to make your list and adjust it as you need. However, after you have finalized your list, you should convert it to a tuple so it cannot get mangled. We’ll come back to this later in the bootcamp.\nSo, I ask you, which is better?\n\n# Should it be a list?\nstop_codons = ['UAA', 'UAG', 'UGA']\n\n# or a tuple?\nstop_codons = ('UAA', 'UAG', 'UGA')",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/lists_and_tuples.html#computing-environment",
    "href": "appendices/python_basics/lists_and_tuples.html#computing-environment",
    "title": "Appendix G — Lists and tuples",
    "section": "G.6 Computing environment",
    "text": "G.6 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Lists and tuples</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/iteration.html",
    "href": "appendices/python_basics/iteration.html",
    "title": "Appendix H — Iteration",
    "section": "",
    "text": "H.1 Introducing the for loop\n| Download notebook\nWe often want a computer program to do a similar operation many times. For example, we may want to analyze many codons, one after another, and find the start and stop codons in order to determine the length of the open reading frame. Or, as a simpler example, we may wish to find the GC content of a specific sequence. We check each base to see if it is G or C and keep a count. Doing these repeated operations in a program is called iteration.\nThe first method of iteration we will discuss is the for loop. As an example of its use, we compute the GC content of an RNA sequence.\n# The sequence we want to analyze\nseq = 'GACAGACUCCAUGCACGUGGGUAUCUGUC'\n\n# Initialize GC counter\nn_gc = 0\n\n# Initialize sequence length\nlen_seq = 0\n\n# Loop through sequence and count G's and C's\nfor base in seq:\n    len_seq += 1\n    if base in 'GCgc':\n        n_gc += 1\n        \n# Divide to get GC content\nn_gc / len_seq\n\n0.5517241379310345\nLet’s look carefully at what we did here. We took a string containing a sequence of nucleotides and then we did something for each character (base) in that string (nucleic acid sequence). A string is a sequence in the sense of the programming language as well; just like a list or tuple, the string is an ordered collection of characters. (So as not to confuse between biological sequences and sequences as a part of the Python language, we will always write the latter in italics.)\nNow, let’s translate the new syntax in the above code to English.\nPython: for base in seq:\nEnglish: for each character in the string whose variable name is seq, do the following, with that character taking the name base\nThis exposes a general way of doing things repeatedly in Python. For every item in a sequence, we do something. That something follows the for clause and is contained in an indentation block. When we do this, we say are “looping over a sequence.” In the context of a for clause, the membership operator, in, means that we consider in order each item in the sequence or iterator (we’ll talk about iterators in a moment).\nNow, looking within the loop, the first thing we do is increment the length of the sequence. For each base we encounter in the loop, we add one to the sequence length. Makes sense!\nNext, we have an if statement. We use the membership operator again. We ask if the current base is a G or a C. To be safe, we also included lower case characters in case the sequence was entered that way. If the base is a G or a C, we increment the counter of GC bases by one.\nFinally, we get the fractional GC content by dividing the number of GC’s by the total length of the sequence.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/iteration.html#introducing-the-for-loop",
    "href": "appendices/python_basics/iteration.html#introducing-the-for-loop",
    "title": "Appendix H — Iteration",
    "section": "",
    "text": "H.1.1 Aside: the len() function\nNote in the last example that we determined the length of the RNA sequence by iterating the len_seq counter within the for loop. This works, but Python has a built-in function (like print() is a built-in function) to compute the length of a string (or list, tuple, or other sequence). To find out the length of a sequence, simply use it as an argument to the len() function.\n\nlen(seq)\n\n29\n\n\n\n\nH.1.2 Example and watchout: modifying a list\nLet’s look at another example of iterating through a list. Say we have a list of integers, and we want to change it by doubling each one. Let’s just do it.\n\n# We'll do one through 5\nmy_integers = [1, 2, 3, 4, 5]\n\n# Double each one\nfor n in my_integers:\n    n *= 2\n    \n# Check out the result\nmy_integers\n\n[1, 2, 3, 4, 5]\n\n\nWhoa! It didn’t seem to double any of the integers! This is because my_integers was converted to an iterator in the for clause, and the iterator returns a copy of the item in a list, not a reference to it. Therefore, the n inside the for block is not a view into the original list, and doubling it does nothing meaningful.\nWe’ve seen how to change individual list elements with indexing:\n\n# Don't do things this way\nmy_integers[0] *= 2\nmy_integers[1] *= 2\nmy_integers[2] *= 2\nmy_integers[3] *= 2\nmy_integers[4] *= 2\n\nmy_integers\n\n[2, 4, 6, 8, 10]\n\n\nBut we’d obviously like a better way to do this, with less typing and without knowing ahead of time the length of the list. Let’s look at a new concept that will help with this example.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/iteration.html#iterators",
    "href": "appendices/python_basics/iteration.html#iterators",
    "title": "Appendix H — Iteration",
    "section": "H.2 Iterators",
    "text": "H.2 Iterators\nIn the previous example, we iterated over a sequence. A sequence is one of many iterable objects, called iterables. Under the hood, the Python interpreter actually converts an iterable to an iterator. An iterator is a special object that gives values in succession. A major difference between a sequence and an iterator is that you cannot index an iterator. This seems like a trivial difference, but iterators make for more efficient computing than directly using a sequence with indexing.\nWe can explicitly convert a sequence to an iterator using the built-in function iter(), but we will not bother with that here because the Python interpreter automatically does this for you when you use a sequence in a loop. (This is incidentally why the previous example didn’t work; when the list is converted to an iterator, a copy is made of each element so the original list is unchanged.)\nInstead, we will now explore how we can create useful iterators using the range(), enumerate(), and zip() functions. I know we have not yet covered functions, but the syntax should not be so complicated that you cannot understand what these functions are doing, just like with the print() and len() functions.\n\nH.2.1 The range() function\nThe range() function gives an iterable that enables counting. Let’s look at an example.\n\nfor i in range(10):\n    print(i, end='  ')\n\n0  1  2  3  4  5  6  7  8  9  \n\n\nWe see that range(10) gives us ten numbers, from 0 to 9. As with indexing, range() inclusively starts at zero by default, and the ending is exclusive.\nIt turns out that the arguments of the range() function work much like indexing. If you have a single argument, you get that many integers, starting at 0 and incrementing by one. If you give two arguments, you start inclusively at the first and increment by one ending exclusively at the second argument. Finally, you can specify a stride with the third argument.\n\n# Print numbers 2 through 9\nfor i in range(2, 10):\n    print(i, end='  ')\n\n# Print a newline\nprint()\n    \n# Print even numbers, 2 through 9\nfor i in range(2, 10, 2):\n    print(i, end='     ')\n\n2  3  4  5  6  7  8  9  \n2     4     6     8     \n\n\nIt is often useful to make a list or tuple that has the same entries that a corresponding range object would have. We can do this with type conversion.\n\nlist(range(10))\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nWe can use the range() function along with the len() function on lists to double the elements of our list from a bit ago:\n\nmy_integers = [1, 2, 3, 4, 5]\n\n# Since len(my_integers) = 5, this takes i from 0 to 4, \n# exactly the indices of my_integers\nfor i in range(len(my_integers)):\n    my_integers[i] *= 2\n    \nmy_integers\n\n[2, 4, 6, 8, 10]\n\n\nThis works, but there is an even better way to do this, with the next function.\n\n\nH.2.2 The enumerate() function\nLet’s say we want to print the indices of all G bases in a DNA sequence. We could do this by modifying our previous program.\n\n# Initialize GC counter\nn_gc = 0\n\n# Initialized sequence length\nlen_seq = 0\n\n# Loop through sequence and print index of G's\nfor base in seq:\n    if base in 'Gg':\n        print(len_seq, end='  ')\n    len_seq += 1\n\n0  4  12  16  18  19  20  26  \n\n\nThis is not so bad, but there is an easier way to do this. The enumerate() function gives an iterator that provides both the index and the item of a sequence. Again, this is best demonstrated in practice.\n\n# Loop through sequence and print index of G's\nfor i, base in enumerate(seq):\n    if base in 'Gg':\n        print(i, end='  ')\n\n0  4  12  16  18  19  20  26  \n\n\nThe enumerate() function allowed us to use an index and a base at the same time. To make it more clear, we can print the index and base type for each base in the sequence.\n\n# Print index and identity of bases\nfor i, base in enumerate(seq):\n    print(i, base)\n\n0 G\n1 A\n2 C\n3 A\n4 G\n5 A\n6 C\n7 U\n8 C\n9 C\n10 A\n11 U\n12 G\n13 C\n14 A\n15 C\n16 G\n17 U\n18 G\n19 G\n20 G\n21 U\n22 A\n23 U\n24 C\n25 U\n26 G\n27 U\n28 C\n\n\nThe enumerate() function is really useful and should be used in favor of just doing indexing. For example, many programmers, especially those first trained in lower-level languages, would write the above code similar to how we did the list doubling, with the range() and len() functions, but this is not good practice in Python.\nUsing enumerate(), the list doubling code looks like this:\n\nmy_integers = [1, 2, 3, 4, 5]\n\n# Double each one\nfor i, _ in enumerate(my_integers):\n    my_integers[i] *= 2\n    \n# Check out the result\nmy_integers\n\n[2, 4, 6, 8, 10]\n\n\nenumerate() is more generic and the overhead for returning a reference to an object isn’t an issue. The range(len()) construct will break on an object without support for len(). In addition, you are more likely to introduce bugs by imposing indexing on objects that are iterable but not unambiguously indexable. It is better to use the enumerate() function.\nNote that we used the underscore, _, as a throwaway variable that we do not use. There is no rule for this, but this is generally accepted Python syntax and helps signal that you are not going to use the variable.\nOne last gotcha: if we tried to do a similar technique with a string, we get a TypeError because a string is immutable. We’ll revisit examples like this in the lesson on string methods.\n\n# Try to convert capital G to lower g\nfor i, base in enumerate(seq):\n    if base == 'G':\n        seq[i] = 'g'\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[13], line 4\n      2 for i, base in enumerate(seq):\n      3     if base == 'G':\n----&gt; 4         seq[i] = 'g'\n\nTypeError: 'str' object does not support item assignment\n\n\n\n\n\nH.2.3 The zip() function\nThe zip() function enables us to iterate over several iterables at once. In the example below we iterate over the jersey numbers, names, and positions of the LAFC players who scored in the 2022 MLS Cup Final.\n\nnames = ('Acosta', 'Murillo', 'Bale')\npositions = ('MF', 'D', 'F')\nnumbers = (23, 3, 11)\n\nfor num, pos, name in zip(numbers, positions, names):\n    print(num, name, pos)\n\n23 Acosta MF\n3 Murillo D\n11 Bale F\n\n\n\n\nH.2.4 The reversed() function\nThis function is useful for giving an iterator that goes in the reverse direction. We’ll see that this can be convenient in the next lesson. For now, let’s pretend we’re NASA and need to count down.\n\ncount_up = ('ignition', 1, 2, 3, 4, 5, 6, 7, 8 ,9, 10)\n\nfor count in reversed(count_up):\n    print(count)\n\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\nignition",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/iteration.html#the-while-loop",
    "href": "appendices/python_basics/iteration.html#the-while-loop",
    "title": "Appendix H — Iteration",
    "section": "H.3 The while loop",
    "text": "H.3 The while loop\nThe for loop is very powerful and allows us to construct iterative calculations. When we use a for loop, we need to set up an iterator. A while loop, on the other hand, allows iteration until a conditional expression evaluates False.\nAs an example of a while loop, we will calculate the length of a sequence before hitting a start codon.\n\n# Define start codon\nstart_codon = 'AUG'\n\n# Initialize sequence index\ni = 0\n\n# Scan sequence until we hit the start codon\nwhile seq[i:i+3] != start_codon:\n    i += 1\n    \n# Show the result\nprint('The start codon starts at index', i)\n\nThe start codon starts at index 10\n\n\nLet’s walk through the while loop. The value of i is changing with each iteration, being incremented by one. Each time we consider doing another iteration, the conditional is checked: do the next three bases match the start codon? We set up the conditional to evaluate to True when the bases are not the start codon, so the iteration continues. In other words, iteration continues in a while loop until the conditional returns False.\nNotice that we sliced the string the same way we sliced lists and tuples. In the case of strings, a slice gives another string, i.e., a sequential collection of characters.\nLet’s try looking for another codon…. But, let’s actually not do that. If you run the code below, it will run forever and nothing will get printed to the screen.\n# Define codon of interest\ncodon = 'GCC'\n\n# Initialize sequence index\ni = 0\n\n# Scan sequence until we hit the start codon, but DON'T DO THIS!!!!!\nwhile seq[i:i+3] != codon:\n    i += 1\n    \n# Show the result\nprint('The codon starts at index', i)\nThe reason this runs forever is that the conditional expression in the while statement never returns False. If we slice a string beyond the length of the string we get an empty string result.\n\nseq[100:103]\n\n''\n\n\nThis does not equal the codon we’re interested in, so the while loop keeps going. Forever. This is called an infinite loop, and you definitely to not want these in your code! We can fix it by making a conditional that will evaluate to False if we reach the end of the string.\n\n# Define codon of interest\ncodon = 'GCC'\n\n# Initialize sequence index\ni = 0\n\n# Scan sequence until we hit the start codon or the end of the sequence\nwhile seq[i:i+3] != codon and i &lt; len(seq):\n    i += 1\n    \n# Show the result\nif i == len(seq):\n    print('Codon not found in sequence.')\nelse:\n    print('The codon starts at index', i)\n\nCodon not found in sequence.\n\n\n\nH.3.1 for vs while\nMost anything that requires a loop can be done with either a for loop or a while loop, but there’s a general rule of thumb for which type of loop to use. If you know how many times you have to do something (or if your program knows), use a for loop. If you don’t know how many times the loop needs to run until you run it, use a while loop. For example, when we want to do something with each character in a string or each entry in a list, the program knows how long the sequence is and a for loop is more appropriate. In the previous examples, we don’t know how long it will be before we hit the start codon; it depends on the sequence you put into the program. That makes it more suited to a while loop.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/iteration.html#the-break-and-else-keywords",
    "href": "appendices/python_basics/iteration.html#the-break-and-else-keywords",
    "title": "Appendix H — Iteration",
    "section": "H.4 The break and else keywords",
    "text": "H.4 The break and else keywords\nIteration stops in a for loop when the iterator is exhausted. It stops in a while loop when the conditional evaluates to False. These is another way to stop iteration: the break keyword. Whenever break is encountered in a for or while loop, the iteration halts and execution continues outside the loop. As an example, we’ll do the calculation above with a for loop with a break instead of a while loop.\n\n# Define start codon\nstart_codon = 'AUG'\n\n# Scan sequence until we hit the start codon\nfor i in range(len(seq)):\n    if seq[i:i+3] == start_codon:\n        print('The start codon starts at index', i)\n        break\nelse:\n    print('Codon not found in sequence.')\n\nThe start codon starts at index 10\n\n\nNotice that we have an else block after the for loop. In Python, for and while loops can have an else statement after the code block to be evaluated in the loop. The contents of the else block are evaluated if the loop completes without encountering a break.\n\nH.4.1 A note about use of the word “codon”\nThe astute biologists among you will note that we have not really been using the word “codon” properly here. We are taking it to mean any three consecutive bases, but the more precise definition of a codon means that it is three consecutive bases that code for an amino acid. That means that for a three-base sequence to be a codon, it must be in-register with the start codon.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/iteration.html#computing-environment",
    "href": "appendices/python_basics/iteration.html#computing-environment",
    "title": "Appendix H — Iteration",
    "section": "H.5 Computing environment",
    "text": "H.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html",
    "href": "appendices/python_basics/intro_to_functions.html",
    "title": "Appendix I — Introduction to functions",
    "section": "",
    "text": "I.1 Basic function syntax\n| Download notebook\nA function is a key element in writing programs. You can think of a function in a computing language in much the same way you think of a mathematical function. The function takes in arguments, performs some operation based on the identities of the arguments, and then returns a result. For example, the mathematical function\n\\[\\begin{align}\nf(x, y) = \\frac{x}{y}\n\\end{align}\\]\ntakes arguments \\(x\\) and \\(y\\) and then returns the ratio between the two, \\(x/y\\). In this lesson, we will learn how to construct functions in Python.\nFor our first example, we will translate the above function into Python. A function is defined using the def keyword. This is best seen by example.\ndef ratio(x, y):\n    \"\"\"The ratio of `x` to `y`.\"\"\"\n    return x / y\nFollowing the def keyword is a function signature which indicates the function’s name and its arguments. Just like in mathematics, the arguments are separated by commas and enclosed in parentheses. The indentation following the def line specifies what is part of the function. As soon as the indentation goes to the left again, aligned with def, the contents of the functions are complete.\nImmediately following the function definition is the doc string (short for documentation string), a brief description of the function. The first string after the function definition is always defined as the doc string. Usually, it is in triple quotes, as doc strings often span multiple lines.\nDoc strings are more than just comments for your code, the doc string is what is returned by the native python function help() when someone is looking to learn more about your function. For example:\nhelp(ratio)\n\nHelp on function ratio in module __main__:\n\nratio(x, y)\n    The ratio of `x` to `y`.\nThey are also printed out when you use the ? in a Jupyter notebook or JupyterLab console.\nratio?\n\n\nSignature: ratio(x, y)\nDocstring: The ratio of `x` to `y`.\nFile:      /var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/ipykernel_7590/1007532851.py\nType:      function\nYou are free to type whatever you like in doc strings, or even omit them, but you should always have a doc string with some information about what your function is doing. True, this example of a function is kind of silly, since it is easier to type x / y than ratio(x, y), but it is still good form to have a doc string. This is worth saying explicitly.\nIn the next line of the function, we see a return keyword. Whatever is after the return statement is, you guessed it, returned by the function. Any code after the return is not executed because the function has already returned!",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#basic-function-syntax",
    "href": "appendices/python_basics/intro_to_functions.html#basic-function-syntax",
    "title": "Appendix I — Introduction to functions",
    "section": "",
    "text": "All functions should have doc strings.\n\n\n\nI.1.1 Calling a function\nNow that we have defined our function, we can call it.\n\nratio(5, 4)\n\n1.25\n\n\n\nratio(4, 2)\n\n2.0\n\n\n\nratio(90.0, 8.4)\n\n10.714285714285714\n\n\nIn each case, the function returns a float with the ratio of its arguments.\n\n\nI.1.2 Functions need not have arguments\nA function does not need arguments. As a silly example, let’s consider a function that just returns 42 every time. Of course, it does not matter what its arguments are, so we can define a function without arguments.\n\ndef answer_to_the_ultimate_question_of_life_the_universe_and_everything():\n    \"\"\"Simpler program than Deep Thought's, I bet.\"\"\"\n    return 42\n\nWe still needed the open and closed parentheses at the end of the function name. Similarly, even though it has no arguments, we still have to call it with parentheses.\n\nanswer_to_the_ultimate_question_of_life_the_universe_and_everything()\n\n42\n\n\n\n\nI.1.3 Functions need not return anything\nJust like they do not necessarily need arguments, functions also do not need to return anything. If a function does not have a return statement (or it is never encountered in the execution of the function), the function runs to completion and returns None by default. None is a special Python keyword which basically means “nothing.” For example, a function could simply print something to the screen.\n\ndef think_too_much():\n    \"\"\"Express Caesar's skepticism about Cassius\"\"\"\n    print(\"\"\"Yond Cassius has a lean and hungry look,\nHe thinks too much; such men are dangerous.\"\"\")\n\nWe call this function as all others, but we can show that the result it returns is None.\n\nreturn_val = think_too_much()\n\n# Print a blank line\nprint()\n\n# Print the return value\nprint(return_val)\n\nYond Cassius has a lean and hungry look,\nHe thinks too much; such men are dangerous.\n\nNone",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#built-in-functions-in-python",
    "href": "appendices/python_basics/intro_to_functions.html#built-in-functions-in-python",
    "title": "Appendix I — Introduction to functions",
    "section": "I.2 Built-in functions in Python",
    "text": "I.2 Built-in functions in Python\nThe Python programming language has several built-in functions. We have already encountered print(), id(), ord(), len(), range(), enumerate(), zip(), and reversed(), in addition to type conversions such as list(). The complete set of built-in functions can be found here. A word of warning about these functions and naming your own.\n\nNever define a function or variable with the same name as a built-in function.\n\nAdditionally, Python has keywords (such as def, for, in, if, True, None, etc.), many of which we have already encountered. A complete list of them is here. The interpreter will throw an error if you try to define a function or variable with the same name as a keyword.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#an-example-function-reverse-complement",
    "href": "appendices/python_basics/intro_to_functions.html#an-example-function-reverse-complement",
    "title": "Appendix I — Introduction to functions",
    "section": "I.3 An example function: reverse complement",
    "text": "I.3 An example function: reverse complement\nLet’s write a function that does not do something so trivial as computing ratios or giving us the Answer to the Ultimate Question of Life, the Universe, and Everything. We’ll write a function to compute the reverse complement of a sequence of DNA. Within the function, we’ll use some of our newly acquired iteration skills.\n\ndef complement_base(base):\n    \"\"\"Returns the Watson-Crick complement of a base.\"\"\"\n    if base in 'Aa':\n        return 'T'\n    elif base in 'Tt':\n        return 'A'\n    elif base in 'Gg':\n        return 'C'\n    else:\n        return 'G'\n\n\ndef reverse_complement(seq):\n    \"\"\"Compute reverse complement of a sequence.\"\"\"\n    # Initialize reverse complement\n    rev_seq = ''\n    \n    # Loop through and populate list with reverse complement\n    for base in reversed(seq):\n        rev_seq += complement_base(base)\n        \n    return rev_seq\n\nNote that we do not have error checking here, which we should definitely do, but we’ll cover that in a future lesson. For now, let’s test it to see if it works.\n\nreverse_complement('GCAGTTGCA')\n\n'TGCAACTGC'\n\n\nIt looks good, but we might want to write yet another function to display the template strand (from 5\\('\\) to 3\\('\\)) above its reverse complement (from 3\\('\\) to 5\\('\\)). This makes it easier to verify.\n\ndef display_complements(seq):\n    \"\"\"Print sequence above its reverse complement.\"\"\"\n    # Compute the reverse complement\n    rev_comp = reverse_complement(seq)\n    \n    # Print template\n    print(seq)\n    \n    # Print \"base pairs\"\n    for base in seq:\n        print('|', end='')\n    \n    # Print final newline character after base pairs\n    print()\n            \n    # Print reverse complement\n    for base in reversed(rev_comp):\n        print(base, end='')\n        \n    # Print final newline character\n    print()\n\nLet’s call this function and display the input sequence and the reverse complement returned by the function.\n\nseq = 'GCAGTTGCA'\ndisplay_complements(seq)\n\nGCAGTTGCA\n|||||||||\nCGTCAACGT\n\n\nOk, now it’s clear that the result looks good! This example demonstrates an important programming principle regarding functions. We used three functions to compute and display the reverse complement.\n\ncomplement_base() gives the Watson-Crick complement of a given base.\nreverse_complement() computes the reverse complement.\ndisplay_complements() displays the sequence and the reverse complement.\n\nWe could very well have written a single function to compute the reverse complement with the if statements included within the for loop. Instead, we split this larger operation up into smaller functions. This is an example of modular programming, in which the desired functionality is split up into small, independent, interchangeable modules. This is a very, very important concept.\n\nWrite small functions that do single, simple tasks.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#pause-and-think-about-testing",
    "href": "appendices/python_basics/intro_to_functions.html#pause-and-think-about-testing",
    "title": "Appendix I — Introduction to functions",
    "section": "I.4 Pause and think about testing",
    "text": "I.4 Pause and think about testing\nLet’s pause for a moment and think about what the complement_base() and reverse_complement() functions do. They do a well-defined operation on string inputs. If we’re doing some bioinformatics, we might use these functions over and over again. We should therefore thoroughly test the functions. For example, we should test that reverse_complement('GCAGTTGCA') returns 'TGCAACTGC'. For now, we will proceed without writing tests, but we will soon cover test-driven development, in which your functions are built around tests. For now, I will tell you this: If your functions are not thoroughly tested, you are entering a world of pain. A world of pain. Test your functions.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#keyword-arguments",
    "href": "appendices/python_basics/intro_to_functions.html#keyword-arguments",
    "title": "Appendix I — Introduction to functions",
    "section": "I.5 Keyword arguments",
    "text": "I.5 Keyword arguments\nNow let’s say that instead of the reverse DNA complement, we want the reverse RNA complement. We could re-write the complement_base() function to do this. Better yet, let’s modify it.\n\ndef complement_base(base, material='DNA'):\n    \"\"\"Returns the Watson-Crick complement of a base.\"\"\"\n    if base in 'Aa':\n        if material == 'DNA':\n            return 'T'\n        elif material == 'RNA':\n            return 'U'\n    elif base in 'TtUu':\n        return 'A'\n    elif base in 'Gg':\n        return 'C'\n    else:\n        return 'G'\n    \ndef reverse_complement(seq, material='DNA'):\n    \"\"\"Compute reverse complement of a sequence.\"\"\"\n    # Initialize reverse complement\n    rev_seq = ''\n    \n    # Loop through and populate list with reverse complement\n    for base in reversed(seq):\n        rev_seq += complement_base(base, material=material)\n        \n    return rev_seq\n\nWe have added a named keyword argument, also known as a named kwarg. The syntax for a named kwarg is\nkwarg_name=default_value\nin the def clause of the function definition. In this case, we say that the default material is DNA, but we could call the function with another material (RNA). Conveniently, when you call the function and omit the kwargs, they take on the default value within the function. So, if we wanted to use the default material of DNA, we don’t have to do anything different in the function call.\n\nreverse_complement('GCAGTTGCA')\n\n'TGCAACTGC'\n\n\nBut, if we want RNA, we can use the kwarg. We use the same syntax to call it that we did when defining it.\n\nreverse_complement('GCAGTTGCA', material='RNA')\n\n'UGCAACUGC'",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#calling-a-function-with-a-splat",
    "href": "appendices/python_basics/intro_to_functions.html#calling-a-function-with-a-splat",
    "title": "Appendix I — Introduction to functions",
    "section": "I.6 Calling a function with a splat",
    "text": "I.6 Calling a function with a splat\nPython offers another convenient way to call functions. Say a function takes three arguments, a, b, and c, taken to be the sides of a triangle, and determines whether or not the triangle is a right triangle. I.e., it checks to see if \\(a^2 + b^2 = c^2\\).\n\ndef is_almost_right(a, b, c):\n    \"\"\"\n    Checks to see if a triangle with side lengths\n    `a`, `b`, and `c` is right.\n    \"\"\"\n    # Use sorted(), which gives a sorted list\n    a, b, c = sorted([a, b, c])\n    \n    # Check to see if it is almost a right triangle\n    if abs(a**2 + b**2 - c**2) &lt; 1e-12:\n        return True\n    else:\n        return False\n\nRemember our warning from before: never use equality checks with floats. We therefore just check to see if the Pythagorean theorem almost holds. The function works as expected.\n\nis_almost_right(13, 5, 12)\n\nTrue\n\n\n\nis_almost_right(1, 1, 1.4)\n\nFalse\n\n\nNow, let’s say we had a tuple with the triangle side lengths in it.\n\nside_lengths = (13, 5, 12)\n\nWe can pass these all in separately by splitting the tuple but putting a * in front of it. A * before a tuple used in this way is referred an unpacking operator, and is referred to by some programmers as a “splat.”\n\nis_almost_right(*side_lengths)\n\nTrue\n\n\nThis can be very convenient, and we will definitely use this feature later in the bootcamp when we do some string formatting.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#anonymous-a.k.a.-lambda-functions",
    "href": "appendices/python_basics/intro_to_functions.html#anonymous-a.k.a.-lambda-functions",
    "title": "Appendix I — Introduction to functions",
    "section": "I.7 Anonymous (a.k.a. lambda) functions",
    "text": "I.7 Anonymous (a.k.a. lambda) functions\nSo far, we have written functions using a def statement, followed by an indented block of code defining what will be executed when the function is called. Sometimes the functions are very short, like the ratio() function at the beginning of this lesson. We might wish to more succinctly define a function like this. Python’s lambda keyword enables this. As an example, let’s look at how we could define the ratio() function from before.\n\nf = lambda x, y: x / y\n\nf(3, 5)\n\n0.6\n\n\nThe syntax for defining an anonymous function, which must be on one line, is as follows.\n\nThe keyword lambda.\nThe arguments of the anonymous function, separated by commas.\nAn expression that is the return value of the function.\n\nYou may be thinking that anonymous functions run contrary to the idea that all functions should have doc strings. You’re right. Anonymous functions are typically only used when another function requires a function as an argument. In the is_almost_right() function above, we employed the sorted() function is used to sort a list. The sorted() function takes a key keyword argument that gives a function that is applied to each entry in the list and then the values returned by that function are used in the sorting. As an example, we can sort a list of names of goalscorers for LAFC in the epic 2022 MLS Cup final.\n\nsorted(['Kellyn Acosta', 'Jesus Murillo', 'Gareth Bale'])\n\n['Gareth Bale', 'Jesus Murillo', 'Kellyn Acosta']\n\n\nThis sorted by their first names, but we want to sort for their last names. As we will learn in the lesson on string methods, we can find the index of a space in a string using my_string.find(' '), so that the letter at the start of a last name for a player with string x is x[x.find(' ')+1]. We can use a lambda function to give this as the key and get a nicely sorted list.\n\nsorted(['Kellyn Acosta', 'Jesus Murillo', 'Gareth Bale'], key=lambda x: x[x.find(' ')+1])\n\n['Kellyn Acosta', 'Gareth Bale', 'Jesus Murillo']",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_functions.html#computing-environment",
    "href": "appendices/python_basics/intro_to_functions.html#computing-environment",
    "title": "Appendix I — Introduction to functions",
    "section": "I.8 Computing environment",
    "text": "I.8 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Introduction to functions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/string_methods.html",
    "href": "appendices/python_basics/string_methods.html",
    "title": "Appendix J — String methods",
    "section": "",
    "text": "J.1 Indexing and slicing of strings\n| Download notebook\nIn the last lesson, we wrote some functions to parse strings and compute things like reverse complements. This helped us practice using functions and the iteration skills we learned.\nYou might think, “Hey, replacing characters in strings sounds like it may be pretty common.” You would be right. You might also think, “I bet someone, possibly someone who is a really good programmer, already has written code to do this.” You would again be right.\nFor common tasks, there are often already methods written by someone smart, and working with strings is no different. In this lesson, we will explore some of the string processing tools that come with Python’s standard library.\nBefore getting into string methods, we pause to note that indexing and slicing of strings works just as it does for lists and tuples.\nmy_str = 'The Dude abides.'\n\nprint(my_str[5])\nprint(my_str[:6])\nprint(my_str[::2])\nprint(my_str[::-1])\n\nu\nThe Du\nTeDd bds\n.sediba eduD ehT",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/string_methods.html#revisiting-previous-examples-using-string-methods",
    "href": "appendices/python_basics/string_methods.html#revisiting-previous-examples-using-string-methods",
    "title": "Appendix J — String methods",
    "section": "J.2 Revisiting previous examples using string methods",
    "text": "J.2 Revisiting previous examples using string methods\nWe’ll start by revisiting some of the examples we’ve seen so far.\n\nJ.2.1 Computing GC content\nIf you remember from the iteration lesson, we started by computing the GC content of a nucleic acid sequence. We counted the occurrences of 'G' and 'C' in the string using a for loop. We can use the count() string method do to this.\n\n# Define sequence\nseq = 'GACAGACUCCAUGCACGUGGGUAUCAUGUC'\n\n# Count G's and C's\nseq.count('G') + seq.count('C')\n\n16\n\n\nThe seq.count() method enabled us to count the number times G and C occurred in the string seq. This notation is new. We have a variable, followed by a dot (.), and then a function. These functions are called methods in the language of object-oriented programming (OOP). If you have a string my_str, and you want to execute one of Python’s built-in string methods on it, the syntax is\nmy_str.string_method_of_choice(*args)\nIn general, the count method gives the number of times a substring appears in a string. We can learn more about its behavior by playing with it.\n\n# Substrings of more than one characater\nseq.count('GAC')\n\n2\n\n\n\n# Substrings cannot overlap\n'AAAAAAA'.count('AA')\n\n3\n\n\n\n# Something that's not there.\nseq.count('nonsense')\n\n0\n\n\n\n\nJ.2.2 Finding the index of a start codon\nAnother task in the iteration lesson was to find the index of the start codon in an RNA sequence. Let’s do it with another string method.\n\nseq.find('AUG')\n\n10\n\n\nWow, that was easy. The find() method gives the index where the substring argument first appears. But, what if a substring is not in the string?\n\nseq.find('nonsense')\n\n-1\n\n\nIn this case, find() returns -1. This is not to be interpreted as index -1! find() always returns positive indices if it finds a substring. Note that you should not use find() to test if a substring is present. Use the in operator we already learned about.\n\n'AUG' in seq\n\nTrue\n\n\n\n\nJ.2.3 Finding the last index of a substring\nLet’s say we wanted to find the last instance of the start codon. We basically want to search from the right. This is exactly what the rfind() method does.\n\nseq.rfind('AUG')\n\n25\n\n\n\n\nJ.2.4 Finding the complementary base\nIn our lesson on functions, we wrote a function to compute a complementary base comparing against both the capital and lowercase letter. Here is that function implemented with some handy string methods.\n\ndef complement_base(base):\n    \"\"\"Returns the Watson-Crick complement of a base.\"\"\"\n    # Convert to lowercase\n    base = base.lower()\n    \n    if base == 'a':\n        return 'T'\n    elif base == 't':\n        return 'A'\n    elif base == 'g':\n        return 'C'\n    else:\n        return 'G'\n\nWe were able to avoid all the “base in 'Tt'”-style operations by just converting the base to lowercase using the lower() method. In general, the lower() method takes a string and converts any capital letters to lower case. The upper() function works analogously.\n\n'LeBron James'.lower()\n\n'lebron james'\n\n\n\n'Make me aLl caPS.'.upper()\n\n'MAKE ME ALL CAPS.'\n\n\n\n\nJ.2.5 Converting RNA to DNA\nWe also updated the complementary base function to account for RNA or DNA. Perhaps an easier way is just to replace all Us in an RNA sequence with Ts to get a DNA sequence. The replace() method makes this easy.\n\nseq.replace('U', 'T')\n\n'GACAGACTCCATGCACGTGGGTATCATGTC'\n\n\nNote that seq did not change. Remember, strings are immutable, so the replace() method returns a new string, as does lower(), upper(), and any other string method that returns a string. So, the characters stored in the variable seq are unchanged.\n\nseq\n\n'GACAGACUCCAUGCACGUGGGUAUCAUGUC'",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/string_methods.html#the-join-method",
    "href": "appendices/python_basics/string_methods.html#the-join-method",
    "title": "Appendix J — String methods",
    "section": "J.3 The join() method",
    "text": "J.3 The join() method\nOne of the most useful string methods is the join() method. Say we have a list of words that we want to craft into a sentence.\n\nword_tuple = ('The', 'Dude', 'abides.')\n\nNow, we would like to concatenate them into a single string. (This is sort of like the opposite of taking a string and making a list of its characters by doing a list() type conversion.) We need to know what we want to put between each word. In this case, we want a space. Here’s the nifty syntax to do that.\n\n' '.join(word_tuple)\n\n'The Dude abides.'\n\n\nWe now have a single string with the elements of the tuple, separated by spaces. The string before the dot (.) specifies what goes between the strings in the list or tuple (or other iterable). If we wanted “*” between each word, we could do that, too.\n\n' * '.join(word_tuple)\n\n'The * Dude * abides.'",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/string_methods.html#the-format-method",
    "href": "appendices/python_basics/string_methods.html#the-format-method",
    "title": "Appendix J — String methods",
    "section": "J.4 The format() method",
    "text": "J.4 The format() method\nThe format() method is very powerful. We not go over all use cases here, but I’ll show you what I think is most intuitive and commonly used. Again, this is best learned by example.\n\nmy_str = \"\"\"\nLet's do a Mad Lib!\nDuring this bootcamp, I feel {adjective}.\nThe instructors give us {plural_noun}.\n\"\"\".format(adjective='truculent', plural_noun='haircuts')\n\nprint(my_str)\n\n\nLet's do a Mad Lib!\nDuring this bootcamp, I feel truculent.\nThe instructors give us haircuts.\n\n\n\nSee the pattern? Given a string, the format() method takes kwargs that are themselves strings. Within the string, the name of the kwargs are given in braces. Then, the arguments in the format() method inserts the strings at the places delimited by braces.\nNow, what if we want to insert a number into a string? We could convert it to a string, but we should instead use string conversions. These are short directives that specify how the number should be represented in a string. A complete list is here. The table below shows some that are commonly used.\n\n\n\nconversion\ndescription\n\n\n\n\nd\ninteger\n\n\n04d\ninteger with four digits, possibly with leading zeros\n\n\nf\nfloat, default to six digits after decimal\n\n\n.8f\nfloat with 8 digits after the decimal\n\n\ne\nscientific notation, default to six digits after decimal\n\n\n.16e\nscientific notation with 16 digits after the decimal\n\n\ns\ndisplay as a string\n\n\n\nBelow are examples of all of these.\n\nprint('There are {n:d} states in the US.'.format(n=50))\nprint('Your file number is {n:d}.'.format(n=23))\nprint('π is approximately {pi:f}.'.format(pi=3.14))\nprint('e is approximately {e:.8f}.'.format(e=2.7182818284590451))\nprint(\"Avogadro's number is approximately {N_A:e}.\".format(N_A=6.022e23))\nprint('ε₀ is approximately {eps_0:.16e} F/m.'.format(eps_0=8.854187817e-12))\nprint('That {thing:s} really tied the room together.'.format(thing='rug'))\n\nThere are 50 states in the US.\nYour file number is 23.\nπ is approximately 3.140000.\ne is approximately 2.71828183.\nAvogadro's number is approximately 6.022000e+23.\nε₀ is approximately 8.8541878170000005e-12 F/m.\nThat rug really tied the room together.\n\n\nNote the syntax. In the braces, we specify the name of the kwarg, and then we put a colon followed by the string conversion. Note also that I used double quotes on the outside of the string containing Avogadro’s number so that I could include an apostrophe in the string. Finally, note that we got a subscript zero using the Unicode character, ₀.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/string_methods.html#f-strings",
    "href": "appendices/python_basics/string_methods.html#f-strings",
    "title": "Appendix J — String methods",
    "section": "J.5 f-strings",
    "text": "J.5 f-strings\nf-strings are strings that are prefixed with an f or F that allow convenient insertion of entries into strings. Here are some examples.\n\nn_states = 50\nfile_number = 23\npi = 3.14\ne = 2.7182818284590451\nN_A = 6.022e23\neps_0=8.854187817e-12\nthing = 'rug'\n\nprint(f'There are {n_states} states in the US.')\nprint(f'Your file number is {file_number}.')\nprint(f'π is approximately {pi}.')\nprint(f'e is approximately {e:.8f}.')\nprint(f\"Avogadro's number is approximately {N_A}.\")\nprint(f'ε₀ is approximately {eps_0} F/m.')\nprint(f'That {thing} really tied the room together.')\n\nThere are 50 states in the US.\nYour file number is 23.\nπ is approximately 3.14.\ne is approximately 2.71828183.\nAvogadro's number is approximately 6.022e+23.\nε₀ is approximately 8.854187817e-12 F/m.\nThat rug really tied the room together.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/string_methods.html#there-are-many-more-string-methods",
    "href": "appendices/python_basics/string_methods.html#there-are-many-more-string-methods",
    "title": "Appendix J — String methods",
    "section": "J.6 There are many more string methods",
    "text": "J.6 There are many more string methods\nYou can find a complete list of string methods from the Python doc pages. Various methods will come in handy when parsing strings going forward.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/string_methods.html#computing-environment",
    "href": "appendices/python_basics/string_methods.html#computing-environment",
    "title": "Appendix J — String methods",
    "section": "J.7 Computing environment",
    "text": "J.7 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>String methods</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html",
    "href": "appendices/python_basics/dictionaries.html",
    "title": "Appendix K — Dictionaries",
    "section": "",
    "text": "K.1 Mapping objects and dictionaries\n| Download notebook\nA mapping object allows an arbitrary collection of objects to be indexed by an arbitrary collection of values. That’s a mouthful. It is easier to understand instead by comparing to a sequence.\nLet’s take a sequence of two strings, say a tuple containing a first and last name.\nWe are restricted on how we reference the sequence. I.e., the first name is name[0], and the last name is name[1]. A mapping object could instead be indexed like name['first name'] and name['last name']. You can imagine this would be very useful! A classic example in biology might be looking up amino acids that are coded for by given codons. E.g., you might want\nto give you 'Leucine'.\nPython’s build-in mapping type is a dictionary. You might imagine that the Oxford English Dictionary might conveniently be stored as a dictionary (obviously). I.e., you would not want to store definitions that have to be referenced like\nRather, you would like to get definitions like this:\nImportantly, note that in Python 3.5 and older dictionaries have no sense of order. In Python 3.6, dictionaries were stored in insertion order as an implementation improvement. In Python 3.7 and beyond, dictionaries are guaranteed to be ordered in the order in which their entries were created. It is therefore advisable be cautious when relying on ordering in dictionaries. For safety’s sake, you may be better off assuming there is no sense of order.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#mapping-objects-and-dictionaries",
    "href": "appendices/python_basics/dictionaries.html#mapping-objects-and-dictionaries",
    "title": "Appendix K — Dictionaries",
    "section": "",
    "text": "name = ('jeffrey', 'lebowski')\n\naa['CTT']\n\n\noed[103829]\n\noed['computer']",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#dictionary-syntax",
    "href": "appendices/python_basics/dictionaries.html#dictionary-syntax",
    "title": "Appendix K — Dictionaries",
    "section": "K.2 Dictionary syntax",
    "text": "K.2 Dictionary syntax\nThe syntax for creating a dictionary, as usual, is best seen through example.\n\nmy_dict = {'a': 6, 'b': 7, 'c': 27.6}\nmy_dict\n\n{'a': 6, 'b': 7, 'c': 27.6}\n\n\nA dictionary is created using curly braces ({}). Each entry has a key, followed by a colon, and then the value associated with the key. In the example above, the keys are all strings, which is the most common use case. Note that the items can be of any type; in the above example, they are ints and a float.\nWe could also create the dictionary using the built-in dict() function, which can take a tuple of 2-tuples, each one containing a key-value pair.\n\ndict((('a', 6), ('b', 7), ('c', 27.6)))\n\n{'a': 6, 'b': 7, 'c': 27.6}\n\n\nFinally, we can make a dictionary with keyword arguments to the dict() function.\n\ndict(a='yes', b='no', c='maybe')\n\n{'a': 'yes', 'b': 'no', 'c': 'maybe'}\n\n\nWe do not need to have strings as the keys. In fact, any immutable object can be a key.\n\nmy_dict = {\n    0: 'zero',\n    1.7: [1, 2, 3],\n    (5, 6, 'dummy string'): 3.14,\n    'strings are immutable': 42\n}\n\nmy_dict\n\n{0: 'zero',\n 1.7: [1, 2, 3],\n (5, 6, 'dummy string'): 3.14,\n 'strings are immutable': 42}\n\n\nHowever, mutable objects cannot be used as keys.\n\nmy_dict = {\n    \"immutable is ok\": 1, \n    [\"mutable\", \"not\", \"ok\"]: 5\n}\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 my_dict = {\n      2     \"immutable is ok\": 1, \n      3     [\"mutable\", \"not\", \"ok\"]: 5\n      4 }\n\nTypeError: unhashable type: 'list'",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#indexing-dictionaries",
    "href": "appendices/python_basics/dictionaries.html#indexing-dictionaries",
    "title": "Appendix K — Dictionaries",
    "section": "K.3 Indexing dictionaries",
    "text": "K.3 Indexing dictionaries\nAs mentioned at the beginning of the lesson, we index dictionaries by key.\n\n# Make a dictionary\nmy_dict = dict(a='yes', b='no', c='maybe')\n\n# Pull out an entry\nmy_dict['b']\n\n'no'\n\n\nBecause the indexing of dictionaries is by key and not by sequential integers, they cannot be sliced; they must be accessed element-by-element. (Actually there are ways to slice keys of dictionaries using itertools, but we will not cover that.)",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#useful-dictionaries-in-bioinformatics",
    "href": "appendices/python_basics/dictionaries.html#useful-dictionaries-in-bioinformatics",
    "title": "Appendix K — Dictionaries",
    "section": "K.4 Useful dictionaries in bioinformatics",
    "text": "K.4 Useful dictionaries in bioinformatics\nIt might be useful to quickly look up 3-letter amino acid codes. Dictionaries are particularly useful for this.\n\naa_dict = {\n    \"A\": \"Ala\",\n    \"R\": \"Arg\",\n    \"N\": \"Asn\",\n    \"D\": \"Asp\",\n    \"C\": \"Cys\",\n    \"Q\": \"Gln\",\n    \"E\": \"Glu\",\n    \"G\": \"Gly\",\n    \"H\": \"His\",\n    \"I\": \"Ile\",\n    \"L\": \"Leu\",\n    \"K\": \"Lys\",\n    \"M\": \"Met\",\n    \"F\": \"Phe\",\n    \"P\": \"Pro\",\n    \"S\": \"Ser\",\n    \"T\": \"Thr\",\n    \"W\": \"Trp\",\n    \"Y\": \"Tyr\",\n    \"V\": \"Val\",\n}\n\nAnother useful dictionary would contain the set of codons and the amino acids they code for. This is built in the code below using the zip() function we learned before. To see the logic on how this is constructed, see the codon table here.\n\n# The set of DNA bases\nbases = ['T', 'C', 'A', 'G']\n\n# Build list of codons\ncodon_list = []\nfor first_base in bases:\n    for second_base in bases:\n        for third_base in bases:\n            codon_list += [first_base + second_base + third_base]\n\n# The amino acids that are coded for (* = STOP codon)\namino_acids = 'FFLLSSSSYY**CC*WLLLLPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG'\n\n# Build dictionary from tuple of 2-tuples (technically an iterator, but it works)\ncodons = dict(zip(codon_list, amino_acids))\n\n# Show that we did it\nprint(codons)\n\n{'TTT': 'F', 'TTC': 'F', 'TTA': 'L', 'TTG': 'L', 'TCT': 'S', 'TCC': 'S', 'TCA': 'S', 'TCG': 'S', 'TAT': 'Y', 'TAC': 'Y', 'TAA': '*', 'TAG': '*', 'TGT': 'C', 'TGC': 'C', 'TGA': '*', 'TGG': 'W', 'CTT': 'L', 'CTC': 'L', 'CTA': 'L', 'CTG': 'L', 'CCT': 'P', 'CCC': 'P', 'CCA': 'P', 'CCG': 'P', 'CAT': 'H', 'CAC': 'H', 'CAA': 'Q', 'CAG': 'Q', 'CGT': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R', 'ATT': 'I', 'ATC': 'I', 'ATA': 'I', 'ATG': 'M', 'ACT': 'T', 'ACC': 'T', 'ACA': 'T', 'ACG': 'T', 'AAT': 'N', 'AAC': 'N', 'AAA': 'K', 'AAG': 'K', 'AGT': 'S', 'AGC': 'S', 'AGA': 'R', 'AGG': 'R', 'GTT': 'V', 'GTC': 'V', 'GTA': 'V', 'GTG': 'V', 'GCT': 'A', 'GCC': 'A', 'GCA': 'A', 'GCG': 'A', 'GAT': 'D', 'GAC': 'D', 'GAA': 'E', 'GAG': 'E', 'GGT': 'G', 'GGC': 'G', 'GGA': 'G', 'GGG': 'G'}\n\n\nThese two dictionaries are particularly useful, so I put them in a little module which we will discuss in the lesson on packages and modules.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#a-dictionary-is-an-implementation-of-a-hash-table",
    "href": "appendices/python_basics/dictionaries.html#a-dictionary-is-an-implementation-of-a-hash-table",
    "title": "Appendix K — Dictionaries",
    "section": "K.5 A dictionary is an implementation of a hash table",
    "text": "K.5 A dictionary is an implementation of a hash table\nIt is useful to stop and think about how a dictionary works. Let’s create a dictionary and look at where the values are stored in memory.\n\n# Create dictionary\nmy_dict = {'a': 6, 'b': 7, 'c':12.6}\n\n# Find where they are stored\nprint(id(my_dict))\nprint(id(my_dict['a']))\nprint(id(my_dict['b']))\nprint(id(my_dict['c']))\n\n4570286848\n4343739664\n4343739696\n4600140880\n\n\nSo, each entry in the dictionary is stored at a different location in memory. The dictionary itself also has its own address. So, when I index a dictionary with a key, how does the dictionary know which address in memory to use to fetch the value I am interested in?\nDictionaries use a hash function to do this. A hash function converts its input to an integer. For example, we can use Python’s built-in hash function to convert the keys to integers.\n\nhash('a'), hash('b'), hash('c')\n\n(199680081378453410, -6901413122848462881, -5789435826028719999)\n\n\nUnder the hood, Python then converts these integers to integers that could correspond to locations in memory. A collection of elements that can be indexed this way is called a hash table. This is a very common data structure in computing. Wikipedia has a pretty good discussion on them.\nGiven what you know about how dictionaries work, why do you think mutable objects are not acceptable as keys?",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#dictionaries-are-mutable",
    "href": "appendices/python_basics/dictionaries.html#dictionaries-are-mutable",
    "title": "Appendix K — Dictionaries",
    "section": "K.6 Dictionaries are mutable",
    "text": "K.6 Dictionaries are mutable\nDictionaries are mutable. This means that they can be changed in place. For example, if we want to add an element to a dictionary, we use simple syntax.\n\n# Remind ourselves what the dictionary is\nprint(my_dict)\n\n# Add an entry\nmy_dict['d'] = 'Bootcamp is so much fun!'\n\n# Look at dictionary again\nprint(my_dict)\n\n# Change an entry\nmy_dict['a'] = 'I was not satisfied with entry a.'\n\n# Look at it again\nprint(my_dict)\n\n{'a': 6, 'b': 7, 'c': 12.6}\n{'a': 6, 'b': 7, 'c': 12.6, 'd': 'Bootcamp is so much fun!'}\n{'a': 'I was not satisfied with entry a.', 'b': 7, 'c': 12.6, 'd': 'Bootcamp is so much fun!'}",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#membership-operators-with-dictionaries",
    "href": "appendices/python_basics/dictionaries.html#membership-operators-with-dictionaries",
    "title": "Appendix K — Dictionaries",
    "section": "K.7 Membership operators with dictionaries",
    "text": "K.7 Membership operators with dictionaries\nThe in and not in operators work with dictionaries, but both only query keys and not values. We see this again by example.\n\n# Make a fresh my_dict\nmy_dict = {'a': 1, 'b': 2, 'c': 3}\n\n# in works with keys\n'b' in my_dict, 'd' in my_dict, 'e' not in my_dict\n\n(True, False, True)\n\n\n\n# Try it with values\n2 in my_dict\n\nFalse\n\n\nYup! We get False. Why? Because 2 is not a key in my_dict. We can also iterate over the keys in a dictionary.\n\nfor key in my_dict:\n    print(key, ':', my_dict[key])\n\na : 1\nb : 2\nc : 3\n\n\nThe best, and preferred, method, is to iterate over key,value pairs in a dictionary using the items() method of a dictionary.\n\nfor key, value in my_dict.items():\n    print(key, ':', value)\n\na : 1\nb : 2\nc : 3\n\n\nNote, however, that like lists, the items that come out of the my_dict.items() iterator are not items in the dictionary, but copies of them. If you make changes within the for loop, you will not change entries in the dictionary.\n\nfor key, value in my_dict.items():\n    value = 'this string will not be in dictionary.'\n    \nmy_dict\n\n{'a': 1, 'b': 2, 'c': 3}\n\n\nYou will, though, if you use the keys.\n\nfor key, _ in my_dict.items():\n    my_dict[key] = 'this will be in the dictionary.'\n    \nmy_dict\n\n{'a': 'this will be in the dictionary.',\n 'b': 'this will be in the dictionary.',\n 'c': 'this will be in the dictionary.'}",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#built-in-functions-for-dictionaries",
    "href": "appendices/python_basics/dictionaries.html#built-in-functions-for-dictionaries",
    "title": "Appendix K — Dictionaries",
    "section": "K.8 Built-in functions for dictionaries",
    "text": "K.8 Built-in functions for dictionaries\nThe built-in len() function and del operation work on dictionaries.\n\nlen(d) gives the number of entries in dictionary d\ndel d[k] deletes entry with key k from dictionary d\n\nThis is the first time we’ve encountered the del keyword. This keyword is used to delete variables and their values from memory. The del keyword can also be to delete items from lists. Let’s see things in practice.\n\n# Create my_list and my_dict for reference\nmy_dict = dict(a=1, b=2, c=3, d=4)\nmy_list = [1, 2, 3, 4]\n\n# Print them\nprint('my_dict:', my_dict)\nprint('my_list:', my_list)\n\n# Get lengths\nprint('length of my_dict:', len(my_dict))\nprint('length of my_list:', len(my_list))\n\n# Delete a key from my_dict\ndel my_dict['b']\n\n# Delete entry from my_list\ndel my_list[1]\n\n# Show post-deleted objects\nprint('post-deleted my_dict:', my_dict)\nprint('post-deleted my_list:', my_list)\n\nmy_dict: {'a': 1, 'b': 2, 'c': 3, 'd': 4}\nmy_list: [1, 2, 3, 4]\nlength of my_dict: 4\nlength of my_list: 4\npost-deleted my_dict: {'a': 1, 'c': 3, 'd': 4}\npost-deleted my_list: [1, 3, 4]\n\n\nNote, though, that you cannot delete an item from a tuple, since it’s immutable.\n\nmy_tuple = (1, 2, 3, 4)\ndel my_tuple[1]\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[19], line 2\n      1 my_tuple = (1, 2, 3, 4)\n----&gt; 2 del my_tuple[1]\n\nTypeError: 'tuple' object doesn't support item deletion",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#dictionary-methods",
    "href": "appendices/python_basics/dictionaries.html#dictionary-methods",
    "title": "Appendix K — Dictionaries",
    "section": "K.9 Dictionary methods",
    "text": "K.9 Dictionary methods\nDictionaries have several built-in methods in addition to the items() you have already seen. Following are a few of them, assuming the dictionary is d.\n\n\n\n\n\n\n\nmethod\neffect\n\n\n\n\nd.keys()\nreturn keys\n\n\nd.pop(key)\nreturn value associated with key and delete key from d\n\n\nd.values()\nreturn the values in d\n\n\nd.get(key, None)\nFetch a value in d by key giving a default value (the second argument) if key is missing\n\n\n\nLet’s try these out.\n\nmy_dict = dict(a=1, b=2, c=3, d=4)\n\nmy_dict.keys()\n\ndict_keys(['a', 'b', 'c', 'd'])\n\n\nNote that this is a dict_keys object. We cannot index it. If, say, we wanted to sort the keys and have them index-able, we would have to convert them to a list.\n\nsorted(list(my_dict.keys()))\n\n['a', 'b', 'c', 'd']\n\n\nThis is not a usual use case, though, and be warned that doing then when this is not explicitly what you want can lead to bugs. Now let’s try popping an entry out of the dictionary.\n\nmy_dict.pop('c')\n\n3\n\n\n\nmy_dict\n\n{'a': 1, 'b': 2, 'd': 4}\n\n\n…and, as we expect, key 'c' is now deleted, and its value was returned in the call to my_dict.pop('c'). Now, let’s look at the values.\n\nmy_dict.values()\n\ndict_values([1, 2, 4])\n\n\nWe get a dict_values object, similar to the dict_keys object we got with the my_dict.keys() method. Finally, let’s consider get().\n\nmy_dict.get('d')\n\n4\n\n\nThis is the same as my_dict['d'], except that if the key 'd' is not there, it will return a default value. Let’s try using my_dict.get() with the deleted entry 'c'.\n\nmy_dict.get('c')\n\nNote that there was no error (there would be if we did my_dict['c']), and we got None. We could specify a default value.\n\nmy_dict.get('c', 3)\n\n3\n\n\nYou should think about what behavior you want when you attempt to get a value out of a dictionary by key. Do you want an error when the key is missing? Then use indexing. Do you want to have a (possibly None) default if the key is missing and no error? Then use my_dict.get().\nYou can get more information about build-in methods from the Python documentation.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#list-methods",
    "href": "appendices/python_basics/dictionaries.html#list-methods",
    "title": "Appendix K — Dictionaries",
    "section": "K.10 List methods",
    "text": "K.10 List methods\nAs you may guess, the dictionary method pop() has an analog that works for lists. (Why don’t the dictionary key() and values() methods work for lists?) We take this opportunity to introduce a few more useful list methods. Imagine the list is called s.\n\n\n\nmethod\neffect\n\n\n\n\ns.pop(i)\nreturn value at index i and delete it from the list\n\n\ns.append(x)\nPut x at the end of the list\n\n\ns.insert(i, x)\nInsert x at index i in the list\n\n\ns.remove(x)\nRemove the first occurrence of x from the list\n\n\ns.reverse()\nReverse the order of items in the list",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#using-dictionaries-as-kwargs",
    "href": "appendices/python_basics/dictionaries.html#using-dictionaries-as-kwargs",
    "title": "Appendix K — Dictionaries",
    "section": "K.11 Using dictionaries as kwargs",
    "text": "K.11 Using dictionaries as kwargs\nA nifty feature of dictionaries is that they can be passed into functions as keyword arguments. We covered named keyword arguments in the lesson on functions. In addition to the named keyword arguments, a function can take in arbitrary keyword arguments (not arbitrary non-keyword arguments). This is specified in the function definition by including a last argument with a double-asterisk, **. The kwargs with the double-asterisk get passed in as a dictionary.\n\ndef concatenate_sequences(a, b, **kwargs):\n    \"\"\"Concatenate (combine) 2 or more sequences.\"\"\"\n    seq = a + b\n\n    for key in kwargs:\n        seq += kwargs[key]\n        \n    return seq\n\nLet’s try it!\n\nconcatenate_sequences('TGACAC', 'CAGGGA', c='GGGGGGGGG', d='AAAATTTTT')\n\n'TGACACCAGGGAGGGGGGGGGAAAATTTTT'\n\n\nNow, imagine we have a dictionary that contains our values.\n\nmy_dict = {\"a\": \"TGACAC\", \"b\": \"CAGGGA\", \"c\": \"GGGGGGGGG\", \"d\": \"AAAATTTTT\"}\n\nWe can now pass this directly into the function by preceding it with a double asterisk.\n\nconcatenate_sequences(**my_dict)\n\n'TGACACCAGGGAGGGGGGGGGAAAATTTTT'\n\n\nBeautiful! This example is kind of trivial, but you can imagine that it can come in handy, e.g. with large sets of sequence fragments that you read in from a file. We will use **kwargs later in the bootcamp.\nQuestion: What is the risk in using a dictionary in this way to concatenate sequences?",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#merging-dictionaries",
    "href": "appendices/python_basics/dictionaries.html#merging-dictionaries",
    "title": "Appendix K — Dictionaries",
    "section": "K.12 Merging dictionaries",
    "text": "K.12 Merging dictionaries\nSaw I have two dictionaries that have no like keys and I want to merge them together. This might be like considering two volumes of an encyclopedia; they do not have and like keys, and we might like to consider them as a single volume. How can we accomplish this?\nThe dict() function, combined with the ** operator in function calls allows for this. We simple call dict() with ** before each dictionary argument.\n\nrestriction_dict = {\"KpnI\": \"GGTACC\", \"HindII\": \"AAGCTT\", \"ecoRI\": \"GAATTC\"}\n\ndict(**my_dict, **restriction_dict, another_seq=\"AGTGTAGTG\")\n\n{'a': 'TGACAC',\n 'b': 'CAGGGA',\n 'c': 'GGGGGGGGG',\n 'd': 'AAAATTTTT',\n 'KpnI': 'GGTACC',\n 'HindII': 'AAGCTT',\n 'ecoRI': 'GAATTC',\n 'another_seq': 'AGTGTAGTG'}",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/dictionaries.html#computing-environment",
    "href": "appendices/python_basics/dictionaries.html#computing-environment",
    "title": "Appendix K — Dictionaries",
    "section": "K.13 Computing environment",
    "text": "K.13 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/comprehensions.html",
    "href": "appendices/python_basics/comprehensions.html",
    "title": "Appendix L — Comprehensions",
    "section": "",
    "text": "L.1 List comprehensions\n| Download notebook\nWe have learned how to build lists, tuples, arrays, etc. by constructing them directly. E.g., list(range(10)) gives us a list of all integers between 0 and 9 inclusive. But what if we want to build a list or array by iterating the contains something a bit more complicated. For example, let’s say we want to get a list of all prime numbers less than 1000. This could be a bit cumbersome, even with sympy’s lovely isprime() function.\nBecause we do not know a priori how many entries there are going to be, we have to keep appending to a list. Under the hood, this means that the Python interpreter has to keep allocating memory as it creates and grows lists. So, in addition to being syntactically clunky, the above way of creating a list is inefficient. It would be nice to have a more convenient way of doing this.\nEnter list comprehensions.\nAs is often the case, this is best seen by example. We will create the same Numpy array of primes using a list comprehension.\nprimes = [x for x in range(n_max) if sympy.isprime(x)]\n\n# Take a look\nprint(primes)\n\n[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541, 547, 557, 563, 569, 571, 577, 587, 593, 599, 601, 607, 613, 617, 619, 631, 641, 643, 647, 653, 659, 661, 673, 677, 683, 691, 701, 709, 719, 727, 733, 739, 743, 751, 757, 761, 769, 773, 787, 797, 809, 811, 821, 823, 827, 829, 839, 853, 857, 859, 863, 877, 881, 883, 887, 907, 911, 919, 929, 937, 941, 947, 953, 967, 971, 977, 983, 991, 997]\nIn one line, we have made our list of primes! The list comprehension is enclosed in brackets. The first part, x, is an expression that will be inserted into the list. Next comes a for statement to produce the iterator. Finally, there is a conditional; if the conditional evaluates True, then the expression expression is included in the list.\nIf a condition is absent, all entries are put in the list. For example, if we didn’t want to just do list(range(100)) to get integers, we could use a list comprehension without a conditional.\n# Give same result as list(range(100))\nmy_list_of_ints = [i for i in range(100)]\n\nprint(my_list_of_ints)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Comprehensions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/comprehensions.html#list-comprehensions",
    "href": "appendices/python_basics/comprehensions.html#list-comprehensions",
    "title": "Appendix L — Comprehensions",
    "section": "",
    "text": "L.1.1 Another example list comprehension\nLet’s say we wanted to build a list containing the information about the 2018 Nobel laureates. We have, in three separate arrays, their names, nationalities, and category for the prize.\n\nnames = (\n    \"Frances Arnold\",\n    \"George Smith\",\n    \"Gregory Winter\",\n    \"postponed\",\n    \"Denis Mukwege\",\n    \"Nadia Murad\",\n    \"Arthur Ashkin\",\n    \"Gérard Mourou\",\n    \"Donna Strickland\",\n    \"James Allison\",\n    \"Tasuku Honjo\",\n    \"William Nordhaus\",\n    \"Paul Romer\",\n)\n\nnationalities = (\n    \"USA\",\n    \"USA\",\n    \"UK\",\n    \"---\",\n    \"DRC\",\n    \"Iraq\",\n    \"USA\",\n    \"France\",\n    \"Canada\",\n    \"USA\",\n    \"Japan\",\n    \"USA\",\n    \"USA\",\n)\n\ncategories = (\n    \"Chemistry\",\n    \"Chemistry\",\n    \"Chemistry\",\n    \"Literature\",\n    \"Peace\",\n    \"Peace\",\n    \"Physics\",\n    \"Physics\",\n    \"Physics\",\n    \"Physiology or Medicine\",\n    \"Physiology or Medicine\",\n    \"Economics\",\n    \"Economics\",\n)\n\nWith these tuples in hand, we can use a list comprehension to build a nice list of tuples containing the information about the laureates.\n\n[(cat, name, nat) for name, nat, cat in zip(names, nationalities, categories)]\n\n[('Chemistry', 'Frances Arnold', 'USA'),\n ('Chemistry', 'George Smith', 'USA'),\n ('Chemistry', 'Gregory Winter', 'UK'),\n ('Literature', 'postponed', '---'),\n ('Peace', 'Denis Mukwege', 'DRC'),\n ('Peace', 'Nadia Murad', 'Iraq'),\n ('Physics', 'Arthur Ashkin', 'USA'),\n ('Physics', 'Gérard Mourou', 'France'),\n ('Physics', 'Donna Strickland', 'Canada'),\n ('Physiology or Medicine', 'James Allison', 'USA'),\n ('Physiology or Medicine', 'Tasuku Honjo', 'Japan'),\n ('Economics', 'William Nordhaus', 'USA'),\n ('Economics', 'Paul Romer', 'USA')]\n\n\nNotice that I do not have to use range(); I can use any iterator, including one that puts out multiple values using zip().\nNow, let’s say we are really interested in the prize in chemistry. We can add an if statement to the comprehension like we did in the prime number example.\n\n[\n    (cat, name, nat)\n    for name, nat, cat in zip(names, nationalities, categories)\n    if cat == \"Chemistry\"\n]\n\n[('Chemistry', 'Frances Arnold', 'USA'),\n ('Chemistry', 'George Smith', 'USA'),\n ('Chemistry', 'Gregory Winter', 'UK')]\n\n\n(Note here that we split the list comprehension over many lines for readability, which is perfectly legal.) We can also nest iterators. For example, let’s say the the chemistry and medicine prize winners got together in Sweden and wanted to play against each other in basketball. There are three chemistry winners, but only two medicine winners. So, to play 2-on-2, we would have to choose only two chemistry laureates. So, let’s make a list of all possible pairs of chemistry winners.\n\n# First get list of chemistry laureates\nchem_names = [name for name, cat in zip(names, categories) if cat == \"Chemistry\"]\n\n# List of all possible pairs of chemistry laureates\n[\n    (n1, n2)\n    for i, n1 in enumerate(chem_names)\n    for j, n2 in enumerate(chem_names)\n    if i &lt; j\n]\n\n[('Frances Arnold', 'George Smith'),\n ('Frances Arnold', 'Gregory Winter'),\n ('George Smith', 'Gregory Winter')]\n\n\nTo summarize this structure of list comprehensions, borrowing from Dave Beazley’s explanation in Python Essential Reference, a list comprehension has the following structure.\n[expression_to_put_in_list for i_1 in iterable_1 if condition_1\n                           for i_2 in iterable_2 if condition_2\n                                     ...\n                           for i_n in iterable_n if condition_n]\nwhich is roughly equivalent to\nmy_list = []\nfor i_1 in iterable_1:\n    if condition_1:\n        for i_2 in iterable_2:\n            if condition_2:\n                ...\n                for i_n in iterable_n:\n                    if condition_n:\n                        my_list += [expression_to_put_in_list]\n\n\nL.1.2 What if you want an else statement in a list comprehension?\nNow, let’s say that we deem “Physiology or Medicine” to be too long of a title for the category of the prize. We instead want to substitute that phrase with “Medicine” for brevity. We might construct the list like this:\n\n[\n    (\"Medicine\", name, nat)\n    for name, nat, cat in zip(names, nationalities, categories)\n    if cat == \"Physiology or Medicine\"\n]\n\n[('Medicine', 'James Allison', 'USA'), ('Medicine', 'Tasuku Honjo', 'Japan')]\n\n\nThis leaves out all of the other prizes. So, we need an else statement. To include all prizes, we might try it like this.\n\n[\n    (\"Medicine\", name, nat)\n    for name, nat, cat in zip(names, nationalities, categories)\n    if cat == \"Physiology or Medicine\" else (cat, name, nat)\n]\n\n\n  Cell In[10], line 4\n    if cat == \"Physiology or Medicine\" else (cat, name, nat)\n                                       ^\nSyntaxError: invalid syntax\n\n\n\n\nSyntax error! This structure of a list comprehension does not match the template shown above. In the conditional expression of list comprehensions, you cannot have an else block.\nHowever, the expression_to_put_in_list can be any valid Python expression. The following is a valid Python expression:\n(\"Medicine\", name, nat) if cat == \"Physiology or Medicine\" else (cat, name, nat)\nSo, we can still use a list comprehension to build the list.\n\n[\n    (\"Medicine\", name, nat) if cat == \"Physiology or Medicine\" else (cat, name, nat)\n    for name, nat, cat in zip(names, nationalities, categories)\n]\n\n[('Chemistry', 'Frances Arnold', 'USA'),\n ('Chemistry', 'George Smith', 'USA'),\n ('Chemistry', 'Gregory Winter', 'UK'),\n ('Literature', 'postponed', '---'),\n ('Peace', 'Denis Mukwege', 'DRC'),\n ('Peace', 'Nadia Murad', 'Iraq'),\n ('Physics', 'Arthur Ashkin', 'USA'),\n ('Physics', 'Gérard Mourou', 'France'),\n ('Physics', 'Donna Strickland', 'Canada'),\n ('Medicine', 'James Allison', 'USA'),\n ('Medicine', 'Tasuku Honjo', 'Japan'),\n ('Economics', 'William Nordhaus', 'USA'),\n ('Economics', 'Paul Romer', 'USA')]\n\n\nTo be clear here, there is no conditional in the list comprehension; the conditional is in the expression to be added to the list, which we have called expression_to_put_in_list.\nList comprehensions will prove very useful, and most Pythonistas use them extensively.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Comprehensions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/comprehensions.html#dictionary-comprehensions",
    "href": "appendices/python_basics/comprehensions.html#dictionary-comprehensions",
    "title": "Appendix L — Comprehensions",
    "section": "L.2 Dictionary comprehensions",
    "text": "L.2 Dictionary comprehensions\nIn addition to list comprehensions, Python also allows for dictionary comprehensions (and set comprehensions, but we will not discuss sets in the bootcamp). To demonstrate a dictionary comprehension, let’s use the name of the laureate as a key and the values in the dictionary are their nationality and category.\n\n{name: (cat, nat) for name, nat, cat in zip(names, nationalities, categories)}\n\n{'Frances Arnold': ('Chemistry', 'USA'),\n 'George Smith': ('Chemistry', 'USA'),\n 'Gregory Winter': ('Chemistry', 'UK'),\n 'postponed': ('Literature', '---'),\n 'Denis Mukwege': ('Peace', 'DRC'),\n 'Nadia Murad': ('Peace', 'Iraq'),\n 'Arthur Ashkin': ('Physics', 'USA'),\n 'Gérard Mourou': ('Physics', 'France'),\n 'Donna Strickland': ('Physics', 'Canada'),\n 'James Allison': ('Physiology or Medicine', 'USA'),\n 'Tasuku Honjo': ('Physiology or Medicine', 'Japan'),\n 'William Nordhaus': ('Economics', 'USA'),\n 'Paul Romer': ('Economics', 'USA')}\n\n\nAaaand we have our dictionary! This is quite a powerful way to construct this, and you may find dictionary comprehensions quite useful. I use them in specifying **kwargs and in creating dictionaries I want to convert to data frames.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Comprehensions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/comprehensions.html#paul-romer-and-jupyter-and-open-source-software",
    "href": "appendices/python_basics/comprehensions.html#paul-romer-and-jupyter-and-open-source-software",
    "title": "Appendix L — Comprehensions",
    "section": "L.3 Paul Romer and Jupyter and open source software",
    "text": "L.3 Paul Romer and Jupyter and open source software\nCoincidentally, one of the laureates featured in this lesson, Paul Romer, is a big fan of Jupyter notebooks. I love this quote from this blog post of his:\n\nIn the larger contest between open and proprietary models, Mathematica versus Jupyter would be a draw if the only concern were their technical accomplishments. In the 1990s, Mathematica opened up an undeniable lead. Now, Jupyter is the unambiguous technical leader.\nThe tie-breaker is social, not technical. The more I learn about the open source community, the more I trust its members. The more I learn about proprietary software, the more I worry that objective truth might perish from the earth.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Comprehensions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/comprehensions.html#computing-environment",
    "href": "appendices/python_basics/comprehensions.html#computing-environment",
    "title": "Appendix L — Comprehensions",
    "section": "L.4 Computing environment",
    "text": "L.4 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\nnumpy     : 1.26.4\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Comprehensions</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html",
    "href": "appendices/python_basics/packages_and_modules.html",
    "title": "Appendix M — Packages and modules",
    "section": "",
    "text": "M.1 Example: I want to compute the mean and median of a list of numbers\n| Download notebook\nThe Python Standard Library has lots of built-in modules that contain useful functions and data types for doing specific tasks. You can also use modules from outside the standard library. And you will undoubtedly write your own modules!\nA module is contained in a file that ends with .py. This file can have classes, functions, and other objects. We will not discuss defining your own classes until much later in the bootcamp, so your modules will essentially just contain functions for now.\nA package contains several related modules that are all grouped together under one name. We will extensively use the NumPy, SciPy, Pandas, and Bokeh packages, among others, in the bootcamp, and I’m sure you will also use them beyond. As such, the first module we will consider is NumPy. We will talk a lot more about NumPy later in the bootcamp.\nSay I have a list of numbers and I want to compute the mean. This happens all the time; you repeat a measurement multiple times and you want to compute the mean. We could write a function to do this.\ndef mean(values):\n    \"\"\"Compute the mean of a sequence of numbers.\"\"\"\n    return sum(values) / len(values)\nAnd it works as expected.\nprint(mean([1, 2, 3, 4, 5]))\nprint(mean((4.5, 1.2, -1.6, 9.0)))\n\n3.0\n3.275\nIn addition to the mean, we might also want to compute the median, the standard deviation, etc. These seem like really common tasks. Remember my advice: if you want to do something that seems really common, a good programmer (or a team of them) probably already wrote something to do that. Means, medians, standard deviations, and lots and lots and lots of other numerical things are included in the Numpy module. To get access to it, we have to import it.\nimport numpy\nThat’s it! We now have the numpy module available for use. Remember, in Python everything is an object, so if we want to access the methods and attributes available in the numpy module, we use dot syntax. In a Jupyter notebook or in the JupyterLab console, you can type\n(note the dot) and hit tab, and we will see what is available. For Numpy, there is a huge number of options!\nSo, let’s try to use Numpy’s numpy.mean() function to compute a mean.\nprint(numpy.mean([1, 2, 3, 4, 5]))\nprint(numpy.mean((4.5, 1.2, -1.6, 9.0)))\n\n3.0\n3.275\nGreat! We get the same values! Now, we can use the numpy.median() function to compute the median.\nprint(numpy.median([1, 2, 3, 4, 5]))\nprint(numpy.median((4.5, 1.2, -1.6, 9.0)))\n\n3.0\n2.85\nThis is nice. It gives the median, including when we have an even number of elements in the sequence of numbers, in which case it automatically interpolates. It is really important to know that it does this interpolation, since if you are not expecting it, it can give unexpected results. So, here is an important piece of advice:\nWe can access the doc string of the numpy.median() function in JupyterLab by typing\nand looking at the output. An important part of that output:\nThis is where the documentation tells you that the median will be reported as the average of two middle values when the number of elements is even. Note that you could also read the documentation here, which is a bit easier to read.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#example-i-want-to-compute-the-mean-and-median-of-a-list-of-numbers",
    "href": "appendices/python_basics/packages_and_modules.html#example-i-want-to-compute-the-mean-and-median-of-a-list-of-numbers",
    "title": "Appendix M — Packages and modules",
    "section": "",
    "text": "numpy.\n\n\n\n\n\n\n\nAlways check the doc strings of functions.\n\n\nnumpy.median?\n\nNotes\n-----\nGiven a vector ``V`` of length ``N``, the median of ``V`` is the\nmiddle value of a sorted copy of ``V``, ``V_sorted`` - i\ne., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the\ntwo middle values of ``V_sorted`` when ``N`` is even.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#the-as-keyword",
    "href": "appendices/python_basics/packages_and_modules.html#the-as-keyword",
    "title": "Appendix M — Packages and modules",
    "section": "M.2 The as keyword",
    "text": "M.2 The as keyword\nWe use Numpy all the time. Typing numpy over and over again can get annoying. So, it is common practice to use the as keyword to import a module with an alias. Numpy’s alias is traditionally np, and this is the only alias you should ever use for Numpy.\n\nimport numpy as np\n\nnp.median((4.5, 1.2, -1.6, 9.0))\n\nnp.float64(2.85)\n\n\nI prefer to do things this way, though some purists differ. We will use traditional aliases for major packages like Numpy and Pandas throughout the bootcamp.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#third-party-packages",
    "href": "appendices/python_basics/packages_and_modules.html#third-party-packages",
    "title": "Appendix M — Packages and modules",
    "section": "M.3 Third party packages",
    "text": "M.3 Third party packages\nStandard Python installations come with the standard library. Numpy and other useful packages are not in the standard library. Outside of the standard library, there are several packages available. Several. Ha! There are currently (September, 2025) about 680,000 packages available through the Python Package Index, PyPI. Usually, you can ask Google about what you are trying to do, and there is often a third party module to help you do it.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#writing-your-own-module",
    "href": "appendices/python_basics/packages_and_modules.html#writing-your-own-module",
    "title": "Appendix M — Packages and modules",
    "section": "M.4 Writing your own module",
    "text": "M.4 Writing your own module\nTo write your own module, you need to create a .py file and save it. You can do this using the text editor in JupyterLab. Let’s call our module na_utils, for “nucleic acid utilities.” So, we create a file called na_utils.py. We’ll build this module to have two functions, based on things we’ve already written. We’ll have a function dna_to_rna(), which converts a DNA sequence to an RNA sequence (just changes T to U), and another function reverse_rna_complement(), which returns the reverse RNA complement of a DNA template. The contents of na_utils.py should look as follows.\n\"\"\"\nUtilities for parsing nucleic acid sequences.\n\"\"\"\n\ndef dna_to_rna(seq):\n    \"\"\"\n    Convert a DNA sequence to RNA.\n    \"\"\"\n    # Determine if original sequence was uppercase\n    seq_upper = seq.isupper()\n\n    # Convert to lowercase\n    seq = seq.lower()\n\n    # Swap out 't' for 'u'\n    seq = seq.replace('t', 'u')\n\n    # Return upper or lower case RNA sequence\n    if seq_upper:\n        return seq.upper()\n    else:\n        return seq\n\n\ndef reverse_rna_complement(seq):\n    \"\"\"\n    Convert a DNA sequence into its reverse complement as RNA.\n    \"\"\"\n    # Determine if original was uppercase\n    seq_upper = seq.isupper()\n\n    # Reverse sequence\n    seq = seq[::-1]\n\n    # Convert to upper\n    seq = seq.upper()\n\n    # Compute complement\n    seq = seq.replace('A', 'u')\n    seq = seq.replace('T', 'a')\n    seq = seq.replace('G', 'c')\n    seq = seq.replace('C', 'g')\n\n    # Return result\n    if seq_upper:\n        return seq.upper()\n    else:\n        return seq\nNote that the file starts with a doc string saying what the module contains.\nI then have my two functions, each with doc strings. We will now import the module and then use these functions. In order for the import to work, the file na_utils.py must be in your present working directory, since this is where the Python interpreter will look for your module. In general, if you execute the code\nimport my_module\nthe Python interpreter will look first in the pwd to find my_module.py.\n\nimport na_utils\n\n# Sequence\nseq = 'GACGATCTAGGCGACCGACTGGCATCG'\n\n# Convert to RNA\nna_utils.dna_to_rna(seq)\n\n'GACGAUCUAGGCGACCGACUGGCAUCG'\n\n\nWe can also compute the reverse RNA complement.\n\nna_utils.reverse_rna_complement(seq)\n\n'CGAUGCCAGUCGGUCGCCUAGAUCGUC'\n\n\nWonderful! You now have your own functioning module!\n\nM.4.1 A quick note on error checking\nThese functions have minimal error checking of the input. For example, the dna_to_rna() function will take gibberish in and give jibberish out.\n\nna_utils.dna_to_rna('You can observe a lot by just watching.')\n\n'you can observe a lou by jusu wauching.'\n\n\nIn general, checking input and handling errors is an essential part of writing functions, and we will cover that in a later lesson.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#importing-modules-in-your-.py-files-and-notebooks",
    "href": "appendices/python_basics/packages_and_modules.html#importing-modules-in-your-.py-files-and-notebooks",
    "title": "Appendix M — Packages and modules",
    "section": "M.5 Importing modules in your .py files and notebooks",
    "text": "M.5 Importing modules in your .py files and notebooks\nThe Python style guide (PEP 8) says:\n\nImports are always put at the top of the file, just after any module comments and docstrings, and before module globals and constants.\nImports should be grouped in the following order:\n\nstandard library imports\nrelated third party imports\nlocal application/library specific imports\n\nYou should put a blank line between each group of imports.\n\nYou should follow this guide. I generally do it for Jupyter notebooks as well, with my first code cell having all of the imports I need. Therefore, going forward all of our lessons will have all necessary imports at the top of the document. The only exception is when we are explicitly demonstrating a concept that requires an import.\n\nM.5.1 Imports and updates\nOnce you have imported a module or package, the interpreter stores its contents in memory. You cannot update the contents of the package and expect the interpreter to know about the changes. You will need to restart the kernel and then import the package again in a fresh instance.\nThis can seem annoying, but it is good design. It ensures that code you are running does not change as you go through executing a notebook. However, when developing modules, it is sometimes convenient to have an imported module be updated as you run through the notebook as you are editing. To enable this, you can use the autoreload extension. To activate it, run the following in a code cell.\n%load_ext autoreload\n%autoreload 2\nWhenever you run a cell, imported packages and modules will be automatically reloaded.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#shareable-reusable-packages",
    "href": "appendices/python_basics/packages_and_modules.html#shareable-reusable-packages",
    "title": "Appendix M — Packages and modules",
    "section": "M.6 Shareable, reusable packages",
    "text": "M.6 Shareable, reusable packages\nWhen we wrote the na_utils module, we stored it in the directory that we were working in, or the pwd. But what if you write a module that you want to use regardless of what directory your are in? To allow this kind of usage, you can use the setuptools module of the standard library to manage your packages. You should read the documentation on Python packages and modules to understand the details of how this is done, but what we present here is sufficient to get simple packages running and installed.\n\nM.6.1 Package architecture\nIn order for the tools in setuptools to effectively install your modules for widespread use, you need to follow a specific architecture for your package. I made an example jb_bootcamp package for a computing bootcamp that I teach.\nThe file structure is of the package is\n/jb_bootcamp\n  /jb_bootcamp\n    __init__.py\n    na_utils.py\n    bioinfo_dicts.py\n    ...\nsetup.py\nREADME.md\nThe ellipsis above signifies that there are other files in there that we are not going to use yet. I am trying to keep it simple for now to show how package management works.\nIt is essential that the name of the root directory be the name of the package, and that there be a subdirectory with the same name. That subdirectory must contain a file __init__.py. This file contains information about the package and how the modules of the package are imported, but it may be empty for simple modules. In this case, I included a string with the name and version of the package, as well as instructions to import appropriate modules. Here are the contents of __init__.py. The first two lines of code tell the interpreter what to import when running import jb_bootcamp.\n\"\"\"Top-level package for utilities for bootcamp.\"\"\"\n\nfrom .na_utils import *\nfrom .bioinfo_dicts import *\n\n__author__ = 'Justin Bois'\n__email__ = 'bois@caltech.edu'\n__version__ = '0.0.1'\nAlso within the subdirectory are the .py files containing the code of the package. In our case, we have, na_utils.py and bioinfo_dicts.py.\nIt is also good practice to have a README file (which I suggest you write in Markdown) that has information about the package and what it does. Since this little demo package is kind of trivial, the README is quite short. Here are the contents I made for README.md (shown in unrendered raw Markdown).\n# jb_bootcamp\n\nUtilities for use in the Introduction to Programming in the Biological Sciences Bootcamp.\nFinally, in the main directory, we need to have a file called setup.py, which contains the instructions for setuptools to install the package. We use the setuptools.setup() function to do the installation.\nimport setuptools\n\nwith open(\"README.md\", \"r\") as f:\n    long_description = f.read()\n\nsetuptools.setup(\n    name='jb_bootcamp',\n    version='0.0.1',\n    author='Justin Bois',\n    author_email='bois@caltech.edu',\n    description='Utilities for use in bootcamp.',\n    long_description=long_description,\n    long_description_content_type='ext/markdown',\n    packages=setuptools.find_packages(),\n    classifiers=(\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: OS Independent\",\n    ),\n)\nThis is a minimal setup.py function, but will be sufficient for most packages you write for your own use. For your use, you make obvious changes to the name, author, etc., fields.\n\n\nM.6.2 Installing your package\nYou can install your package locally using\npip install -e path_to_my_package\nThe -e flag means it is editable, such that pip installs the package locally and is aware of edits you make to the files of the package. Once your package is mature, you should deposit it on the Python Package Index, in which case you can directly install it using\npip install my_package_name\n…Though in both cases, you should probably use a package manager to do so.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#package-management",
    "href": "appendices/python_basics/packages_and_modules.html#package-management",
    "title": "Appendix M — Packages and modules",
    "section": "M.7 Package management",
    "text": "M.7 Package management\nYour workflows may require many packages. These packages depend on each other in various ways. For example, Pandas requires NumPy, python-dateutil, and pytz, plus loads of optional dependencies. To make matters complicated, different versions of various packages depend on specific versions of their dependencies, which can form a tangled web of requirements. How can we handle this mess?\nPackage management systems solve this problem. The package management system we are using is Pixi. When you want to install a new package, you can do so with a command like\npixi add the-package-i-want\nand Pixi will make sure that all of the version numbers line up, updating or downgrading already installed packages to accommodate the new one. Unless you set it up otherwise, Pixi uses packages in the conda-forge channel. Pixi also plays nicely with pip, which can also be used to install packages. To do this, you need to include the --pypi flag.\npixi add --pypi the-pypi-package-i-want\nIf you want to add an editable package you are working on, you should use Pixi to handle the project. The Pixi documentation goes over how to do this.\nThe smaller the set of packages you need to manage, the better. Therefore, Pixi allows you to set up environments. (The Pixi developers call environments “projects” to emphasize that the environments in Pixi are more granular than typical of other package management systems.) Each environment contains a set of packages with versions in them. In the lesson in which you set up your computer, you set up an environment for this course that we have been using. Using environments for your projects, as opposed to a single monolithic base environment that has tons and tons of packages, is advantageous for several reasons.\n\nBy keeping the number of packages limited to those you need, you can avoid version clashes.\nProjects may require specific versions of packages, which can be explicitly installed. In other environments, you can use different versions.\nYou can encode your environment in a pixi.toml file that you can share. This allows your collaborators to readily set up environments mirroring yours and more easily share packages.\n\nI will not go through the details of how to use Pixi here, but rather refer you to Pixi’s extensive documentation.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/packages_and_modules.html#computing-environment",
    "href": "appendices/python_basics/packages_and_modules.html#computing-environment",
    "title": "Appendix M — Packages and modules",
    "section": "M.8 Computing environment",
    "text": "M.8 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.13.7\nIPython version      : 9.5.0\n\nnumpy     : 2.2.6\njupyterlab: 4.4.7",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>M</span>  <span class='chapter-title'>Packages and modules</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/exceptions_and_error_handling.html",
    "href": "appendices/python_basics/exceptions_and_error_handling.html",
    "title": "Appendix N — Errors and exception handling",
    "section": "",
    "text": "N.1 Kinds of errors\n| Download notebook\nAnd now we’ll proceed to discussing errors and exception handling.\nSo far, we have encountered errors when we did something wrong. For example, when we tried to change a character in a string, we got a TypeError.\nIn this case, the TypeError indicates that we tried to do something that is legal in Python for some types, but we tried to do it to a type for which it is illegal (strings are immutable). In Python, an error detected during execution is called an exception. We say that the interpreter “raised an exception.” There are many kinds of built-in exceptions, and you can find a list of them, with descriptions here. You can write your own kinds of exceptions, but we will not cover that in bootcamp.\nIn this lesson, we will investigate how to handle errors in your code. Importantly, we will also touch on the different kinds of errors and how to avoid them. Or, more specifically, you will learn how to use exceptions to help you write better, more bug-free code.\nIn computer programs, we can break down errors into three types.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/exceptions_and_error_handling.html#kinds-of-errors",
    "href": "appendices/python_basics/exceptions_and_error_handling.html#kinds-of-errors",
    "title": "Appendix N — Errors and exception handling",
    "section": "",
    "text": "N.1.1 Syntax errors\nA syntax error means you wrote something nonsensical, something the Python interpreter cannot understand. An example of a syntax error in English would be the following.\n\nSir Tristram, violer d’amores, fr’over the short sea, had passen-core rearrived from North Armorica on this side the scraggy isthmus of Europe Minor to wielderfight his penisolate war: nor had topsawyer’s rocks by the stream Oconee exaggerated themselse to Laurens County’s gorgios while they went doublin their mumper all the time: nor avoice from afire bellowsed mishe mishe to tauftauf thuartpeatrick: not yet, though venissoon after, had a kidscad buttended a bland old isaac: not yet, though all’s fair in vanessy, were sosie sesthers wroth with twone nathandjoe.\n\nThis is recognizable as English. In fact, it is the second sentence of a very famous novel (Finnegans Wake by James Joyce). Clearly, many spelling and punctuation rules of English are violated here. To many of us, it is nonsensical, but I do know of some people who have read the book and understand it. So, English is fairly tolerant of syntax errors. A simpler example would be\n\nBoootcamp is fun!\n\nThis has a syntax error (“Boootcamp” is not in the English language), but we understand what it means. A syntax error in Python would be this:\nmy_list = [1, 2, 3\nWe know what this means. We are trying to create a list with three items, 1, 2, and 3. However, we forgot the closing bracket. Unlike users of the English language, the Python interpreter is not forgiving; it will raise a SyntaxError exception.\n\nmy_list = [1, 2, 3\n\n\n  Cell In[3], line 1\n    my_list = [1, 2, 3\n                      ^\nSyntaxError: incomplete input\n\n\n\n\nSyntax errors are often the easiest to deal with, since the program will not run at all if any are present.\n\n\nN.1.2 Runtime errors\nRuntime errors occur when a program is syntactically correct, so it can run, but the interpreter encountered something wrong. The example at the start of the tutorial, trying to change a character in a string, is an example of a runtime error. This particular one was a TypeError, which is a more specific type of runtime error. Python does have a RuntimeError, which just indicates a generic runtime (non-syntax) error.\nRuntime errors are more difficult to spot than syntax errors because it is possible that a program could run all the way through without encountering the error for some inputs, but for other inputs, you get an error. Let’s consider the example of a simple function meant to add two numbers.\n\ndef add_two_things(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\nSyntactically, this function is just fine. We can use it and it works.\n\nadd_two_things(6, 7)\n\n13\n\n\nWe can even add strings, even though it was meant to add two numbers.\n\nadd_two_things('Hello, ', 'world.')\n\n'Hello, world.'\n\n\nHowever, when we try to add a string and a number, we get a TypeError, the kind of runtime error we saw before.\n\nadd_two_things('a string', 5.7)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 add_two_things('a string', 5.7)\n\nCell In[4], line 3, in add_two_things(a, b)\n      1 def add_two_things(a, b):\n      2     \"\"\"Add two numbers.\"\"\"\n----&gt; 3     return a + b\n\nTypeError: can only concatenate str (not \"float\") to str\n\n\n\n\n\nN.1.3 Semantic errors\nSemantic errors are perhaps the most nefarious. They occur when your program is syntactically correct, executes without runtime errors, and then produces the wrong result. These errors are the hardest to find and can do the most damage. After all, when your program does not do what you designed it to do, you want it to scream out with an exception!\nFollowing is a common example of a semantic error in which we change a mutable object within a function and then try to reuse it.\n\n# A function to append a list onto itself, with the intention of \n# returning a new list, but leaving the input unaltered\ndef double_list(in_list):\n    \"\"\"Append a list to itself.\"\"\"\n    in_list += in_list\n    return in_list\n\n# Make a list\nmy_list = [3, 2, 1]\n\n# Double it\nmy_list_double = double_list(my_list)\n\n# Later on in our program, we want a sorted my_list\nmy_list.sort()\n\n# Let's look at my_list:\nprint('We expect [1, 2, 3]')\nprint('We get   ', my_list)\n\nWe expect [1, 2, 3]\nWe get    [1, 1, 2, 2, 3, 3]\n\n\nYikes! We changed my_list within the function unintentionally. Question: How would you re-rewrite double_list() to avoid this issue?",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/exceptions_and_error_handling.html#handling-errors-in-your-code",
    "href": "appendices/python_basics/exceptions_and_error_handling.html#handling-errors-in-your-code",
    "title": "Appendix N — Errors and exception handling",
    "section": "N.2 Handling errors in your code",
    "text": "N.2 Handling errors in your code\nIf you have a syntax error, your code will not even run. So, we will assume we are without syntax errors in this discussion on how to handle errors. So, how can we handle runtime errors? In most use cases, we just write our code and let the Python interpreter tell us about these exceptions. However, sometimes we want to use the fact that we know we might encounter a runtime error within our code. A common example of this is when importing modules that are convenient, but not essential, for your code to run. Errors are handled in your code using a try statement.\nLet’s try importing a module that computes GC content. This doesn’t exist, so we will get an ImportError.\n\nimport gc_content\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 import gc_content\n\nModuleNotFoundError: No module named 'gc_content'\n\n\n\nNow, if we had the gc_content module, we would like to use it. But if not, we will just hand-code a calculation of the GC content of a sequence. We use a try statement.\n\n# Try to get the gc_content module\ntry:\n    import gc_content\n    have_gc = True\nexcept ImportError as e:\n    have_gc = False\nfinally:\n    # Do whatever is necessary here, like close files\n    pass\n\nseq = 'ACGATCTACGATCAGCTGCGCGCATCG'\n    \nif have_gc:\n    print(gc_content(seq))\nelse:\n    print(seq.count('G') + seq.count('C'))\n\n16\n\n\nThe program now runs just fine! The try statement consists of an initial try clause. Everything under the try clause is attempted to be executed. If it succeeds, the rest of the try statement is skipped, and the interpreter goes to the seq = ... line.\nIf, however, there is an ImportError, the code within the except ImportError as e clause is executed. The exception does not halt the program. If there is some other kind of error other than an ImportError, the interpreter will raise an exception after it does whatever code is in the finally clause. The finally clause is useful to tidy things up, like closing open file handles. While it is possible for a try statement to handle any generic exception by not specifying ImportError as e, it is good practice to explicitly specify the exception(s) that you anticipate in try statements as shown here. In this case, we only want to have control over ImportErrors. We want the interpreter to scream at us for any other, unanticipated errors.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/exceptions_and_error_handling.html#issuing-warnings",
    "href": "appendices/python_basics/exceptions_and_error_handling.html#issuing-warnings",
    "title": "Appendix N — Errors and exception handling",
    "section": "N.3 Issuing warnings",
    "text": "N.3 Issuing warnings\nWe may want to issue a warning instead of silently continuing. For this, the warnings module from the standard library is useful. We use the warnings.warn() method to issue the warning.\n\n# Try to get the gc_content module\ntry:\n    import gc_content\n\n    have_gc = True\nexcept ImportError as e:\n    have_gc = False\n    warnings.warn(\n        \"Failed to load gc_content. Using custom function.\", UserWarning\n    )\nfinally:\n    pass\n\nseq = \"ACGATCTACGATCAGCTGCGCGCATCG\"\n\nif have_gc:\n    print(gc_content(seq))\nelse:\n    print(seq.count(\"G\") + seq.count(\"C\"))\n\n16\n\n\n/var/folders/8h/qwnxpqcx6vldhxr71n1582d00000gn/T/ipykernel_16676/3620265156.py:8: UserWarning: Failed to load gc_content. Using custom function.\n  warnings.warn(\n\n\nNormally, we would use an ImportWarning, but those are ignored by default, so we have used a UserWarning.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/exceptions_and_error_handling.html#checking-input",
    "href": "appendices/python_basics/exceptions_and_error_handling.html#checking-input",
    "title": "Appendix N — Errors and exception handling",
    "section": "N.4 Checking input",
    "text": "N.4 Checking input\nIt is often the case that you want to check the input of a function to ensure that it works properly. In other words, you want to anticipate errors that the user (or you) might make in running your function, and you want to give descriptive error messages. For example, let’s say you are writing a code that processes protein sequences that contain only the 20 naturally occurring amino acids represented by their one-letter abbreviation. You may wish to check that the amino acid sequence is legitimate. In particular, the letters B, J, O, U, X, and Z, are not valid abbreviations for standard amino acids. (We will not use the ambiguity code, e.g. B for aspartic acid or asparagine, Z for glutamine or glutamic acid, or X for any amino acid.)\nTo illustrate the point, we will write a simple function that converts the sequence of one-letter amino acids to the three-letter abbreviation. We’ll use the dictionary that converts single-letter amino acid codes to triple letter that we encountered in the lesson on dictionaries that is now included in the bootcamp_utils package.\n\ndef one_to_three(seq):\n    \"\"\"\n    Converts a protein sequence using one-letter abbreviations\n    to one using three-letter abbreviations.\n    \"\"\"\n    # Convert seq to upper case\n    seq = seq.upper()\n\n    aa_list = []\n    for amino_acid in seq:\n        # Check if the `amino_acid` is in our dictionary `bootcamp_utils.aa`\n        if amino_acid not in bootcamp_utils.aa.keys():\n            raise RuntimeError(f\"{amino_acid} is not a valid amino acid\")\n        # Add the `amino_acid` to our aa_list\n        aa_list.append(bootcamp_utils.aa[amino_acid])\n\n    # Return the amino acids, joined together, with a dash as a separator.\n    return \"-\".join(aa_list)\n\nSo, if we put in a legitimate amino acid sequence, the function works as expected.\n\none_to_three('waeifnsdfklnsae')\n\n'Trp-Ala-Glu-Ile-Phe-Asn-Ser-Asp-Phe-Lys-Leu-Asn-Ser-Ala-Glu'\n\n\nBut, it we put in an improper amino acid, we will get a descriptive error.\n\none_to_three('waeifnsdfzklnsae')\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[14], line 1\n----&gt; 1 one_to_three('waeifnsdfzklnsae')\n\nCell In[12], line 13, in one_to_three(seq)\n     10 for amino_acid in seq:\n     11     # Check if the `amino_acid` is in our dictionary `bootcamp_utils.aa`\n     12     if amino_acid not in bootcamp_utils.aa.keys():\n---&gt; 13         raise RuntimeError(f\"{amino_acid} is not a valid amino acid\")\n     14     # Add the `amino_acid` to our aa_list\n     15     aa_list.append(bootcamp_utils.aa[amino_acid])\n\nRuntimeError: Z is not a valid amino acid\n\n\n\nGood code checks for errors and gives useful error messages. We will use exception handling extensively when we go over test driven development in future lessons.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/exceptions_and_error_handling.html#computing-environment",
    "href": "appendices/python_basics/exceptions_and_error_handling.html#computing-environment",
    "title": "Appendix N — Errors and exception handling",
    "section": "N.5 Computing environment",
    "text": "N.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p bootcamp_utils,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\nbootcamp_utils: 0.0.7\njupyterlab    : 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>N</span>  <span class='chapter-title'>Errors and exception handling</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/file_io.html",
    "href": "appendices/python_basics/file_io.html",
    "title": "Appendix O — File I/O",
    "section": "",
    "text": "O.1 File objects\n| Download notebook\nReading data in from files and then writing your results out again is one of the most common practices in scientific computing. In this tutorial, we will learn about some of Python’s File I/O capabilities. We will use a PDB file as an example. The PDB file contains the crystal structure for the tetramerization domain of p53.It is stored in the file ~/git/bootcamp/data/1OLG.pdb. (Make sure you launch your notebook from the ~/git/bootcamp/ directory.) Note that 1OLG is its unique Protein Databank identifier.\nTo open a file, we use the built-in open() function. When opening files, we should do this using context management. I will demonstrate how to open a file and then describe the syntax.\nwith open('data/1OLG.pdb', 'r') as f:\n    print(type(f))\n\n&lt;class '_io.TextIOWrapper'&gt;\nPython has a wonderful keyword, with. This keyword enables context management. Upon entry into a with block, variables have certain meaning. In this case, the variable f has the meaning of an open file, an instance of the _io.TextIOWrapper class. Upon exit, certain operations take place. For file objects created by opening them, the file is automatically closed upon exit, even if there is an error. This is important. If your program raises an exception before you have a chance to close the file, it won’t get closed and you could be in trouble. If you use context management, the file will still get closed. So here is an important tip:\nLet’s focus for a moment on the variable f in the above code cell. It is a Python file object, which has methods and attributes, just like any other object. We’ll explore those in a moment, but first, let’s look at how we opened the file. The first argument to open() is a string that has the name of the file, with the full path if necessary. The second argument is a string that says what we will be doing with the file. I.e., are we reading or writing to the file? The possible strings for this second argument are\nWe will mostly be working with text files in the bootcamp, so the first three are the most useful. A big warning, though….",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>O</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/file_io.html#file-objects",
    "href": "appendices/python_basics/file_io.html#file-objects",
    "title": "Appendix O — File I/O",
    "section": "",
    "text": "Use context management using with when working with files.\n\n\n\n\n\n\n\n\n\nstring\nmeaning\n\n\n\n\n'r'\nopen a text file for reading\n\n\n'w'\ncreate and open a text file for writing\n\n\n'a'\nappend an existing text file\n\n\n'r+'\nopen a text file for reading and writing\n\n\nappend 'b' to any of the above\nsame as above, except for binary files\n\n\n\n\n\nTrying to open an existing file with ‘w’ will wipe it out and create a new file.\n\n\nO.1.1 Reading data out of the file with file object methods\nWe will focus on the methods f.read() and f.readlines(). What do they do?\n\n\n\n\n\n\n\nmethod\ntask\n\n\n\n\nf.read()\nRead the entire contents of the file into a string\n\n\nf.readlines()\nRead the entire file into a list with each item being a string representing a line\n\n\n\nFirst, we’ll try using the first method to get a single string with the entire contents of the file.\n\n# Read file into string\nwith open('data/1OLG.pdb', 'r') as f:\n    f_str = f.read()\n\n# Let's look at the first 1000 characters\nf_str[:1000]\n\n'HEADER    ANTI-ONCOGENE                           13-JUN-94   1OLG              \\nTITLE     HIGH-RESOLUTION SOLUTION STRUCTURE OF THE OLIGOMERIZATION             \\nTITLE    2 DOMAIN OF P53 BY MULTI-DIMENSIONAL NMR                               \\nCOMPND    MOL_ID: 1;                                                            \\nCOMPND   2 MOLECULE: TUMOR SUPPRESSOR P53 (OLIGOMERIZATION DOMAIN);             \\nCOMPND   3 CHAIN: A, B, C, D;                                                   \\nCOMPND   4 ENGINEERED: YES                                                      \\nSOURCE    MOL_ID: 1;                                                            \\nSOURCE   2 ORGANISM_SCIENTIFIC: HOMO SAPIENS;                                   \\nSOURCE   3 ORGANISM_COMMON: HUMAN;                                              \\nSOURCE   4 ORGANISM_TAXID: 9606                                                 \\nKEYWDS    ANTI-ONCOGENE                                                         \\nEXPDTA    SOLUTION NMR      '\n\n\nWe see lots of \\n, which signifies a new line. The backslash is known as an escape character, meaning that the n after it does not signify the letter n, but that \\n together means a new line.\nNow, let’s try reading it in as a list.\n\n# Read contents of the file in as a list\nwith open('data/1OLG.pdb', 'r') as f:\n    f_list = f.readlines()\n\n# Look at the list (first ten entries)\nf_list[:10]\n\n['HEADER    ANTI-ONCOGENE                           13-JUN-94   1OLG              \\n',\n 'TITLE     HIGH-RESOLUTION SOLUTION STRUCTURE OF THE OLIGOMERIZATION             \\n',\n 'TITLE    2 DOMAIN OF P53 BY MULTI-DIMENSIONAL NMR                               \\n',\n 'COMPND    MOL_ID: 1;                                                            \\n',\n 'COMPND   2 MOLECULE: TUMOR SUPPRESSOR P53 (OLIGOMERIZATION DOMAIN);             \\n',\n 'COMPND   3 CHAIN: A, B, C, D;                                                   \\n',\n 'COMPND   4 ENGINEERED: YES                                                      \\n',\n 'SOURCE    MOL_ID: 1;                                                            \\n',\n 'SOURCE   2 ORGANISM_SCIENTIFIC: HOMO SAPIENS;                                   \\n',\n 'SOURCE   3 ORGANISM_COMMON: HUMAN;                                              \\n']\n\n\nWe see that each entry is a line, including the newline character. To look at lines in files, the rstrip() method for strings can come it handy. It strips all whitespace, including newlines, from the end of a string.\n\nf_list[0].rstrip()\n\n'HEADER    ANTI-ONCOGENE                           13-JUN-94   1OLG'\n\n\n\n\nO.1.2 Reading line-by-line\nWhat if we do not want to read the entire file into a list? For example, if a file is several gigabytes, we do not want to spend all of our RAM storing a list. Instead, we can read it line-by-line. Conveniently, the file object can be used as an iterator.\n\n# Print the first ten lines of the file\nwith open('data/1OLG.pdb', 'r') as f:\n    for i, line in enumerate(f):\n        print(line.rstrip())\n        if i &gt;= 10:\n            break\n\nHEADER    ANTI-ONCOGENE                           13-JUN-94   1OLG\nTITLE     HIGH-RESOLUTION SOLUTION STRUCTURE OF THE OLIGOMERIZATION\nTITLE    2 DOMAIN OF P53 BY MULTI-DIMENSIONAL NMR\nCOMPND    MOL_ID: 1;\nCOMPND   2 MOLECULE: TUMOR SUPPRESSOR P53 (OLIGOMERIZATION DOMAIN);\nCOMPND   3 CHAIN: A, B, C, D;\nCOMPND   4 ENGINEERED: YES\nSOURCE    MOL_ID: 1;\nSOURCE   2 ORGANISM_SCIENTIFIC: HOMO SAPIENS;\nSOURCE   3 ORGANISM_COMMON: HUMAN;\nSOURCE   4 ORGANISM_TAXID: 9606\n\n\nAlternatively, we can use the method f.readline() to read a single line in the file and return it as a string.\n\n# Print the first ten lines of the file\nwith open('data/1OLG.pdb', 'r') as f:\n    i = 0\n    while i &lt; 10:\n        print(f.readline().rstrip())\n        i += 1\n\nHEADER    ANTI-ONCOGENE                           13-JUN-94   1OLG\nTITLE     HIGH-RESOLUTION SOLUTION STRUCTURE OF THE OLIGOMERIZATION\nTITLE    2 DOMAIN OF P53 BY MULTI-DIMENSIONAL NMR\nCOMPND    MOL_ID: 1;\nCOMPND   2 MOLECULE: TUMOR SUPPRESSOR P53 (OLIGOMERIZATION DOMAIN);\nCOMPND   3 CHAIN: A, B, C, D;\nCOMPND   4 ENGINEERED: YES\nSOURCE    MOL_ID: 1;\nSOURCE   2 ORGANISM_SCIENTIFIC: HOMO SAPIENS;\nSOURCE   3 ORGANISM_COMMON: HUMAN;\n\n\nEach subsequent call to f.readline() reads in the next line of the file. (As we read through a file, we keep moving forward in the bytes of the file and we have to use f.seek() to rewind.)",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>O</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/file_io.html#writing-to-a-file",
    "href": "appendices/python_basics/file_io.html#writing-to-a-file",
    "title": "Appendix O — File I/O",
    "section": "O.2 Writing to a file",
    "text": "O.2 Writing to a file\nWriting to a file has similar syntax. We already saw how to open a file for writing. Again, context management is useful. However, before trying to open a file, we should check to make sure a file of the same name does not exist before opening it. The os.path module is useful. The function os.path.isfile() function checks to see if a file exists.\n\nos.path.isfile('data/1OLG.pdb')\n\nTrue\n\n\nNow that we know how to check existence of a file so we do not overwrite it, we can open and write a file.\n\nif os.path.isfile('yogi.txt'):\n    raise RuntimeError('File yogi.txt already exists.')\n\nwith open('yogi.txt', 'w') as f:\n    f.write('When you come to a fork in the road, take it.')\n    f.write('You can observe a lot by just watching.')\n    f.write('I never said most of the things I said.')\n\nNote that we can use the f.write() method to write strings to a file. Let’s look at the file contents.\n\n!cat yogi.txt\n\nWhen you come to a fork in the road, take it.You can observe a lot by just watching.I never said most of the things I said.\n\n\nAh! There are no newlines! When writing to a file, unlike when you use the print() function, you must include the newline characters. Let’s try again, intentionally obliterating our first attempt.\n\nwith open('yogi.txt', 'w') as f:\n    f.write('When you come to a fork in the road, take it.\\n')\n    f.write('You can observe a lot by just watching.\\n')\n    f.write('I never said most of the things I said.\\n')\n    \n!cat yogi.txt\n\nWhen you come to a fork in the road, take it.\nYou can observe a lot by just watching.\nI never said most of the things I said.\n\n\nThat’s better. Note also that f.write() only takes strings as arguments. You cannot pass numbers. They must be converted to strings first.\n\n# This will result in an exception\nwith open('gimme_phi.txt', 'w') as f:\n    f.write('The golden ratio is φ = ')\n    f.write(1.61803398875)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[12], line 4\n      2 with open('gimme_phi.txt', 'w') as f:\n      3     f.write('The golden ratio is φ = ')\n----&gt; 4     f.write(1.61803398875)\n\nTypeError: write() argument must be str, not float\n\n\n\nYup. It must be a string. Let’s try again.\n\nwith open('gimme_phi.txt', 'w') as f:\n    f.write('The golden ratio is φ = ')\n    f.write('{phi:.8f}'.format(phi=1.61803398875))\n\n!cat gimme_phi.txt\n\nThe golden ratio is φ = 1.61803399\n\n\nThat works!",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>O</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/file_io.html#an-exercise-extract-atomic-coordinates-for-first-chain-in-tetramer",
    "href": "appendices/python_basics/file_io.html#an-exercise-extract-atomic-coordinates-for-first-chain-in-tetramer",
    "title": "Appendix O — File I/O",
    "section": "O.3 An exercise: extract atomic coordinates for first chain in tetramer",
    "text": "O.3 An exercise: extract atomic coordinates for first chain in tetramer\nAs an example on how to do file I/O, we will take the PDB file and extract only the ATOM records for the first chain of the tetramer and write only those entries to a new file.\nIt is useful to know that according to the PDB format specification, column 21 in the ATOM entry gives the ID of the chain.\nWe also conveniently use the fact that we can have multiple files open in our with block, separating them with commas.\n\nwith open('data/1OLG.pdb', 'r') as f, open('atoms_chain_A.txt', 'w') as f_out:\n    # Put the ATOM lines from chain A in new file\n    for line in f:\n        if len(line) &gt; 21 and line[:4] == 'ATOM' and line[21] == 'A':\n            f_out.write(line)\n\nLet’s see how we did!\n\n!head -10 atoms_chain_A.txt\n\nATOM      1  N   LYS A 319      18.634  25.437  10.685  1.00  4.81           N  \nATOM      2  CA  LYS A 319      17.984  25.295   9.354  1.00  4.32           C  \nATOM      3  C   LYS A 319      18.160  23.876   8.818  1.00  3.74           C  \nATOM      4  O   LYS A 319      19.259  23.441   8.537  1.00  3.67           O  \nATOM      5  CB  LYS A 319      18.609  26.282   8.371  1.00  4.67           C  \nATOM      6  CG  LYS A 319      18.003  26.056   6.986  1.00  5.15           C  \nATOM      7  CD  LYS A 319      16.476  26.057   7.091  1.00  5.90           C  \nATOM      8  CE  LYS A 319      16.014  27.341   7.784  1.00  6.51           C  \nATOM      9  NZ  LYS A 319      16.388  28.518   6.952  1.00  7.33           N  \nATOM     10  H1  LYS A 319      18.414  24.606  11.281  1.00  5.09           H  \n\n\n\n!tail -10 atoms_chain_A.txt\n\nATOM    689  HD2 PRO A 359       0.183  25.663  13.542  1.00  4.71           H  \nATOM    690  HD3 PRO A 359       0.246  23.956  13.062  1.00  4.53           H  \nATOM    691  N   GLY A 360      -3.984  26.791  10.832  1.00  5.45           N  \nATOM    692  CA  GLY A 360      -4.489  28.138  10.445  1.00  5.95           C  \nATOM    693  C   GLY A 360      -5.981  28.236  10.765  1.00  6.77           C  \nATOM    694  O   GLY A 360      -6.401  27.621  11.732  1.00  7.24           O  \nATOM    695  OXT GLY A 360      -6.679  28.924  10.039  1.00  7.15           O  \nATOM    696  H   GLY A 360      -4.589  26.020  10.828  1.00  5.72           H  \nATOM    697  HA2 GLY A 360      -3.950  28.896  10.995  1.00  5.99           H  \nATOM    698  HA3 GLY A 360      -4.341  28.288   9.386  1.00  6.05           H  \n\n\nNice!",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>O</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/file_io.html#finding-files-and-with-glob",
    "href": "appendices/python_basics/file_io.html#finding-files-and-with-glob",
    "title": "Appendix O — File I/O",
    "section": "O.4 Finding files and with glob",
    "text": "O.4 Finding files and with glob\nIn the above snippet of code, we extracted all atom records from a PDB file. We might want to do this (or some other operation) for many files. For example, the directory ~/git/data/ has four PDB files in it. For the present discussion, let’s say we want to pull the sequence of chain A out of each PDB file.\nThe glob module from the standard library enables us to get a list of all files that match a pattern. In our case, we want all files matching data/*.pdb, where * is a wild card character, meaning that any matches of characters where * appears are allowed. Let’s see what glob.glob() gives us.\n\nfile_list = glob.glob('data/*.pdb')\n\nfile_list\n\n['data/1OLG.pdb', 'data/1J6Z.pdb', 'data/1FAG.pdb', 'data/2ERK.pdb']\n\n\nWe have the four PDB files. We can now loop over them and pull out the sequences.\n\n# Dictionary to hold sequences\nseqs = {}\n\n# Loop through all matching files\nfor file_name in file_list:\n    # Extract PDB ID\n    pdb_id = file_name[file_name.find('/')+1:file_name.rfind('.')]\n    \n    # Initialize sequence string, which we build as we go along\n    seq = ''\n    with open(file_name, 'r') as f:\n        for line in f:\n            if len(line) &gt; 11 and line[:6] == 'SEQRES' and line[11] == 'A':\n                seq += line[19:].rstrip() + ' '\n\n    # Build sequence with dash-joined three letter codes\n    seq = '-'.join(seq.split())\n\n    # Store in the dictionary\n    seqs[pdb_id] = seq\n\nLet’s take a look at what we got. We’ll look at actin.\n\nseqs['1J6Z']\n\n'ASP-GLU-ASP-GLU-THR-THR-ALA-LEU-VAL-CYS-ASP-ASN-GLY-SER-GLY-LEU-VAL-LYS-ALA-GLY-PHE-ALA-GLY-ASP-ASP-ALA-PRO-ARG-ALA-VAL-PHE-PRO-SER-ILE-VAL-GLY-ARG-PRO-ARG-HIS-GLN-GLY-VAL-MET-VAL-GLY-MET-GLY-GLN-LYS-ASP-SER-TYR-VAL-GLY-ASP-GLU-ALA-GLN-SER-LYS-ARG-GLY-ILE-LEU-THR-LEU-LYS-TYR-PRO-ILE-GLU-HIC-GLY-ILE-ILE-THR-ASN-TRP-ASP-ASP-MET-GLU-LYS-ILE-TRP-HIS-HIS-THR-PHE-TYR-ASN-GLU-LEU-ARG-VAL-ALA-PRO-GLU-GLU-HIS-PRO-THR-LEU-LEU-THR-GLU-ALA-PRO-LEU-ASN-PRO-LYS-ALA-ASN-ARG-GLU-LYS-MET-THR-GLN-ILE-MET-PHE-GLU-THR-PHE-ASN-VAL-PRO-ALA-MET-TYR-VAL-ALA-ILE-GLN-ALA-VAL-LEU-SER-LEU-TYR-ALA-SER-GLY-ARG-THR-THR-GLY-ILE-VAL-LEU-ASP-SER-GLY-ASP-GLY-VAL-THR-HIS-ASN-VAL-PRO-ILE-TYR-GLU-GLY-TYR-ALA-LEU-PRO-HIS-ALA-ILE-MET-ARG-LEU-ASP-LEU-ALA-GLY-ARG-ASP-LEU-THR-ASP-TYR-LEU-MET-LYS-ILE-LEU-THR-GLU-ARG-GLY-TYR-SER-PHE-VAL-THR-THR-ALA-GLU-ARG-GLU-ILE-VAL-ARG-ASP-ILE-LYS-GLU-LYS-LEU-CYS-TYR-VAL-ALA-LEU-ASP-PHE-GLU-ASN-GLU-MET-ALA-THR-ALA-ALA-SER-SER-SER-SER-LEU-GLU-LYS-SER-TYR-GLU-LEU-PRO-ASP-GLY-GLN-VAL-ILE-THR-ILE-GLY-ASN-GLU-ARG-PHE-ARG-CYS-PRO-GLU-THR-LEU-PHE-GLN-PRO-SER-PHE-ILE-GLY-MET-GLU-SER-ALA-GLY-ILE-HIS-GLU-THR-THR-TYR-ASN-SER-ILE-MET-LYS-CYS-ASP-ILE-ASP-ILE-ARG-LYS-ASP-LEU-TYR-ALA-ASN-ASN-VAL-MET-SER-GLY-GLY-THR-THR-MET-TYR-PRO-GLY-ILE-ALA-ASP-ARG-MET-GLN-LYS-GLU-ILE-THR-ALA-LEU-ALA-PRO-SER-THR-MET-LYS-ILE-LYS-ILE-ILE-ALA-PRO-PRO-GLU-ARG-LYS-TYR-SER-VAL-TRP-ILE-GLY-GLY-SER-ILE-LEU-ALA-SER-LEU-SER-THR-PHE-GLN-GLN-MET-TRP-ILE-THR-LYS-GLN-GLU-TYR-ASP-GLU-ALA-GLY-PRO-SER-ILE-VAL-HIS-ARG-LYS-CYS-PHE'\n\n\nExcellent! Our function worked, and we now have the protein sequence.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>O</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/file_io.html#computing-environment",
    "href": "appendices/python_basics/file_io.html#computing-environment",
    "title": "Appendix O — File I/O",
    "section": "O.5 Computing environment",
    "text": "O.5 Computing environment\n\n%load_ext watermark\n%watermark -v -p jupyterlab\n\nPython implementation: CPython\nPython version       : 3.11.9\nIPython version      : 8.20.0\n\njupyterlab: 4.0.13",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>O</span>  <span class='chapter-title'>File I/O</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "",
    "text": "P.1 A very brief introduction to NumPy arrays\n| Download notebook\nHere you will learn about NumPy, arguably the most important package for scientific computing, and SciPy, a package containing lots of goodies for scientific computing, like special functions and numerical integrators.\nThe central object for NumPy and SciPy is the ndarray, commonly referred to as a “NumPy array.” This is an array object that is convenient for scientific computing. We will go over it in depth in the next lesson, but for now, let’s just create some NumPy arrays and see how operators work on them.\nJust like with type conversions with lists, tuples, and other data types we’ve looked at, we can convert a list to a NumPy array using\nNote that above we imported the NumPy package with the np alias. This is for convenience; it allow us to use np as a prefix instead of numpy. NumPy is in very widespread use, and the convention is to use the np abbreviation.\n# Create a NumPy array from a list\nmy_ar = np.array([1, 2, 3, 4])\n\n# Look at it\nmy_ar\n\narray([1, 2, 3, 4])\nWe see that the list has been converted, and it is explicitly shown as an array. It has several attributes and lots of methods. The most important attributes are probably the data type of its elements and the shape of the array.\n# The data type of stored entries\nmy_ar.dtype\n\ndtype('int64')\n# The shape of the array\nmy_ar.shape\n\n(4,)\nThere are also lots of methods. The one we use most often is astype(), which converts the data type of the array.\nmy_ar.astype(float)\n\narray([1., 2., 3., 4.])\nThere are many others. For example, we can compute summary statistics about the entries in the array, very similar to what we have see with Pandas.\nprint(my_ar.max())\nprint(my_ar.min())\nprint(my_ar.sum())\nprint(my_ar.mean())\nprint(my_ar.std())\n\n4\n1\n10\n2.5\n1.118033988749895\nImportantly, NumPy arrays can be arguments to NumPy functions. In this case, these functions do the same operations as the methods we just looked at.\nprint(np.max(my_ar))\nprint(np.min(my_ar))\nprint(np.sum(my_ar))\nprint(np.mean(my_ar))\nprint(np.std(my_ar))\n\n4\n1\n10\n2.5\n1.118033988749895",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#a-very-brief-introduction-to-numpy-arrays",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#a-very-brief-introduction-to-numpy-arrays",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "",
    "text": "np.array()",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#other-ways-to-make-numpy-arrays",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#other-ways-to-make-numpy-arrays",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.2 Other ways to make NumPy arrays",
    "text": "P.2 Other ways to make NumPy arrays\nThere are many other ways to make NumPy arrays besides just converting lists or tuples. Below are some examples.\n\n# How long our arrays will be\nn = 10\n\n# Make a NumPy array of length n filled with zeros\nnp.zeros(n)\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\n# Make a NumPy array of length n filled with ones\nnp.ones(n)\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\n# Make an empty NumPy array of length n without initializing entries\n# (while it initially holds whatever values were previously in the memory\n# locations assigned, ones will be displayed)\nnp.empty(n)\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\n# Make a NumPy array filled with zeros the same shape as another NumPy array\nmy_ar = np.array([[1, 2], [3, 4]])\nnp.zeros_like(my_ar)\n\narray([[0, 0],\n       [0, 0]])\n\n\nAs we work through the rest of this exercise, we will use more interesting arrays (not just zeroes, ones, and counting).\n\nx = np.array(\n    [1683, 2061, 1792, 1852, 2091, 1781, 1912, 1802, 1751, 1731, 1892,\n     1951, 1809, 1683, 1787, 1840, 1821, 1910, 1930, 1800, 1833, 1683,\n     1671, 1680, 1692, 1800, 1821, 1882, 1642, 1749, 1712, 1661, 1701,\n     2141, 1863, 1752, 1740, 1721, 1660, 1930, 2030, 1851, 2131, 1828])\n\ny = np.array(\n    [1840, 2090, 2169, 1988, 2212, 2339, 1989, 2144, 2290, 1920, 2280,\n     1809, 2158, 1800, 2133, 2060, 2160, 2001, 2030, 2088, 1951, 2460])",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#slicing-numpy-arrays",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#slicing-numpy-arrays",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.3 Slicing NumPy arrays",
    "text": "P.3 Slicing NumPy arrays\nWe can slice NumPy arrays like lists and tuples. Here are a few examples.\n\n# Reversed array\nx[::-1]\n\narray([1828, 2131, 1851, 2030, 1930, 1660, 1721, 1740, 1752, 1863, 2141,\n       1701, 1661, 1712, 1749, 1642, 1882, 1821, 1800, 1692, 1680, 1671,\n       1683, 1833, 1800, 1930, 1910, 1821, 1840, 1787, 1683, 1809, 1951,\n       1892, 1731, 1751, 1802, 1912, 1781, 2091, 1852, 1792, 2061, 1683])\n\n\n\n# Every 5th element, starting at index 3\nx[3::5]\n\narray([1852, 1751, 1683, 1930, 1680, 1642, 2141, 1660, 1828])\n\n\n\n# Entries 10 to 20\nx[10:21]\n\narray([1892, 1951, 1809, 1683, 1787, 1840, 1821, 1910, 1930, 1800, 1833])\n\n\n\nP.3.1 Fancy indexing\nNumPy arrays also allow fancy indexing, where we can slice out specific values. For example, say we wanted indices 1, 19, and 6 (in that order) from x. We just index with a list of the indices we want.\n\nx[[1, 19, 6]]\n\narray([2061, 1800, 1912])\n\n\nInstead of a list, we could also use a NumPy array.\n\nx[np.array([1, 19, 6])]\n\narray([2061, 1800, 1912])\n\n\nAs a very nice feature, we can use Boolean indexing with Numpy arrays. Say we only want entries greater than 2000.\n\n# Just slice out the big ones\nx[x &gt; 2000]\n\narray([2061, 2091, 2141, 2030, 2131])\n\n\nIf we want to know the indices where the values are high, we can use the np.where() function.\n\nnp.where(x &gt; 2000)\n\n(array([ 1,  4, 33, 40, 42]),)",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#numpy-arrays-are-mutable",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#numpy-arrays-are-mutable",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.4 NumPy arrays are mutable",
    "text": "P.4 NumPy arrays are mutable\nYes, NumPy arrays are mutable. Let’s look at some consequences.\n\n# Make an array\nmy_ar = np.array([1, 2, 3, 4])\n\n# Change an element\nmy_ar[2] = 6\n\n# See the result\nmy_ar\n\narray([1, 2, 6, 4])\n\n\nNow, let’s try attaching another variable to the NumPy array.\n\n# Attach a new variable\nmy_ar2 = my_ar\n\n# Set an entry using the new variable\nmy_ar2[3] = 9\n\n# Does the original change? (yes.)\nmy_ar\n\narray([1, 2, 6, 9])\n\n\nLet’s see how messing with NumPy in functions affects things.\n\n# Re-instantiate my_ar\nmy_ar = np.array([1, 2, 3, 4]).astype(float)\n\n# Function to normalize x (note that /= works with mutable objects)\ndef normalize(x):\n    x /= np.sum(x)\n\n# Pass it through a function\nnormalize(my_ar)\n\n# Is it normalized even though we didn't return anything? (Yes.)\nmy_ar\n\narray([0.1, 0.2, 0.3, 0.4])\n\n\nSo, be careful when writing functions. What you do to your NumPy array inside the function will happen outside of the function as well. Always remember that:\n\nNumPy arrays are mutable.\n\n\nP.4.1 Slices of NumPy arrays are views, not copies\nA very important distinction between NumPy arrays and lists is that slices of NumPy arrays are views into the original NumPy array, NOT copies. To illustrate this, we will again use out 1, 2, 3, 4 array for simplicity and clarity.\n\n# Make list and array\nmy_list = [1, 2, 3, 4]\nmy_ar = np.array(my_list)\n\n# Slice out of each\nmy_list_slice = my_list[1:-1]\nmy_ar_slice = my_ar[1:-1]\n\n# Mess with the slices\nmy_list_slice[0] = 9\nmy_ar_slice[0] = 9\n\n# Look at originals\nprint(my_list)\nprint(my_ar)\n\n[1, 2, 3, 4]\n[1 9 3 4]\n\n\nMessing with an element of a slice of a NumPy array messes with that element in the original! This is not the case with lists. Let’s issue a warning.\n\nSlices of NumPy arrays are views, not copies.\n\nFortunately, you can make a copy of an array using the np.copy() function.\n\n# Make a copy\nx_copy = np.copy(x)\n\n# Mess with an entry\nx_copy[10] = 2000\n\n# Check equality\nnp.allclose(x, x_copy)\n\nFalse\n\n\nSo, messing with an entry in the copy did not affect the original.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#mathematical-operations-with-arrays",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#mathematical-operations-with-arrays",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.5 Mathematical operations with arrays",
    "text": "P.5 Mathematical operations with arrays\nMathematical operations on arrays are done elementwise to all elements.\n\n# Divide one array be another\nnp.array([5, 6, 7, 8]) / np.array([1, 2, 3, 4])\n\narray([5.        , 3.        , 2.33333333, 2.        ])\n\n\n\n# Multiply by scalar\n-4 * x\n\narray([-6732, -8244, -7168, -7408, -8364, -7124, -7648, -7208, -7004,\n       -6924, -7568, -7804, -7236, -6732, -7148, -7360, -7284, -7640,\n       -7720, -7200, -7332, -6732, -6684, -6720, -6768, -7200, -7284,\n       -7528, -6568, -6996, -6848, -6644, -6804, -8564, -7452, -7008,\n       -6960, -6884, -6640, -7720, -8120, -7404, -8524, -7312])\n\n\n\n# Raise to power\nx**2\n\narray([2832489, 4247721, 3211264, 3429904, 4372281, 3171961, 3655744,\n       3247204, 3066001, 2996361, 3579664, 3806401, 3272481, 2832489,\n       3193369, 3385600, 3316041, 3648100, 3724900, 3240000, 3359889,\n       2832489, 2792241, 2822400, 2862864, 3240000, 3316041, 3541924,\n       2696164, 3059001, 2930944, 2758921, 2893401, 4583881, 3470769,\n       3069504, 3027600, 2961841, 2755600, 3724900, 4120900, 3426201,\n       4541161, 3341584])",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#indexing-2d-numpy-arrays",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#indexing-2d-numpy-arrays",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.6 Indexing 2D NumPy arrays",
    "text": "P.6 Indexing 2D NumPy arrays\nNumPy arrays need not be one-dimensional. We’ll create a two-dimensional NumPy array by reshaping our x array from having shape (44,) to having shape (11, 4). That is, it will become an array with 11 rows and 4 columns. (The 2D nature of this array has no meaning in this case; it’s just meant for demonstration.)\n\n# New 2D array using the reshape() method\nmy_ar = x.reshape((11, 4))\n\n# Look at it\nmy_ar\n\narray([[1683, 2061, 1792, 1852],\n       [2091, 1781, 1912, 1802],\n       [1751, 1731, 1892, 1951],\n       [1809, 1683, 1787, 1840],\n       [1821, 1910, 1930, 1800],\n       [1833, 1683, 1671, 1680],\n       [1692, 1800, 1821, 1882],\n       [1642, 1749, 1712, 1661],\n       [1701, 2141, 1863, 1752],\n       [1740, 1721, 1660, 1930],\n       [2030, 1851, 2131, 1828]])\n\n\nNotice that it is represented as an array made out of a list of lists. If we had a list of lists, we would index it like this:\nlist_of_lists[i][j]\n\n# Make list of lists\nlist_of_lists = [[1, 2], [3, 4]]\n\n# Pull out value in first row, second column\nlist_of_lists[0][1]\n\n2\n\n\nThough this will work with NumPy arrays, this is not how NumPy arrays are indexed. They are indexed much more conveniently.\n\nmy_ar[0, 1]\n\n2061\n\n\nWe essentially have a tuple in the indexing brackets. Now, say we wanted the second row (indexing starting at 0).\n\nmy_ar[2, :]\n\narray([1751, 1731, 1892, 1951])\n\n\nWe can use Boolean indexing as before.\n\nmy_ar[my_ar &gt; 2000]\n\narray([2061, 2091, 2141, 2030, 2131])\n\n\nNote that this gives a one-dimensional array of the entries greater than 2000. If we wanted indices where this is the case, we can again use np.where().\n\nnp.where(my_ar &gt; 2000)\n\n(array([ 0,  1,  8, 10, 10]), array([1, 0, 1, 0, 2]))\n\n\nThis tuple of NumPy arrays is how we would index using fancy indexing to pull those values out using fancy indexing.\n\nmy_ar[(np.array([ 0,  1,  8, 10, 10]), np.array([1, 0, 1, 0, 2]))]\n\narray([2061, 2091, 2141, 2030, 2131])\n\n\nNumPy arrays can be of arbitrary integer dimension, and these principles extrapolate to 3D, 4D, etc., arrays.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#concatenating-arrays",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#concatenating-arrays",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.7 Concatenating arrays",
    "text": "P.7 Concatenating arrays\nLet’s say we want to study all cross sectional areas and don’t care if the mother was well-fed or not. We would want to concatenate our arrays. The np.concatenate() function accomplishes this. We simply have to pass it a tuple containing the NumPy arrays we want to concatenate.\n\ncombined = np.concatenate((x, y))\n\n# Look at it\ncombined\n\narray([1683, 2061, 1792, 1852, 2091, 1781, 1912, 1802, 1751, 1731, 1892,\n       1951, 1809, 1683, 1787, 1840, 1821, 1910, 1930, 1800, 1833, 1683,\n       1671, 1680, 1692, 1800, 1821, 1882, 1642, 1749, 1712, 1661, 1701,\n       2141, 1863, 1752, 1740, 1721, 1660, 1930, 2030, 1851, 2131, 1828,\n       1840, 2090, 2169, 1988, 2212, 2339, 1989, 2144, 2290, 1920, 2280,\n       1809, 2158, 1800, 2133, 2060, 2160, 2001, 2030, 2088, 1951, 2460])",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#numpy-has-useful-mathematical-functions",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#numpy-has-useful-mathematical-functions",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.8 NumPy has useful mathematical functions",
    "text": "P.8 NumPy has useful mathematical functions\nSo far, we have not done much mathematics with Python. We have done some adding and division, but nothing like computing a logarithm or cosine. The NumPy functions also work elementwise on the arrays when it is intuitive to do so. That is, they apply the function to each entry in the array. Check it out.\n\n# Exponential\nnp.exp(x / 1000)\n\narray([5.38167681, 7.8538197 , 6.00144336, 6.37255189, 8.09300412,\n       5.93578924, 6.76660849, 6.06175887, 5.76036016, 5.64629738,\n       6.63262067, 7.03571978, 6.10434004, 5.38167681, 5.97151103,\n       6.29653826, 6.1780334 , 6.7530888 , 6.88951024, 6.04964746,\n       6.2526164 , 5.38167681, 5.31748262, 5.36555597, 5.43033051,\n       6.04964746, 6.1780334 , 6.56662499, 5.16549017, 5.74885095,\n       5.54003047, 5.26457279, 5.47942408, 8.50794132, 6.44303692,\n       5.7661234 , 5.69734342, 5.59011579, 5.25931084, 6.88951024,\n       7.61408636, 6.36618252, 8.42328589, 6.22143134])\n\n\n\n# Cosine\nnp.cos(x)\n\narray([ 0.62656192,  0.9933696 ,  0.27501843,  0.03112568,  0.26681725,\n       -0.96021239, -0.33430744,  0.29228295, -0.42404251, -0.99984597,\n        0.72399324, -0.99748325,  0.84865001,  0.62656192, -0.84393482,\n        0.56257847,  0.43231386,  0.99610114,  0.48702972, -0.99122275,\n       -0.11903049,  0.62656192,  0.94691648, -0.73027654, -0.24968607,\n       -0.99122275,  0.43231386, -0.98275172, -0.49500319, -0.64703425,\n       -0.98592179, -0.61963892, -0.17156886,  0.00460656, -0.99936794,\n        0.53296056,  0.90375673,  0.82939405,  0.3256673 ,  0.48702972,\n        0.86222727, -0.824246  ,  0.5401501 ,  0.91834245])\n\n\n\n# Square root\nnp.sqrt(x)\n\narray([41.02438299, 45.39823785, 42.33202098, 43.03486958, 45.72745346,\n       42.20189569, 43.72642222, 42.44997055, 41.84495191, 41.60528813,\n       43.49712634, 44.17012565, 42.53234064, 41.02438299, 42.27292278,\n       42.89522118, 42.67317659, 43.70354677, 43.93176527, 42.42640687,\n       42.81354926, 41.02438299, 40.87786687, 40.98780306, 41.1339276 ,\n       42.42640687, 42.67317659, 43.38202393, 40.52159918, 41.82104733,\n       41.37632173, 40.75536774, 41.24318125, 46.27094121, 43.16248371,\n       41.85689907, 41.71330723, 41.48493703, 40.74309757, 43.93176527,\n       45.0555213 , 43.02324953, 46.16275555, 42.75511665])\n\n\nWe can even do some matrix operations (which are obviously not done elementwise), like dot products.\n\nnp.dot(x, x)\n\n146360195\n\n\nNumPy also has useful attributes, like np.pi.\n\nnp.pi\n\n3.141592653589793",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#scipy-has-even-more-useful-functions-in-modules",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#scipy-has-even-more-useful-functions-in-modules",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.9 SciPy has even more useful functions (in modules)",
    "text": "P.9 SciPy has even more useful functions (in modules)\nSciPy actually began life as a library of special functions that operate on NumPy arrays. For example, we can compute an error function using the scipy.special module, which contains lots of special functions. Note that you often have to individually import the SciPy module you want to use, for example with\nimport scipy.special\n\nscipy.special.erf(x / 2000)\n\narray([0.76597747, 0.8549794 , 0.7948931 , 0.80965587, 0.86074212,\n       0.79209865, 0.8236209 , 0.79740973, 0.78433732, 0.77904847,\n       0.81905337, 0.83227948, 0.79915793, 0.76597747, 0.7936263 ,\n       0.80676772, 0.8021292 , 0.82316805, 0.8276577 , 0.79690821,\n       0.80506817, 0.76597747, 0.76262579, 0.76514271, 0.76846912,\n       0.79690821, 0.8021292 , 0.81673693, 0.7543863 , 0.78381257,\n       0.77393853, 0.75980693, 0.77094188, 0.86995276, 0.81227529,\n       0.78459935, 0.78143985, 0.77636944, 0.75952376, 0.8276577 ,\n       0.84883448, 0.80941641, 0.86814949, 0.80384751])\n\n\nThere are many SciPy submodules which give plenty or rich functionality for scientific computing. You can check out the SciPy docs to learn about all of the functionality. Particularly useful modules that have come up in our work in systems biology include:\n\nscipy.special: Special functions.\nscipy.stats: Functions for statistical analysis.\nscipy.optimize: Numerical optimization.\nscipy.integrate: Numerical solutions to differential equations.\nscipy.interpolate: Smooth interpolation of functions.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#numpy-and-scipy-are-highly-optimized",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#numpy-and-scipy-are-highly-optimized",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.10 NumPy and SciPy are highly optimized",
    "text": "P.10 NumPy and SciPy are highly optimized\nImportantly, NumPy and SciPy routines are often fast. To understand why, we need to think a bit about how your computer actually runs code you write.\n\nP.10.1 Interpreted and compiled languages\nWe have touched on the fact that Python is an interpreted language. This means that the Python interpreter reads through your code, line by line, translates the commands into instructions that your computer’s processor can execute, and then these are executed. It also does garbage collection, which manages memory usage in your programs for you. As an interpreted language, code is often much easier to write, and development time is much shorter. It is often easier to debug. By contrast, with compiled languages (the dominant ones being Fortran, C, and C++), your entire source code is translated into machine code before you ever run it. When you execute your program, it is already in machine code. As a result, compiled code is often much faster than interpreted code. The speed difference depends largely on the task at hand, but there is often over a 100-fold difference.\nFirst, we’ll demonstrate the difference between compiled and interpreted languages by looking at a function to sum the elements of an array. Note that Python is dynamically typed, so the function below works for multiple data types, but the C function works only for double precision floating point numbers.\n\n# Python code to sum an array and print the result to the screen\nprint(sum(my_ar))\n\n[19793 20111 20171 19978]\n\n\n/* C code to sum an array and print the result to the screen */\n\n#include &lt;stdio.h&gt;\n\nvoid sum_array(double a[], int n);\n\nvoid sum_array(double a[], int n) {\n   int i; \n   double sum=0;\n   for (i = 0; i &lt; n; i++){\n       sum += a[i];\n   }\n   printf(\"%g\\n\", sum);\n}\nThe C code won’t even execute without another function called main to call it. You should notice the difference in complexity of the code. Interpreted code is very often much easier to write!\n\n\nP.10.2 NumPy and SciPy use compiled code!\nUnder the hood, when you call a NumPy or SciPy function, or use one of the methods, the Python interpreter passes the arrays into pre-compiled functions. (They are usually C or Fortran functions.) That means that you get to use an interpreted language with near-compiled speed! We can demonstrate the speed by comparing an explicit sum of elements of an array using a Python for loop versus NumPy. We will use the np.random module to generate a large array of random numbers (we will visit random number generation in a coming section). We then use the %timeit magic function of IPython to time the execution of the sum of the elements of the array.\n\n# Make array of 10,000 random numbers\nrng = np.random.default_rng()\nx = rng.random(10000)\n\n# Sum with Python for loop\ndef python_sum(x):\n    x_sum = 0.0\n    for y in x:\n        x_sum += y\n    return x_sum\n\n# Test speed\n%timeit python_sum(x)\n\n877 µs ± 9.01 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nNow we’ll do the same test with the NumPy implementation.\n\n%timeit np.sum(x)\n\n7.84 µs ± 99.1 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nWow! We went from a millisecond to microseconds!",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#word-of-advice-use-numpy-and-scipy",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#word-of-advice-use-numpy-and-scipy",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.11 Word of advice: use NumPy and SciPy",
    "text": "P.11 Word of advice: use NumPy and SciPy\nIf you are writing code and you think to yourself, “This seems like a pretty common things to do,” there is a good chance the someone really smart has written code to do it. If it’s something numerical, there is a good chance it is in NumPy or SciPy. Use these packages. Do not reinvent the wheel. It is very rare you can beat them for performance, error checking, etc.\nFurthermore, NumPy and SciPy are very well tested. In general, you do not need to write unit tests for well-established packages. Obviously, if you use NumPy or SciPy within your own functions, you still need to test what you wrote.",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  },
  {
    "objectID": "appendices/python_basics/intro_to_numpy_and_scipy.html#computing-environment",
    "href": "appendices/python_basics/intro_to_numpy_and_scipy.html#computing-environment",
    "title": "Appendix P — Introduction to Numpy and Scipy",
    "section": "P.12 Computing environment",
    "text": "P.12 Computing environment\n\n%load_ext watermark\n%watermark -v -p numpy,scipy,jupyterlab\n\nPython implementation: CPython\nPython version       : 3.10.9\nIPython version      : 8.10.0\n\nnumpy     : 1.23.5\nscipy     : 1.10.0\njupyterlab: 3.5.3",
    "crumbs": [
      "Appendices",
      "Computing",
      "<span class='chapter-number'>P</span>  <span class='chapter-title'>Introduction to Numpy and Scipy</span>"
    ]
  }
]